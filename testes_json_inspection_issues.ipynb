{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO//ZRzRSndyPCoQwR1838M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armandossrecife/teste/blob/main/testes_json_inspection_issues.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "MY_ISSUES_ZIP = \"https://github.com/Technical-Debt-Large-Scale/my_validation/raw/main/cassandra/my_issues_to_inspection_cassandra.zip\"\n",
        "DIRETORIO_CORRENTE = os.getcwd()\n",
        "filename = \"my_issues_to_inspection_cassandra.zip\"\n",
        "DIRETORIO_ISSUES = os.path.join(DIRETORIO_CORRENTE,\"downloads\")\n",
        "PATH_ARQUIVO_LOCAL = os.path.join(DIRETORIO_ISSUES,filename)\n",
        "PATH_ARQUIVOS_DESCOMPACTADOS =  os.path.join(DIRETORIO_CORRENTE, \"my_issues\")"
      ],
      "metadata": {
        "id": "N1yzPtPiVjA9"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dada uma url e um local de arquivo (destination) faz o download do conteudo da url no arquivo\n",
        "def download_file(url, destination):\n",
        "    try:\n",
        "        response = requests.get(url) # Faz a requisicao do arquivo\n",
        "        response.raise_for_status()  # Verifica se houve algum erro na requisição\n",
        "        conteudo = response.content  # Guarda o conteudo binario da resposta da requisicao\n",
        "        # Coloca o conteudo da requisicao em um arquivo local\n",
        "        # Cria um novo arquivo e insere o conteudo neste arquivo\n",
        "        with open(destination, mode='wb') as file:\n",
        "            file.write(conteudo)\n",
        "    except requests.exceptions.MissingSchema:\n",
        "        # Caso seja uma excecao de url invalida\n",
        "        print(\"URL inválida. Certifique-se de fornecer uma URL válida.\")\n",
        "        print(\"Download cancelado!\")\n",
        "        raise ValueError(\"URL inválida. Certifique-se de fornecer uma URL válida.\")\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        # Caso seja uma excecao de comunicacao de rede\n",
        "        print(f\"Erro na conexão!\")\n",
        "        print(\"Download cancelado!\")\n",
        "        raise ValueError(\"Erro na conexão!\")\n",
        "    except IOError:\n",
        "        # Caso aconteca um erro de IO do arquivo\n",
        "        print(f\"Arquivo {destination} inválido!\")\n",
        "        print(\"Download cancelado!\")\n",
        "        raise ValueError(f\"Arquivo {destination} inválido!\")\n",
        "\n",
        "def unzip_file(my_file, path_to_unzip=None):\n",
        "    try:\n",
        "        with zipfile.ZipFile(my_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(path_to_unzip)\n",
        "    except Exception as ex:\n",
        "        raise ValueError(f\"Erro ao descompactar: {str(ex)}\")\n",
        "\n",
        "def get_field_content(field_name, file_content):\n",
        "    try:\n",
        "        my_field = file_content.split(f\"{field_name}:\")[1]\n",
        "        my_field = my_field.split('\\n')[0]\n",
        "        return my_field\n",
        "    except Exception:\n",
        "        print(f\"{field_name} não existe\")"
      ],
      "metadata": {
        "id": "FTeqzxOFVzGV"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "7Ki3crLBVhJM"
      },
      "outputs": [],
      "source": [
        "def convert_list_of_dict_in_json(data):\n",
        "    # Open the JSON file for writing in write mode ('w')\n",
        "    with open(\"issues_to_inspection.json\", \"w\") as outfile:\n",
        "        # Use json.dump() to write the list of dictionaries to the file\n",
        "        json.dump(data, outfile, indent=4)  # Optional: Add indentation for readability\n",
        "\n",
        "def generate_selected_issues(filename):\n",
        "    print('Aguarde...')\n",
        "    try:\n",
        "        print(f'Fazendo o download do arquivo {filename}')\n",
        "        download_file(url=MY_ISSUES_ZIP, destination=PATH_ARQUIVO_LOCAL)\n",
        "        print(f\"Arquivo salvo em: {PATH_ARQUIVO_LOCAL}\")\n",
        "        print(f'Descompactando o arquivo {filename}')\n",
        "        unzip_file(PATH_ARQUIVO_LOCAL)\n",
        "        print(f\"Arquivo {filename} descompactado com sucesso\")\n",
        "    except Exception as ex:\n",
        "        print(f\"Erro: {str(ex)}\")\n",
        "\n",
        "def convert_issues_to_json(filename, path_arquivos_descompactados):\n",
        "  # Lista de issues para serem inspecionados [dict1, dict2, .., dictN]\n",
        "  issues_to_inspection = []\n",
        "  print(\"Ler os arquivos\")\n",
        "  # Get list of files\n",
        "  filenames = os.listdir(path_arquivos_descompactados)\n",
        "  print(f\"Lendo {len(filenames)} arquivos...\")\n",
        "  for filename in filenames:\n",
        "    issue = {}\n",
        "    file_path = os.path.join(path_arquivos_descompactados, filename)\n",
        "    with open(file_path, 'r') as file:\n",
        "      file_content = file.read()\n",
        "      issue_type, summary, description, comments = None, None, None, None\n",
        "      try:\n",
        "        issue_type = get_field_content('issue_type', file_content)\n",
        "        summary = get_field_content('summary', file_content)\n",
        "        description = get_field_content('description', file_content)\n",
        "        comments = get_field_content('comments', file_content)\n",
        "        issue[\"issue_id\"] = filename\n",
        "        issue[\"issue_type\"] = issue_type\n",
        "        issue[\"summary\"] = summary\n",
        "        issue[\"description\"] = description\n",
        "        issue[\"comments\"] = comments\n",
        "        issues_to_inspection.append(issue)\n",
        "      except Exception as ex:\n",
        "        print(f\"{filename} com problema em: {str(ex)}\")\n",
        "  convert_list_of_dict_in_json(issues_to_inspection)\n",
        "  print(\"List of dictionaries converted to JSON file: issues_to_inspection.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_comment_to_text(comments_string):\n",
        "  # Remove the unnecessary parts from the string\n",
        "  comments_string = comments_string.split(\": \")[-1]  # Assuming the colon and space separate the key-value pair\n",
        "  comments_list = comments_string.split(\",\")\n",
        "  comments = \"\"\n",
        "  for comment in comments_list:\n",
        "    comment = comment.replace(\"['\", \"\")\n",
        "    comment = comment.replace(\"']\", \"\")\n",
        "    comment = comment.replace(\", '\", \" \")\n",
        "    comment = comment.replace(\"'\", \"\")\n",
        "    # Replace all occurrences of '\\n' with a new line character\n",
        "    comment = comment.replace(\"\\\\n\", \"\\n\")\n",
        "    comment = comment.replace(\"\\\\r\", \"\\r\")\n",
        "    comments = comments + comment\n",
        "  return comments"
      ],
      "metadata": {
        "id": "7q24dnSdcrW_"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_selected_issues(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubx6-Ph7WRPc",
        "outputId": "cc570da9-cec5-4359-d2c7-0229f2a8969a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aguarde...\n",
            "Fazendo o download do arquivo my_issues_to_inspection_cassandra.zip\n",
            "Arquivo salvo em: /content/downloads/my_issues_to_inspection_cassandra.zip\n",
            "Descompactando o arquivo my_issues_to_inspection_cassandra.zip\n",
            "Arquivo my_issues_to_inspection_cassandra.zip descompactado com sucesso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convert_issues_to_json(filename, PATH_ARQUIVOS_DESCOMPACTADOS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gI6ECt82WUNR",
        "outputId": "ebf68a26-ff39-447d-f088-87201713c84b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ler os arquivos\n",
            "Lendo 226 arquivos...\n",
            "description não existe\n",
            "comments não existe\n",
            "description não existe\n",
            "comments não existe\n",
            "description não existe\n",
            "comments não existe\n",
            "description não existe\n",
            "comments não existe\n",
            "description não existe\n",
            "comments não existe\n",
            "description não existe\n",
            "comments não existe\n",
            "description não existe\n",
            "comments não existe\n",
            "description não existe\n",
            "comments não existe\n",
            "description não existe\n",
            "comments não existe\n",
            "description não existe\n",
            "comments não existe\n",
            "description não existe\n",
            "comments não existe\n",
            "description não existe\n",
            "comments não existe\n",
            "description não existe\n",
            "comments não existe\n",
            "description não existe\n",
            "comments não existe\n",
            "List of dictionaries converted to JSON file: issues_to_inspection.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comments_string = \"\"\"comments: ['Working on this.', 'Still working on this, Todd?', ... , 'Committed.', 'Integrated in Cassandra #567 (See [https://hudson.apache.org/hudson/job/Cassandra/567/])']\"\"\"\n",
        "\n",
        "print(comments_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wwovhpesamtm",
        "outputId": "d4c408c8-3b6e-418b-9e84-6b27b8667ca1"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comments: ['Working on this.', 'Still working on this, Todd?', ... , 'Committed.', 'Integrated in Cassandra #567 (See [https://hudson.apache.org/hudson/job/Cassandra/567/])']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_comment = convert_comment_to_text(comments_string)\n",
        "print(my_comment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhUaDXY3gyAt",
        "outputId": "3c80c45b-9783-4bf7-f76e-fa63f78bd55e"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on this. Still working on this Todd? ...  Committed. Integrated in Cassandra #567 (See [https://hudson.apache.org/hudson/job/Cassandra/567/])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"issues_to_inspection.json\", \"r\") as json_file:\n",
        "    json_data = json.load(json_file)"
      ],
      "metadata": {
        "id": "4RAd-IFedWK-"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for issue in json_data:\n",
        "  print(f\"issue_id: {issue['issue_id']}\")\n",
        "  print(f\"issue_type: {issue['issue_type']}\")\n",
        "  print(f\"summary: {issue['summary']}\")\n",
        "  print(f\"description: {issue['description']}\")\n",
        "  print(f\"comments: {issue['comments']}\")\n",
        "  if issue[\"comments\"]:\n",
        "    my_comment = convert_comment_to_text(issue['comments'])\n",
        "    print(f\"my_comment: {my_comment}\")\n",
        "  print(\"---\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfN_olDmd3LI",
        "outputId": "2a5e9293-6a6d-4e8b-aa8f-16e909909c23"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "issue_id: CASSANDRA-6012\n",
            "issue_type:  Bug \n",
            "summary:  CAS does not always correctly replay inProgress rounds \n",
            "description:  Paxos says that on receiving the result of a prepare from a quorum of acceptors, the proposer should propose the value of the higher-number proposal accepted amongst the ones returned by the acceptors, and only propose his own value if no acceptor has send us back a previously accepted value.\n",
            "comments:  ['Attaching fix: as far as checking if we should finish an inProgress round, we only need to keep the most recent inProgress commit that has a value. But so as to not break the optimization of CASSANDRA-5667, the patch also keep the most recent inProgress, regardless of whether it has a value or not.\\n', '+1', 'Committed, thanks'] \n",
            "my_comment: as far as checking if we should finish an inProgress round we only need to keep the most recent inProgress commit that has a value. But so as to not break the optimization of CASSANDRA-5667 the patch also keep the most recent inProgress regardless of whether it has a value or not.\n",
            " +1 Committed thanks \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5426\n",
            "issue_type:  Improvement \n",
            "summary:  Redesign repair messages \n",
            "description:  Many people have been reporting 'repair hang' when something goes wrong.\n",
            "comments:  ['Work in progress is pushed to: https://github.com/yukim/cassandra/commits/5426-1\\n\\nOnly implemented for normal case that works.\\n\\n--\\n\\nFirst of all, ActiveRepairService is broken down to several classes and placed into o.a.c.repair to make my work easier.\\n\\nThe main design change around messages is that, all repair related message is packed into RepairMessage and handled in RepairMessageVerbHandler, which is executed in ANTY_ENTROPY stage. RepairMessage carries RepairMessageHeader and its content(if any). RepairMessageHeader is basically to indicate that the message belongs to which repair job and to specify content type. Repair message content type currently has 6 types defined in RepairMessageType: VALIDATION_REQUEST, VALIDATION_COMPLETE, VALIDATION_FAILED, SYNC_REQUEST, SYNC_COMPLETE, and SYNC_FAILED.\\n\\n*VALIDATION_REQUEST*\\n\\nVALIDATION_REQUEST is sent from repair initiator(coordinator) to request Merkle tree.\\n\\n*VALIDATION_COMPLETE*/*VALIDATION_FAILED*\\n\\nCalculated Merkle tree is sent back using VALIDATION_COMPLETE message. VALIDATION_FAILED message is used when something goes wrong in remote node.\\n\\n*SYNC_REQUEST*\\n\\nSYNC_REQUEST is sent when we have to repair remote two nodes. This is forwarded StreamingRepairTask we have today.\\n\\n*SYNC_COMPLETE*/*SYNC_FAILED*\\n\\nWhen there is no need to exchange data, or need to exchange but completed streaming, the node(this includes the node that received SYNC_REQUEST) sends back SYNC_COMPLETE. If streaming data fails, sends back SYNC_FAILED.\\n\\nThe whole repair process is depend on async message exchange using MessagingService, so there is still the chance to hang when the node fail to deliver message(see CASSANDRA-5393).\\n\\nAny feedback is appreciated.', 'Pushed completed version to: https://github.com/yukim/cassandra/commits/5426-2\\n\\nThis time, failure handling is implemented and added some unit tests for new classes.\\n', 'Could any of this be added to other processes using streaming?  Bootstrap/Decommission/Move/sstableloader?', \"[~jjordan] For the streaming improvements, I'm working on CASSANDRA-5286.\", 'The approach looks good to me: I definitively like the idea of having a common message/header for all repair message. Same for breaking down ARS in separate files.\\n\\nOne thing I\\'m not sure of is that it seems that when we get an error, we log it but we doesn\\'t error out the repair session itself. Maybe we should, otherwise I fear most people won\\'t notice something went wrong.\\n\\nAlso, when we fail, maybe we could send an error message (typically the exception message) for easier debugging/reporting.\\n\\nI also wonder if maybe we should have more of a fail-fast policy when there is errors. For instance, if one node fail it\\'s validation phase, maybe it might be worth failing right away and let the user re-trigger a repair once he has fixed whatever was the source of the error, rather than still differencing/syncing the other nodes (but I admit that both solutions are possible).\\n\\nGoing a bit further, I think we should add 2 messages to interrupt the validation and sync phase. If only because that could be useful to users if they need to stop a repair for some reason, but also, if we get an error during validation from one node, we could use that to interrupt the other nodes and thus fail fast while minimizing the amount of work done uselessly. But anyway, I guess that part can be done in a follow up ticket.\\n\\nOther than that, a few remarks/nits on the refactor.:\\n- In RepairMessageType, if gossip is any proof, then it could be wise to add more \"FUTURE\" type, say 4 or 5 \"just in case\". As an aside, I tend to not be a fan of relying on an enum ordinal for serialization since it\\'s extra fragile (you should not reorder stuffs for instance, which could easily slip by mistake imo). I personally prefer assigning the ordinal manually (like in transport.Message.Type for instance) even if that\\'s a bit more verbose.  Anyway, if people like it the way it is, so be it, but  \n",
            "my_comment: I definitively like the idea of having a common message/header for all repair message. Same for breaking down ARS in separate files.\n",
            "\n",
            "One thing I\\m not sure of is that it seems that when we get an error we log it but we doesn\\t error out the repair session itself. Maybe we should otherwise I fear most people won\\t notice something went wrong.\n",
            "\n",
            "Also when we fail maybe we could send an error message (typically the exception message) for easier debugging/reporting.\n",
            "\n",
            "I also wonder if maybe we should have more of a fail-fast policy when there is errors. For instance if one node fail it\\s validation phase maybe it might be worth failing right away and let the user re-trigger a repair once he has fixed whatever was the source of the error rather than still differencing/syncing the other nodes (but I admit that both solutions are possible).\n",
            "\n",
            "Going a bit further I think we should add 2 messages to interrupt the validation and sync phase. If only because that could be useful to users if they need to stop a repair for some reason but also if we get an error during validation from one node we could use that to interrupt the other nodes and thus fail fast while minimizing the amount of work done uselessly. But anyway I guess that part can be done in a follow up ticket.\n",
            "\n",
            "Other than that a few remarks/nits on the refactor.:\n",
            "- In RepairMessageType if gossip is any proof then it could be wise to add more \"FUTURE\" type say 4 or 5 \"just in case\". As an aside I tend to not be a fan of relying on an enum ordinal for serialization since it\\s extra fragile (you should not reorder stuffs for instance which could easily slip by mistake imo). I personally prefer assigning the ordinal manually (like in transport.Message.Type for instance) even if that\\s a bit more verbose.  Anyway if people like it the way it is so be it but  \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-13526\n",
            "issue_type:  Bug \n",
            "summary:  nodetool cleanup on KS with no replicas should remove old data, not silently complete \n",
            "description:  From the user list:\n",
            "comments:  ['The issue I am seeing on C* cluster with the below setup\\n\\nCassandra version : 2.1.16\\nDatacenters: 4 DC\\nRF: NetworkTopologyStrategy with 3 RF in each DC\\nKeyspaces: 50 keyspaces, few replicating to one DC and few replicating to multiple DC\\n\\n', '| branch | unit | [dtest|https://github.com/jasonstack/cassandra-dtest/commits/CASSANDRA-13526] |\\n| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526] |  [pass|https://circleci.com/gh/jasonstack/cassandra/182] | bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test known |\\n| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526-3.11]|  [pass|https://circleci.com/gh/jasonstack/cassandra/186] | pass |\\n| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526-3.0]|  [pass|https://circleci.com/gh/jasonstack/cassandra/181] | offline_tools_test.TestOfflineTools.sstableofflinerelevel_test  auth_test.TestAuth.system_auth_ks_is_alterable_test |\\n| [2.2|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526-2.2]|  [pass|https://circleci.com/gh/jasonstack/cassandra/185] |  ttl_test.TestTTL.collection_list_ttl_test |\\n\\nunit test all passed, some irrelevant dtests failed.\\n\\nwhen no local range && node has joined token ring,  clean up will remove all base local sstables.  ', '[~jjirsa] could you review ? thanks..', \"Thanks [~jasonstack] .  I glanced at it and it looked reasonable, though I'll do a more thorough review next week.\\n\\nSince it's a bug fix (and a pretty serious one at that), it seems like we should have patches for at least 3.0 and 3.11 , and perhaps even 2.1 and 2.2. Are you able to port your fix to 3.0 and 3.11? \\n\\nWe should also add a unit test to make sure we prevent this sort of regression again in the future.\\n\\n\\n\\n\", '[~jjirsa] thanks for reviewing. {{trunk}} was draft for review. I will prepare for older branches and more tests.', \"[~jasonstack] if you give me a few days I'll do a real review, and you can backport after that if it's easier\\n\", \"sure. it's not urgent.\", 'Patch looks good to me, dtest looks good as well, with two \n",
            "my_comment: 50 keyspaces few replicating to one DC and few replicating to multiple DC\n",
            "\n",
            " | branch | unit | [dtest|https://github.com/jasonstack/cassandra-dtest/commits/CASSANDRA-13526] |\n",
            "| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526] |  [pass|https://circleci.com/gh/jasonstack/cassandra/182] | bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test known |\n",
            "| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526-3.11]|  [pass|https://circleci.com/gh/jasonstack/cassandra/186] | pass |\n",
            "| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526-3.0]|  [pass|https://circleci.com/gh/jasonstack/cassandra/181] | offline_tools_test.TestOfflineTools.sstableofflinerelevel_test  auth_test.TestAuth.system_auth_ks_is_alterable_test |\n",
            "| [2.2|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526-2.2]|  [pass|https://circleci.com/gh/jasonstack/cassandra/185] |  ttl_test.TestTTL.collection_list_ttl_test |\n",
            "\n",
            "unit test all passed some irrelevant dtests failed.\n",
            "\n",
            "when no local range && node has joined token ring  clean up will remove all base local sstables.   [~jjirsa] could you review ? thanks.. \"Thanks [~jasonstack] .  I glanced at it and it looked reasonable though Ill do a more thorough review next week.\n",
            "\n",
            "Since its a bug fix (and a pretty serious one at that) it seems like we should have patches for at least 3.0 and 3.11  and perhaps even 2.1 and 2.2. Are you able to port your fix to 3.0 and 3.11? \n",
            "\n",
            "We should also add a unit test to make sure we prevent this sort of regression again in the future.\n",
            "\n",
            "\n",
            "\n",
            "\" [~jjirsa] thanks for reviewing. {{trunk}} was draft for review. I will prepare for older branches and more tests. \"[~jasonstack] if you give me a few days Ill do a real review and you can backport after that if its easier\n",
            "\" \"sure. its not urgent.\" Patch looks good to me dtest looks good as well with two \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14319\n",
            "issue_type:  Improvement \n",
            "summary:  nodetool rebuild from DC lets you pass invalid datacenters  \n",
            "description:  If you pass an invalid datacenter to nodetool rebuild, you'll get an error like this:\n",
            "comments:  [\"Hi [~rustyrazorblade],\\r\\n\\r\\nAre you looking for something like below when we provide invalid datacenter as an option to rebuild?\\r\\n{code:java}\\r\\n$ bin/nodetool rebuild no_dc\\r\\nnodetool: Provided datacenter: no_dc is not a valid datacenter, available datacenters are: datacenter1, datacenter2\\r\\nSee 'nodetool help' or 'nodetool help <command>'.\\r\\n$ \\r\\n{code}\\r\\nAnd for\\r\\n{quote}3. Ideally, we indicate which keyspaces are set to replicate to this DC and which aren't\\r\\n{quote}\\r\\nAre you referring {{this}} as the datacenter from where the {{rebuild}} command is being executed from or the one which is provided as an option (e.g., {{no_dc}} in above example)? if it is the later, invalid dc would not have any keyspaces, so what is expected for #3 in this scenario?\\r\\n\\r\\n\\xa0\\r\\n\\r\\nI am interested in working on this ticket. Can you assign this ticket to me?\\xa0\", 'Hi [~rustyrazorblade],\\r\\n\\r\\nI took a stab at it and implemented #1 and #2 from the list above, attached the patch with this implementation. Waiting for more information on #3. ', \"I'm not sure #3 is something that we can do as you pointed out, but I'm +1 on what you have.\", 'The fix looks good to me. I think it would be good to have some test to verify the behavior. \\r\\n[~vinaykumarcse] could you add a test to your patch? ', '[~vinaykumarcse] do you plan to finish this? Am I ok to assign this to myself? It seems like this patch is abandoned.', 'Assigning to myself. [~vinaykumarcse] feel free to take it from me if you feel like it though.', 'I believe the logic should be moved a little bit up so we fail earlier, in StorageService.rebuild method before anything is actually executed where all other checks are.\\r\\n\\r\\nhttps://github.com/apache/cassandra/pull/2309', 'trunk [https://github.com/apache/cassandra/pull/2309]\\r\\nj11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2185/workflows/c0b0e974-fdb1-4410-a180-fc8890c9a7e5]\\r\\nj8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2185/workflows/4fc8308c-3f07-4b41-9ad3-3a20a547c0e9]\\r\\n\\r\\n4.1 [https://github.com/apache/cassandra/pull/2323]\\r\\nj11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2194/workflows/e6aee8b6-95e7-446f-879d-a66bb4275255]\\r\\nj8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2194/workflows/bae89aec-bbe4-4705-bcdf-fc4fdecbfe3a]\\r\\n\\r\\n4.0 [https://github.com/apache/cassandra/pull/2324]\\r\\nj11 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2195/workflows/4e9e290d-9ada-4aa7-a2a9-68d66b1961fe\\r\\nj8 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2195/workflows/87cb1f9a-b71e-414b-b270-81d934390ab0\\r\\n\\r\\n3.11 [https://github.com/apache/cassandra/pull/2325]\\r\\nj8 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2196/workflows/7d692de2-3f73-4247-b4ec-fb8f6031c681\\r\\n\\r\\n3.0 [https://github.com/apache/cassandra/pull/2332]\\r\\nj8 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2197/workflows/831b7247-b5aa-4402-b9e9-9748602240f5', '[~brandon.williams] [~blerer] could you please review? Thank you.', '+1 from me.', \"This seems to set {{isRebuilding}} to false when validating the input or failing to {{isRebuilding.compareAndSet}} as part of the {{try/catch}} that is introduced. Just because this thread is not able to proceed with rebuild doesn't mean another thread isn't still proceeding.\\r\\n\\r\\nI think the new {{try/catch}} should be removed and the {{isRebuilding.compareAndSet}} should be in the existing {{try/catch}} that sets {{isRebuilding}} to {{false}} if actually doing the rebuild fails.\", 'Ah, good catch, I will take a closer look in a moment. We will need a new ticket for this though.\\n\\n\\nSent from ProtonMail mobile\\n\\n\\n\\n\\\\', \"What I said about moving the {{isRebuilding.compareAndSet}} into the {{try/catch}} doesn't make sense. It should be outside the {{try/catch}} but after all the up front validation is complete.\", '[~aweisberg] you mean it like this? https://g \n",
            "my_comment: datacenter1 datacenter2\r\n",
            "See nodetool help or nodetool help <command>.\r\n",
            "$ \r\n",
            "{code}\r\n",
            "And for\r\n",
            "{quote}3. Ideally we indicate which keyspaces are set to replicate to this DC and which arent\r\n",
            "{quote}\r\n",
            "Are you referring {{this}} as the datacenter from where the {{rebuild}} command is being executed from or the one which is provided as an option (e.g. {{no_dc}} in above example)? if it is the later invalid dc would not have any keyspaces so what is expected for #3 in this scenario?\r\n",
            "\r\n",
            "\\xa0\r\n",
            "\r\n",
            "I am interested in working on this ticket. Can you assign this ticket to me?\\xa0\" Hi [~rustyrazorblade]\r\n",
            "\r\n",
            "I took a stab at it and implemented #1 and #2 from the list above attached the patch with this implementation. Waiting for more information on #3.  \"Im not sure #3 is something that we can do as you pointed out but Im +1 on what you have.\" The fix looks good to me. I think it would be good to have some test to verify the behavior. \r\n",
            "[~vinaykumarcse] could you add a test to your patch?  [~vinaykumarcse] do you plan to finish this? Am I ok to assign this to myself? It seems like this patch is abandoned. Assigning to myself. [~vinaykumarcse] feel free to take it from me if you feel like it though. I believe the logic should be moved a little bit up so we fail earlier in StorageService.rebuild method before anything is actually executed where all other checks are.\r\n",
            "\r\n",
            "https://github.com/apache/cassandra/pull/2309 trunk [https://github.com/apache/cassandra/pull/2309]\r\n",
            "j11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2185/workflows/c0b0e974-fdb1-4410-a180-fc8890c9a7e5]\r\n",
            "j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2185/workflows/4fc8308c-3f07-4b41-9ad3-3a20a547c0e9]\r\n",
            "\r\n",
            "4.1 [https://github.com/apache/cassandra/pull/2323]\r\n",
            "j11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2194/workflows/e6aee8b6-95e7-446f-879d-a66bb4275255]\r\n",
            "j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2194/workflows/bae89aec-bbe4-4705-bcdf-fc4fdecbfe3a]\r\n",
            "\r\n",
            "4.0 [https://github.com/apache/cassandra/pull/2324]\r\n",
            "j11 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2195/workflows/4e9e290d-9ada-4aa7-a2a9-68d66b1961fe\r\n",
            "j8 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2195/workflows/87cb1f9a-b71e-414b-b270-81d934390ab0\r\n",
            "\r\n",
            "3.11 [https://github.com/apache/cassandra/pull/2325]\r\n",
            "j8 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2196/workflows/7d692de2-3f73-4247-b4ec-fb8f6031c681\r\n",
            "\r\n",
            "3.0 [https://github.com/apache/cassandra/pull/2332]\r\n",
            "j8 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2197/workflows/831b7247-b5aa-4402-b9e9-9748602240f5 [~brandon.williams] [~blerer] could you please review? Thank you. +1 from me. \"This seems to set {{isRebuilding}} to false when validating the input or failing to {{isRebuilding.compareAndSet}} as part of the {{try/catch}} that is introduced. Just because this thread is not able to proceed with rebuild doesnt mean another thread isnt still proceeding.\r\n",
            "\r\n",
            "I think the new {{try/catch}} should be removed and the {{isRebuilding.compareAndSet}} should be in the existing {{try/catch}} that sets {{isRebuilding}} to {{false}} if actually doing the rebuild fails.\" Ah good catch I will take a closer look in a moment. We will need a new ticket for this though.\n",
            "\n",
            "\n",
            "Sent from ProtonMail mobile\n",
            "\n",
            "\n",
            "\n",
            "\\\\ \"What I said about moving the {{isRebuilding.compareAndSet}} into the {{try/catch}} doesnt make sense. It should be outside the {{try/catch}} but after all the up front validation is complete.\" [~aweisberg] you mean it like this? https://g \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2280\n",
            "issue_type:  Improvement \n",
            "summary:  Request specific column families using StreamIn \n",
            "description:  StreamIn.requestRanges only specifies a keyspace, meaning that requesting a range will request it for all column families: if you have a large number of CFs, this can cause quite a headache. \n",
            "comments:  ['Attaching a patch that modifies a few streaming messages to (optionally) specify the CFs to repair. Only AES actually uses this feature.', \"StreamOut.transferRangesForRequest() flushes all SSTables for the keyspace even if we know the CFs. Can it flush just the CF's it is sending? \\n\\nAlthough CompactionManager.doValidation() also forces the CF to flush, so it may bo not be necessary when streaming for repair. May still be necessary for StreamOut.transferRanges() as it is used during move and decomission. \\n\\nOtherwise no problems.\\n \\nJonathan has moved CASSANDRA-2088 to 0.8 because the counters make it difficult to share compaction code with 0.7. I'll now do that ticket on top of this one.  \\n\", 'Adding 0002 to only flush matching CFS.', 'Cannot see any problems, good to go.', 'I\\'d prefer to avoid special-casing empty list and just pass all CF names when that is what we want.  A constructor or factory method that does not take a columnFamilies parameter could default to that, too.  (This is easy w/ CFS.all, but that would require using CFS objects instead of Strings. Which may be better anyway but it is hard to tell w/o trying it.)\\n\\nNit: we usually use \"cfs\" to abbreviate ColumnFamilyStore, suggest expanding to columnFamilies.', \"The reason for special casing the empty list is that it is backwards compatible with older MessagingService versions: otherwise we'd have to perform a lookup during deserialization to build the list for localhost. I think the code delta is about equal either way.\\n\\nRenamed cfs -> columnFamilies and squashed to one patch.\", \"bq. otherwise we'd have to perform a lookup during deserialization to build the list for localhost\\n\\nRight. That's a cleaner approach, since the semantics of what a columnFamilies list is doesn't have to be special cased outside the deserialize, which is the right place to deal with this.\\n\\nSpeaking of which, don't we need some code in StreamOutSession.create to force the cF list to all, if target is an old-version node?  Otherwise what we send, will not be what target expects.\", \"> Speaking of which, don't we need some code in StreamOutSession.create to force the cF list to all\\nNo, because the default is the empty list, which sends all. We either have to generate the list of CFs on the source or on the destination, so moving it from one side to the other doesn't save us any code, and requires special casing for backwards compatibility.\", 'bq. the default is the empty list, which sends all\\n\\nBut AES does NOT send the empty list, so an old-version target will reply with the wrong data.', '> But AES does NOT send the empty list, so an old-version target will reply with the wrong data.\\nIt will reply with \"all of the data\". The only alternative would be for it to not send anything at all since we have no way for an old-version and new-version to communicate which CFs they want to send or receive. I think that \"do what we\\'ve always done\" is a reasonable backwards compatibility strategy.', 'bq. It will reply with \"all of the data\"\\n\\nRight, which is not what is expected.  I\\'m saying we should make what is expected, match what we\\'ll receive, or we\\'re asking for regressions later (even if it happens to work now as by accident).', \"As discussed in IRC: StreamInSession (the session created by the requesting node) doesn't record any of the information about the request, so there isn't a place to add the list of expected CFs at the moment. We could fill out StreamInSession some more so that we could add this information, but I feel that that is speculative.\", 'Rebased for trunk.', 'bq. StreamInSession (the session created by the requesting node) doesn\\'t record any of the information about the request\\n\\nThat\\'s what I was missing. Thanks for clarifying.\\n\\nv3 rebases and removes special casing of empty CF list. As expected, this improves encapsulation of special cases (only streaming code needs to care, instead of leaking to anything that might touch a list of CF names).  Als \n",
            "my_comment: StreamInSession (the session created by the requesting node) doesnt record any of the information about the request so there isnt a place to add the list of expected CFs at the moment. We could fill out StreamInSession some more so that we could add this information but I feel that that is speculative.\" Rebased for trunk. bq. StreamInSession (the session created by the requesting node) doesn\\t record any of the information about the request\n",
            "\n",
            "That\\s what I was missing. Thanks for clarifying.\n",
            "\n",
            "v3 rebases and removes special casing of empty CF list. As expected this improves encapsulation of special cases (only streaming code needs to care instead of leaking to anything that might touch a list of CF names).  Als \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-12984\n",
            "issue_type:  Bug \n",
            "summary:  MVs are built unnecessarily again after bootstrap \n",
            "description:  After bootstrap MVs are enqueued to be built but they have been already created by the bootstrap.\n",
            "comments:  ['https://github.com/Jaumo/cassandra/commits/12984-3.0\\n+ \\nhttps://github.com/Jaumo/cassandra/commits/12984-3.x', 'Code looks good -- just running CI on these:\\n\\n||3.0|[utest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-3.0-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-3.0-dtest/]|\\n||3.11|[utest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-3.11-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-3.11-dtest/]|\\n||3.X|[utest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-3.X-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-3.X-dtest/]|\\n||trunk|[utest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-trunk-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-trunk-dtest/]|', 'Thanks for the patch, [~brstgt]! Tests looked reasonable, so I committed the patch as [e9b7a0f|https://git1-us-west.apache.org/repos/asf/cassandra/?p=cassandra.git;a=commit;h=e9b7a0f2546579244ffc167c56122b0a47d4b4b0].'] \n",
            "my_comment:  https://github.com/Jaumo/cassandra/commits/12984-3.0\n",
            "+ \n",
            "https://github.com/Jaumo/cassandra/commits/12984-3.x Code looks good -- just running CI on these:\n",
            "\n",
            "||3.0|[utest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-3.0-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-3.0-dtest/]|\n",
            "||3.11|[utest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-3.11-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-3.11-dtest/]|\n",
            "||3.X|[utest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-3.X-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-3.X-dtest/]|\n",
            "||trunk|[utest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-trunk-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/carlyeks/job/carlyeks-review-12984-trunk-dtest/]| Thanks for the patch [~brstgt]! Tests looked reasonable so I committed the patch as [e9b7a0f|https://git1-us-west.apache.org/repos/asf/cassandra/?p=cassandra.git;a=commit;h=e9b7a0f2546579244ffc167c56122b0a47d4b4b0]. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7107\n",
            "issue_type:  Improvement \n",
            "summary:  General minor tidying of CollationController path \n",
            "description:  There is a lot of unnecessary boiler plate when grabbing an iterator from an in-memory column family. This patch:\n",
            "comments:  [\"Whilst the main point of this is to tidy up, I also find a roughly 15-20% performance bump for the following stress command:\\n\\nbq. cassandra-stress read n=10000000 -key populate=1..100 -col slice n=fixed\\\\(1000\\\\) size=fixed\\\\(1\\\\) -rate threads=50 -mode thrift\\n\\nI've pushed the changes [here|https://github.com/belliottsmith/cassandra/tree/7107]\\n\\n\", 'Have only skimmed it so far. Will have a deep look once the issues behind these two unit tests are fixed (caused by the patch):\\n- org.apache.cassandra.cli.CliTest\\n- org.apache.cassandra.db.ColumnFamilyStoreTest\\n\\nThere are other tests failing, but those fail w/ and w/out the patch, both.', \"Weird. I was sure I'd run all of the unit tests before uploading.\\n\\nTwo line patch uploaded that fixes those two tests.\", \"Overall LGTM, some very welcome cleanup. Nice catch with the empty check in CollationController#collectTimeOrderedData(), too.\\n\\nSquashed commit #1 and #3 together and added to extra commits with suggested changes, pushed to https://github.com/iamaleksey/cassandra/commits/7107\\n\\nThe second original commit is not included b/c I'm very uncomfortable with leaving a ColumnFamily#append() method around - it's going to be abused by some innocent soul, and there will be hard to debug suffering as a result. Inlined it into a new CF#maybeAppendColumn() method instead, that's less likely to be called by mistake (see the last commit).\\n\\nThe second commit has a bugfix, some improvements, nits fixed, some extra cleanup and prettiness:\\n- discovered only one bug - ABSC.CellCollection#size() was not calling maybeSortCells() - fixed in the commit #2\\n- refactored ABSC#slice() and ABTC#slice() for obviousness\\n- made the second binary search in ABSC#slice() use the updated lowerBound, reducing the range to search\\n- in CollationController#collectAllData(), in the memtable loop, using Iterables#transform() instead of copying the cells into a temporary ArrayList, to potentially reduce the amount of localCopy()-ing (esp. for low query LIMITs)\\n- killed off MergeIterator#getSimple(), instead simply using toCollate.get(0) in QF#collateColumns(), avoiding creating a Reducer instance in the trivial case w/ a single source\", \"Slightly worried about a potentiall off by one at https://github.com/iamaleksey/cassandra/blob/7107/src/java/org/apache/cassandra/db/ArrayBackedSortedColumns.java#L526, but the more I stare at it, the more benign it looks to me. Wouldn't mind a second look by a third person at this particular code just to help me sleep better.\", \"LGTM. I don't think there's any off-by-1 error, unless I'm missing something: do you have a specific case you think may be off?\", \"Only the 'invert' case, although both do still seem fine to me. Don't trust myself much ¯\\\\_(ツ)_/¯\", 'Committed, thanks.', \"Well, just to be sure:\\n\\n* We translate a slice to the range \\\\[lb..ub\\\\) - i.e. lb is inclusive; \\n* We search in the range [0..lb) to translate the next slice - lb is exclusive here\\n\\nSo I'm pretty sure it's fine - lb always gets returned in the first iterator, and is never searched for building the following iterator, but is the exact boundary at which we start our search.\"] \n",
            "my_comment: do you have a specific case you think may be off?\" \"Only the invert case although both do still seem fine to me. Dont trust myself much ¯\\\\_(ツ)_/¯\" Committed thanks. \"Well just to be sure:\n",
            "\n",
            "* We translate a slice to the range \\\\[lb..ub\\\\) - i.e. lb is inclusive; \n",
            "* We search in the range [0..lb) to translate the next slice - lb is exclusive here\n",
            "\n",
            "So Im pretty sure its fine - lb always gets returned in the first iterator and is never searched for building the following iterator but is the exact boundary at which we start our search.\"] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-13010\n",
            "issue_type:  New Feature \n",
            "summary:  nodetool compactionstats should say which disk a compaction is writing to \n",
            "description: None\n",
            "comments: None\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2759\n",
            "issue_type:  Bug \n",
            "summary:  Scrub could lose increments and replicate that loss \n",
            "description:  If scrub cannot 'repair' a corrupted row, it will skip it. On node A, if the row contains some sub-count for A id, those will be lost forever since A is the source of truth on it's current id. We should thus renew node A id when that happens to avoid this (not unlike we do in cleanup). \n",
            "comments:  [\"Attached patch against 0.8.\\n\\nThe patch also add a new startup option to renew the node id on startup. This could be useful if someone lose one of it's sstable (because of a bad disk for instance) and don't want to fully decommission that node.\\n\\nThis could arguably be splitted in another ticket though.\", 'what is \"renewing a node id?\"', \"It's picking a new UUID for the current node to use for new counter increment.\\n\\nThe problem is that on a given node we store deltas for it's current nodeId (to avoid synchronized read-before-write, but I'm starting to wonder is that was the smartest ever). Anyway, if scrub skips a row, it may skip some of those deltas. Let's say at first there is no increments coming for this row for A as 'first distinguished replica'. So far we are still kind of good, because on a read (with CL > ONE) the result coming from A will have a 'version' for it's own sub-count smaller that the one on the other replica, so we will us the sub-count on those replica and return the correct value.\\n\\nHowever, as soon as A acknowledge new increments for this row, it will start inserting new deltas while he is not intrinsically up to date. Which will result in an definitive undercount.\\n\\nThe goal of renewing the node id of A is to make sure that second part never happen (because after the renew A will add new deltas as A', not A anymore).\\n\\nAnyway, now that I've plugged the brain this patch doesn't really works because A will never be repaired by the other nodes of it's now inconsistent value.\\n\\nSo I have no clue how to actually fix that.\", \"It may be that the best short fix here is to make scrub *not* skipping row on counter column families (though CASSANDRA-2614 would change that to 'never ever skipping row') and just throw a RuntimeException.\", \"bq. make scrub not skip rows on counter column families\\n\\n+1\\n\\nbq. CASSANDRA-2614 would change that to 'never ever skipping row'\\n\\nOnly if you actually did have a counter in the column_metadata, right?\", 'Attaching patch to simply re-throw the exception instead of skipping the row for counter column families.\\n\\nbq. Only if you actually did have a counter in the column_metadata, right?\\n\\nright.', '+1\\n\\ncan you add a link to this issue in the \"dangerous\" comment?', 'Committed with suggested comment update.', 'Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])\\n    '] \n",
            "my_comment:  [\"Attached patch against 0.8.\n",
            "\n",
            "The patch also add a new startup option to renew the node id on startup. This could be useful if someone lose one of its sstable (because of a bad disk for instance) and dont want to fully decommission that node.\n",
            "\n",
            "This could arguably be splitted in another ticket though.\" what is \"renewing a node id?\" \"Its picking a new UUID for the current node to use for new counter increment.\n",
            "\n",
            "The problem is that on a given node we store deltas for its current nodeId (to avoid synchronized read-before-write but Im starting to wonder is that was the smartest ever). Anyway if scrub skips a row it may skip some of those deltas. Lets say at first there is no increments coming for this row for A as first distinguished replica. So far we are still kind of good because on a read (with CL > ONE) the result coming from A will have a version for its own sub-count smaller that the one on the other replica so we will us the sub-count on those replica and return the correct value.\n",
            "\n",
            "However as soon as A acknowledge new increments for this row it will start inserting new deltas while he is not intrinsically up to date. Which will result in an definitive undercount.\n",
            "\n",
            "The goal of renewing the node id of A is to make sure that second part never happen (because after the renew A will add new deltas as A not A anymore).\n",
            "\n",
            "Anyway now that Ive plugged the brain this patch doesnt really works because A will never be repaired by the other nodes of its now inconsistent value.\n",
            "\n",
            "So I have no clue how to actually fix that.\" \"It may be that the best short fix here is to make scrub *not* skipping row on counter column families (though CASSANDRA-2614 would change that to never ever skipping row) and just throw a RuntimeException.\" \"bq. make scrub not skip rows on counter column families\n",
            "\n",
            "+1\n",
            "\n",
            "bq. CASSANDRA-2614 would change that to never ever skipping row\n",
            "\n",
            "Only if you actually did have a counter in the column_metadata right?\" Attaching patch to simply re-throw the exception instead of skipping the row for counter column families.\n",
            "\n",
            "bq. Only if you actually did have a counter in the column_metadata right?\n",
            "\n",
            "right. +1\n",
            "\n",
            "can you add a link to this issue in the \"dangerous\" comment? Committed with suggested comment update. Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])\n",
            "     \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7078\n",
            "issue_type:  Improvement \n",
            "summary:  Make sstable2json output more readable \n",
            "description:  sstable2json writes the entire file as a single JSON line.  also, local timestamp for delete is given as hex bytes instead of an int. \n",
            "comments:  ['fix attached', 'also renames \"columns\" field in output to \"cells\"', 'I feel like you are missing matching changes to SSTableImport. Can you make SSTableExportTest pass first before I have a look?', 'v2', 'Now SSTableImportTest. v3?', 'v3 also removes \"old\" format support from import (pre-1.0 stuff) and supercolumn support.', 'LGTM, +1.', 'committed'] \n",
            "my_comment:  fix attached also renames \"columns\" field in output to \"cells\" I feel like you are missing matching changes to SSTableImport. Can you make SSTableExportTest pass first before I have a look? v2 Now SSTableImportTest. v3? v3 also removes \"old\" format support from import (pre-1.0 stuff) and supercolumn support. LGTM +1. committed \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5605\n",
            "issue_type:  Bug \n",
            "summary:  Crash caused by insufficient disk space to flush \n",
            "description:  A few times now I have seen our Cassandra nodes crash by running themselves out of memory. It starts with the following exception:\n",
            "comments:  ['Do you have multiple data directories?  If you only have one, it may have been blacklisted and marked read-only by a previous issue, can you check the logs for anything like that?', 'We have only one data directory. There is nothing in the log about it being blacklisted.', 'Am not sure if the following information helps but we too hit this issue in production today. We were running with cassandra 1.2.4 and two patches CASSANDRA-5554 & CASSANDRA-5418. \\n\\nWe were running with RF=3 and LCS. \\n\\nWe ran into this issue while using sstablelaoder to push data from  remote 1.2.4 cluster nodes to another cluster\\n\\nWe cross checked using JMX if blacklisting is the cause of this bug and it looks like it is definitely not the case. \\n\\nWe however saw a pile up of pending compactions ~ 1800 pending compactions per node when node crashed. Surprising thing is that the \"Insufficient disk space to write xxxx bytes\" appears much before the node crashes. For us it started appearing aprrox 3 hours before the node crashed. \\n\\nThe cluster which showed this behavior was having loads of writes occurring ( We were using multiple SSTableLoaders to stream data into this cluster. ). We pushed in almost 15 TB worth data ( including the RF =3 ) in a matter of 16 hours. We were not serving any reads from this cluster as we were still migrating data to it. \\n\\nAnother interesting behavior observed that nodes were neighbors in most of the time. \\n\\nAm not sure if the above information helps but wanted to add it to the context of the ticket.  ', \"Apologies if this isn't directly relevant, but I seem to be experiencing the same issue using 1.2.8 launched via ccm for integration testing. One differentiating feature here is that this happened the first time the node was ever brought up (0 data). The integration tests attempting to use the ccm cluster never completed due to this hang, but all they do is test creation of a fairly simple schema and then attempt to write and then read back a single row. There is plenty of free disk space available...\\n\\nHere's what was in the log:\\n INFO [main] 2013-08-07 14:56:46,763 CassandraDaemon.java (line 118) Logging initialized\\n INFO [main] 2013-08-07 14:56:46,807 CassandraDaemon.java (line 145) JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.7.0_25\\n INFO [main] 2013-08-07 14:56:46,808 CassandraDaemon.java (line 183) Heap size: 8248098816/8248098816\\n INFO [main] 2013-08-07 14:56:46,808 CassandraDaemon.java (line 184) Classpath: /opt/ccmlib_cassandra/ccm/lwcdbng_test_cluster/node1/conf:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/build/classes/main:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/build/classes/thrift:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/antlr-3.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/avro-1.4.0-fixes.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/avro-1.4.0-sources-fixes.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/commons-cli-1.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/commons-codec-1.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/commons-lang-2.6.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/compress-lzf-0.8.4.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/concurrentlinkedhashmap-lru-1.3.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/guava-13.0.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/high-scale-lib-1.1.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jackson-core-asl-1.9.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jackson-mapper-asl-1.9.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jamm-0.2.5.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jbcrypt-0.3m.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jline-1.0.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/json-simple-1.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/libthrift-0.7.0.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8 \n",
            "my_comment: /opt/ccmlib_cassandra/ccm/lwcdbng_test_cluster/node1/conf:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/build/classes/main:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/build/classes/thrift:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/antlr-3.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/avro-1.4.0-fixes.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/avro-1.4.0-sources-fixes.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/commons-cli-1.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/commons-codec-1.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/commons-lang-2.6.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/compress-lzf-0.8.4.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/concurrentlinkedhashmap-lru-1.3.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/guava-13.0.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/high-scale-lib-1.1.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jackson-core-asl-1.9.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jackson-mapper-asl-1.9.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jamm-0.2.5.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jbcrypt-0.3m.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jline-1.0.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/json-simple-1.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/libthrift-0.7.0.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8 \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8147\n",
            "issue_type:  Bug \n",
            "summary:  Secondary indexing of map keys does not work properly when mixing contains and contains_key \n",
            "description:  If you have a table with a map column and an index on the map key selecting data using a contains key and a contains will not return the expected data.\n",
            "comments:  ['I think there is not currently support for indexing both values and keys (ref: http://www.datastax.com/dev/blog/cql-in-2-1)\\n\\nIn the example above the index is on keys only, but the select is attempting to query a values index too. So the empty result is sort of correct, but there should probably be a warning message to prevent confusion about this (\"index does not exist\" or something like that).\\n\\n/cc [~slebresne]', 'The empty result is not correct, no matter how you look at it. There are only two things that can happen in response to the query:\\nyou either get the good result back or you get an error with a message telling you what you did wrong.\\nI agree that querying a key and a value and on map does not make a lot of sense, so an error message will be perfectly acceptable.  ', \"I don't totally agree with all this. It is correct that we currently only allow one index per CQL column, and so one cannot index both the keys and values of a given map, but that's not what the test does. The test only has an index on the keys.\\n\\nRegarding the {{SELECT}}, provided you do have an indexed clause (which that example has), it's allowed to have other non-indexed clause (it will require {{ALLOW FILTERING}} but it's used in the example too).  So I'm not sure why this doesn't work, but it should (it's worth testing on current 2.1 branch though, maybe this has been fixed since 2.1.0).\\n\\nbq. I agree that querying a key and a value and on map does not make a lot of sense\\n\\nOut of curiosity, why wouldn't that make sense?\", \"{quote}(it's worth testing on current 2.1 branch though, maybe this has been fixed since 2.1.0).{quote}\\nI tested it on the latest 2.1\\n\\n{quote}Out of curiosity, why wouldn't that make sense?{quote}\\nAs a map can only have one value associated to a given key, using such a query means that you want to check that the key exists and that the value is the one you think it should be. If you select using a contains key only you will be able to have the same information but you will also know if it is the key which is missing or the value which is not what you expect.\\nThat is why I think that it does not make a lot of sense and that an error message will be fine for me if I was a user.\\nNow as a user it is also true that it will also give me a better sense of robustness if the query was handled properly ;-)   \\n\", \"bq. As a map can only have one value associated to a given key, using such a query means that you want to check that the key exists and that the value is the one you think it should be.\\n\\nThat's not what the query means, no. Asking for maps that contains a given key and a given value does not imply that said given value must be associated to said given key.\\nBesides, even if that was what the query means, the query still make sense. It might not be terribly useful, but it make sense, so I'd still not think throwing an error would not be very user friendly.\", 'The problem was cause by the fact that the IndexSearcher was trying to use the key index to search for the contains value.\\nThis patch fix this problem and also fix the SelectStatement check for the indexed column (if we do a select contains on a map were only the key is indexed we should reject the query).\\nThe patch also replace the index option Strings with constants to avoid typos issues.', 'Hmm, so this makes some of the same fixes as CASSANDRA-8155, but in a different way.  Personally, I prefer putting the logic for \"does this index support this operator\" in the index code instead of the operator code (as the 8155 patch does).  However, it would be good to include your test cases and the constants instead of String options.\\n\\nDo you want to take a look at that patch and see if you agree?', 'I like the {{supportsOperator}} approach of CASSANDRA-8155 but I think that we should do the validation during the preparation phase of the select statement and not at execution time. It seems more user friendly to me.\\n\\nSo personally I would kind of merge the \n",
            "my_comment: http://www.datastax.com/dev/blog/cql-in-2-1)\n",
            "\n",
            "In the example above the index is on keys only but the select is attempting to query a values index too. So the empty result is sort of correct but there should probably be a warning message to prevent confusion about this (\"index does not exist\" or something like that).\n",
            "\n",
            "/cc [~slebresne] The empty result is not correct no matter how you look at it. There are only two things that can happen in response to the query:\n",
            "you either get the good result back or you get an error with a message telling you what you did wrong.\n",
            "I agree that querying a key and a value and on map does not make a lot of sense so an error message will be perfectly acceptable.   \"I dont totally agree with all this. It is correct that we currently only allow one index per CQL column and so one cannot index both the keys and values of a given map but thats not what the test does. The test only has an index on the keys.\n",
            "\n",
            "Regarding the {{SELECT}} provided you do have an indexed clause (which that example has) its allowed to have other non-indexed clause (it will require {{ALLOW FILTERING}} but its used in the example too).  So Im not sure why this doesnt work but it should (its worth testing on current 2.1 branch though maybe this has been fixed since 2.1.0).\n",
            "\n",
            "bq. I agree that querying a key and a value and on map does not make a lot of sense\n",
            "\n",
            "Out of curiosity why wouldnt that make sense?\" \"{quote}(its worth testing on current 2.1 branch though maybe this has been fixed since 2.1.0).{quote}\n",
            "I tested it on the latest 2.1\n",
            "\n",
            "{quote}Out of curiosity why wouldnt that make sense?{quote}\n",
            "As a map can only have one value associated to a given key using such a query means that you want to check that the key exists and that the value is the one you think it should be. If you select using a contains key only you will be able to have the same information but you will also know if it is the key which is missing or the value which is not what you expect.\n",
            "That is why I think that it does not make a lot of sense and that an error message will be fine for me if I was a user.\n",
            "Now as a user it is also true that it will also give me a better sense of robustness if the query was handled properly ;-)   \n",
            "\" \"bq. As a map can only have one value associated to a given key using such a query means that you want to check that the key exists and that the value is the one you think it should be.\n",
            "\n",
            "Thats not what the query means no. Asking for maps that contains a given key and a given value does not imply that said given value must be associated to said given key.\n",
            "Besides even if that was what the query means the query still make sense. It might not be terribly useful but it make sense so Id still not think throwing an error would not be very user friendly.\" The problem was cause by the fact that the IndexSearcher was trying to use the key index to search for the contains value.\n",
            "This patch fix this problem and also fix the SelectStatement check for the indexed column (if we do a select contains on a map were only the key is indexed we should reject the query).\n",
            "The patch also replace the index option Strings with constants to avoid typos issues. Hmm so this makes some of the same fixes as CASSANDRA-8155 but in a different way.  Personally I prefer putting the logic for \"does this index support this operator\" in the index code instead of the operator code (as the 8155 patch does).  However it would be good to include your test cases and the constants instead of String options.\n",
            "\n",
            "Do you want to take a look at that patch and see if you agree? I like the {{supportsOperator}} approach of CASSANDRA-8155 but I think that we should do the validation during the preparation phase of the select statement and not at execution time. It seems more user friendly to me.\n",
            "\n",
            "So personally I would kind of merge the \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-15004\n",
            "issue_type:  Bug \n",
            "summary:  Anti-compaction briefly corrupts sstable state for reads \n",
            "description:  Since we use multiple sstable rewriters in anticompaction, the first call to prepareToCommit will remove the original sstables from the tracker view before the other rewriters add their sstables. This creates a brief window where reads can miss data. \n",
            "comments:  ['|[3.0|https://github.com/bdeggleston/cassandra/tree/15004-3.0]|[3.11|https://github.com/bdeggleston/cassandra/tree/15004-3.11]|[trunk|https://github.com/bdeggleston/cassandra/tree/15004-trunk]|\\r\\n|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F15004-3.0]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F15004-3.11]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F15004-trunk]|', '+1\\r\\n\\r\\npushed a unit test: [3.0|https://github.com/krummas/cassandra/commits/blake/15004-3.0] [3.11|https://github.com/krummas/cassandra/commits/blake/15004-3.11] [trunk|https://github.com/krummas/cassandra/commits/blake/15004-trunk]\\r\\n\\r\\nedit: not sure what is going on with the dtests though, probably need a restart', 'Nice catch, and it looks like a good fix to me.\\r\\n\\r\\n(+1)', \"Blake realised there was an issue with the patch he posted, so I have put together an alternative patch with input from [~krummas].\\r\\n\\r\\n[3.0|https://github.com/belliottsmith/cassandra/tree/15004-3.0] [3.11|https://github.com/belliottsmith/cassandra/tree/15004-3.11] [4.0|https://github.com/belliottsmith/cassandra/tree/15004-4.0]\\r\\n\\r\\nThese patches extract an interface for {{LifecycleTransaction}} and no-op the relevant calls ({{prepareToCommit}} and {{obsoleteOriginals}}) so that {{SSTableRewriter.prepareToCommit}} does not update the tracker - these are then invoked directly once each rewriter has finished its other preparatory work.\\r\\n\\r\\nIt's a bit ugly and still finicky, but probably better/safer than more invasive surgery at this point in time.\", 'updated unit tests [3.0|https://github.com/krummas/cassandra/tree/15004-3.0] [3.11|https://github.com/krummas/cassandra/tree/15004-3.11] [trunk|https://github.com/krummas/cassandra/tree/15004-trunk] also adds checks that the files on disk are what we expect', 'lgtm, just need a few comments explaining what is going on and the comment mentioning {{permitRedundantTransitions}} needs to be removed/updated', \"Thanks.  I've pushed branches with updated comments.\", '+1', 'Thanks, committed to [3.0|https://github.com/apache/cassandra/commit/44785dd2eec5697eec7e496ed3a73d2573f4fe6a], [3.11|https://github.com/apache/cassandra/commit/9199e591c6148d14f3d12784af8ce5342f118161] and [4.0|https://github.com/apache/cassandra/commit/df62169d1b6a5bfff2bc678ffbeb0883a3a576b5]'] \n",
            "my_comment: not sure what is going on with the dtests though probably need a restart Nice catch and it looks like a good fix to me.\r\n",
            "\r\n",
            "(+1) \"Blake realised there was an issue with the patch he posted so I have put together an alternative patch with input from [~krummas].\r\n",
            "\r\n",
            "[3.0|https://github.com/belliottsmith/cassandra/tree/15004-3.0] [3.11|https://github.com/belliottsmith/cassandra/tree/15004-3.11] [4.0|https://github.com/belliottsmith/cassandra/tree/15004-4.0]\r\n",
            "\r\n",
            "These patches extract an interface for {{LifecycleTransaction}} and no-op the relevant calls ({{prepareToCommit}} and {{obsoleteOriginals}}) so that {{SSTableRewriter.prepareToCommit}} does not update the tracker - these are then invoked directly once each rewriter has finished its other preparatory work.\r\n",
            "\r\n",
            "Its a bit ugly and still finicky but probably better/safer than more invasive surgery at this point in time.\" updated unit tests [3.0|https://github.com/krummas/cassandra/tree/15004-3.0] [3.11|https://github.com/krummas/cassandra/tree/15004-3.11] [trunk|https://github.com/krummas/cassandra/tree/15004-trunk] also adds checks that the files on disk are what we expect lgtm just need a few comments explaining what is going on and the comment mentioning {{permitRedundantTransitions}} needs to be removed/updated \"Thanks.  Ive pushed branches with updated comments.\" +1 Thanks committed to [3.0|https://github.com/apache/cassandra/commit/44785dd2eec5697eec7e496ed3a73d2573f4fe6a] [3.11|https://github.com/apache/cassandra/commit/9199e591c6148d14f3d12784af8ce5342f118161] and [4.0|https://github.com/apache/cassandra/commit/df62169d1b6a5bfff2bc678ffbeb0883a3a576b5] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1404\n",
            "issue_type:  Improvement \n",
            "summary:  Improve pre-cleanup estimate of disk space required \n",
            "description:  Compaction sums the sizes of all ranges to estimate the output size in order to pick a target drive. Anti-compactions directly drop ranges from sstables, and the volume of data dropped can easily be calculated using the SSTable index. Using this knowledge in our estimate in ColumnFamilyStore.getExpectedCompactedFileSize would allow 'nodetool cleanup' to run when less than 50% of disk is available, in the case where ranges are being dropped (post node-move). \n",
            "comments:  [\"That's a reasonable improvement to make; I'd also like to make cleanup operate on one sstable at a time, and _just_ clean out unneeded rows, rather than doing clean + major-compaction-merging.  That way you can continue to rely on minor compactions if that fits your needs.\", \"bq. I'd also like to make cleanup operate on one sstable at a time\\n\\nthis is done in CASSANDRA-1916\", 'Bumping this one: with 60% full nodes, it can become impossible to cleanup the largest sstable (256GB or so).', \"(Targetting 1.1.1 because I think it's a minor change. If that is not the case then let's put it back on the back burner.)\", 'Attaching patch to calculate estimated compacted file size with eliminating ranges when performing cleanup. ', 'I think this makes the \" / 2\" guess obsolete?', \"You are right. There is no need to halve since we are actually calculating the exact size of cleanup'ed sstable.\", 'committed'] \n",
            "my_comment: with 60% full nodes it can become impossible to cleanup the largest sstable (256GB or so). \"(Targetting 1.1.1 because I think its a minor change. If that is not the case then lets put it back on the back burner.)\" Attaching patch to calculate estimated compacted file size with eliminating ranges when performing cleanup.  I think this makes the \" / 2\" guess obsolete? \"You are right. There is no need to halve since we are actually calculating the exact size of cleanuped sstable.\" committed \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-10866\n",
            "issue_type:  Improvement \n",
            "summary:  Column Family should expose count metrics for dropped mutations. \n",
            "description:  Please take a look at the discussion in CASSANDRA-10580. This is opened so that the latency on dropped mutations is exposed as a metric on column families. \n",
            "comments:  [\"I don't see much sense in exposing latency metrics for dropped mutations on a per-cf basis, since this won't vary much on a per-cf basis, but rather on a per-node basis (also internal vs cross node), so the global dropped mutation latency metric should suffice to tune node timeouts accordingly. Keeping a count for dropped mutations per CF may make sense though, as you may want to monitor the distribution of dropped mutations across different CFs.\", 'That makes sense. Updating title to reflect.', \"I have provided a patch on top of https://issues.apache.org/jira/secure/attachment/12777927/Trunk-All-Comments.patch\\n\\nI can add Unit Tests in messaging service but I did not see a pattern for checking metrics thorough unit tests so skipping those. The change is verified on a local installation through visual VM. \\n\\nIt is possible to optimize the KS/CF lookup in MessagingService.java (maybe through a Map), but I am hoping that's not necessary.\", 'Any updates here ?', \"Thanks for the patch. Some comments below:\\n- Please rebase to latest trunk.\\n- In {{MessagingService.updateDroppedMutationCount}} use {{Keyspace.open(mutation.getKeyspaceName()).getColumnFamilyStore(UUID)}} to fetch CFS instead of iterating in {{ColumnFamilyStore.all()}}, and also perform null check if CFS is null (if table was dropped for example).\\n- In {{updateDroppedMutationCount(MessageIn message)}}, no need to check if {{message.payload instanceof Collection<?>}}, since there are no {{DROPPABLE_VERBS}} which operates on a collection of mutations.\\n- In {{StorageProxy.performLocally}}, add an {{Optional<Mutation>}} argument that receives an {{Optional.absent(}} if it's not a mutation. Similarly, {{LocalMutationRunnable}} should receive an {{Optional<Mutation>}} and only count if {{!mutationOpt.isEmpty()}}\\n- In {{TableStats}} you removed the {{Maximum tombstones per slice}} metric by mistake.\", \"Thanks. I included the Collection because I did not realize that SCHEMA_* verb isn't part of DROPPABLE_VERBs. Good point.\\n\\nI'll submit a rebased patch shortly.\", 'Attached 10866-Trunk.patch.', \"Thanks for the patch. It's looking better. You'll need to print the {{DroppedMutations}} metric in the {{TableStats}} command and verify it's being correctly displayed in {{nodetool tablestats}} command. Also, on {{LocalMutationRunnable}} you'll need to make the {{mutationOpt}} field is final, and initialize it in the empty constructor (otherwise there will be {{NullPointerException}}.\\n\\nSome style nits:\\n* Remove {{updateDroppedMutationCount(MessageIn message)}} (since it's only used once) and instead do the conditional {{message.payload instanceof IMutation}} check on {{incrementDroppedMessages(MessageIn message, long timeTaken)}} and call {{updateDroppedMutationCount((IMutation)message.payload)}} from there.\\n* On {{LocalMutationRunnable}}, pass the optional to {{MessagingService.incrementDroppedMutations}} and perform the conditional from there.\\n* On {{updateDroppedMutationCount(IMutation mutation)}} replace the null mutation check with an assertion, since it should never be null.\\n** Also no need to create the variable {{columnFamilyIds}}, you can iterate on the loop directly on {{mutation.getColumnFamilyIds()}}.\\n* Use meaningful commit message\", 'Please submit patch again once those are addressed.', 'Attached. Please take a look when you get a chance !', 'Any updates here ?', \"LGTM, let's wait test results before marking as ready to commit.\\n\\n||trunk||\\n|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-10686]|\\n|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-10686-testall/lastCompletedBuild/testReport/]|\\n|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-10686-dtest/lastCompletedBuild/testReport/]|\", 'Tests look good. Marking as ready to commit.', 'Thanks.', 'Looks like this needs to be rebased following CASSANDRA-10477.', 'Rebased.', 'Thanks, [~anubhavk] and [~pa \n",
            "my_comment:  [\"I dont see much sense in exposing latency metrics for dropped mutations on a per-cf basis since this wont vary much on a per-cf basis but rather on a per-node basis (also internal vs cross node) so the global dropped mutation latency metric should suffice to tune node timeouts accordingly. Keeping a count for dropped mutations per CF may make sense though as you may want to monitor the distribution of dropped mutations across different CFs.\" That makes sense. Updating title to reflect. \"I have provided a patch on top of https://issues.apache.org/jira/secure/attachment/12777927/Trunk-All-Comments.patch\n",
            "\n",
            "I can add Unit Tests in messaging service but I did not see a pattern for checking metrics thorough unit tests so skipping those. The change is verified on a local installation through visual VM. \n",
            "\n",
            "It is possible to optimize the KS/CF lookup in MessagingService.java (maybe through a Map) but I am hoping thats not necessary.\" Any updates here ? \"Thanks for the patch. Some comments below:\n",
            "- Please rebase to latest trunk.\n",
            "- In {{MessagingService.updateDroppedMutationCount}} use {{Keyspace.open(mutation.getKeyspaceName()).getColumnFamilyStore(UUID)}} to fetch CFS instead of iterating in {{ColumnFamilyStore.all()}} and also perform null check if CFS is null (if table was dropped for example).\n",
            "- In {{updateDroppedMutationCount(MessageIn message)}} no need to check if {{message.payload instanceof Collection<?>}} since there are no {{DROPPABLE_VERBS}} which operates on a collection of mutations.\n",
            "- In {{StorageProxy.performLocally}} add an {{Optional<Mutation>}} argument that receives an {{Optional.absent(}} if its not a mutation. Similarly {{LocalMutationRunnable}} should receive an {{Optional<Mutation>}} and only count if {{!mutationOpt.isEmpty()}}\n",
            "- In {{TableStats}} you removed the {{Maximum tombstones per slice}} metric by mistake.\" \"Thanks. I included the Collection because I did not realize that SCHEMA_* verb isnt part of DROPPABLE_VERBs. Good point.\n",
            "\n",
            "Ill submit a rebased patch shortly.\" Attached 10866-Trunk.patch. \"Thanks for the patch. Its looking better. Youll need to print the {{DroppedMutations}} metric in the {{TableStats}} command and verify its being correctly displayed in {{nodetool tablestats}} command. Also on {{LocalMutationRunnable}} youll need to make the {{mutationOpt}} field is final and initialize it in the empty constructor (otherwise there will be {{NullPointerException}}.\n",
            "\n",
            "Some style nits:\n",
            "* Remove {{updateDroppedMutationCount(MessageIn message)}} (since its only used once) and instead do the conditional {{message.payload instanceof IMutation}} check on {{incrementDroppedMessages(MessageIn message long timeTaken)}} and call {{updateDroppedMutationCount((IMutation)message.payload)}} from there.\n",
            "* On {{LocalMutationRunnable}} pass the optional to {{MessagingService.incrementDroppedMutations}} and perform the conditional from there.\n",
            "* On {{updateDroppedMutationCount(IMutation mutation)}} replace the null mutation check with an assertion since it should never be null.\n",
            "** Also no need to create the variable {{columnFamilyIds}} you can iterate on the loop directly on {{mutation.getColumnFamilyIds()}}.\n",
            "* Use meaningful commit message\" Please submit patch again once those are addressed. Attached. Please take a look when you get a chance ! Any updates here ? \"LGTM lets wait test results before marking as ready to commit.\n",
            "\n",
            "||trunk||\n",
            "|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-10686]|\n",
            "|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-10686-testall/lastCompletedBuild/testReport/]|\n",
            "|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-10686-dtest/lastCompletedBuild/testReport/]|\" Tests look good. Marking as ready to commit. Thanks. Looks like this needs to be rebased following CASSANDRA-10477. Rebased. Thanks [~anubhavk] and [~pa \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3381\n",
            "issue_type:  Bug \n",
            "summary:  StorageProxy does not log correctly when schema is not in agreement \n",
            "description:  \"logger.debug(\"%s disagrees (%s)\", host, entry.getKey());\"\n",
            "comments:  ['Fixed the erroneous debug logging statement by replacing %s with {}, as supported by SLF4J. Also made use of the {}-notation on some of the other debug logging statements in the class.', 'committed.  Thanks, Jackson and Tommy!'] \n",
            "my_comment:  Fixed the erroneous debug logging statement by replacing %s with {} as supported by SLF4J. Also made use of the {}-notation on some of the other debug logging statements in the class. committed.  Thanks Jackson and Tommy! \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-17808\n",
            "issue_type:  Improvement \n",
            "summary:  Optionally avoid hint transfer during decommission \n",
            "description:  Both because they aren’t strictly necessary to maintain consistency, and because throttling induced by their rate-limiter (see {{hinted_handoff_throttle}}) may stall progress, transferring hints during decommission (specifically unbootstrap) rather than just pausing, disabling, and truncating them probably doesn’t make sense. The only other concern would be the BatchLog, which nominally depends on hint delivery to maintain its \"guarantees\". However, during BatchLog replay on unbootstrap, {{ReplayingBatch}} ignores batches older the gcgs anyway.\n",
            "comments:  [\"I may try to address CASSANDRA-16679 while I'm in this corner of the codebase...\", \"Did a 300-iteration run of {{HintedHandoffAddRemoveNodesTest}} w/ some minor fixes to make checking for hint totals more tolerant: https://app.circleci.com/pipelines/github/maedhroz/cassandra?branch=CASSANDRA-17808\\r\\n\\r\\nThings look clean, but I'll probably do another run after making the rest of the changes I need to make for this issue...\", 'Reading back in CASSANDRA-16679, unfortunately the CircleCI logs are gone but I noticed we were hitting the following issue:\\r\\n{code:java}\\r\\nIf I remove node 4 never to get back up, I still can see in the logs of node 2 that node 4 goes down and then a connection is established between node 2 and node4 and node 4 is marked immediately up in the logs of node 2...{code}\\r\\nDid this got resolved somewhere down the road? :D\\xa0That would be awesome', 'Looks like I have, in fact, found the cause of the flakes in CASSANDRA-16679. Moving over to fix that issue, then will rebase and finalize this.', 'Going to try to get this into review, now that CASSANDRA-16679 is committed...', '[~stefan.miklosovic] Not sure if you were interested in reviewing further, but [the PR is ready|https://github.com/apache/cassandra/pull/1835].', '+1', '+1', 'Committed as https://github.com/apache/cassandra/commit/d6aee7e08c658db9d394a6b7e3e27791b4d6854f', \"[~maedhroz], [~smiklosovic] \\xa0Hello, can we also do the same exercise and cherry-pick this issue as we did for the virtual tables running queries? I think this is a helpful flag for users.\\r\\n\\r\\nI have prepared changes here:\\r\\nhttps://github.com/apache/cassandra/pull/2448\\r\\n\\r\\nThe test results on Jenkins:\\r\\n[https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2522/]\\r\\n\\r\\nAnd again, I'm a bit limited with getting CircleCi results for this, so help is needed :)\", '[~maedhroz] are you ok with backporting to 4.1? How is this ad-hoc addition of features done? We just agree on that?'] \n",
            "my_comment: https://app.circleci.com/pipelines/github/maedhroz/cassandra?branch=CASSANDRA-17808\r\n",
            "\r\n",
            "Things look clean but Ill probably do another run after making the rest of the changes I need to make for this issue...\" Reading back in CASSANDRA-16679 unfortunately the CircleCI logs are gone but I noticed we were hitting the following issue:\r\n",
            "{code:java}\r\n",
            "If I remove node 4 never to get back up I still can see in the logs of node 2 that node 4 goes down and then a connection is established between node 2 and node4 and node 4 is marked immediately up in the logs of node 2...{code}\r\n",
            "Did this got resolved somewhere down the road? :D\\xa0That would be awesome Looks like I have in fact found the cause of the flakes in CASSANDRA-16679. Moving over to fix that issue then will rebase and finalize this. Going to try to get this into review now that CASSANDRA-16679 is committed... [~stefan.miklosovic] Not sure if you were interested in reviewing further but [the PR is ready|https://github.com/apache/cassandra/pull/1835]. +1 +1 Committed as https://github.com/apache/cassandra/commit/d6aee7e08c658db9d394a6b7e3e27791b4d6854f \"[~maedhroz] [~smiklosovic] \\xa0Hello can we also do the same exercise and cherry-pick this issue as we did for the virtual tables running queries? I think this is a helpful flag for users.\r\n",
            "\r\n",
            "I have prepared changes here:\r\n",
            "https://github.com/apache/cassandra/pull/2448\r\n",
            "\r\n",
            "The test results on Jenkins:\r\n",
            "[https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2522/]\r\n",
            "\r\n",
            "And again Im a bit limited with getting CircleCi results for this so help is needed :)\" [~maedhroz] are you ok with backporting to 4.1? How is this ad-hoc addition of features done? We just agree on that? \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6109\n",
            "issue_type:  New Feature \n",
            "summary:  Consider coldness in STCS compaction \n",
            "description:  I see two options:\n",
            "comments:  ['bq. The latter is better if you have cold data that may become hot again... but it\\'s confusing if you have a workload such that you can\\'t keep up with all compaction, but you can keep up with hot sstable. (Compaction backlog stat becomes useless since we fall increasingly behind.)\\n\\nThe pending compactions stat is already pretty wonky, so I\\'m not sure we should prioritize keeping that sane.\\n\\nOption 1 (don\\'t compact cold sstables) seems dangerous as a first step compared to option 2, especially because it\\'s hard to decide what is \"cold\".  Prioritizing compaction of hotter sstables seems like the better first step.\\n\\nWhen comparing hotness of sstables, I think a good measure is {{avg_reads_per_sec / number_of_keys}} rather than just {{avg_reads_per_sec}} so that large sstables aren\\'t over-weighted.  When I mention the hotness of a bucket of sstables below, I\\'m talking about the sum of the hotness measure across the individual sstables.\\n\\nFor prioritizing compaction of hotter sstables, it seems like there are a few levels this can operate at:\\n# Picking sstable members for compaction buckets\\n# Picking the most \"interesting\" bucket to submit to the compaction executor (currently the smallest sstables are considered the most interesting)\\n# At the compaction executor level, prioritizing tasks in the queue (the queue is not currently prioritized)\\n\\n(1) seems like the most difficult point to make good decisions at.  I can imagine a scheme like dropping members that are below {{2 * stdev}} of the mean hotness for the bucket working decently, but some of the efficiency of compacting many sstables at once is lost, and some of the drops would be poor when there is little variance among the sstables.\\n\\n(2) would probably work well by itself, although, as discussed below, sstable overlap is a better measure than hotness for this.\\n\\n(3) requires (2) to be somewhat fair.  Each table submits its hottest buckets for compaction, and the executor prioritizes the hottest buckets in the queue (regardless of which table they came from).  There is a potential for starvation among colder tables when compaction falls behind, but that may be mitigated by a few things:\\n* If the compaction of the hotter sstables is very effective at merging rows, the hotness of future buckets for that table should be lower.  Since the hotness of a bucket is the sum of its members, if four totally overlapping sstables are merged into one sstable, the hotness of the new sstable should be 1/4 of the hotness of the previous bucket.  I\\'ll point out that tracking how much overlap there is among sstables would be a much better measure than hotness for picking which compactions to prioritize; in the worst case here (no overlap), the hotness of the newly compacted sstable could be the same as the bucket it came from.\\n* If we were willing to discard cold items in the queue when hotter items came in and the queue was full, colder tables would eventually submit new tasks with more sstables in them (thus having greater hotness).\\n\\nWhile I\\'m thinking about it, do we have any tickets or features in place to track sstable overlap (beyond average number of sstables hit per read at the table level)?', 'I guess whether hotness or overlap is a more important criterion depends on your goal:\\n# prioritizing by hotness helps speed reads up more, especially when you have a lot of cold data sitting around\\n# prioritizing by overlap ratio reduces disk space and helps throw away obsolete cells faster\\n\\nI was hoping to tackle #1 here, but maybe that needs a separate strategy a la CASSANDRA-5561.\\n\\nFor #2, CASSANDRA-5906 adds a HyperLogLog component that does a fantastic job of letting us estimate overlap ratios.\\n\\n', \"I think I have some clearer ideas about how to do this now.  We should be able to combine hotness and overlap concerns at the different levels.\\n\\nAt level (1), avoid compacting comparatively cold data by dropping sstables from buckets when their hotness is l \n",
            "my_comment:  bq. The latter is better if you have cold data that may become hot again... but it\\s confusing if you have a workload such that you can\\t keep up with all compaction but you can keep up with hot sstable. (Compaction backlog stat becomes useless since we fall increasingly behind.)\n",
            "\n",
            "The pending compactions stat is already pretty wonky so I\\m not sure we should prioritize keeping that sane.\n",
            "\n",
            "Option 1 (don\\t compact cold sstables) seems dangerous as a first step compared to option 2 especially because it\\s hard to decide what is \"cold\".  Prioritizing compaction of hotter sstables seems like the better first step.\n",
            "\n",
            "When comparing hotness of sstables I think a good measure is {{avg_reads_per_sec / number_of_keys}} rather than just {{avg_reads_per_sec}} so that large sstables aren\\t over-weighted.  When I mention the hotness of a bucket of sstables below I\\m talking about the sum of the hotness measure across the individual sstables.\n",
            "\n",
            "For prioritizing compaction of hotter sstables it seems like there are a few levels this can operate at:\n",
            "# Picking sstable members for compaction buckets\n",
            "# Picking the most \"interesting\" bucket to submit to the compaction executor (currently the smallest sstables are considered the most interesting)\n",
            "# At the compaction executor level prioritizing tasks in the queue (the queue is not currently prioritized)\n",
            "\n",
            "(1) seems like the most difficult point to make good decisions at.  I can imagine a scheme like dropping members that are below {{2 * stdev}} of the mean hotness for the bucket working decently but some of the efficiency of compacting many sstables at once is lost and some of the drops would be poor when there is little variance among the sstables.\n",
            "\n",
            "(2) would probably work well by itself although as discussed below sstable overlap is a better measure than hotness for this.\n",
            "\n",
            "(3) requires (2) to be somewhat fair.  Each table submits its hottest buckets for compaction and the executor prioritizes the hottest buckets in the queue (regardless of which table they came from).  There is a potential for starvation among colder tables when compaction falls behind but that may be mitigated by a few things:\n",
            "* If the compaction of the hotter sstables is very effective at merging rows the hotness of future buckets for that table should be lower.  Since the hotness of a bucket is the sum of its members if four totally overlapping sstables are merged into one sstable the hotness of the new sstable should be 1/4 of the hotness of the previous bucket.  I\\ll point out that tracking how much overlap there is among sstables would be a much better measure than hotness for picking which compactions to prioritize; in the worst case here (no overlap) the hotness of the newly compacted sstable could be the same as the bucket it came from.\n",
            "* If we were willing to discard cold items in the queue when hotter items came in and the queue was full colder tables would eventually submit new tasks with more sstables in them (thus having greater hotness).\n",
            "\n",
            "While I\\m thinking about it do we have any tickets or features in place to track sstable overlap (beyond average number of sstables hit per read at the table level)? I guess whether hotness or overlap is a more important criterion depends on your goal:\n",
            "# prioritizing by hotness helps speed reads up more especially when you have a lot of cold data sitting around\n",
            "# prioritizing by overlap ratio reduces disk space and helps throw away obsolete cells faster\n",
            "\n",
            "I was hoping to tackle #1 here but maybe that needs a separate strategy a la CASSANDRA-5561.\n",
            "\n",
            "For #2 CASSANDRA-5906 adds a HyperLogLog component that does a fantastic job of letting us estimate overlap ratios.\n",
            "\n",
            " \"I think I have some clearer ideas about how to do this now.  We should be able to combine hotness and overlap concerns at the different levels.\n",
            "\n",
            "At level (1) avoid compacting comparatively cold data by dropping sstables from buckets when their hotness is l \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2633\n",
            "issue_type:  Bug \n",
            "summary:  Keys get lost in bootstrap \n",
            "description:  When bootstrapping a new node, the key at the upper end of the new node's range can get lost.  To reproduce:\n",
            "comments:  [\"So say we have a node A with rows A B C D on it.\\n\\nWe bootstrap a node C.\\n\\nC requests (A, C] from A.\\n\\nA will do a GT scan starting with A.  So a cache hit will result in [A, C] being transferred instead. That is a bug, I'll see if I can create a unit test that demonstrates that separately.\\n\\nBut I don't see how this affects the C row?\\n\", 'A cache hit results in [A, C) being returned.  All GT scans with cache hits give positions at the start of the row rather than the end.  The above patch fixes both ends - skip over A, but include C.', 'getPosition only affects start of scan, not end.', 'Good catch. Attaching a unit test to catch the bug.', \"It looks to me that for client reads, getPosition is just used for the start of an iterator, as you say.  But for streaming, getPosition is used for the end position too in SSTableReader.getPositionsForRanges.  Or have I misunderstood what's going on?\", \"bq. But I don't see how this affects the C row?\\n\\nThis affects the C row because it will use the position of C found as the position where to stop scanning. But the position of C is the start of C, so when used as an end position, it excludes it. That is, getPositionForRanges will return (start of A, start of C), which results in scanning [A, C) as Richard says.\\n\\nSo +1 on this.\", 'got it.  committed, thanks!', 'Integrated in Cassandra-0.7 #480 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/480/])\\n    '] \n",
            "my_comment:  [\"So say we have a node A with rows A B C D on it.\n",
            "\n",
            "We bootstrap a node C.\n",
            "\n",
            "C requests (A C] from A.\n",
            "\n",
            "A will do a GT scan starting with A.  So a cache hit will result in [A C] being transferred instead. That is a bug Ill see if I can create a unit test that demonstrates that separately.\n",
            "\n",
            "But I dont see how this affects the C row?\n",
            "\" A cache hit results in [A C) being returned.  All GT scans with cache hits give positions at the start of the row rather than the end.  The above patch fixes both ends - skip over A but include C. getPosition only affects start of scan not end. Good catch. Attaching a unit test to catch the bug. \"It looks to me that for client reads getPosition is just used for the start of an iterator as you say.  But for streaming getPosition is used for the end position too in SSTableReader.getPositionsForRanges.  Or have I misunderstood whats going on?\" \"bq. But I dont see how this affects the C row?\n",
            "\n",
            "This affects the C row because it will use the position of C found as the position where to stop scanning. But the position of C is the start of C so when used as an end position it excludes it. That is getPositionForRanges will return (start of A start of C) which results in scanning [A C) as Richard says.\n",
            "\n",
            "So +1 on this.\" got it.  committed thanks! Integrated in Cassandra-0.7 #480 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/480/])\n",
            "     \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-12786\n",
            "issue_type:  Bug \n",
            "summary:  Fix a bug in CASSANDRA-11005(Split consisten range movement flag) \n",
            "description:  I missed a place in the code where we need to split this flag for bootstrap \n",
            "comments:  [\"Patch merged cleanly from 2.2 -> trunk, assuming those are the branches you're targeting [~kohlisankalp] ? \\n\\nLooks pretty straight forward, but pushed to my github to kick off CI:\\n\\n| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-dtest/] |\\n| [3.X|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786-3.X] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.X-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.X-dtest/] |\\n| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786-3.0] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.0-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.0-dtest/] |\\n| [2.2|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786-2.2] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-2.2-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-2.2-dtest/] |\\n\", 'Yes..please commit it. ', 'lgtm, committed as {{28713778abe29c1d9120d2127354b7fd5ee8fff1}}'] \n",
            "my_comment:  [\"Patch merged cleanly from 2.2 -> trunk assuming those are the branches youre targeting [~kohlisankalp] ? \n",
            "\n",
            "Looks pretty straight forward but pushed to my github to kick off CI:\n",
            "\n",
            "| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-dtest/] |\n",
            "| [3.X|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786-3.X] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.X-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.X-dtest/] |\n",
            "| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786-3.0] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.0-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.0-dtest/] |\n",
            "| [2.2|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786-2.2] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-2.2-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-2.2-dtest/] |\n",
            "\" Yes..please commit it.  lgtm committed as {{28713778abe29c1d9120d2127354b7fd5ee8fff1}} \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3862\n",
            "issue_type:  Bug \n",
            "summary:  RowCache misses Updates \n",
            "description:  While performing stress tests to find any race problems for CASSANDRA-2864 I guess I (re-)found one for the standard on-heap row cache.\n",
            "comments:  [\"Dunno if there's a better way to do it...\", \"I believe you are absolutely right that this is a bug.\\n\\nUnfortunately I don't think including the memtables during cache reads really solves it. If you miss an update, it won't ever get added to the cached row, but the update itself will be flushed at some point and thus not be in any memtable anymore.\\n\\nOne partial solution I see could be that when a read 'reads for caching', it starts adding some sentinel object in the cache for the given row key. That sentinel would need to be an actual (empty) row but marked with the fact it's only a sentinel. When a write look if the row is cache, if it's a sentinel we would add the write to the sentinel. Once the read returns and we actually put the row in cache, we would it (atomically) with the content of the sentinel. A read that check the cache and see a sentinel would just skip the cache (and would not put it's result into the cache). Adapting that to the serializingCache is trivial.\\n\\nUnfortunately, this is not perfect because this would screw counters. Though I guess for counters we could do the same thing as we would do for the serializingCache, i.e, if a read that 'reads for caching' see that the sentinel is not empty, we would just not cache the result (i.e, a row would be cache only if we are sure no write were done concurrently to the read).\", \"bq.  If you miss an update, it won't ever get added to the cached row, but the update itself will be flushed at some point and thus not be in any memtable anymore.\\n\\nVery true ...\\n\\nHow about adopting the strategy we apply with CASSANDRA-2864:\\n\\n- Writers dont update the cache at all\\n- Readers merge cache with memtables\\n- Upon flush merge memtables with cache\", \"{quote}\\nHow about adopting the strategy we apply with CASSANDRA-2864:\\n\\n* Writers dont update the cache at all\\n* Readers merge cache with memtables\\n* Upon flush merge memtables with cache\\n{quote}\\nThe problem with that is that I don't see how we can make that work for counters at all. I also think it would be nice not having to merge on reads if we can avoid it (even if it's in-memory, it still uses CPU).\\n\\nAs a side note, I also suspect it's not bulletproof in theory, as a memtable could be fully flushed while a 'read to be cached' happens and with a bad timing during that, we could still miss an update. Of course, that kind of timing have almost no chance to happen. But in the case where a user triggers a flush manually, a memtable with only a handful of columns could be flushed very quickly, and I suspect the behavior could be observed. However unlikely that is, it'd be better if we can fix this problem once and for all.\\n\\nI'll probably give a shot to my 'sentinel' proposal described above, I don't think it's too much code.\", \"Hmokay ... don't want to abuse Jira as an educational forum but maybe as a reward for the bugreport :-) ... are you saying that a reader could see a memtable view where flushing memtables are gone (flushed) and sstables don't contain the flushed memtables?\\n\\nIf that's the case than yes the cache would lose an update. But that what also imply that a read could miss an update without caching being in place at all no?\\n\\nOtherwise (and that's how I read the code) given that the memtable switch will only happen after the merge the reader will read all updates because they are either in (flushing) memtables or in sstables and the cache will be in fact valid.\\n\\n\", 'To be precise, what I\\'m saying is that (at least in theory) the following scenario would be possible:\\n* A read-for-cache read the memtables grabing updates\\n* then it start reading the sstables\\n* while the previous happens, a new update arrives. The memtable is then flushed and happens to be fully flushed *before* our read-for-cache completes.\\n\\nIn that case, the new update won\\'t be part of the cached row (ever) because during the flush (when we would merge the memtable to the cache) the row was not in the cache yet. That may se \n",
            "my_comment:  [\"Dunno if theres a better way to do it...\" \"I believe you are absolutely right that this is a bug.\n",
            "\n",
            "Unfortunately I dont think including the memtables during cache reads really solves it. If you miss an update it wont ever get added to the cached row but the update itself will be flushed at some point and thus not be in any memtable anymore.\n",
            "\n",
            "One partial solution I see could be that when a read reads for caching it starts adding some sentinel object in the cache for the given row key. That sentinel would need to be an actual (empty) row but marked with the fact its only a sentinel. When a write look if the row is cache if its a sentinel we would add the write to the sentinel. Once the read returns and we actually put the row in cache we would it (atomically) with the content of the sentinel. A read that check the cache and see a sentinel would just skip the cache (and would not put its result into the cache). Adapting that to the serializingCache is trivial.\n",
            "\n",
            "Unfortunately this is not perfect because this would screw counters. Though I guess for counters we could do the same thing as we would do for the serializingCache i.e if a read that reads for caching see that the sentinel is not empty we would just not cache the result (i.e a row would be cache only if we are sure no write were done concurrently to the read).\" \"bq.  If you miss an update it wont ever get added to the cached row but the update itself will be flushed at some point and thus not be in any memtable anymore.\n",
            "\n",
            "Very true ...\n",
            "\n",
            "How about adopting the strategy we apply with CASSANDRA-2864:\n",
            "\n",
            "- Writers dont update the cache at all\n",
            "- Readers merge cache with memtables\n",
            "- Upon flush merge memtables with cache\" \"{quote}\n",
            "How about adopting the strategy we apply with CASSANDRA-2864:\n",
            "\n",
            "* Writers dont update the cache at all\n",
            "* Readers merge cache with memtables\n",
            "* Upon flush merge memtables with cache\n",
            "{quote}\n",
            "The problem with that is that I dont see how we can make that work for counters at all. I also think it would be nice not having to merge on reads if we can avoid it (even if its in-memory it still uses CPU).\n",
            "\n",
            "As a side note I also suspect its not bulletproof in theory as a memtable could be fully flushed while a read to be cached happens and with a bad timing during that we could still miss an update. Of course that kind of timing have almost no chance to happen. But in the case where a user triggers a flush manually a memtable with only a handful of columns could be flushed very quickly and I suspect the behavior could be observed. However unlikely that is itd be better if we can fix this problem once and for all.\n",
            "\n",
            "Ill probably give a shot to my sentinel proposal described above I dont think its too much code.\" \"Hmokay ... dont want to abuse Jira as an educational forum but maybe as a reward for the bugreport :-) ... are you saying that a reader could see a memtable view where flushing memtables are gone (flushed) and sstables dont contain the flushed memtables?\n",
            "\n",
            "If thats the case than yes the cache would lose an update. But that what also imply that a read could miss an update without caching being in place at all no?\n",
            "\n",
            "Otherwise (and thats how I read the code) given that the memtable switch will only happen after the merge the reader will read all updates because they are either in (flushing) memtables or in sstables and the cache will be in fact valid.\n",
            "\n",
            "\" To be precise what I\\m saying is that (at least in theory) the following scenario would be possible:\n",
            "* A read-for-cache read the memtables grabing updates\n",
            "* then it start reading the sstables\n",
            "* while the previous happens a new update arrives. The memtable is then flushed and happens to be fully flushed *before* our read-for-cache completes.\n",
            "\n",
            "In that case the new update won\\t be part of the cached row (ever) because during the flush (when we would merge the memtable to the cache) the row was not in the cache yet. That may se \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-16837\n",
            "issue_type:  New Feature \n",
            "summary:  Add node tool commands to recompress sstables and upgrade all sstables created before given timestamp  \n",
            "description:  When changing compression parameters and testing their results, it is useful to be able to recompress only sstables that need to be recompressed. \n",
            "comments:  ['Patch:\\r\\n|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-16837-trunk]|[tests|https://app.circleci.com/pipelines/github/ifesdjeen/cassandra?branch=CASSANDRA-16837-trunk]|', '+1 assuming clean cci', 'Thank you for the review!\\xa0\\r\\n\\r\\nCommitted to trunk with [cfc402d26a628bbc9e005c370f5707f4985207aa|https://github.com/apache/cassandra/commit/cfc402d26a628bbc9e005c370f5707f4985207aa].'] \n",
            "my_comment:  Patch:\r\n",
            "|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-16837-trunk]|[tests|https://app.circleci.com/pipelines/github/ifesdjeen/cassandra?branch=CASSANDRA-16837-trunk]| +1 assuming clean cci Thank you for the review!\\xa0\r\n",
            "\r\n",
            "Committed to trunk with [cfc402d26a628bbc9e005c370f5707f4985207aa|https://github.com/apache/cassandra/commit/cfc402d26a628bbc9e005c370f5707f4985207aa]. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14647\n",
            "issue_type:  Bug \n",
            "summary:  Reading cardinality from Statistics.db failed \n",
            "description:  There is some issue with sstable metadata which is visible in system.log, the messages says:\n",
            "comments:  ['Hey Vitali, thanks for the report. [~krummas] any idea what is going on here? Would changing compaction strategies cause this issue?', '[~nezdali] could you post logs?', 'This is not due to STCS -> LCS. I have the same behavior on one cluster with LCS and heavy writes.\\xa0STCS has never been configured on it.', 'Looks like this can happen when the table metric `EstimatedPartitionCount` is [queried |https://github.com/apache/cassandra/blob/d049c6b9b4af4f663aac2bf90d860c3b0c20684a/src/java/org/apache/cassandra/metrics/TableMetrics.java#L307] - it grabs the CANONICAL sstables without referencing them, so if there are very many sstables some might get compacted away while calculating the partition count and we get this warning\\r\\n\\r\\nIf this is the case it is not really a problem (other than the annoying warn message in the log files)\\r\\n\\r\\n[~rha] could you verify that you are querying this metric?\\r\\n\\r\\n[~rha] / [~nezdali] could you pause querying this metric and check if the error stops appearing?\\r\\n\\r\\nThanks for providing the logs over email btw [~nezdali]', 'Thanks Marcus. Stopped quering\\xa0EstimatedPartitionCount metric from on 1 node.', 'Unfortunately this did no help, the failed cardinality message is still in the log.', \"I do query partition count through Datadog JMX agent, so it happens all the time.\\xa0I'll try to disabled it - although it didn't work for [~nezdali]\", \"After removing {{EstimatedPartitionCount}} from Datadog's cassandra.yaml on one node, I don't see any warning for 4 days.\\r\\n\\r\\n[~nezdali] can you double check that your change was effective (e.g. monitoring service restarted, etc.)?\\xa0\", \"there is an alias for the metric called {{EstimatedRowCount}} which should also be disabled\\r\\n\\r\\nI'll work on a fix\", 'Tested again, and there are no more warning in the log for the whole day. Turned out that monitoring system is querying all keys via jmx and then filters them based on the configuration file.', '[~krummas] I have created a patch for this and uploaded to this ticket\\xa0 [^14647-trunk-1.patch]', \"The {{TableMetrics}} change looks good to me (grabbing refs to the sstables before checking key count), but I don't think we need the {{SSTableReader.refAndGetApproximateKeyCount}} part - in the cases this is called we always already have a ref to the sstable.\", '[~krummas] Updated the patch.\\xa0\\r\\n||Branch||CircleCI||\\r\\n|[14647-trunk|https://github.com/apache/cassandra/compare/trunk...nvharikrishna:14647-trunk]|[link|https://circleci.com/gh/nvharikrishna/cassandra/3#tests/containers/2]|', '+1\\r\\n\\r\\nre-ran the tests and failures look unrelated;\\r\\n\\r\\nhttps://circleci.com/workflow-run/264bb5cb-fd18-4ea2-8b1d-768d9ab96d94', 'sorry for the delay on this - committed as {{ded62076e7fdfd1cfdcf96447489ea607ca796a0}} to trunk'] \n",
            "my_comment:  Hey Vitali thanks for the report. [~krummas] any idea what is going on here? Would changing compaction strategies cause this issue? [~nezdali] could you post logs? This is not due to STCS -> LCS. I have the same behavior on one cluster with LCS and heavy writes.\\xa0STCS has never been configured on it. Looks like this can happen when the table metric `EstimatedPartitionCount` is [queried |https://github.com/apache/cassandra/blob/d049c6b9b4af4f663aac2bf90d860c3b0c20684a/src/java/org/apache/cassandra/metrics/TableMetrics.java#L307] - it grabs the CANONICAL sstables without referencing them so if there are very many sstables some might get compacted away while calculating the partition count and we get this warning\r\n",
            "\r\n",
            "If this is the case it is not really a problem (other than the annoying warn message in the log files)\r\n",
            "\r\n",
            "[~rha] could you verify that you are querying this metric?\r\n",
            "\r\n",
            "[~rha] / [~nezdali] could you pause querying this metric and check if the error stops appearing?\r\n",
            "\r\n",
            "Thanks for providing the logs over email btw [~nezdali] Thanks Marcus. Stopped quering\\xa0EstimatedPartitionCount metric from on 1 node. Unfortunately this did no help the failed cardinality message is still in the log. \"I do query partition count through Datadog JMX agent so it happens all the time.\\xa0Ill try to disabled it - although it didnt work for [~nezdali]\" \"After removing {{EstimatedPartitionCount}} from Datadogs cassandra.yaml on one node I dont see any warning for 4 days.\r\n",
            "\r\n",
            "[~nezdali] can you double check that your change was effective (e.g. monitoring service restarted etc.)?\\xa0\" \"there is an alias for the metric called {{EstimatedRowCount}} which should also be disabled\r\n",
            "\r\n",
            "Ill work on a fix\" Tested again and there are no more warning in the log for the whole day. Turned out that monitoring system is querying all keys via jmx and then filters them based on the configuration file. [~krummas] I have created a patch for this and uploaded to this ticket\\xa0 [^14647-trunk-1.patch] \"The {{TableMetrics}} change looks good to me (grabbing refs to the sstables before checking key count) but I dont think we need the {{SSTableReader.refAndGetApproximateKeyCount}} part - in the cases this is called we always already have a ref to the sstable.\" [~krummas] Updated the patch.\\xa0\r\n",
            "||Branch||CircleCI||\r\n",
            "|[14647-trunk|https://github.com/apache/cassandra/compare/trunk...nvharikrishna:14647-trunk]|[link|https://circleci.com/gh/nvharikrishna/cassandra/3#tests/containers/2]| +1\r\n",
            "\r\n",
            "re-ran the tests and failures look unrelated;\r\n",
            "\r\n",
            "https://circleci.com/workflow-run/264bb5cb-fd18-4ea2-8b1d-768d9ab96d94 sorry for the delay on this - committed as {{ded62076e7fdfd1cfdcf96447489ea607ca796a0}} to trunk \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-12717\n",
            "issue_type:  Bug \n",
            "summary:  IllegalArgumentException in CompactionTask \n",
            "description:  When I was ran LargePartitionsTest.test_11_1G at trunk, I found that this test fails due to a java.lang.IllegalArgumentException during compaction.\n",
            "comments:  ['Patch is here. Could you please review this?\\n\\nFix IllegalArgumentException in CompactionTask\\nhttps://github.com/matope/cassandra/commit/d6c40dd3d4d95dba8b9c3f88de1015315e45990d', 'Took your patch and added the same fix to cleanup compaction. CI triggered.\\n\\n||cassandra-3.X|[branch|https://github.com/apache/cassandra/compare/cassandra-3.X...snazy:12717-3.x]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-12717-3.x-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-12717-3.x-dtest/lastSuccessfulBuild/]\\n', 'Thanks for the patch! CI looks good.\\n\\nCommitted as [433dd1c0ab77d296dafcc6c2079aa9445a6c1b2a|https://github.com/apache/cassandra/commit/433dd1c0ab77d296dafcc6c2079aa9445a6c1b2a] to [cassandra-3.X|https://github.com/apache/cassandra/tree/cassandra-3.X]\\n'] \n",
            "my_comment:  Patch is here. Could you please review this?\n",
            "\n",
            "Fix IllegalArgumentException in CompactionTask\n",
            "https://github.com/matope/cassandra/commit/d6c40dd3d4d95dba8b9c3f88de1015315e45990d Took your patch and added the same fix to cleanup compaction. CI triggered.\n",
            "\n",
            "||cassandra-3.X|[branch|https://github.com/apache/cassandra/compare/cassandra-3.X...snazy:12717-3.x]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-12717-3.x-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-12717-3.x-dtest/lastSuccessfulBuild/]\n",
            " Thanks for the patch! CI looks good.\n",
            "\n",
            "Committed as [433dd1c0ab77d296dafcc6c2079aa9445a6c1b2a|https://github.com/apache/cassandra/commit/433dd1c0ab77d296dafcc6c2079aa9445a6c1b2a] to [cassandra-3.X|https://github.com/apache/cassandra/tree/cassandra-3.X]\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8757\n",
            "issue_type:  Bug \n",
            "summary:  IndexSummaryBuilder should construct itself offheap, and share memory between the result of each build() invocation \n",
            "description: None\n",
            "comments: None\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-71\n",
            "issue_type:  New Feature \n",
            "summary:  Range query support \n",
            "description:  Scan for keys by range (between X and Y) or prefix (starting with P). \n",
            "comments:  ['Add range query support, which requires using an OrderPreservingPartitioner.  (Keys are returned in the order defined by the partitioner collation.)  The fundamental approach (in table.getKeyRange) is simple: create a CollatedIterator [See http://commons.apache.org/collections/api/org/apache/commons/collections/IteratorUtils.html#collatedIterator(java.util.Comparator,%20java.util.Collection)] that will return unique keys from different sources, each of which is already sorted.  Then we just need Iterators for different key sources.  \\n\\nFor SSTables, this means adding seekTo and an Iterator interface to FileStruct.  For Memtable, this means adding a DestructivePQIterator since unlike SSTable keys those are not inherently ordered already.  This means that we only do M log N work sorting the memtable keys where M is the number of keys we actually read, and N is the total number of keys, where a naive sort-everything-first iterator would be N log N.\\n\\nThis does not implement ranges spanning multiple nodes.  I will implement that for another ticket.', \"I reviewed this patch and here are my \n",
            "my_comment: create a CollatedIterator [See http://commons.apache.org/collections/api/org/apache/commons/collections/IteratorUtils.html#collatedIterator(java.util.Comparator%20java.util.Collection)] that will return unique keys from different sources each of which is already sorted.  Then we just need Iterators for different key sources.  \n",
            "\n",
            "For SSTables this means adding seekTo and an Iterator interface to FileStruct.  For Memtable this means adding a DestructivePQIterator since unlike SSTable keys those are not inherently ordered already.  This means that we only do M log N work sorting the memtable keys where M is the number of keys we actually read and N is the total number of keys where a naive sort-everything-first iterator would be N log N.\n",
            "\n",
            "This does not implement ranges spanning multiple nodes.  I will implement that for another ticket. \"I reviewed this patch and here are my \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-10805\n",
            "issue_type:  New Feature \n",
            "summary:  Additional Compaction Logging \n",
            "description:  Currently, viewing the results of past compactions requires parsing the log and looking at the compaction history system table, which doesn't have information about, for example, flushed sstables not previously compacted.\n",
            "comments:  [\"I've pushed a [branch|https://github.com/carlyeks/cassandra/tree/ticket/10805]. It adds a CompactionLogger class which the compaction strategies can use. Currently, most of the work happens in either the Flush runnable or the CompactionTask, but more statements could be added for the strategies to use.\\n\\nThere is an added table parameter {{log_all}} which, if set to true, will start logging right away. Otherwise, there is a JMX operation to start compaction logging for a CF.\", '* Can we use logback to do the logging to file? It should be possible to create a special logger that goes to a separate file. Feels wrong to implement our own log rotation etc for this\\n* To enable/disable, can we just change the log level of that logger?\\n* Would be nice if the logging could be a bit more self-describing and human readable - JSON?', \"* I was initially using logback, but changed because I was getting incomplete files (since I didn't know when a new file was created). Looking at the [logback docs|http://logback.qos.ch/manual/appenders.html#RollingFileAppender] it seems like I probably just need to implement these two classes to make sure the logs are complete\\n* That will work well; I'll make sure the logger has the name of the table it is assigned to in order to capture just the output from one table\\n* Good point; this could also simplify some of the multiple-line events\", \"I had the idea for this JIRA a few months ago, but was too busy/distracted to do anything with it.  Really glad to see that it's not only been added by others, but that it's also actively being addressed.  Fantastic.\\n\\nWe're doing some comparative studies of STCS and DTCS for a huge C* user and these enhancements will really provide actionable metrics for people desiring to tune their compaction procedures.  \", \"I've been looking at logback and how we might be able to use logback directly. The biggest problem is that there isn't a notification for when the log file changes. We need to know that the logfile is changing so that we can log out the sstables that are already on disk so that each logfile is independent (and old ones can be deleted).\\nWe won't be able to use the logback loggers, or logback.xml, because we won't be able to use the loggers as they are currently defined, since additional information needs to be passed on creation of the logger to know what tables' files to log on a new file.\\nWe can still use the infrastructure of logback to be able to execute the mechanics of doing the log rotations so that we aren't responsible for it; it just won't be as seamless as updating the logback.xml files and having it reread it.\", \"I've pushed a new version of this branch which is updated for the CASSANDRA-6696 changes, and outputs JSON objects. I haven't looked at how to use just the parts of logback that we would want to yet, but if the current approach looks OK except for reimplementing the log rolling, I'll take a look at it early next week.\", 'it looks good, a few comments;\\n\\n* timestamps on all log entries\\n* it could perhaps be useful to log if the compaction strategy handles repaired or unrepaired data and which data directory it is handling instead of the strategy id? Or perhaps log the mapping on startup so we can figure it out?\\n\\nand a nit - remove redundant \"this.\" in CompactionLogger.\\n\\nAnd an idea - feel free to ignore, could we make the serialization \\'pluggable\\' in CompactionLogger? Then we could for example have all nodes in a cluster write to a socket somewhere so that we don\\'t have to ship log files to visualize? We could do this in a followup ticket when/if anyone needs it though', \"This looks really promising. I've played around with the branch and did some minor changes in a [PR|https://github.com/carlyeks/cassandra/pull/1/files].\\n\\nHowever, I'm still not sure why you plan to implement your own file rolling logic. Getting files rolled by logback and archive them manually afterwards would work perfectly fine for me.\", \"[~spodxx@gmai \n",
            "my_comment:  [\"Ive pushed a [branch|https://github.com/carlyeks/cassandra/tree/ticket/10805]. It adds a CompactionLogger class which the compaction strategies can use. Currently most of the work happens in either the Flush runnable or the CompactionTask but more statements could be added for the strategies to use.\n",
            "\n",
            "There is an added table parameter {{log_all}} which if set to true will start logging right away. Otherwise there is a JMX operation to start compaction logging for a CF.\" * Can we use logback to do the logging to file? It should be possible to create a special logger that goes to a separate file. Feels wrong to implement our own log rotation etc for this\n",
            "* To enable/disable can we just change the log level of that logger?\n",
            "* Would be nice if the logging could be a bit more self-describing and human readable - JSON? \"* I was initially using logback but changed because I was getting incomplete files (since I didnt know when a new file was created). Looking at the [logback docs|http://logback.qos.ch/manual/appenders.html#RollingFileAppender] it seems like I probably just need to implement these two classes to make sure the logs are complete\n",
            "* That will work well; Ill make sure the logger has the name of the table it is assigned to in order to capture just the output from one table\n",
            "* Good point; this could also simplify some of the multiple-line events\" \"I had the idea for this JIRA a few months ago but was too busy/distracted to do anything with it.  Really glad to see that its not only been added by others but that its also actively being addressed.  Fantastic.\n",
            "\n",
            "Were doing some comparative studies of STCS and DTCS for a huge C* user and these enhancements will really provide actionable metrics for people desiring to tune their compaction procedures.  \" \"Ive been looking at logback and how we might be able to use logback directly. The biggest problem is that there isnt a notification for when the log file changes. We need to know that the logfile is changing so that we can log out the sstables that are already on disk so that each logfile is independent (and old ones can be deleted).\n",
            "We wont be able to use the logback loggers or logback.xml because we wont be able to use the loggers as they are currently defined since additional information needs to be passed on creation of the logger to know what tables files to log on a new file.\n",
            "We can still use the infrastructure of logback to be able to execute the mechanics of doing the log rotations so that we arent responsible for it; it just wont be as seamless as updating the logback.xml files and having it reread it.\" \"Ive pushed a new version of this branch which is updated for the CASSANDRA-6696 changes and outputs JSON objects. I havent looked at how to use just the parts of logback that we would want to yet but if the current approach looks OK except for reimplementing the log rolling Ill take a look at it early next week.\" it looks good a few comments;\n",
            "\n",
            "* timestamps on all log entries\n",
            "* it could perhaps be useful to log if the compaction strategy handles repaired or unrepaired data and which data directory it is handling instead of the strategy id? Or perhaps log the mapping on startup so we can figure it out?\n",
            "\n",
            "and a nit - remove redundant \"this.\" in CompactionLogger.\n",
            "\n",
            "And an idea - feel free to ignore could we make the serialization \\pluggable\\ in CompactionLogger? Then we could for example have all nodes in a cluster write to a socket somewhere so that we don\\t have to ship log files to visualize? We could do this in a followup ticket when/if anyone needs it though \"This looks really promising. Ive played around with the branch and did some minor changes in a [PR|https://github.com/carlyeks/cassandra/pull/1/files].\n",
            "\n",
            "However Im still not sure why you plan to implement your own file rolling logic. Getting files rolled by logback and archive them manually afterwards would work perfectly fine for me.\" \"[~spodxx@gmai \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-578\n",
            "issue_type:  Bug \n",
            "summary:  get_range_slice NPE \n",
            "description:  If I call get_range_slice with arguments in the SliceRange structure, then it seems to NPE.  I think it only does it when there is nothing in the range specified in the column slice start and end.\n",
            "comments:  ['02\\n    make Row contain a single, final CF reference\\n\\n01\\n    r/m unused Row code, and move table variable into callers rather than serializing it redundantly\\n', 'Still get this error with the patches applied.\\n\\nDEBUG - range_slice\\nDEBUG - reading org.apache.cassandra.db.RangeSliceCommand@4ef18d37 from 31@/127.0.0.1\\nERROR - Error in ThreadPoolExecutor\\njava.lang.RuntimeException: java.lang.NullPointerException\\n        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:53)\\n        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\\n        at java.lang.Thread.run(Thread.java:619)\\nCaused by: java.lang.NullPointerException\\n        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:58)\\n        at org.apache.cassandra.db.RowSerializer.serialize(Row.java:92)\\n        at org.apache.cassandra.db.RangeSliceReply.getReply(RangeSliceReply.java:53)\\n        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:46)\\n        ... 4 more\\nERROR - Fatal exception in thread Thread[ROW-READ-STAGE:11,5,main]\\njava.lang.RuntimeException: java.lang.NullPointerException\\n        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:53)\\n        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\\n        at java.lang.Thread.run(Thread.java:619)\\nCaused by: java.lang.NullPointerException\\n        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:58)\\n        at org.apache.cassandra.db.RowSerializer.serialize(Row.java:92)\\n        at org.apache.cassandra.db.RangeSliceReply.getReply(RangeSliceReply.java:53)\\n        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:46)\\n        ... 4 more\\n', 'can you reproduce against the default set of columnfamilies?', 'Let me try to do that.\\n\\njust fyi in org.apache.cassandra.db.row.RowSerializer row.cf is null which is causing the NPE.', \"I can reproduce it here, I'm good.\", '03\\n    allow serializing null CF; add get_range_slice test exercising this', \"I am getting a couple errors that I wasn't getting before and I think its related.\\n\\n\\n2009-11-24_05:06:54.65928 java.lang.NullPointerException\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.db.Row.digest(Row.java:75)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:103)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:46)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:88)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:453)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:381)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:170)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:245)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:208)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:758) \n",
            "my_comment: java.lang.NullPointerException\n",
            "        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:58)\n",
            "        at org.apache.cassandra.db.RowSerializer.serialize(Row.java:92)\n",
            "        at org.apache.cassandra.db.RangeSliceReply.getReply(RangeSliceReply.java:53)\n",
            "        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:46)\n",
            "        ... 4 more\n",
            " can you reproduce against the default set of columnfamilies? Let me try to do that.\n",
            "\n",
            "just fyi in org.apache.cassandra.db.row.RowSerializer row.cf is null which is causing the NPE. \"I can reproduce it here Im good.\" 03\n",
            "    allow serializing null CF; add get_range_slice test exercising this \"I am getting a couple errors that I wasnt getting before and I think its related.\n",
            "\n",
            "\n",
            "2009-11-24_05:06:54.65928 java.lang.NullPointerException\n",
            "2009-11-24_05:06:54.65928       at org.apache.cassandra.db.Row.digest(Row.java:75)\n",
            "2009-11-24_05:06:54.65928       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:103)\n",
            "2009-11-24_05:06:54.65928       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:46)\n",
            "2009-11-24_05:06:54.65928       at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:88)\n",
            "2009-11-24_05:06:54.65928       at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:453)\n",
            "2009-11-24_05:06:54.65928       at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:381)\n",
            "2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)\n",
            "2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:170)\n",
            "2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:245)\n",
            "2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:208)\n",
            "2009-11-24_05:06:54.65928       at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:758) \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5050\n",
            "issue_type:  Bug \n",
            "summary:  Cql3 token queries broken \n",
            "description:  Currently any select statement that uses a token() predicate breaks with \"Bad Input\"\n",
            "comments:  ['+1, committed, thanks'] \n",
            "my_comment:  +1 committed thanks \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1789\n",
            "issue_type:  Improvement \n",
            "summary:  Clean up (and make sane) key/row cache loading logspam \n",
            "description:  //Start\n",
            "comments:  ['saner logging', 'backported to 0.6 and committed', 'Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])\\n    '] \n",
            "my_comment:  saner logging backported to 0.6 and committed Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])\n",
            "     \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5571\n",
            "issue_type:  Improvement \n",
            "summary:  Reject bootstrapping endpoints that are already in the ring with different gossip data \n",
            "description:  The ring can be silently broken by improperly bootstrapping an endpoint that has an existing entry in the gossip table. In the case where a node attempts to bootstrap with the same IP address as an existing ring member, the old token metadata is dropped without warning, resulting in range shifts for the cluster.\n",
            "comments:  ['Rick, are you still planning to take a stab at this?', 'Hi, Jingsi Zhu is no longer at Facebook so this email address is no longer being monitored. If you need assistance, please contact another person who is currently at the company.\\n', \"And that's why you shouldn't use work email accounts for OSS participation.\", \"I will note that in working on CASSANDRA-5916, I realize this won't be as straightforward as I originally thought, either, and will likely need to take the same approach we end up taking there.\", 'With CASSANDRA-5916 done, we probably need to rearrange it a bit so we always do a shadow gossip round, then punt if the entry exists in gossip, otherwise hand off the info when doing a replace.', '5571-2.0-v1.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-5571]) uses a shadow gossip round to check for endpoint collisions.', \"The problem with this patch is that it checks gossip regardless of whether it's going to bootstrap or not; this means you can't just simply restart an existing node.\", 'Bah, not sure how I missed that.  5517-2.0-v2.patch (the branch is also updated) only checks for collisions when bootstrapping.', 'We probably ought to make Gossiper.isDeadState public, and then check that not only is the state not null, but also not a dead state.  This way the IP can be reused without waiting 3 days for the dead state to be evicted/', '5571-2.0-v3.patch tolerates dead nodes when checking for endpoint collisions on bootstrap.', \"Committed, with a minor change of consolidating the 'should we bootstrap' into a single function, since maintaining it in one place is scary enough.\", 'To anyone using vnodes and looking to reduce their exposure to this before 2.0.2, you have two obvious workarounds :\\n\\n1) manually pick tokens, and bootstrap your new vnode node with all 256 of them in in intiial_token comma delimited list\\n\\nOR\\n\\n1) continue to let num_tokens pick tokens for you and bootstrap\\n2) collect all this node\\'s tokens into a comma delimited list with a command like\\n{noformat}\\nIP=__NODE_IP__ ; nodetool ring | fgrep -w \"$IP\" | awk \\'{print $NF}\\' |xargs -d,\\n\\nor\\n\\nIP =__NODE_IP__ ; nodetool -h $IP info -T |grep Token | awk \\'{print $NF}\\' | tr \\'\\\\n\\' \\',\\'\\n{noformat}\\n3) put the comma delimited list from 2 into the initial_token line in cassandra.yaml before the next time the node restarts', 'I noticed that checkForEndpointCollision() is called before starting Gossiper (that is started immediately after this check) . \\nThis causes a problem the first time I bootstrap a cluster since all nodes call checkForEndpointCollision() but no one have yet started Gossiper so no one answer the gossip messages leading all nodes to timeout and die with an \"Unable to gossip with any seeds\" RTE.\\n\\nThis is an issue especially using Cassandra with Priam (https://github.com/Netflix/Priam) were all nodes starts automatically at the very same time with the same configuration. \\nUnfortunately working around the problem in Priam is fairly complicated since it would require synchronizing the whole cluster to bootstrap in a specific order with different configuration. \\n\\nThe question then is: may we move the checkForEndpointCollision() call after Gossiper is started (about ten lines later in StorageService)?\\n\\nOn the contrary if this check need to happen before Gossiper is started, another option could be to allow GossipDigestSynVerbHandler.doVerb() to respond even if Gossiper is not yet enabled (right now it checks for Gossiper.instance.isEnabled() or it silently discard the request).\\n\\n', \"bq. This causes a problem the first time I bootstrap a cluster since all nodes call checkForEndpointCollision() but no one have yet started Gossiper\\n\\nOnly bootstrapping nodes call it, and seeds never bootstrap, which is what the other nodes are trying to communicate with.  You can probably either a) make sure your seeds have started first, or b) just not bootstrap at all, since it's a fresh cluster where it doesn't m \n",
            "my_comment: may we move the checkForEndpointCollision() call after Gossiper is started (about ten lines later in StorageService)?\n",
            "\n",
            "On the contrary if this check need to happen before Gossiper is started another option could be to allow GossipDigestSynVerbHandler.doVerb() to respond even if Gossiper is not yet enabled (right now it checks for Gossiper.instance.isEnabled() or it silently discard the request).\n",
            "\n",
            " \"bq. This causes a problem the first time I bootstrap a cluster since all nodes call checkForEndpointCollision() but no one have yet started Gossiper\n",
            "\n",
            "Only bootstrapping nodes call it and seeds never bootstrap which is what the other nodes are trying to communicate with.  You can probably either a) make sure your seeds have started first or b) just not bootstrap at all since its a fresh cluster where it doesnt m \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-303\n",
            "issue_type:  Improvement \n",
            "summary:  allow get_slice to operate on SC subcolumns \n",
            "description:  again, post CASSANDRA-185 \n",
            "comments:  ['Currently, there is no index at subcolumn level. So, do we want to just add scan-based get_slice or do we want to add subcolumn index? The latter further complicates the storage format.', 'for this ticket i will be happy to just say \"we still assume supercolumns have to fit in memory\"\\n\\njust want to expose the slice api at that level, since get_columns_since goes away w/ CASSANDRA-185', '    add get_slice for supercolumn, tests.  add back updated TimeSortTest.  we need to pass gcBefore to the filters so Slice can count correctly.\\n', 'All tests pass, looks good. +1', 'committed'] \n",
            "my_comment:  Currently there is no index at subcolumn level. So do we want to just add scan-based get_slice or do we want to add subcolumn index? The latter further complicates the storage format. for this ticket i will be happy to just say \"we still assume supercolumns have to fit in memory\"\n",
            "\n",
            "just want to expose the slice api at that level since get_columns_since goes away w/ CASSANDRA-185     add get_slice for supercolumn tests.  add back updated TimeSortTest.  we need to pass gcBefore to the filters so Slice can count correctly.\n",
            " All tests pass looks good. +1 committed \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4495\n",
            "issue_type:  Improvement \n",
            "summary:  Don't tie client side use of AbstractType to JDBC \n",
            "description:  We currently expose the AbstractType to java clients that want to reuse them though the cql.jdbc.* classes. I think this shouldn't be tied to the JDBC standard. JDBC was make for SQL DB, which Cassandra is not (CQL is not SQL and will never be). Typically, there is a fair amount of the JDBC standard that cannot be implemented with C*, and there is a number of specificity of C* that are not in JDBC (typically the set and maps collections).\n",
            "comments:  ['bq. So I propose to extract simple type classes with just a compose and decompose method\\n\\nwhy not just expose the AbstractType classes at that point?', 'bq. why not just expose the AbstractType classes at that point?\\n\\nI though about that, but I think that at least currently that means pull pretty mull all of Cassandra (why the goal is to allow clients to pull just the minimum useful to them).', 'I would suggest adding {{getString(ByteBuffer bytes)}} and {{getType()}} as well. The JDBC specific stuff like {{isCurrency()}} and {{isSigned()}} are of course easily moved over to client side.', \"[~ardot] [~mfiguiere] [~urandom] I don't suppose I can interest one of you in this?\", \"First cut of this is over at:\\n\\nhttps://github.com/carlyeks/cassandra/tree/4495\\n\\nI don't like the name (composer), but using abstracttype et al caused a lot of conflicts. I'm open to a better name or, if AbstractType really is the best name, I'll just fully qualify the names.\", 'Could this solution be enhanced to address the collection types? (List, Set, and Map). This is really awkward to do on the client side. \\n\\n{{getType()}} in each class would also be handy when you are passed the AbstractType.\\n\\nIs there a reason you did not remove the {{o.a.c.cql.jdbc}} package from the build?\\n\\nGreat job BTW...', 'Just pushed up a couple of new commits.\\n\\n- Removed the o.a.c.cql.jdbc namespace\\n- Added getType\\n- Added List,Set,Map implementations\\n  - Haven\\'t figured out how to merge the {List,Set,Map}Type\\'s compose and decompose because of their usage of validate\\n- Added a \"asCompose()\" call to AbstractType which returns the AbstractComposer for each type', \"bq. Haven't figured out how to merge the {List,Set,Map}\\n\\nCan you elaborate?\", 'Just meant that the Collection-types cannot use the Collection-composers because they depend on validation, which the composers do not have. Not sure if it is worth adding, or if we are happy without it.', \"Can't you just move the {{validate()}} method in the classes in {{o.a.c.types}} from the ones on {{o.a.c.db.marshal}}? Then the collections classes in {{o.a.c.types}} will have access to them.\", \"This adds validate to the composer.\\n\\nA couple of things to be aware of:\\n- Replaced o.a.c.db.marshal.MarshalException with o.a.c.types.MarshalException as it may be thrown to a client\\n- Collection composer validate no ops, reasoning below\\n\\nI'm not sure that there is any reason to validate the Collections. It seems that the previous validation would fail on, for example, Map<Map<TimeUUID, string>, TimeUUID>, as the validation of a Map is actually validating using the Value type of the Map, rather than iterating through the values and making sure that an entry is valid. This entry-wise validation happens when we call compose.\", \"Haven't really look at the detail of the patch, but for what it's worth, I've somehow never been a fan of the compose/decompose terminology. I'd prefer say encode/decode or serialize/deserialize. And BooleanCodec or BooleanSerializer sounds better to my hear than BooleanComposer. But do feel free to discard that opinion if it's just me being french and if composer sounds perfectly fine to you guys.\", 'I like that name a lot more. Attached an updated version which renames to *Serializer, and renames the methods to (de)serialize.', 'WDYT, Rick?', 'LGTM. Should be no problem to incorporate into client side work. Thanks for the enhancements!', 'Alright, committed then.', \"For info, took the liberty to do the following renames:\\n* renamed the package from type to serializers, since that's what the classes are called.\\n* made AbstractSerializer an interface since I didn't see a good reason to have it an abstract class. Renamed into TypeSerializer too.\\n* renamed the asComposer() method in AbstractType to getSerializer() (I suspect that was a left-over of the initial patch)\"] \n",
            "my_comment:  bq. So I propose to extract simple type classes with just a compose and decompose method\n",
            "\n",
            "why not just expose the AbstractType classes at that point? bq. why not just expose the AbstractType classes at that point?\n",
            "\n",
            "I though about that but I think that at least currently that means pull pretty mull all of Cassandra (why the goal is to allow clients to pull just the minimum useful to them). I would suggest adding {{getString(ByteBuffer bytes)}} and {{getType()}} as well. The JDBC specific stuff like {{isCurrency()}} and {{isSigned()}} are of course easily moved over to client side. \"[~ardot] [~mfiguiere] [~urandom] I dont suppose I can interest one of you in this?\" \"First cut of this is over at:\n",
            "\n",
            "https://github.com/carlyeks/cassandra/tree/4495\n",
            "\n",
            "I dont like the name (composer) but using abstracttype et al caused a lot of conflicts. Im open to a better name or if AbstractType really is the best name Ill just fully qualify the names.\" Could this solution be enhanced to address the collection types? (List Set and Map). This is really awkward to do on the client side. \n",
            "\n",
            "{{getType()}} in each class would also be handy when you are passed the AbstractType.\n",
            "\n",
            "Is there a reason you did not remove the {{o.a.c.cql.jdbc}} package from the build?\n",
            "\n",
            "Great job BTW... Just pushed up a couple of new commits.\n",
            "\n",
            "- Removed the o.a.c.cql.jdbc namespace\n",
            "- Added getType\n",
            "- Added ListSetMap implementations\n",
            "  - Haven\\t figured out how to merge the {ListSetMap}Type\\s compose and decompose because of their usage of validate\n",
            "- Added a \"asCompose()\" call to AbstractType which returns the AbstractComposer for each type \"bq. Havent figured out how to merge the {ListSetMap}\n",
            "\n",
            "Can you elaborate?\" Just meant that the Collection-types cannot use the Collection-composers because they depend on validation which the composers do not have. Not sure if it is worth adding or if we are happy without it. \"Cant you just move the {{validate()}} method in the classes in {{o.a.c.types}} from the ones on {{o.a.c.db.marshal}}? Then the collections classes in {{o.a.c.types}} will have access to them.\" \"This adds validate to the composer.\n",
            "\n",
            "A couple of things to be aware of:\n",
            "- Replaced o.a.c.db.marshal.MarshalException with o.a.c.types.MarshalException as it may be thrown to a client\n",
            "- Collection composer validate no ops reasoning below\n",
            "\n",
            "Im not sure that there is any reason to validate the Collections. It seems that the previous validation would fail on for example Map<Map<TimeUUID string> TimeUUID> as the validation of a Map is actually validating using the Value type of the Map rather than iterating through the values and making sure that an entry is valid. This entry-wise validation happens when we call compose.\" \"Havent really look at the detail of the patch but for what its worth Ive somehow never been a fan of the compose/decompose terminology. Id prefer say encode/decode or serialize/deserialize. And BooleanCodec or BooleanSerializer sounds better to my hear than BooleanComposer. But do feel free to discard that opinion if its just me being french and if composer sounds perfectly fine to you guys.\" I like that name a lot more. Attached an updated version which renames to *Serializer and renames the methods to (de)serialize. WDYT Rick? LGTM. Should be no problem to incorporate into client side work. Thanks for the enhancements! Alright committed then. \"For info took the liberty to do the following renames:\n",
            "* renamed the package from type to serializers since thats what the classes are called.\n",
            "* made AbstractSerializer an interface since I didnt see a good reason to have it an abstract class. Renamed into TypeSerializer too.\n",
            "* renamed the asComposer() method in AbstractType to getSerializer() (I suspect that was a left-over of the initial patch)\"] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-11580\n",
            "issue_type:  Sub-task \n",
            "summary:  remove DatabaseDescriptor dependency from SegmentedFile \n",
            "description:  Several configurable parameters are pulled from {{DatabaseDescriptor}} from {{SegmentedFile}} and its subclasses. \n",
            "comments:  [\"||branch||testall||dtest||\\n|[11580|https://github.com/yukim/cassandra/tree/11580]|[testall|http://cassci.datastax.com/view/Dev/view/yukim/job/yukim-11580-testall/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/yukim/job/yukim-11580-dtest/lastCompletedBuild/testReport/]|\\n\\nThe patch that removes DatabaseDescriptor dependency and {{ChunkCache}} singleton access from {{SegmentedFile}}.\\n- All subclasses of {{SegmentedFile}} are merged into {{SegmentedFile}} since difference among those is in {{Rebufferer}} after CASSANDRA-5863.\\n- Introduced {{DiskOptimizationStrategy}} for {{disk_optimization_strategy}} config.\\n- {{SegmentedFile.Builder}}'s {{serializeBound}}/{{deserializedBound}} seem no longer used, so I removed them.\\n\", 'Really great stuff Yuki, I could only find minor things and I had to look really hard to find them :) The items marked as optional are left to your decision since they are more down to individual stylistic preferences:\\n\\n*SegmentedFile:*\\n\\n* Class comments are no longer up-to-date\\n* protected accessors can be private\\n* Optional: class can be final\\n* Comments of onDiskLength mention SegmentIterator that no longer exists\\n* Trivial: alignment error in the constructor parameters at line 66\\n* Cleanup.Tidy(): the chunk cache is invalidated only when the metadata is not null, this was the existing behavior but is it correct? Index files use a chunk cache even if they are not compressed?\\n* Cleanup.Tidy(): can either metdata.close or the cache invalidation throw? \\n* Optional: should we rename metadata to compressionMetadata in Cleanup?\\n* Class comments of {{Builder}} are also no longer up-to-date\\n* Suppress resource warnings for rebufferer in Builder.complete() since it is owned by the SegmentedFile\\n\\n*Other files:*\\n\\n* SStableReader ln 436: {{// special implementation of load to use non-pooled SegmentedFile builders}} can be removed\\n* The EMPTY BufferHolder in Rebuffered.java at line 62 can still be static final\\n* SegmentedFileTest should be renamed to DiskOptimizationStrategyTest\\n* Optional: There is a bit of code duplications in BigTableWriter openFinal and openForBatch as well as SSTableReader openForBatch and load. I wonder if we could introduce helper methods in SSTable to create the index and data builders and to create the index and data segmented files.', 'Thanks for review.\\nI feel like the name {{SegmentedFile}} is not appropriate anymore, and it just can be integrated with {{RandomAccessReader.Builder}}.\\nLet me work on that along with fixing your review points.\\n', 'bq. I feel like the name {{SegmentedFile}} is not appropriate anymore, and it just can be integrated with {{RandomAccessReader.Builder}}.\\n\\nI totally agree, sounds like a good plan.', \"I ended up renaming {{SegmentedFile}} to {{FileHandle}}. If anyone has better name, I'm open to suggestion.\\nI think I fixed he points in the review, except: The EMPTY BufferHolder in Rebuffered.java at line 62 can still be static final. variable in interface is implicitly declared as public static final, and my IntelliJ gives me warning if I left those.\\n\\n||branch||testall||dtest||\\n|[11580|https://github.com/yukim/cassandra/tree/11580]|[testall|http://cassci.datastax.com/view/Dev/view/yukim/job/yukim-11580-testall/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/yukim/job/yukim-11580-dtest/lastCompletedBuild/testReport/]|\\n\", \"I've pushed some nits [here|https://github.com/stef1927/cassandra/commit/ddee27868592076c7c30c285eb92938450eafe9e]: mostly edits in the comments, unused imports and more restrictive access modifiers in RAR. I've also fixed some resource management problems in unit tests, and in the two new {{open()}} methods in case of exceptions. I've rebased, which resulted in a couple of conflicts, especially in {{CommitLogReader}} where some code has been moved around. \\n\\nIf you're +1 on my changes and the CI results are OK, then I'm also + 1 and we can commit thi \n",
            "my_comment: mostly edits in the comments unused imports and more restrictive access modifiers in RAR. Ive also fixed some resource management problems in unit tests and in the two new {{open()}} methods in case of exceptions. Ive rebased which resulted in a couple of conflicts especially in {{CommitLogReader}} where some code has been moved around. \n",
            "\n",
            "If youre +1 on my changes and the CI results are OK then Im also + 1 and we can commit thi \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4624\n",
            "issue_type:  Bug \n",
            "summary:  ORDER BY validation is not restrictive enough \n",
            "description:  We're not able to do order by on anything that is a key range. However, we only refuse queries that have an empty where clause, but that doesn't exclude all key ranges at all. \n",
            "comments:  ['+1', 'Committed, thanks'] \n",
            "my_comment:  +1 Committed thanks \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6170\n",
            "issue_type:  New Feature \n",
            "summary:  Modify AbstractCassandraDaemon.initLog4j() to allow for hotfixing log level on on a cassandra class \n",
            "description:  When customer wants to bump up log level of a cassandra class, here is the procedure they follow:\n",
            "comments:  [\"Don't see any reason not to allow a symlink here, trivial patch attached.\", '+1', 'Committed.  For trunk, I assumed logback would just work.'] \n",
            "my_comment:  [\"Dont see any reason not to allow a symlink here trivial patch attached.\" +1 Committed.  For trunk I assumed logback would just work. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1600\n",
            "issue_type:  New Feature \n",
            "summary:  Merge get_indexed_slices with get_range_slices \n",
            "description:  From a comment on 1157:\n",
            "comments:  ['the semantics of \"give me all the rows with column value X\" and \"give me all the rows between Y and Z\" are different enough that smashing them together feels like the wrong thing to do.  Nor is stop_key on index scans something we want to optimize for.', '> \"give me all the rows between Y and Z\"\\nThe semantics of get_indexed_slices is still \"give me all the rows between Y and Z\", with the one addition that it applies a filter. I\\'m going to make a patch that does this: if it is not ugly, then I\\'m going to reopen this issue.', 'Patchset to merge get_range_slices and get_indexed_slices: drops ~200 hand written lines and ~1400 thrift generated lines.', \"From an API perspective, I think it does make sense to merge the two.\\n\\nThey are fundamentally the same operation.  As Stu says, get_indexed_slices() only adds a filter to get_range_slices().  This is close enough (and the filter is self-contained enough) that I don't think a separate function is needed.  I think the merged version would probably be easier for a new developer to grok, as well.\\n\\nI can't comment on the backend parts of this.  If those are troublesome enough to keep the two separate, then it might make sense to do this at the client level.\", '+1 I agree this should be done in one call.\\n', \"Sounds like the ayes have it.  Stu, if you can rebase we'll get this in.\", 'Rebased for trunk.', \"am i missing something?  i'm not seeing where this either respects the end_key during an index scan, or applies row_filter during a row scan, both of which are implied by the api change.  if we're not going to merge semantics then they really should remain separate calls.\", \"Adding 0007 to respect 'end_key' for filtered get_range_slices calls.\", '> applies row_filter during a row scan\\nI\\'m willing to say that this should be \"considered harmful\". Without a toggle to disable the RPC_TIMEOUT, we\\'d be setting people up for their unbounded scans to succeed in testing, and then cause cascading failures in production by going into retry loops that scan (70 MB * 10s) of data before timing out.\\n\\nIndexed scans are safe, in that the worst case is that you don\\'t match anything in your index, and you have to get empty results from every node in your cluster. Empty results are cheap.\\n\\nEDIT: And yes, I realize that our current scheme for boolean operations between clauses ends up reverting to a scan, but that is fixable via a merge join of the indexes (or 1472), which would preserve the safety I mention.', \"> we'd be setting people up for their unbounded scans to succeed in testing, and then cause cascading failures in production by going into retry loops\\n\\nwe already have this problem with the existing get_range_slices and excessively large count values.  it turns out that allowing people to do more powerful/efficient things is the right choice even when it is potentially dangerous.\", \"> we already have this problem with the existing get_range_slices and excessively large count values.\\nIn that case, the user is explicitly saying, give me a lot of stuff. The fix (in production) would be a one line code change, not the emergency addition of an index.\\n\\n> it turns out that allowing people to do more powerful/efficient things is the right choice even when it is potentially dangerous\\nI disagree. http://jsomers.net/blog/it-turns-out\\n\\nIf we're talking about the specific case of adhoc analytics queries, then we should discuss them independently, because they really are a whole different beast. For instance, if the idea here is that you would perform filtering in Cassandra rather than in the Hadoop process, you are not saving anything but ser/de time, since the recommended way to deploy Hadoop is directly on localhost.\", \"You seem to be arguing that there exists a body of users who somehow know to avoid misusing get_range_slices outside Hadoop now, but won't after this change.  I don't see that.  Is there a change in degree? Yes. But in kind? No.\", \"> You seem to be arguing that there exi \n",
            "my_comment: And yes I realize that our current scheme for boolean operations between clauses ends up reverting to a scan but that is fixable via a merge join of the indexes (or 1472) which would preserve the safety I mention. \"> wed be setting people up for their unbounded scans to succeed in testing and then cause cascading failures in production by going into retry loops\n",
            "\n",
            "we already have this problem with the existing get_range_slices and excessively large count values.  it turns out that allowing people to do more powerful/efficient things is the right choice even when it is potentially dangerous.\" \"> we already have this problem with the existing get_range_slices and excessively large count values.\n",
            "In that case the user is explicitly saying give me a lot of stuff. The fix (in production) would be a one line code change not the emergency addition of an index.\n",
            "\n",
            "> it turns out that allowing people to do more powerful/efficient things is the right choice even when it is potentially dangerous\n",
            "I disagree. http://jsomers.net/blog/it-turns-out\n",
            "\n",
            "If were talking about the specific case of adhoc analytics queries then we should discuss them independently because they really are a whole different beast. For instance if the idea here is that you would perform filtering in Cassandra rather than in the Hadoop process you are not saving anything but ser/de time since the recommended way to deploy Hadoop is directly on localhost.\" \"You seem to be arguing that there exists a body of users who somehow know to avoid misusing get_range_slices outside Hadoop now but wont after this change.  I dont see that.  Is there a change in degree? Yes. But in kind? No.\" \"> You seem to be arguing that there exi \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1702\n",
            "issue_type:  Improvement \n",
            "summary:  handle skipping bad rows in LazilyCompacted path \n",
            "description:  it's easy to handle skipping bad rows during compation in the PreCompacted (merged-in-memory) path and we have done this for a long time.  It is harder in the LazilyCompacted path since we have already started writing data when we discover that some of the source rows cannot be deserialized.  This adds mark/reset to SSTableWriter so compaction can skip back to the beginning in these circumstances. \n",
            "comments:  ['the corner case is when a bad row extends past where we have good data, so we have to truncate at the end of the good data.', '+1', 'committed', \"Would it be possible to differentiate read errors (recoverable) from write errors (non-recoverable) here? If for some reason we can't write to the destination, won't we drop all data in the rest of the inputs?\", 'It\\'s difficult, because they are both happening during \"writer.append(row)\" on the Lazy path.  Open to suggestions.', 'If you wrapped the compaction read path (mostly inside iterators) with a known (runtime?) exception, you could differentiate that way. But IMO, this should wait until we fix the file format so that we can always discard corrupted data at the block level.', \"bq. If you wrapped the compaction read path (mostly inside iterators) with a known (runtime?) exception, you could differentiate that way\\n\\nstarted out doing that (still might) but for now I'm going to revert this.  feels like silently dropping data on the floor is the wrong thing to do.  let's make a separate utility that can expunge corrupt rows from individual sstables, if/when we need that.\"] \n",
            "my_comment:  the corner case is when a bad row extends past where we have good data so we have to truncate at the end of the good data. +1 committed \"Would it be possible to differentiate read errors (recoverable) from write errors (non-recoverable) here? If for some reason we cant write to the destination wont we drop all data in the rest of the inputs?\" It\\s difficult because they are both happening during \"writer.append(row)\" on the Lazy path.  Open to suggestions. If you wrapped the compaction read path (mostly inside iterators) with a known (runtime?) exception you could differentiate that way. But IMO this should wait until we fix the file format so that we can always discard corrupted data at the block level. \"bq. If you wrapped the compaction read path (mostly inside iterators) with a known (runtime?) exception you could differentiate that way\n",
            "\n",
            "started out doing that (still might) but for now Im going to revert this.  feels like silently dropping data on the floor is the wrong thing to do.  lets make a separate utility that can expunge corrupt rows from individual sstables if/when we need that.\"] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-359\n",
            "issue_type:  Bug \n",
            "summary:  CFS readStats_ and diskReadStats_ are missing \n",
            "description: None\n",
            "comments: None\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3229\n",
            "issue_type:  Improvement \n",
            "summary:  Remove ability to disable dynamic snitch entirely \n",
            "description:  We've moved dynamic snitch from \"new, default to off\" to \"well tested, default to true,\" and it's time now to take the next step to \"there is no reason to disable it, and keeping the option around just lets people shoot their foot off.\" \n",
            "comments:  ['Ben Coverston points out that a more conservative approach would be to leave the option in but remove it from the example config file first.', \"I'm for the more conservative approach. In general I'm not in favor of fully disabling options by fear of people shooting themselves in the foot because I think that this always end up resulting in pissing off power users at one point or the other. In the case of the DS, being able to disable it if only for debugging purposes may prove useful. But I'm totally for having hidden options that are only listed in advanced docs with the proper warnings.\", \"With it defaulting to on in the last two major releases, I've never heard of anyone disabling it, so I'm fine with removing the option.\", 'patch removes dynamic_snitch boolean from example configuration (defaulting to true) and sets default badness threshold to 0.1\\n', '+1', 'committed', 'I agree that the dynamic snitch is super stable and 99.99999 % of the time everyone wants it. However it should always remain an option. I can imagine a situation where you do not want the dynamic snitch. For example a user may want to a snitch that always sends reads to predictable places. In that case not being able to disable dynamic would be bad.', \"You can't have that anyway because the failure detector will force reads to a different replica when it thinks your preferred one is down.\", \"we are experimenting with workload split between OLTP and OLAP workload using network topology strategy. The OLTP keyspace has replicas in both (virtual)DCs. OLTP would only be written to nodes in DC1 and only being read as OLAP input in DC2. While we don't know yet if this is the right approach, with dynamic snitch off (and read repair chance:0) in case no nodes are down we hope to avoid OLAP reads to show up in DC1. Without being able to switch off dynamic snitch - I am not sure how we are able to achieve workload split (to be proven if that gives a performance benefit).\", 'You can use the badness threshold from CASSANDRA-1519'] \n",
            "my_comment:  Ben Coverston points out that a more conservative approach would be to leave the option in but remove it from the example config file first. \"Im for the more conservative approach. In general Im not in favor of fully disabling options by fear of people shooting themselves in the foot because I think that this always end up resulting in pissing off power users at one point or the other. In the case of the DS being able to disable it if only for debugging purposes may prove useful. But Im totally for having hidden options that are only listed in advanced docs with the proper warnings.\" \"With it defaulting to on in the last two major releases Ive never heard of anyone disabling it so Im fine with removing the option.\" patch removes dynamic_snitch boolean from example configuration (defaulting to true) and sets default badness threshold to 0.1\n",
            " +1 committed I agree that the dynamic snitch is super stable and 99.99999 % of the time everyone wants it. However it should always remain an option. I can imagine a situation where you do not want the dynamic snitch. For example a user may want to a snitch that always sends reads to predictable places. In that case not being able to disable dynamic would be bad. \"You cant have that anyway because the failure detector will force reads to a different replica when it thinks your preferred one is down.\" \"we are experimenting with workload split between OLTP and OLAP workload using network topology strategy. The OLTP keyspace has replicas in both (virtual)DCs. OLTP would only be written to nodes in DC1 and only being read as OLAP input in DC2. While we dont know yet if this is the right approach with dynamic snitch off (and read repair chance:0) in case no nodes are down we hope to avoid OLAP reads to show up in DC1. Without being able to switch off dynamic snitch - I am not sure how we are able to achieve workload split (to be proven if that gives a performance benefit).\" You can use the badness threshold from CASSANDRA-1519 \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6008\n",
            "issue_type:  Bug \n",
            "summary:  Getting 'This should never happen' error at startup due to sstables missing \n",
            "description:  Exception encountered during startup: \"Unfinished compactions reference missing sstables. This should never happen since compactions are marked finished before we start removing the old sstables\"\n",
            "comments:  [\"Now that I think about this more: Doesn't this new cleanup code make it hard to restore a CF from a backup?  If there was a compaction for this CF in progress when you took down the system, when you bring it back up with new sstables for this CF, then this check will prevent you from starting.\", 'I have got the same issue on one of my nodes. I was running a long CQL query (probably too long to be completed successfully anyway) and restarted Cassandra while it was running. \\n\\nI am wondering if there is a way to restore that node? It consistently fails on startup with this error.', 'I think we worked around it by deleting all the data from system.compactions_in_progress', 'Thanks, I was able to recover the node. I could not truncate that table because the node was down but I have deleted all the rows. And I have got them back when I restarted the node and it crashed again with the same error :) After looking briefly at the code I have decided to delete all the compaction directories on that node (\"compactions_in_progress\" ones), clean the table again and restart. That did seem to help :)', 'Hi, we are able to consistently reproduce this issue:\\n{noformat}\\nERROR 23:14:06,001 Exception encountered during startup\\njava.lang.IllegalStateException: Unfinished compactions reference missing sstables. This should never happen since compactions are marked finished before we start removing the old sstables.\\n       at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:489)\\n       at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:264)\\n       at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:461)\\n       at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:504)\\njava.lang.IllegalStateException: Unfinished compactions reference missing sstables. This should never happen since compactions are marked finished before we start removing the old sstables.\\n       at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:489)\\n       at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:264)\\n       at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:461)\\n       at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:504)\\nException encountered during startup: Unfinished compactions reference missing sstables. This should never happen since compactions are marked finished before we start removing the old sstables.\\n{noformat}\\n\\nHere are the two ways in which we have found to reproduce this issue:\\n# Buffer a large amount of CL.LOCAL_QUORUM writes into a Cassandra CF, then drop the CF while the writes are still buffered.\\n# Buffer a large amount of CL.LOCAL_QUORUM writes into a Cassandra CF, then restart some nodes while before the buffer has finished draining.', 'We were able to recover the node by deleting all the data, commit, and saved_cache directories on the affected node and letting it rebuild.', 'I can reproduce this fairly consistently (on trunk, at least) using only {{cassandra-stress}}.  Just let it run for a few minutes, CTRL-C the cassandra process, then restart it.', \"Just for your information, we've run into this issue with v2.0.2 on our dev environment and the workaround worked.\\n\\nHope the fix could be included in the next patch v2.0.3.\\nThanks a lot for your help.\", \"6008-trunk-v1.patch should resolve the issue on trunk.  I'll have a 2.0 patch shortly.\\n\\nThe root of the problem was that LazilyCompactedRow was merging row tombstones incorrectly, essentially just discarding them.  There are a lot of documentation improvements in the patch (as it took me a while to understand everything sufficiently well to make a good fix); the only real code changes are in LazilyCompactedRow and a unit test to reproduce the issue.\\n\\nWe should probably change the title of this ticket to match the actual proble \n",
            "my_comment: Unfinished compactions reference missing sstables. This should never happen since compactions are marked finished before we start removing the old sstables.\n",
            "{noformat}\n",
            "\n",
            "Here are the two ways in which we have found to reproduce this issue:\n",
            "# Buffer a large amount of CL.LOCAL_QUORUM writes into a Cassandra CF then drop the CF while the writes are still buffered.\n",
            "# Buffer a large amount of CL.LOCAL_QUORUM writes into a Cassandra CF then restart some nodes while before the buffer has finished draining. We were able to recover the node by deleting all the data commit and saved_cache directories on the affected node and letting it rebuild. I can reproduce this fairly consistently (on trunk at least) using only {{cassandra-stress}}.  Just let it run for a few minutes CTRL-C the cassandra process then restart it. \"Just for your information weve run into this issue with v2.0.2 on our dev environment and the workaround worked.\n",
            "\n",
            "Hope the fix could be included in the next patch v2.0.3.\n",
            "Thanks a lot for your help.\" \"6008-trunk-v1.patch should resolve the issue on trunk.  Ill have a 2.0 patch shortly.\n",
            "\n",
            "The root of the problem was that LazilyCompactedRow was merging row tombstones incorrectly essentially just discarding them.  There are a lot of documentation improvements in the patch (as it took me a while to understand everything sufficiently well to make a good fix); the only real code changes are in LazilyCompactedRow and a unit test to reproduce the issue.\n",
            "\n",
            "We should probably change the title of this ticket to match the actual proble \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1128\n",
            "issue_type:  Bug \n",
            "summary:  sstable2json spews because it uses DatabaseDescriptor before loadSchemas() is called \n",
            "description:  sstable2json depends on DatabaseDescriptor for ColumnFamily meta data.  DD requires loadSchemas() is called before the CFMD can be accesed.  nothing in the code path in sstable2json calls loadSchemas(). \n",
            "comments:  ['patch against -r948111', \"+1\\n\\nDD.loadSchemas() requires that system tables are present.  I wish there were a better way to do this, but I can't think of one.\", \"I'm going to hold off on committing this while I investigate some more.  There's got to be a better way.  Also, I want to make sure that sstableimport isn't broken in the same fundamental way.\", 'let me know if there is anything I can do to help...', 'Matthew, can you add this same fix to SSTableImport as well as a check immediately after the DD.loadSchemas() to verify that >0 non-system tables are defined?  The relevant call is DD.getNonSystemTables().', 'patch2 against r948964', 'committed with minor revisions.', 'Integrated in Cassandra #449 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/449/])\\n    have sstable import/export load schema from local storage. Patch by Matthew Dennis, reviewed by Gary Dusbabek. CASSANDRA-1128\\n'] \n",
            "my_comment:  patch against -r948111 \"+1\n",
            "\n",
            "DD.loadSchemas() requires that system tables are present.  I wish there were a better way to do this but I cant think of one.\" \"Im going to hold off on committing this while I investigate some more.  Theres got to be a better way.  Also I want to make sure that sstableimport isnt broken in the same fundamental way.\" let me know if there is anything I can do to help... Matthew can you add this same fix to SSTableImport as well as a check immediately after the DD.loadSchemas() to verify that >0 non-system tables are defined?  The relevant call is DD.getNonSystemTables(). patch2 against r948964 committed with minor revisions. Integrated in Cassandra #449 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/449/])\n",
            "    have sstable import/export load schema from local storage. Patch by Matthew Dennis reviewed by Gary Dusbabek. CASSANDRA-1128\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-11272\n",
            "issue_type:  Bug \n",
            "summary:  NullPointerException (NPE) during bootstrap startup in StorageService.java \n",
            "description:  After bootstrapping fails due to stream closed error, the following error results:\n",
            "comments:  ['Which Cassandra version are you using? ', 'This was in 3.0.3.', 'Same problem in 2.2.5 \\n\\nINFO   | jvm 1    | 2016/05/09 21:44:28 | May 09, 2016 9:44:28 PM com.google.common.util.concurrent.ExecutionList executeListener\\nINFO   | jvm 1    | 2016/05/09 21:44:28 | SEVERE: RuntimeException while executing runnable com.google.common.util.concurrent.Futures$4@5d81f1af with executor com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService@6992bd30\\nINFO   | jvm 1    | 2016/05/09 21:44:28 | java.lang.NullPointerException\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at org.apache.cassandra.service.StorageService$2.onFailure(StorageService.java:1243)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at com.google.common.util.concurrent.Futures$4.run(Futures.java:1172)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:202)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at org.apache.cassandra.streaming.StreamResultFuture.maybeComplete(StreamResultFuture.java:210)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete(StreamResultFuture.java:186)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:434)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at org.apache.cassandra.streaming.StreamSession.onError(StreamSession.java:529)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at org.apache.cassandra.streaming.StreamReceiveTask$OnCompletionRunnable.run(StreamReceiveTask.java:181)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at java.util.concurrent.FutureTask.run(FutureTask.java:266)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 |       at java.lang.Thread.run(Thread.java:745)\\nINFO   | jvm 1    | 2016/05/09 21:44:28 | \\n', 'From what I can see we have a {{NullPointerException}} in the callback listener {{onFailure}}, which results into the {{SEVERE}} level error within executor itself. Unfortunately, this does not really indicate what exactly happened during the bootstrap process. \\n\\nI\\'ve removed the cause fetch from all branches, which should surface the exception cause:\\n\\n{code}\\nWARN  07:44:06 Warn Message\\njava.lang.RuntimeException: Exception Message\\n\\tat ...(ExceptionTest.java:44) [test/:na]\\n\\n        ... stacktrace...\\n\\nCaused by: java.lang.RuntimeException: Cause Message\\n\\t... 29 common frames omitted\\n{code}\\n\\nIt doesn\\'t fix the \"cause\", although I guess it\\'s the best I can do not being able to find out what actually causes the bootstrap error.\\n\\n|[2.2|https://github.com/ifesdjeen/cassandra/tree/11272-2.2]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-11272-2.2-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-11272-2.2-dtest/]|\\n|[3.0|https://github.com/ifesdjeen/cassandra/tree/11272-3.0]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-11272-3.0-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-11272-3.0-dtest/]|\\n|[trunk|https://github.com/ifesdjeen \n",
            "my_comment: Cause Message\n",
            "\\t... 29 common frames omitted\n",
            "{code}\n",
            "\n",
            "It doesn\\t fix the \"cause\" although I guess it\\s the best I can do not being able to find out what actually causes the bootstrap error.\n",
            "\n",
            "|[2.2|https://github.com/ifesdjeen/cassandra/tree/11272-2.2]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-11272-2.2-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-11272-2.2-dtest/]|\n",
            "|[3.0|https://github.com/ifesdjeen/cassandra/tree/11272-3.0]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-11272-3.0-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-11272-3.0-dtest/]|\n",
            "|[trunk|https://github.com/ifesdjeen \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-78\n",
            "issue_type:  Bug \n",
            "summary:  Interrupted recovery requires manual intervention to fix \n",
            "description:  Originally reported by Alexander Staubo: \"If you kill the server while it is going through its initial \"row recovery\" phase, you risk ending up with a database that's corrupt and will fail with \"negative seek\" exceptions and similar.\"\n",
            "comments:  [\"Prashant's memory seems to be wrong here -- the only place getTempFileName is in the anticompaction (bootstrap) code.  Everything else uses getNextFileName directly.\", \"while writing these patches I checked and closeRename does fsync (via FileChannel.force).  Have not audited commitlog similarly.\\n\\nAdditionally, the bootstrap code uses the potentially unsafe CFS.getFileName instead of getTempFileName.  Not sure if there's actually a problem there.  (Just a note to self to come back to this after 0.3)\", '+1', 'Integrated in Cassandra #59 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/59/])\\n    use getTempFileName / closeRename to avoid problems w/ half-written sstables.\\npatch by jbellis; reviewed by Eric Evans for \\nclean up anticompaction code a little.\\npatch by jbellis; reveiewed by Eric Evans for \\n', 'committed'] \n",
            "my_comment:  [\"Prashants memory seems to be wrong here -- the only place getTempFileName is in the anticompaction (bootstrap) code.  Everything else uses getNextFileName directly.\" \"while writing these patches I checked and closeRename does fsync (via FileChannel.force).  Have not audited commitlog similarly.\n",
            "\n",
            "Additionally the bootstrap code uses the potentially unsafe CFS.getFileName instead of getTempFileName.  Not sure if theres actually a problem there.  (Just a note to self to come back to this after 0.3)\" +1 Integrated in Cassandra #59 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/59/])\n",
            "    use getTempFileName / closeRename to avoid problems w/ half-written sstables.\n",
            "patch by jbellis; reviewed by Eric Evans for \n",
            "clean up anticompaction code a little.\n",
            "patch by jbellis; reveiewed by Eric Evans for \n",
            " committed \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-708\n",
            "issue_type:  New Feature \n",
            "summary:  Set Cache Capacity via nodeprobe \n",
            "description:  CASSANDRA-688 added the ability to set capacity via JMX. Adding this to nodeprobe will be useful for changing a node's capacity on fly. \n",
            "comments:  [\"03\\n    better division of aggregated key cache capacity among sstable caches\\n\\n02\\n    add cache info to cfstats; add setcachecapacity\\n\\n01\\n    use 0-capacity cache instead of null to indicate no caching; this means we don't need to worry about creating & destroying caches from JMX (and synchronizing on that)\\n\", '+1', 'rebased & committed', \"Integrated in Cassandra #335 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/335/])\\n    better division of aggregated capacity among sstable caches\\npatch by jbellis; reviewed by goffinet for \\nadd cache info to cfstats; add setcachecapacity\\npatch by jbellis; reviewed by goffinet for \\nuse 0-capacity cache instead of null to indicate no caching; this means we don't need to worry about creating & destroying cache objects from JMX\\npatch by jbellis; reviewed by goffinet for \\n\"] \n",
            "my_comment:  [\"03\n",
            "    better division of aggregated key cache capacity among sstable caches\n",
            "\n",
            "02\n",
            "    add cache info to cfstats; add setcachecapacity\n",
            "\n",
            "01\n",
            "    use 0-capacity cache instead of null to indicate no caching; this means we dont need to worry about creating & destroying caches from JMX (and synchronizing on that)\n",
            "\" +1 rebased & committed \"Integrated in Cassandra #335 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/335/])\n",
            "    better division of aggregated capacity among sstable caches\n",
            "patch by jbellis; reviewed by goffinet for \n",
            "add cache info to cfstats; add setcachecapacity\n",
            "patch by jbellis; reviewed by goffinet for \n",
            "use 0-capacity cache instead of null to indicate no caching; this means we dont need to worry about creating & destroying cache objects from JMX\n",
            "patch by jbellis; reviewed by goffinet for \n",
            "\"] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-12956\n",
            "issue_type:  Bug \n",
            "summary:  CL is not replayed on custom 2i exception \n",
            "description:  If during the node shutdown / drain the custom (non-cf) 2i throws an exception, CommitLog will get correctly preserved (segments won't get discarded because segment tracking is correct). \n",
            "comments:  ['The problem is only present in Cassandra starting from 3.0. Versions before that will replay commit log despite the exception, possibly generating multiple indentical sstables.', \"Patch for {{3.0}} is quite different and is much bigger. Main problem is that there's no transactionality on the same level as in {{3.X}}. {{3.0}} memtables are flushed and renamed to non-tmp names, readers are returned. We need a bit better granularity, since after we may have to abort all the flushed sstables if 2i failed. I've changed it a bit in {{3.x}} fashion, although since we flush to just one sstable, I thought that extracting {{txn}} to the top level will not give us anything.\\n\\nBoth patches introduce the second latch. I'm usually not the biggest fan of two threads that have to wait for one another, but here the ordering is an issue. Problem is that post-flush executor is single-threaded (for ordering), and flush executor is multi-threaded, so we can't return future backed with that multi-threaded executor as it will break order. On the other hand, if we move 2i flush to flush executor, we'll have to sequentially wait for 2i, then all memtables. Current approach allows to keep these actions parallel. \\n\\nWe only need to synchronise the non-cf 2i flush with memtable holding data for current cf. All the cf-index memtables will be in sync with data one anyways since they're combined in the transaction. \\n\\n|[3.X|https://github.com/ifesdjeen/cassandra/tree/12956-3.X]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.X-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.X-dtest/]|\\n|[3.0|https://github.com/ifesdjeen/cassandra/tree/12956-3.0-v2]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.0-v2-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.0-v2-dtest/]|\\n|[trunk|https://github.com/ifesdjeen/cassandra/tree/12956-trunk]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-trunk-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-trunk-dtest/]|\\n\", '[~blambov] to review.', 'Looking at the 3.X patch, as flush threads waiting for post-flush waiting for flush threads is a recipe for disaster (poor performance and deadlocks in particular), I would much prefer the 2i flush to be done on a different thread. In particular, as the flush runnables doing the actual work proceed on their per-disk executors, the flush thread itself [here|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:12956-3.X#diff-98f5acb96aa6d684781936c141132e2aR1152] looks like a better candidate.\\n\\nCould the inverse of the current problem also cause issues (i.e. 2i flushing without the data being in sstables)? It appears that the 2i flush also needs to eventually become transactional -- doing the above would make that easier too.', 'Thanks for the review!\\n\\nI was also skeptical about two threads waiting for one another. Also, tried the approach you\\'ve suggested.\\nI hesitated mostly because it\\'d be blocking the flush thread (although you\\'re right that it\\'s going to be\\nwaiting for flushes anyways) and because {{flushMemtable}} is called from loop, so I wasn\\'t sure if it\\'s\\na good place.\\n\\nIn retrospect, I think your suggestion is much better than the previous version. I\\'ve re-implemented a patch\\nfor 3.0 as well, it got much simpler. Now, we do 2i flush before memtable flush during flush of \"data\" memtable\\n(first one). We could bring back changes that expose sstable writer and run 2i flush on the other\\nthread and commit sstable only on successful 2i flush, although since all cf memtables are flushed sequentially\\nanyways and it might be a bit out of scope of the bugfix I decided to leave it this way. Simply running 2i\\nflush in a different thread is not enough, as we need to ensure it\\'s in sync with \"data\" memtable flush.\\n\\nOrder of 2i/memtable flush \n",
            "my_comment:  The problem is only present in Cassandra starting from 3.0. Versions before that will replay commit log despite the exception possibly generating multiple indentical sstables. \"Patch for {{3.0}} is quite different and is much bigger. Main problem is that theres no transactionality on the same level as in {{3.X}}. {{3.0}} memtables are flushed and renamed to non-tmp names readers are returned. We need a bit better granularity since after we may have to abort all the flushed sstables if 2i failed. Ive changed it a bit in {{3.x}} fashion although since we flush to just one sstable I thought that extracting {{txn}} to the top level will not give us anything.\n",
            "\n",
            "Both patches introduce the second latch. Im usually not the biggest fan of two threads that have to wait for one another but here the ordering is an issue. Problem is that post-flush executor is single-threaded (for ordering) and flush executor is multi-threaded so we cant return future backed with that multi-threaded executor as it will break order. On the other hand if we move 2i flush to flush executor well have to sequentially wait for 2i then all memtables. Current approach allows to keep these actions parallel. \n",
            "\n",
            "We only need to synchronise the non-cf 2i flush with memtable holding data for current cf. All the cf-index memtables will be in sync with data one anyways since theyre combined in the transaction. \n",
            "\n",
            "|[3.X|https://github.com/ifesdjeen/cassandra/tree/12956-3.X]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.X-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.X-dtest/]|\n",
            "|[3.0|https://github.com/ifesdjeen/cassandra/tree/12956-3.0-v2]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.0-v2-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.0-v2-dtest/]|\n",
            "|[trunk|https://github.com/ifesdjeen/cassandra/tree/12956-trunk]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-trunk-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-trunk-dtest/]|\n",
            "\" [~blambov] to review. Looking at the 3.X patch as flush threads waiting for post-flush waiting for flush threads is a recipe for disaster (poor performance and deadlocks in particular) I would much prefer the 2i flush to be done on a different thread. In particular as the flush runnables doing the actual work proceed on their per-disk executors the flush thread itself [here|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:12956-3.X#diff-98f5acb96aa6d684781936c141132e2aR1152] looks like a better candidate.\n",
            "\n",
            "Could the inverse of the current problem also cause issues (i.e. 2i flushing without the data being in sstables)? It appears that the 2i flush also needs to eventually become transactional -- doing the above would make that easier too. Thanks for the review!\n",
            "\n",
            "I was also skeptical about two threads waiting for one another. Also tried the approach you\\ve suggested.\n",
            "I hesitated mostly because it\\d be blocking the flush thread (although you\re right that it\\s going to be\n",
            "waiting for flushes anyways) and because {{flushMemtable}} is called from loop so I wasn\\t sure if it\\s\n",
            "a good place.\n",
            "\n",
            "In retrospect I think your suggestion is much better than the previous version. I\\ve re-implemented a patch\n",
            "for 3.0 as well it got much simpler. Now we do 2i flush before memtable flush during flush of \"data\" memtable\n",
            "(first one). We could bring back changes that expose sstable writer and run 2i flush on the other\n",
            "thread and commit sstable only on successful 2i flush although since all cf memtables are flushed sequentially\n",
            "anyways and it might be a bit out of scope of the bugfix I decided to leave it this way. Simply running 2i\n",
            "flush in a different thread is not enough as we need to ensure it\\s in sync with \"data\" memtable flush.\n",
            "\n",
            "Order of 2i/memtable flush \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1500\n",
            "issue_type:  Improvement \n",
            "summary:  Consolidated identical code in CassandraDaemon.setup() into super class \n",
            "description:  There is identical code in the setup() methods of both avro and thrift CassandraDaemon classes. I moved that code into the AbstractCassandraDaemon class for cleanliness. \n",
            "comments:  ['diff attached', 'patch for refactoring common setup code in CassandraDaemon', 'committed'] \n",
            "my_comment:  diff attached patch for refactoring common setup code in CassandraDaemon committed \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-915\n",
            "issue_type:  Bug \n",
            "summary:  disallow column family names containing hyphens \n",
            "description:  You cannot use use hyphens in column family names because hyphens are used as delimiters in sstable filenames (which are derived from the CF name). \n",
            "comments:  ['The attached patch should take care of 0.6 (and trunk for the time being), but this ticket will need to be added as a dependency to CASSANDRA-44 in order to make sure that the new-shiny handles this case as well.', '+1', 'committed to 0.6 and trunk', \"Integrated in Cassandra #389 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/389/])\\n    don't allow hyphens in column family names\\n\\nPatch by eevans; reviewed by jbellis for \\n\"] \n",
            "my_comment:  The attached patch should take care of 0.6 (and trunk for the time being) but this ticket will need to be added as a dependency to CASSANDRA-44 in order to make sure that the new-shiny handles this case as well. +1 committed to 0.6 and trunk \"Integrated in Cassandra #389 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/389/])\n",
            "    dont allow hyphens in column family names\n",
            "\n",
            "Patch by eevans; reviewed by jbellis for \n",
            "\"] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4919\n",
            "issue_type:  Bug \n",
            "summary:  StorageProxy.getRangeSlice sometimes returns incorrect number of columns \n",
            "description:  When deployed on a single node, number of columns is correct.\n",
            "comments:  ['Attaching a patch fixing paged column iteration.', 'Is there a dtest for this?', \"There's a wide row test and a range slice test, but not a combination of the two.\", 'committed, and create https://github.com/riptano/cassandra-dtest/issues/5 to follow up w/ dtest.', 'I note for the testing part that this only concern getRangeSlice with the isPaging option (i.e. for thrift, get_paged_slice) and that currently CQL never calls that so this is not reproducible with CQL. We may end up using the isPaging thing in CQL with CASSANDRA-4851 however.', 'Just added wide_slice_test to putget_test.py in the dtests.'] \n",
            "my_comment:  Attaching a patch fixing paged column iteration. Is there a dtest for this? \"Theres a wide row test and a range slice test but not a combination of the two.\" committed and create https://github.com/riptano/cassandra-dtest/issues/5 to follow up w/ dtest. I note for the testing part that this only concern getRangeSlice with the isPaging option (i.e. for thrift get_paged_slice) and that currently CQL never calls that so this is not reproducible with CQL. We may end up using the isPaging thing in CQL with CASSANDRA-4851 however. Just added wide_slice_test to putget_test.py in the dtests. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4992\n",
            "issue_type:  Bug \n",
            "summary:  TTL/WRITETIME function against collection column returns invalid value \n",
            "description:  Since we cannot query individual content of collection in 1.2, TTL/WRITETIME function on collection column does not make sense. But currently we can perform those function on collection and get deserialization error like:\n",
            "comments:  ['Simple patch attached to just refuse said function on collection columns.', '+1', 'Commited, thanks'] \n",
            "my_comment:  Simple patch attached to just refuse said function on collection columns. +1 Commited thanks \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-17220\n",
            "issue_type:  Improvement \n",
            "summary:  Make startup checks configurable \n",
            "description:  This ticket was created from needs discovered in CASSANDRA-17180. We want to be able to configure a startup check so we figured out that it is necessary to treat all startup checks same - to be able to configure them. This ticket is about making startup checks configurable.\n",
            "comments:  ['https://github.com/apache/cassandra/pull/1448', 'hey [~dcapwell] [~paulo] , would you mind to review this? It touches the enum configs we spoke about recently.', 'This is looking very good! I really like that we\\'re making the legacy checks {{non_configurable_check}} while allowing new \"configurable\" checks to be easily added by registering a new {{{}StartupCheckType{}}}.\\r\\n\\r\\nI think in order for this to be ready we need to make the checks with deprecated properties configurable via the {{startup_checks}} yaml (ie. cassandra.ignore_dc, cassandra.ignore_rack). Also, can you please update {{cassandra.yaml}} with a commented out example on how to configure startup checks?\\r\\n\\r\\nIt would be nice to have a definition on CASSANDRA-17292 before merging this to ensure it will be consistent with the new property grouping, even though I think {{startup_checks}} seems to make sense on it\\'s own macro-group.', 'https://github.com/apache/cassandra/pull/1448', 'This looks great +1. If you agree can you incorporate [this commit|https://github.com/pauloricardomg/cassandra/commit/956f6def4f12622efa914f3f08d6747bfc278953] updating the {{cassandra.yaml}} wording? I tried to reduce the amount of text between the properties.\\r\\n\\r\\nAlso, one final nitpicking before we merge:\\r\\n * Should we make startup check names be prefixed with {{check_}} ? Ie. {{{}check_dc{}}}, {{{}check_rack{}}}, etc (instead of simply {{dc}} or {{{}rack{}}})? This will make it easier to figure out what the startup check does. The renamed checks would be:\\r\\n{code:yaml}\\r\\nstartup_checks: \\r\\n  check_filesystem_ownership:\\r\\n\\r\\n      enabled: false\\r\\n      ownership_token: \"sometoken\" # (overriden by \"CassandraOwnershipToken\" system property)\\r\\n      ownership_filename: \".cassandra_fs_ownership\" # (overriden by \"cassandra.fs_ownership_filename\")\\r\\n  check_dc:\\r\\n\\r\\n      enabled: true # (overriden by cassandra.ignore_dc system property)\\r\\n  check_rack:\\r\\n\\r\\n      enabled: true # (overriden by cassandra.ignore_rack system property)\\r\\n{code}\\r\\n\\r\\nWhat do you think [~smiklosovic] \\xa0[~dcapwell] ?', 'We probably need to create a documentation ticket to explain what each startup check does on the documentation.', 'Thanks mate, I am for simple configs, it is obvious it is a check. Yes config ticket will follow.', '{quote}\\xa0Thanks mate, I am for simple configs, it is obvious it is a check.\\r\\n{quote}\\r\\nOk, even though I still prefer {{check_dc}} and check_rack ;)\\r\\n\\r\\n\\r\\n\\r\\n\\xa0Can you attach some CI results?', 'once this is settable via system property it will be\\r\\n\\r\\n-Dstartup_checks.dc.enabled=true\\r\\n\\r\\ninstead of \\r\\n\\r\\n-Dstartup_checks.check_dc.enabled=true\\r\\n\\r\\nThe former makes more sense to me.', 'https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1457/'] \n",
            "my_comment: true # (overriden by cassandra.ignore_rack system property)\r\n",
            "{code}\r\n",
            "\r\n",
            "What do you think [~smiklosovic] \\xa0[~dcapwell] ? We probably need to create a documentation ticket to explain what each startup check does on the documentation. Thanks mate I am for simple configs it is obvious it is a check. Yes config ticket will follow. {quote}\\xa0Thanks mate I am for simple configs it is obvious it is a check.\r\n",
            "{quote}\r\n",
            "Ok even though I still prefer {{check_dc}} and check_rack ;)\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\\xa0Can you attach some CI results? once this is settable via system property it will be\r\n",
            "\r\n",
            "-Dstartup_checks.dc.enabled=true\r\n",
            "\r\n",
            "instead of \r\n",
            "\r\n",
            "-Dstartup_checks.check_dc.enabled=true\r\n",
            "\r\n",
            "The former makes more sense to me. https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1457/ \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-12499\n",
            "issue_type:  Bug \n",
            "summary:  Row cache does not cache partitions on tables without clustering keys \n",
            "description:  {code}\n",
            "comments:  ['Working as intended in:\\n- 2.1.12\\n- 2.2.7\\n\\nBroken in:\\n- 3.0.8\\n- 3.7\\n- Trunk\\n\\n', 'Branch here: https://github.com/jeffjirsa/cassandra/commit/e7cb6c3409b889174d32c4eec9f60da380f39d3d\\n\\nBelieve patch applies cleanly to 3.0 -> trunk (or it did a few days ago when written)\\n\\ntestall shows no failures: http://cassci.datastax.com/job/jeffjirsa-cassandra-12499-testall/lastBuild/testReport/\\ndtest shows one failure that appears unrelated: http://cassci.datastax.com/job/jeffjirsa-cassandra-12499-dtest/\\n\\n\\n\\n', \"The change lgtm but a few small remarks:\\n* It's probably worth a comment as to why we have to special case tables without clustering columns.\\n* For the test, instead of adding a new {{standardCFMD()}} method with a new parameter, I'd just make {{clusteringType == null}} mean no clustering. You can also pull the {{.addClusteringColumn()}} call on its own and call just that conditionally rather than duplicate 3 lines. Lastly, {{insertData}} don't really need a new parameter, it can decide if it needs a clustering based on the {{CFMetaData}}, which would be less error prone.\\n\", 'Thanks [~slebresne]. Implemented your changes, force pushed.\\n\\nPatch here: https://github.com/apache/cassandra/compare/trunk...jeffjirsa:cassandra-12499.patch\\nCassci running, test reports should be here: http://cassci.datastax.com/job/jeffjirsa-cassandra-12499-testall/lastBuild/testReport/ and http://cassci.datastax.com/job/jeffjirsa-cassandra-12499-dtest/lastBuild/testReport/ in a few hours. \\n\\n', '+1', 'Committed in {{eace9aaddfdd0059f52b1eb9b6902f999f04a447}}\\n'] \n",
            "my_comment: http://cassci.datastax.com/job/jeffjirsa-cassandra-12499-testall/lastBuild/testReport/ and http://cassci.datastax.com/job/jeffjirsa-cassandra-12499-dtest/lastBuild/testReport/ in a few hours. \n",
            "\n",
            " +1 Committed in {{eace9aaddfdd0059f52b1eb9b6902f999f04a447}}\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6972\n",
            "issue_type:  Improvement \n",
            "summary:  Throw an ERROR when auto_bootstrap: true and bootstrapping node is listed in seeds \n",
            "description:  Obviously when this condition exists the node will not bootstrap.  But it is not obvious from the logs why it is not bootstrapping.  Throwing an error would make it obvious and therefore faster to correct. \n",
            "comments:  ['The catch with doing this is, now everyone has to go around and put autobootstrap: false in their seed configs.', 'Yes, the right fix is to just log it explicitly at info.  We added that back in CASSANDRA-746 but it got undone at some point.', 'Committed basically the same thing from that ticket.  The circle of life is complete.'] \n",
            "my_comment: false in their seed configs. Yes the right fix is to just log it explicitly at info.  We added that back in CASSANDRA-746 but it got undone at some point. Committed basically the same thing from that ticket.  The circle of life is complete. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-15861\n",
            "issue_type:  Bug \n",
            "summary:  Mutating sstable component may race with entire-sstable-streaming(ZCS) causing checksum validation failure \n",
            "description:  Flaky dtest: [test_dead_sync_initiator - repair_tests.repair_test.TestRepair|https://ci-cassandra.apache.org/view/all/job/Cassandra-devbranch-dtest/143/testReport/junit/dtest.repair_tests.repair_test/TestRepair/test_dead_sync_initiator/]\n",
            "comments:  ['[~marcuse] [~djoshi] it looks like you are pretty experienced with compaction code, do you see any issue with first proposal? ', 'If reading this correctly, I wonder if this should also be a issue with org.apache.cassandra.io.sstable.format.SSTableReader#cloneWithNewSummarySamplingLevel which is called by org.apache.cassandra.io.sstable.IndexSummaryRedistribution; this modifies the summary file in place.', \"[~dcapwell]\\xa0you are right. {{IndexSummary}} can definitely cause trouble for entire-sstable-streaming.. Then the only option we have is to apply first approach to {{IndexSummary}}\\xa0because we can't make {{IndexSummary}}\\xa0fixed-length encoding..\\r\\n\\r\\nOr we can consider a lock approach\", 'One way could be to mark the sstable compacting while we stream the index summary and sstable metadata components', 'bq. One way could be to mark the sstable compacting while we stream the index summary and sstable metadata components\\r\\n\\r\\nif the sstables are already in compacting state, does it mean entire-sstable-streaming will be blocked until compaction is finished? \\r\\n\\r\\nIt\\'d be nice to minize the lock scope, so \"critical section\" only include \"metadata mutation\" (rewrite index summary and stats metadata) which should be fast.', \"I took a quick look and have a few comments\\r\\n\\r\\n* https://github.com/apache/cassandra/pull/642/files#diff-de503ddc819368b078a86f6d9e3921aeR211-R228. isn't the write async; we are just storing on a buffer to be written later?  Looking at org.apache.cassandra.net.AsyncStreamingOutputPlus#writeFileToChannelZeroCopy it calls channel.writeAndFlush but doesn't look at the future, so the write is async.  \\r\\n* have you done any longevity testing of this?  My fear is that compaction will get blocked while streaming is running which could cause slowness or stability issues.\", 'bq. https://github.com/apache/cassandra/pull/642/files#diff-de503ddc819368b078a86f6d9e3921aeR211-R228. isn\\'t the write async; we are just storing on a buffer to be written later? Looking at org.apache.cassandra.net.AsyncStreamingOutputPlus#writeFileToChannelZeroCopy it calls channel.writeAndFlush but doesn\\'t look at the future, so the write is async.\\r\\n\\r\\n{{writeFileToChannelZeroCopy}} is async, but if I remember correctly about unix file system: once a file is opened, reader won\\'t be affected by file deletion or rewrite. So when the synchronized block completes, all component file-channels are already opened and in-sync with {{ComponentManifest}}, even if they are not flushed from netty outbound buffer to kernel.\\r\\n\\r\\nI agree with you that the atomicity should be made more obvious rather than relying on underlying FS. I will find a cleaner approach.\\r\\n\\r\\nbq. have you done any longevity testing of this? My fear is that compaction will get blocked while streaming is running which could cause slowness or stability issues.\\r\\n\\r\\nNot yet, ticket is still \"in-progress\".  It\\'d be nice to reuse the tests did in CASSANDRA-14556..\\r\\n\\r\\nbq. Make STATS mutation as a proper compaction to create hard link on the compacting sstable components with a new descriptor, except STATS files which will be copied entirely. Then mutation will be applied on the new STATS file. At the end, old sstable will be released. This ensures all sstable components are immutable and shouldn\\'t make these special compaction tasks slower.\\r\\n\\r\\nI had a [prototype|https://github.com/jasonstack/cassandra/blob/cb9bdaf037fd550b84fe5b7da89f9c56dc729c35/src/java/org/apache/cassandra/db/compaction/StatsMutationCompaction.java#L72] using the hardlink approach which should avoid blocking between compaction and streaming. The only thing I don\\'t like is redistributing index summary every hour will explode sstable generation.. what do you guys think?', \"bq. writeFileToChannelZeroCopy is async, but if I remember correctly about unix file system: once a file is opened, reader won't be affected by file deletion or rewrite. So when the synchronized block completes, al \n",
            "my_comment: once a file is opened reader wont be affected by file deletion or rewrite. So when the synchronized block completes al \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-13257\n",
            "issue_type:  New Feature \n",
            "summary:  Add repair streaming preview \n",
            "description:  It would be useful to be able to estimate the amount of repair streaming that needs to be done, without actually doing any streaming. Our main motivation for this having something this is validating CASSANDRA-9143 in production, but I’d imagine it could also be a useful tool in troubleshooting. \n",
            "comments:  ['|[branch|https://github.com/bdeggleston/cassandra/tree/13257]|[dtest|http://cassci.datastax.com/view/Dev/view/bdeggleston/job/bdeggleston-13257-dtest/]|[testall|http://cassci.datastax.com/view/Dev/view/bdeggleston/job/bdeggleston-13257-testall/]|', '[~pauloricardomg], [~yukim] - would one of you be interested in reviewing?', 'Is it possible to add \"dry-run\" feature to stremaing instead of TreeDifference? That way, we can use the feature to preview other streaming operations.\\nFor example:\\n\\n{code}\\nStreamPlan newPlan = new StreamPlan(dryRun=true);\\nnewPlan.addTransferRanges(range);\\nStreamResultFuture future = newPlan.execute();\\n\\n// if dryRun, only run streaming until PREPARE phase so that exactly send and *receive* size are known, and return calculated StreamState based on those.\\nStreamState dryRunResult = future.get();\\n{code}\\n\\nOr, when streaming is only for out going (like decommission) we may be able to return without exchanging messages.\\n\\nWDYT?', 'bq. Is it possible to add \"dry-run\" feature to stremaing instead of TreeDifference?\\n\\nThat’s a really good idea. It will also be more accurate than TreeDifference, since it will measure actual sstable sizes, not the size of the partitions that come out of the validation compaction.', \"[~yukim] I've pushed commits converting the preview to use streaming instead of merkle tree diffs. A squashed version is [here|https://github.com/bdeggleston/cassandra/tree/13257-squashed]\\n\\nNew ci runs are here:\\n|[dtest|http://cassci.datastax.com/job/bdeggleston-13257-squashed-dtest/]|[testall|http://cassci.datastax.com/job/bdeggleston-13257-squashed-testall/]|\", 'I wonder what is the usage of \"Perform preview on repaired data\"/{{PreviewKind.REPAIRED}}.\\n\\nI kind of want to separate context of \"preview streaming\" and \"preview repair\". So I\\'d like to keep {{PreviewKind}} in repair package, and have boolean {{isPreview}} in streaming. It seems to me that the reason we have {{PreviewKind}} in streaming right now is to add above functionality.\\n', 'The use cases I have in mind for the preview types {{ALL}}, {{UNREPAIRED}}, {{REPAIRED}} are: estimate streaming required for a full repair, estimate streaming for an incremental repair, and validate that repaired data is in sync, respectively. \\n\\nThe immediate need this ticket addresses is validating CASSANDRA-9143 in a large and active cluster, where the main use of preview will be validating that repaired data is in sync\\n\\nUsing just a boolean {{isPreview}} on the streaming side doesn’t provide enough information to perform an accurate preview. We’d only ever be able to preview the full repair case. The existing stream session sstable selection logic either selects all sstables for a token range, or (post CASSANDRA-13328) only the sstables in a token range which are part of an in-progress repair. Selecting only the repaired or unrepaired sstables is not supported. Starting an actual incremental repair won’t work because it will perform anti-compaction before it does anything. Making {{StreamSession}} aware of {{PreviewKind}} is the most straightforward way to do this.\\n\\nBetween that and the potential for previewing streaming for things like decommission, etc, I think the best place for {{PreviewKind}} is in the streaming package. Supporting some repair related operations isn’t a stretch, given repair and streaming are already fairly closely coupled.\\n', 'bq. validate that repaired data is in sync\\n\\nI\\'m not sure this should go to {{nodetool repair}} command. If we have subcommand, then {{nodetool repair validate}} would be the right command.\\nDoes this need to \"preview\" streaming as well? Seems validating repaired SSTables is enough.\\n\\nAbout {{PreviewKind}} in streaming, it is how SSTables are selected and I\\'m fine with it for now until we have more cleaner way to decouple from streaming itself.\\n\\nbq. We’d only ever be able to preview the full repair case.\\n\\nWould it be so? I will look up CASSANDRA-13328 as well.', 'bq. If we have s \n",
            "my_comment: estimate streaming required for a full repair estimate streaming for an incremental repair and validate that repaired data is in sync respectively. \n",
            "\n",
            "The immediate need this ticket addresses is validating CASSANDRA-9143 in a large and active cluster where the main use of preview will be validating that repaired data is in sync\n",
            "\n",
            "Using just a boolean {{isPreview}} on the streaming side doesn’t provide enough information to perform an accurate preview. We’d only ever be able to preview the full repair case. The existing stream session sstable selection logic either selects all sstables for a token range or (post CASSANDRA-13328) only the sstables in a token range which are part of an in-progress repair. Selecting only the repaired or unrepaired sstables is not supported. Starting an actual incremental repair won’t work because it will perform anti-compaction before it does anything. Making {{StreamSession}} aware of {{PreviewKind}} is the most straightforward way to do this.\n",
            "\n",
            "Between that and the potential for previewing streaming for things like decommission etc I think the best place for {{PreviewKind}} is in the streaming package. Supporting some repair related operations isn’t a stretch given repair and streaming are already fairly closely coupled.\n",
            " bq. validate that repaired data is in sync\n",
            "\n",
            "I\\m not sure this should go to {{nodetool repair}} command. If we have subcommand then {{nodetool repair validate}} would be the right command.\n",
            "Does this need to \"preview\" streaming as well? Seems validating repaired SSTables is enough.\n",
            "\n",
            "About {{PreviewKind}} in streaming it is how SSTables are selected and I\\m fine with it for now until we have more cleaner way to decouple from streaming itself.\n",
            "\n",
            "bq. We’d only ever be able to preview the full repair case.\n",
            "\n",
            "Would it be so? I will look up CASSANDRA-13328 as well. bq. If we have s \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1919\n",
            "issue_type:  Improvement \n",
            "summary:  Add shutdownhook to flush commitlog \n",
            "description:  this replaces the periodic_with_flush approach from CASSANDRA-1780 / CASSANDRA-1917 \n",
            "comments:  [\"The approach I took was to add a shutdownBlocking to CommitLog (that in turn shuts down the CL executor; the code to enable this is most of the patch), have drain call CL.shutdownBlocking, and add a shutdown hook to call drain.\\n\\nThis looks like it works fine in standalone tests but it breaks the hell out of the test suite.  Not sure what's going on:\\n\\n{noformat}\\n    [junit] Testsuite: org.apache.cassandra.auth.SimpleAuthorityTest\\n    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 0.076 sec\\n    [junit] \\n    [junit] Testsuite: org.apache.cassandra.cli.CliTest\\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 1.136 sec\\n    [junit] \\n    [junit] ------------- Standard Error -----------------\\n    [junit]  WARN 23:31:54,490 Generated random token Token(bytes[761bf80745d12d316f77fbb493168428]). Random tokens will result in an unbalanced ring; see http://wiki.apache.org/cassandra/Operations\\n    [junit] ------------- ---------------- ---------------\\n    [junit] Testcase: testCli(org.apache.cassandra.cli.CliTest):\\tCaused an ERROR\\n    [junit] Unable to create thrift socket to /127.0.0.1:9170\\n    [junit] java.io.IOException: Unable to create thrift socket to /127.0.0.1:9170\\n    [junit] \\tat org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:72)\\n    [junit] \\tat org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:207)\\n    [junit] \\tat org.apache.cassandra.service.EmbeddedCassandraService.init(EmbeddedCassandraService.java:64)\\n    [junit] \\tat org.apache.cassandra.cli.CliTest.setup(CliTest.java:219)\\n    [junit] \\tat org.apache.cassandra.cli.CliTest.testCli(CliTest.java:149)\\n    [junit] Caused by: org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address /127.0.0.1:9170.\\n    [junit] \\tat org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:99)\\n    [junit] \\tat org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:85)\\n    [junit] \\tat org.apache.cassandra.thrift.TCustomServerSocket.<init>(TCustomServerSocket.java:59)\\n    [junit] \\tat org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:65)\\n{noformat}\\n\\nFirst test runs fine, second errors out because (I'm guessing from the error message) the first JVM is still hanging around, and the third test doesn't start at all, even long after the junit timeout.\", 'Aha, the batch CL executor shutdown thread was being created but not started.  v2 attached.\\n\\nCliTest is still failing though with the same error, along with EmbeddedCassandraServiceTest.  Not sure how to tell junit, \"wait for the previous JVM to exit completely before starting the next test.\"', \"+1. \\n\\nI'm beginning to think something was introduced in the SSL patch that altered the behavior the sockets.  I've seen odd socket errors twice in the last few days while running the unit tests, I think in RemoveTest.  fwiw, I didn't see any errors while running the tests with this patch.\", \"bq. I'm beginning to think something was introduced in the SSL patch that altered the behavior the sockets\\n\\nNow I'm getting CliTest failures w/o this patch, too.  I think you might be on to something.\", \"hmm, this isn't quite right, drain is similar to what we want but not the same (flushing every CF could take a while).\", 'v3 just shuts down mutation stage + commitlog in the hook', '+1', 'committed', 'Integrated in Cassandra-0.7 #216 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/216/])\\n    add JVM shutdownhook to sync commitlog\\npatch by jbellis; reviewed by gdusbabek for CASSANDRA-1919\\n'] \n",
            "my_comment: Could not create ServerSocket on address /127.0.0.1:9170.\n",
            "    [junit] \\tat org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:99)\n",
            "    [junit] \\tat org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:85)\n",
            "    [junit] \\tat org.apache.cassandra.thrift.TCustomServerSocket.<init>(TCustomServerSocket.java:59)\n",
            "    [junit] \\tat org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:65)\n",
            "{noformat}\n",
            "\n",
            "First test runs fine second errors out because (Im guessing from the error message) the first JVM is still hanging around and the third test doesnt start at all even long after the junit timeout.\" Aha the batch CL executor shutdown thread was being created but not started.  v2 attached.\n",
            "\n",
            "CliTest is still failing though with the same error along with EmbeddedCassandraServiceTest.  Not sure how to tell junit \"wait for the previous JVM to exit completely before starting the next test.\" \"+1. \n",
            "\n",
            "Im beginning to think something was introduced in the SSL patch that altered the behavior the sockets.  Ive seen odd socket errors twice in the last few days while running the unit tests I think in RemoveTest.  fwiw I didnt see any errors while running the tests with this patch.\" \"bq. Im beginning to think something was introduced in the SSL patch that altered the behavior the sockets\n",
            "\n",
            "Now Im getting CliTest failures w/o this patch too.  I think you might be on to something.\" \"hmm this isnt quite right drain is similar to what we want but not the same (flushing every CF could take a while).\" v3 just shuts down mutation stage + commitlog in the hook +1 committed Integrated in Cassandra-0.7 #216 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/216/])\n",
            "    add JVM shutdownhook to sync commitlog\n",
            "patch by jbellis; reviewed by gdusbabek for CASSANDRA-1919\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-11038\n",
            "issue_type:  Bug \n",
            "summary:  Is node being restarted treated as node joining? \n",
            "description:  Hi, \n",
            "comments:  [\"If we do send a {{NEW_NODE}} notification for a simple restart, then that do qualify as a bug (though I'll changed the priority to minor since as you mentioned, modern drivers should easily ignore that). I'll note however than 2.0 is not supported anymore and that's too minor for 2.1, so we'll need to first check if that's still a thing in 2.2.\\n\", 'And this does seem to still be a thing since we have [a dtest|https://github.com/riptano/cassandra-dtest/blob/master/pushed_notifications_test.py#L155-L179] that actively assert that a {{NEW_NODE}} is send in that case (not sure why no red flags were raised when writing that test but that sound obviously wrong to me).', 'Not all drivers have that short circuit to avoid node refresh for known nodes.\\n\\nOne other thing I noticed related to this: in addition to status+topo events for starting existing nodes, we also receive both messages when adding a new node, in the same order (i.e. \"status up\" before \"topology_change new_node\").', \"Pushed branches with fixes for 2.2/3.0/3.7/trunk - though the fix merges forward cleanly except for conflicts where I've cleaned up imports. Basically, these preserve the existing behaviour of delivering both {{NEW_NODE}} and {{UP}} events when a node first joins the cluster & of delaying both until after the node becomes available for clients. The erroneous {{NEW_NODE}} when a known node is restarted has been removed. The tracking of pushed notifications in {{EventNotifier}} is still necessary at the moment (because [reasons|https://issues.apache.org/jira/browse/CASSANDRA-7816?focusedCommentId=14346387&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14346387]), but they will go away with CASSANDRA-9156. See CASSANDRA-11731 for some related discussion.\\n\\ndtest branch [here|https://github.com/beobal/cassandra-dtest/tree/11038]\\n\\n||branch||testall||dtest||\\n|[11038-2.2|https://github.com/beobal/cassandra/tree/11038-2.2]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-2.2-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-2.2-dtest]|\\n|[11038-3.0|https://github.com/beobal/cassandra/tree/11038-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.0-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.0-dtest]|\\n|[11038-3.7|https://github.com/beobal/cassandra/tree/11038-3.7]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.7-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.7-dtest]|\\n|[11038-trunk|https://github.com/beobal/cassandra/tree/11038-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-trunk-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-trunk-dtest]|\\n\\n(so far I've only kicked off CI for the 2.2 branch, just in case there's some problem I didn't run into locally, will kick off the other jobs when that finishes).\\n\\nedit: pushed an additional commit to the 2.2 branch as I forgot to switch to java 7 during dev and accidentally included an 8ism.\", \"See this dtest [pull request|https://github.com/riptano/cassandra-dtest/pull/983] for fixing the behavior of the restart node tests in pushed_notifications_test.py. You may also want to add a new dtest to check that NEW_NODE and REMOVED_NODE are sent when a node joins or leaves respectively, I don't think we have a test specific for this at the moment.\", 'thanks everyone for fixing this issue!', \"I've rebased (which should fix the dtest failures) and kicked off another set of CI runs. \\nFTR, the dtest jobs are using [this branch|https://github.com/beobal/cassandra-dtest/tree/11731] , which also includes [~Stefania]'s tests for CASSANDRA-11731.\\n\\n||branch||testall||dtest||\\n|[11038-2.2|https://github.com/beobal/cassandra/tree/11038-2.2]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-2.2-testall]|[dtest|http://cassc \n",
            "my_comment: pushed an additional commit to the 2.2 branch as I forgot to switch to java 7 during dev and accidentally included an 8ism.\" \"See this dtest [pull request|https://github.com/riptano/cassandra-dtest/pull/983] for fixing the behavior of the restart node tests in pushed_notifications_test.py. You may also want to add a new dtest to check that NEW_NODE and REMOVED_NODE are sent when a node joins or leaves respectively I dont think we have a test specific for this at the moment.\" thanks everyone for fixing this issue! \"Ive rebased (which should fix the dtest failures) and kicked off another set of CI runs. \n",
            "FTR the dtest jobs are using [this branch|https://github.com/beobal/cassandra-dtest/tree/11731]  which also includes [~Stefania]s tests for CASSANDRA-11731.\n",
            "\n",
            "||branch||testall||dtest||\n",
            "|[11038-2.2|https://github.com/beobal/cassandra/tree/11038-2.2]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-2.2-testall]|[dtest|http://cassc \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4874\n",
            "issue_type:  Improvement \n",
            "summary:  Possible authorizaton handling impovements \n",
            "description:  I'll create another issue with my suggestions about fixing/improving IAuthority interfaces. This one lists possible improvements that aren't related to grant/revoke methods.\n",
            "comments:  ['bq. CREATE COLUMNFAMILY: P.CREATE on the KS in CQL2 vs. P.CREATE on the CF in CQL3 and Thrift\\n\\nCQL2 sounds correct to me, how can you have permissions on an object that doesn\\'t exist yet?\\n\\nBut that would imply that KS create permissioning is also broken, which you mention.\\n\\nMaybe we should have a \"cluster\" or \"all\" top-level permission: having create on all, allows creating keyspaces.  This would fit with the heirarchy design you describe too (GRANT UPDATE ON ALL TO foo), and gives a nice shorthand for granting system-wide permissions (or a subset of them) w/o making someone a superuser.\\n\\nbq. BATCH: P.UPDATE or P.DELETE on CF in CQL2 vs. P.UPDATE in CQL3 and Thrift (despite remove* in Thrift asking for P.DELETE)\\n\\nISTM that the correct behavior is to permission-check each statement in the batch separately.\\n\\nbq. DELETE: P.DELETE in CQL2 and Thrift vs. P.UPDATE in CQL3\\n\\nSeems obvious.\\n\\nbq. DROP INDEX: no checks in CQL2 vs. P.ALTER on the CF in CQL3\\n\\nALTER sounds reasonable.\\n\\nbq. We should move it to one place. SomeClassWithABetterName.authorize\\n\\nI\\'m not sure this really improves things, you\\'ve just created an abstraction layer with different names but fundamentally you still have to insert the correct auth call in each query processing path.\\n\\nbq. P.UPDATE on the KS should allow you to do updates on KS\\'s cfs\\n\\n+1\\n\\n', \"{quote}\\nbq.DELETE: P.DELETE in CQL2 and Thrift vs. P.UPDATE in CQL3\\n\\nSeems obvious.\\n{quote}\\n\\nNot to me. Should be P.DELETE in Thrift and CQL3? If so, then what do you think about requiring both P.UPDATE and P.DELETE for inserts/updates with TTL set?\\n\\n{quote}\\nbq. We should move it to one place. SomeClassWithABetterName.authorize\\n\\nI'm not sure this really improves things, you've just created an abstraction layer with different names but fundamentally you still have to insert the correct auth call in each query processing path.\\n{quote}\\n\\nYou are right. This won't be needed once we fix permission inheritance. Then one method in ClientState (modified current hasAccees) will be sufficient. No need for another enum.\\n\\n[~jbellis] Can you look at CASSANDRA-4875 as well please?\", 'bq. Should be P.DELETE in Thrift and CQL3? \\n\\nYes.\\n\\nbq. If so, then what do you think about requiring both P.UPDATE and P.DELETE for inserts/updates with TTL set?\\n\\nYes, we should.  (This is why I\\'m not 100% sure it makes sense to distinguish between UPDATE and DELETE at all, but \"require both for ttl\" is probably the best compromise.)', \"bq. I'm not 100% sure it makes sense to distinguish between UPDATE and DELETE at all\\n\\nI'd agree with that. From a security perspective I don't see the difference between updating a value with crap or deleting it, so imo both permission will always be set together and so it seems to me that having both only help people at making the mistake of revoking one permission without the other. Just my 2 cents though.\", \"bq. I don't see the difference between updating a value with crap or deleting it\\n\\nMost time there is no difference, unless you have a null-column in the first place and only care about the column name.\", 'bq. unless you have a null-column in the first place and only care about the column name\\n\\nNot sure I follow.', \"Imagine a wide row representing a time series, where every column's name is a timestamp and every column's value is already null - you don't care about the value. In this case there is a difference between overwriting the value with crap (doesn't matter) and removing the column entirely (matters).\", \"Btw I'm not advocating for keeping P.DELETE, just saying that there is sometimes a difference from security standpoint.\", \"Ok, I understand that example. It's a fairly specific use case imo (in term of security I mean), and there will always been cases where whatever permissions we allow won't be precise enough for someone. I'm still of the opinion that it's not worth the potential foot shooting of setting P.DELETE without P.UPDATE by mistake, b \n",
            "my_comment: P.DELETE in CQL2 and Thrift vs. P.UPDATE in CQL3\n",
            "\n",
            "Seems obvious.\n",
            "{quote}\n",
            "\n",
            "Not to me. Should be P.DELETE in Thrift and CQL3? If so then what do you think about requiring both P.UPDATE and P.DELETE for inserts/updates with TTL set?\n",
            "\n",
            "{quote}\n",
            "bq. We should move it to one place. SomeClassWithABetterName.authorize\n",
            "\n",
            "Im not sure this really improves things youve just created an abstraction layer with different names but fundamentally you still have to insert the correct auth call in each query processing path.\n",
            "{quote}\n",
            "\n",
            "You are right. This wont be needed once we fix permission inheritance. Then one method in ClientState (modified current hasAccees) will be sufficient. No need for another enum.\n",
            "\n",
            "[~jbellis] Can you look at CASSANDRA-4875 as well please?\" bq. Should be P.DELETE in Thrift and CQL3? \n",
            "\n",
            "Yes.\n",
            "\n",
            "bq. If so then what do you think about requiring both P.UPDATE and P.DELETE for inserts/updates with TTL set?\n",
            "\n",
            "Yes we should.  (This is why I\\m not 100% sure it makes sense to distinguish between UPDATE and DELETE at all but \"require both for ttl\" is probably the best compromise.) \"bq. Im not 100% sure it makes sense to distinguish between UPDATE and DELETE at all\n",
            "\n",
            "Id agree with that. From a security perspective I dont see the difference between updating a value with crap or deleting it so imo both permission will always be set together and so it seems to me that having both only help people at making the mistake of revoking one permission without the other. Just my 2 cents though.\" \"bq. I dont see the difference between updating a value with crap or deleting it\n",
            "\n",
            "Most time there is no difference unless you have a null-column in the first place and only care about the column name.\" bq. unless you have a null-column in the first place and only care about the column name\n",
            "\n",
            "Not sure I follow. \"Imagine a wide row representing a time series where every columns name is a timestamp and every columns value is already null - you dont care about the value. In this case there is a difference between overwriting the value with crap (doesnt matter) and removing the column entirely (matters).\" \"Btw Im not advocating for keeping P.DELETE just saying that there is sometimes a difference from security standpoint.\" \"Ok I understand that example. Its a fairly specific use case imo (in term of security I mean) and there will always been cases where whatever permissions we allow wont be precise enough for someone. Im still of the opinion that its not worth the potential foot shooting of setting P.DELETE without P.UPDATE by mistake b \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-15027\n",
            "issue_type:  Bug \n",
            "summary:  Handle IR prepare phase failures less race prone by waiting for all results \n",
            "description:  Handling incremental repairs as a coordinator begins by sending a {{PrepareConsistentRequest}} message to all participants, which may also include the coordinator itself. Participants will run anti-compactions upon receiving such a message and report the result of the operation back to the coordinator.\n",
            "comments:  ['* [\\xa0[trunk|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-15027] ][ [circleci|https://circleci.com/workflow-run/2b027f87-cf45-48ee-8eae-45a563701bc6] ]', 'Thanks [~spodxx@gmail.com]. I’ve extended your code so that in addition to waiting for other anti-compactions to complete, the coordinator also pro-actively cancels ongoing anti-compactions on the other participants. This avoids wasting time waiting for anti-compactions on other machines. The code does 3 things:\\r\\n * Adds a session state check to the\\xa0{{isStopRequested}} method in the anti-compaction iterator.\\r\\n * The coordinator now sends failure messages to all participants when it receives a failure message from one of them in the prepare phase. It does not mark these participants as having failed internally though, since that would cause the nodetool session to immediately complete. Instead, it waits until it’s received messages from all the other nodes.\\r\\n * The participants will now respond with a failed prepare message if the anti-compaction completes, but the session was failed in the mean time. This prevents a dead lock on the coordinator in the case where the participant received a failure message between the time the anti-compaction completes and the callback fires.\\r\\n\\r\\nLet me know what you think. If everything looks ok to you, I’m +1 on committing.\\r\\n\\r\\n[trunk|https://github.com/bdeggleston/cassandra/tree/15027-trunk]\\r\\n [circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15027-trunk]', \"Your updates look like valuable improvement over the initial\\xa0patch. I'm +1 in general\\xa0as for the changes, but also\\xa0fixed some additional minor issues and added a new tests:\\r\\n\\r\\n* [CASSANDRA-15027|https://github.com/spodkowinski/cassandra/commits/CASSANDRA-15027]\\r\\n* [https://circleci.com/workflow-run/2b444c33-a54c-46b5-9923-bcded8bcf465]\\r\\n\\r\\nPlease see comments with each commit in branch above for details.\\r\\n\\r\\nAlso happy to discuss any of the changes (most likely the last commit) in another jira, if you feel it's out of scope for this ticket.\\r\\n\\xa0\", \"Nice. Your follow on changes look good to me, I have 2 nits, but those can just be fixed on commit.\\r\\n\\r\\n* We should log the session id in compaction manager when an anti-compaction is cancelled (and probably when there's an error as well)\\r\\n* Some error handling should be added to the commit fixing the race between proposeFuture and hasFailure so nodetool doesn't hang if there's an error in the callback\\r\\n\\r\\nedit: proposed fixes [here|https://github.com/bdeggleston/cassandra/commit/02d7d9e09983db0d4661486b17adc375e17be24f]\", 'LGTM +1', 'Committed to trunk as\\xa09bde713ee8883f70d130efb6290ec0e6daea524f, thanks'] \n",
            "my_comment: proposed fixes [here|https://github.com/bdeggleston/cassandra/commit/02d7d9e09983db0d4661486b17adc375e17be24f]\" LGTM +1 Committed to trunk as\\xa09bde713ee8883f70d130efb6290ec0e6daea524f thanks \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9858\n",
            "issue_type:  Bug \n",
            "summary:  SelectStatement.Parameters fields should be inspectable by custom indexes and query handlers \n",
            "description:  SelectStatement.Parameters fields should be inspectable by custom indexes and query handlers \n",
            "comments:  ['https://github.com/JeremiahDJordan/cassandra/tree/c9858-21\\nhttps://github.com/JeremiahDJordan/cassandra/tree/c9858-22\\nhttps://github.com/JeremiahDJordan/cassandra/tree/c9858-30\\n', 'Committed to 2.1 as {{e726cf6d6b1a21abd0b7cf35775b7a980b1009ed}} and merged into 2.2 and trunk, thanks.'] \n",
            "my_comment:  https://github.com/JeremiahDJordan/cassandra/tree/c9858-21\n",
            "https://github.com/JeremiahDJordan/cassandra/tree/c9858-22\n",
            "https://github.com/JeremiahDJordan/cassandra/tree/c9858-30\n",
            " Committed to 2.1 as {{e726cf6d6b1a21abd0b7cf35775b7a980b1009ed}} and merged into 2.2 and trunk thanks. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14145\n",
            "issue_type:  Improvement \n",
            "summary:   Detecting data resurrection during read \n",
            "description:  We have seen several bugs in which deleted data gets resurrected. We should try to see if we can detect this on the read path and possibly fix it. Here are a few examples which brought back data\n",
            "comments:  [\"I agree with the problem description and that we should detect inconsistencies in the repaired set when reading data. But, as described in CASSANDRA-13912, I'm not convinced that correcting data as part of read repairs is the way to go for already repaired data. \\r\\n\\r\\nThere could be many different reasons for inconsistencies. Corrupted or missing sstables are relatively easy to reason about in this context. What I'm more concerned of are internal consistency issues due to rare edge cases, e.g. during upgrade paths or race conditions. Returning inconsistent results in rare cases might be less of an issue, compared to risking greater harm by running repair code which was created with other situations in mind.\\r\\n\\r\\nI'm wondering if we shouldn't just start by logging inconsistencies, allowing operators to further investigate. If there's some serious issue with a node, it needs to be replaced anyways. If the hardware is fine and there are no external causes that lead to inconsistencies, we may have a bigger problem to fix.\\r\\n\", \"I agree with Stefan that logging inconsistencies so that operators can investigate further is the sensible way to approach this initially so I've taken a pass at this in the branch linked below.\\r\\n\\r\\nOn digest mismatch, the coordinator adds a new parameter to the requests for the full data reads. When executing the query, replicas generate a digest of the portion of the data read from their repaired sstables. When the coordinator resolves the data requests, it checks for multiple digests and logs + increments a metric if it finds > 1. To mitigate against false positives caused by sstables moving from pending to repaired at slightly different times on each replica, we also track if the reads touched any tables with pending, but locally uncommitted, repair sessions. If any replica had pending sessions during the read, we increment a different metric (I called these confirmed and unconfirmed inconsistencies).\\r\\n \\r\\nPartition range reads don't make digest requests, so in order to detect inconsistency on that side of the read path the coordinator always adds the parameter to request the info on the repaired status. Although the overhead of tracking the repaired status should be minimal, this means the every range read will perform the additional work. With that in mind, to be conservative I've added separate config option/JMX operations to enable/disable it for single partition and range reads.\\r\\n\\r\\nI haven't added any dtests yet, but there's quite a bit of unit test coverage in the patch\\r\\n\\r\\n||branch||utest||dtest||\\r\\n|[14145-trunk|https://github.com/beobal/cassandra/tree/14145-trunk]|[utest|https://circleci.com/gh/beobal/cassandra/300]|[vnodes|https://circleci.com/gh/beobal/cassandra/301] / [no_vnodes|https://circleci.com/gh/beobal/cassandra/302]|\\r\\n\", \"pushed a branch [here|https://github.com/krummas/cassandra/commits/sam/14145] with initial \n",
            "my_comment:  [\"I agree with the problem description and that we should detect inconsistencies in the repaired set when reading data. But as described in CASSANDRA-13912 Im not convinced that correcting data as part of read repairs is the way to go for already repaired data. \r\n",
            "\r\n",
            "There could be many different reasons for inconsistencies. Corrupted or missing sstables are relatively easy to reason about in this context. What Im more concerned of are internal consistency issues due to rare edge cases e.g. during upgrade paths or race conditions. Returning inconsistent results in rare cases might be less of an issue compared to risking greater harm by running repair code which was created with other situations in mind.\r\n",
            "\r\n",
            "Im wondering if we shouldnt just start by logging inconsistencies allowing operators to further investigate. If theres some serious issue with a node it needs to be replaced anyways. If the hardware is fine and there are no external causes that lead to inconsistencies we may have a bigger problem to fix.\r\n",
            "\" \"I agree with Stefan that logging inconsistencies so that operators can investigate further is the sensible way to approach this initially so Ive taken a pass at this in the branch linked below.\r\n",
            "\r\n",
            "On digest mismatch the coordinator adds a new parameter to the requests for the full data reads. When executing the query replicas generate a digest of the portion of the data read from their repaired sstables. When the coordinator resolves the data requests it checks for multiple digests and logs + increments a metric if it finds > 1. To mitigate against false positives caused by sstables moving from pending to repaired at slightly different times on each replica we also track if the reads touched any tables with pending but locally uncommitted repair sessions. If any replica had pending sessions during the read we increment a different metric (I called these confirmed and unconfirmed inconsistencies).\r\n",
            " \r\n",
            "Partition range reads dont make digest requests so in order to detect inconsistency on that side of the read path the coordinator always adds the parameter to request the info on the repaired status. Although the overhead of tracking the repaired status should be minimal this means the every range read will perform the additional work. With that in mind to be conservative Ive added separate config option/JMX operations to enable/disable it for single partition and range reads.\r\n",
            "\r\n",
            "I havent added any dtests yet but theres quite a bit of unit test coverage in the patch\r\n",
            "\r\n",
            "||branch||utest||dtest||\r\n",
            "|[14145-trunk|https://github.com/beobal/cassandra/tree/14145-trunk]|[utest|https://circleci.com/gh/beobal/cassandra/300]|[vnodes|https://circleci.com/gh/beobal/cassandra/301] / [no_vnodes|https://circleci.com/gh/beobal/cassandra/302]|\r\n",
            "\" \"pushed a branch [here|https://github.com/krummas/cassandra/commits/sam/14145] with initial \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6487\n",
            "issue_type:  Improvement \n",
            "summary:  Log WARN on large batch sizes \n",
            "description:  Large batches on a coordinator can cause a lot of node stress. I propose adding a WARN log entry if batch sizes go beyond a configurable size. This will give more visibility to operators on something that can happen on the developer side. \n",
            "comments:  [\"If it's not out of the way, it would help to include the keyspace and column family and maybe the session ID/info.\", \"Sure. Can't see any reason not to add more info if it's easy to add. \", \"Added the batch_size_warn_threshold setting to cassandra.yaml and altered QueryProcessor#processBatch to log a WARN is the batch's size is more than the added setting.\\n\\nSome example output of what the warning looks like (keep in mind that multiple kss/cfs can be updated with one batch)\\n{code}\\nWARN  14:29:12 Batch of statements for ks.cf pairs to be updated [test.cf, test.cf2, test2.cf] is of size 11024 and exceeds specified threshold of 7168.\\nWARN  14:31:59 Batch of statements for ks.cf pairs to be updated [test.cf] is of size 1448 and exceeds specified threshold of 1024.\\n{code}\", 'Can you make it configure by 1s instead of 1000s?\\n\\nBikeshed: would prefer format of\\n\\n{noformat}\\nBatch of statements for [test.cf, test.cf2, test2.cf] is of size 11024, exceeding specified threshold of 7168\\n{noformat}', 'Sure thing, changed from kb to bytes and updated the warning message in v2.', 'Oops, I skimmed too fast and thought we were counting statements not bytes.  Is that what you were thinking when you estimated 5k [~pmcfadin]?', \"Yes that was in bytes. Just in my own experience, I don't recommend more than ~100 mutations per batch. Doing some quick math I came up with 5k as 100 x 50 byte mutations. \\n\\nTotally up for debate.\", \"Okay, then the first patch is actually what we want for that.\\n\\nProblem is, we can't compute size-in-bytes without MemoryMeter, which is reflection-based so I wouldn't want to put it in the fast path.\\n\\nIf you're okay with counting statements instead I think that will be more lightweight.\", 'We can count statements but the problem there is that if a batch has identical statements grouped together, then counting statements means we are just guessing whether the batch is large or not. Example:\\n\\n{noformat}\\n// 10 statements in a batch when measured with memory meter creates a batch of size 81912 bytes.\\n// for a table with schema \"CREATE TABLE tbl (col1 text PRIMARY KEY);\"\\nPreparedStatement prepStatement = session.prepare(\"INSERT INTO db.tbl (col1) VALUES (?)\");\\nBatchStatement batch = new BatchStatement();\\nbatch.add(prepStatement.bind(\"val1\"));\\n...\\nbatch.add(prepStatement.bind(\"val10\"));\\n{noformat}\\n\\nIncreasing it to 100 produces a batch of size 82456, _544_ bytes more for 10x statements. That being said, this is for batches where queries are identical, and we are only displaying a warning, not actually restricting such batches, so I\\'ll attach the patch with the updated message style suggest by [~jbellis] where the default is set to 50 statements per batch (based on twissandra\\'s model below it allows for 16 posts in a batch where the batch of 48 queries is of size 89kb which seems reasonable.\\n\\n{noformat}\\nINSERT INTO tweets (tweet_id, username, body) VALUES (8ffb88b3-ae60-48c2-bb96-c4f2d08c4ceb, \\'lyubent\\', \\'epic msg\\');\\nINSERT INTO userline (username, tweet_id, time) VALUES (\\'lyubent\\', 8ffb88b3-ae60-48c2-bb96-c4f2d08c4ceb, 66bd94a0-c17d-11e3-9c7a-4366e868fc79);\\nINSERT INTO timeline (username, tweet_id, time) VALUES (\\'follower\\', 8ffb88b3-ae60-48c2-bb96-c4f2d08c4ceb, 66bd94a0-c17d-11e3-9c7a-4366e868fc79);\\n{noformat}\\n\\np.s. Example warn output: \\n\\n{noformat}\\nWARN 17:28:19,652 Batch of statements for [db.timeline, db.userline, db.tweets] is of size 51, exceeding specified threshold of 50 by 1.\\n{noformat}', 'Not saying that we should, but we can calculate the size of the resulting processed collection of Mutation-s w/out using reflection, and warn based on that.', 'Is this something important enough that an Ops team might want to monitor in an automated manner, like with an mbean, for OpsCenter and other monitoring tools? Maybe count of batch size warnings, largest batch size seen, most recent batch size over the limit.', \"[~iamaleksey] I assume you mean calling {{ByteBuffer#limit}} in {{Batch \n",
            "my_comment: \n",
            "\n",
            "{noformat}\n",
            "WARN 17:28:19652 Batch of statements for [db.timeline db.userline db.tweets] is of size 51 exceeding specified threshold of 50 by 1.\n",
            "{noformat} Not saying that we should but we can calculate the size of the resulting processed collection of Mutation-s w/out using reflection and warn based on that. Is this something important enough that an Ops team might want to monitor in an automated manner like with an mbean for OpsCenter and other monitoring tools? Maybe count of batch size warnings largest batch size seen most recent batch size over the limit. \"[~iamaleksey] I assume you mean calling {{ByteBuffer#limit}} in {{Batch \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-11971\n",
            "issue_type:  Improvement \n",
            "summary:  More uses of DataOutputBuffer.RECYCLER \n",
            "description:  There are a few more possible use cases for {{DataOutputBuffer.RECYCLER}}, which prevents a couple of (larger) allocations.\n",
            "comments:  ['Patch uses recycled {{DataOutputBuffer}}s instead of allocating new ones.\\nAlso introduces {{DataOutputBuffer.asNewBuffer()}} to replace some {{ByteBuffer.wrap(out.getData(), 0, out.getLength())}}.\\n\\n||trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:11971-more-recycler-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-11971-more-recycler-trunk-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-11971-more-recycler-trunk-dtest/lastSuccessfulBuild/]', '+1', 'Thanks!\\nCommitted as [063e91754b22a28a43efccb0c238c577a6bd0b8a|https://github.com/apache/cassandra/commit/063e91754b22a28a43efccb0c238c577a6bd0b8a] to [trunk|https://github.com/apache/cassandra/tree/trunk]\\n'] \n",
            "my_comment:  Patch uses recycled {{DataOutputBuffer}}s instead of allocating new ones.\n",
            "Also introduces {{DataOutputBuffer.asNewBuffer()}} to replace some {{ByteBuffer.wrap(out.getData() 0 out.getLength())}}.\n",
            "\n",
            "||trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:11971-more-recycler-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-11971-more-recycler-trunk-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-11971-more-recycler-trunk-dtest/lastSuccessfulBuild/] +1 Thanks!\n",
            "Committed as [063e91754b22a28a43efccb0c238c577a6bd0b8a|https://github.com/apache/cassandra/commit/063e91754b22a28a43efccb0c238c577a6bd0b8a] to [trunk|https://github.com/apache/cassandra/tree/trunk]\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-10580\n",
            "issue_type:  Improvement \n",
            "summary:  Add latency metrics for dropped messages \n",
            "description:  In our production cluster, we are seeing a large number of dropped mutations. At a minimum, we should print the time the thread took to get scheduled thereby dropping the mutation (We should also print the Message / Mutation so it helps in figuring out which column family was affected). This will help find the right tuning parameter for write_timeout_in_ms. \n",
            "comments:  ['Thanks for the patch. Can you please rebase/merge to latest 2.1 HEAD? Some \n",
            "my_comment:  Thanks for the patch. Can you please rebase/merge to latest 2.1 HEAD? Some \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7479\n",
            "issue_type:  Bug \n",
            "summary:  Consistency level ANY does not send Commit to all endpoints for LOCAL_SERIAL \n",
            "description:  If the consistency level is ANY and using LOCAL_SERIAL, the Commit is only send to all local endpoints. \n",
            "comments:  ['[~slebresne] to review', 'Committed (with minor code style update), thanks'] \n",
            "my_comment:  [~slebresne] to review Committed (with minor code style update) thanks \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8180\n",
            "issue_type:  Improvement \n",
            "summary:  Optimize disk seek using min/max column name meta data when the LIMIT clause is used \n",
            "description:  I was working on an example of sensor data table (timeseries) and face a use case where C* does not optimize read on disk.\n",
            "comments:  [\"Querying for all the rows with the primary key {{WHERE id=1}} indeed requires scanning all the rows. By limiting your query only to those clustering column rows that also have {{col > 25}} is precisely the kind of query optimization that users should be doing with well thought out schema and queries that answer efficiently. This is by design. If I'm misunderstanding something here, please re-open and explain further :)\", \"I'm also really curious as to how you expect this optimization to work, because I don't see how this could work.\", \"Oh, unless you mean to order the sstables by mean min column names and to query them one at a time (like we do for names filter) in the specific case where their column names interval don't intersect. In which case, yes, that could be indeed useful with the compaction strategy from CASSANDRA-6602.\", 'The idea is:\\n\\nIf there is no restriction on clustering columns but only on LIMIT:\\n\\n1) Order SSTables by min/max column depending on the first clustering order\\n2) Hit the first SSTable and start sequential read until reaching LIMIT\\n3) If LIMIT is large enough, switch to another SSTable and so on\\n\\n', 'Another test showing that it could be optimized further:\\n\\n{code}\\ncqlsh:test> SELECT * FROM test WHERE id=1 AND col<40 LIMIT 1;\\n\\n id | col | val\\n----+-----+-----\\n  1 |  30 |  30\\n\\n(1 rows)\\n\\n\\nTracing session: 2725c710-5b86-11e4-aeed-814585a29e7b\\n\\n activity                                                                  | timestamp    | source    | source_elapsed\\n---------------------------------------------------------------------------+--------------+-----------+----------------\\n                                                        execute_cql3_query | 16:00:46,850 | 127.0.0.1 |              0\\n                 Parsing SELECT * FROM test WHERE id=1 AND col<40 LIMIT 1; | 16:00:46,850 | 127.0.0.1 |             77\\n                                                       Preparing statement | 16:00:46,850 | 127.0.0.1 |            244\\n                                  Executing single-partition query on test | 16:00:46,851 | 127.0.0.1 |           1485\\n                                              Acquiring sstable references | 16:00:46,851 | 127.0.0.1 |           1500\\n                                               Merging memtable tombstones | 16:00:46,851 | 127.0.0.1 |           1547\\n                                               Key cache hit for sstable 3 | 16:00:46,852 | 127.0.0.1 |           1641\\n                         Seeking to partition indexed section in data file | 16:00:46,852 | 127.0.0.1 |           1651\\n                                               Key cache hit for sstable 2 | 16:00:46,854 | 127.0.0.1 |           4054\\n                         Seeking to partition indexed section in data file | 16:00:46,854 | 127.0.0.1 |           4068\\n                                               Key cache hit for sstable 1 | 16:00:46,855 | 127.0.0.1 |           5232\\n                         Seeking to partition indexed section in data file | 16:00:46,855 | 127.0.0.1 |           5249\\n Skipped 0/3 non-slice-intersecting sstables, included 0 due to tombstones | 16:00:46,855 | 127.0.0.1 |           5499\\n                                Merging data from memtables and 3 sstables | 16:00:46,855 | 127.0.0.1 |           5515\\n                                        Read 2 live and 0 tombstoned cells | 16:00:46,855 | 127.0.0.1 |           5598\\n                                                          Request complete | 16:00:46,855 | 127.0.0.1 |           5997\\n{code}\\n\\n Now reversing the inequality on the clustering column, C* does scan 3 SSTables instead of just one (since LIMIT = 1)', \"[~slebresne], [~thobbs], [~iamaleksey] : \\n\\nI think it might make sense if I implement this change directly on a branch based on {{8099_engine_refactor}}? First of all I found it *much easier* to understand and secondly I don't particularly want to rebase or merge later on once 8099 is merged  \n",
            "my_comment: \n",
            "\n",
            "I think it might make sense if I implement this change directly on a branch based on {{8099_engine_refactor}}? First of all I found it *much easier* to understand and secondly I dont particularly want to rebase or merge later on once 8099 is merged  \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-18697\n",
            "issue_type:  Bug \n",
            "summary:  Skip ColumnFamilyStore#topPartitions initialization when client or tool mode \n",
            "description:  In {{org.apache.cassandra.db.ColumnFamilyStore}} the {{topPartitions}} is initialized when the keyspace is not a system keyspace. However, when running the cassandra library as client mode or tool mode, the initialization also happens. However, {{TopPartitionTracker}} performs queries to the {{system}} keyspace, which might not be available in most of the cases. For that reason, we should skip initialization of {{topPartitions}} when running on client mode or tool mode.\n",
            "comments:  ['CI: [https://app.circleci.com/pipelines/github/frankgh/cassandra/124/workflows/5f9bd478-41d2-43dc-988a-cb2bb0199560]', '+1 on the patch. ', '4.1 PR: [https://github.com/apache/cassandra/pull/2535]', '+1, thanks for the patch.', 'Preparing to commit\\r\\n||Branch||CI||\\r\\n|[4.1|https://github.com/yifan-c/cassandra/tree/r/upstream/pr2535-4.1]|[CI|https://app.circleci.com/pipelines/github/yifan-c/cassandra?branch=r%2Fupstream%2Fpr2535-4.1]|\\r\\n|[trunk|https://github.com/yifan-c/cassandra/tree/r/upstream/pr2519-trunk]|[CI|https://app.circleci.com/pipelines/github/yifan-c/cassandra?branch=r%2Fupstream%2Fpr2519-trunk]|\\r\\n\\r\\n-- Update --\\r\\n\\r\\nBoth CI runs look green. There are some _known_ test failures due to \"failed to create test certs\" on trunk. They are to be fixed in another ticket.\\xa0', 'Committed [9c796dfb2|https://github.com/apache/cassandra/commit/9c796dfb272daa3ce57a2dc5cbeadd9273e1ac72] into Cassandra-4.1 and merged up to trunk. '] \n",
            "my_comment: [https://github.com/apache/cassandra/pull/2535] +1 thanks for the patch. Preparing to commit\r\n",
            "||Branch||CI||\r\n",
            "|[4.1|https://github.com/yifan-c/cassandra/tree/r/upstream/pr2535-4.1]|[CI|https://app.circleci.com/pipelines/github/yifan-c/cassandra?branch=r%2Fupstream%2Fpr2535-4.1]|\r\n",
            "|[trunk|https://github.com/yifan-c/cassandra/tree/r/upstream/pr2519-trunk]|[CI|https://app.circleci.com/pipelines/github/yifan-c/cassandra?branch=r%2Fupstream%2Fpr2519-trunk]|\r\n",
            "\r\n",
            "-- Update --\r\n",
            "\r\n",
            "Both CI runs look green. There are some _known_ test failures due to \"failed to create test certs\" on trunk. They are to be fixed in another ticket.\\xa0 Committed [9c796dfb2|https://github.com/apache/cassandra/commit/9c796dfb272daa3ce57a2dc5cbeadd9273e1ac72] into Cassandra-4.1 and merged up to trunk.  \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5121\n",
            "issue_type:  Bug \n",
            "summary:  system.peers.tokens is empty after node restart \n",
            "description:  Using a 2 nodes fresh cluster (127.0.0.1 & 127.0.0.2) running latest 1.2, I’m querying system.peers to get the nodes of the cluster and their respective token. But it seems there is a problem after either node restart.\n",
            "comments:  ['In StorageService.handleStateNormal, when we see an endpoint come up which we already knew about: \\n\\n{noformat}\\nelse if (endpoint.equals(currentOwner))\\n{\\n    // set state back to normal, since the node may have tried to leave, but failed and is now back up\\n    // no need to persist, token/ip did not change\\n{noformat}\\n\\nI think the bug is that then we call \\n{noformat}\\nSystemTable.updateTokens(endpoint, tokensToUpdateInSystemTable);\\n{noformat}\\nwith an empty collection and SystemTable.updateTokens overwrites the current entry rather than adding to it.\\n\\nFix would be\\n{noformat}\\n- // no need to persist, token/ip did not change\\n+ if (!isClientMode)\\n+    tokensToUpdateInSystemTable.add(token);\\n{noformat}', \"I agree on Sam's analysis but I would suggest the slightly different patch attached, because I think the intend was that if tokensToUpdateInSystemTable is empty, we don't update anything. In particular, in the case where we are relocating, I don't think we want remove the tokens from the system table either (and in the case where there is a token conflict I think the initial intent was also to leave things as they are, even though in that case maybe actually removing the token is not a bad idea?).\\n\", '+1', 'Committed, thanks', 'Hi, commit ec35427fdfbc46a8adeafc042651f552b9bcc1a0 breaks RelocateTest:\\n\\n{noformat}\\n$ ant clean build test -Dtest.name=RelocateTest\\n...\\n    [junit] Testsuite: org.apache.cassandra.service.RelocateTest\\n    [junit] Tests run: 2, Failures: 2, Errors: 0, Time elapsed: 6.215 sec\\n    [junit] \\n    [junit] Testcase: testWriteEndpointsDuringRelocate(org.apache.cassandra.service.RelocateTest):\\tFAILED\\n    [junit] removeTokens should be used instead\\n    [junit] junit.framework.AssertionFailedError: removeTokens should be used instead\\n    [junit] \\tat org.apache.cassandra.db.SystemTable.updateTokens(SystemTable.java:324)\\n    [junit] \\tat org.apache.cassandra.db.SystemTable.updateLocalTokens(SystemTable.java:342)\\n    [junit] \\tat org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1393)\\n    [junit] \\tat org.apache.cassandra.service.StorageService.onChange(StorageService.java:1166)\\n    [junit] \\tat org.apache.cassandra.service.RelocateTest.createInitialRing(RelocateTest.java:106)\\n    [junit] \\tat org.apache.cassandra.service.RelocateTest.testWriteEndpointsDuringRelocate(RelocateTest.java:128)\\n    [junit] \\n    [junit] \\n    [junit] Testcase: testRelocationSuccess(org.apache.cassandra.service.RelocateTest):\\tFAILED\\n    [junit] removeTokens should be used instead\\n    [junit] junit.framework.AssertionFailedError: removeTokens should be used instead\\n    [junit] \\tat org.apache.cassandra.db.SystemTable.updateTokens(SystemTable.java:324)\\n    [junit] \\tat org.apache.cassandra.db.SystemTable.updateLocalTokens(SystemTable.java:342)\\n    [junit] \\tat org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1393)\\n    [junit] \\tat org.apache.cassandra.service.StorageService.onChange(StorageService.java:1166)\\n    [junit] \\tat org.apache.cassandra.service.RelocateTest.createInitialRing(RelocateTest.java:106)\\n    [junit] \\tat org.apache.cassandra.service.RelocateTest.testRelocationSuccess(RelocateTest.java:177)\\n    [junit] \\n    [junit] \\n    [junit] Test org.apache.cassandra.service.RelocateTest FAILED\\n\\nBUILD FAILED\\n...\\n{noformat}\\n\\n\\nAfter commit e6b6eaa583e8fc15f03c3e27664bf7fc06b3af0a, testWriteEndpointsDuringRelocate passes but testRelocationSuccess still fails:\\n{noformat}\\n$ ant clean build test -Dtest.name=RelocateTest\\n...\\n    [junit] Testcase: testRelocationSuccess(org.apache.cassandra.service.RelocateTest):\\tFAILED\\n    [junit] removeEndpoint should be used instead\\n    [junit] junit.framework.AssertionFailedError: removeEndpoint should be used instead\\n    [junit] \\tat org.apache.cassandra.db.SystemTable.updateTokens(SystemTable.java:316)\\n    [junit] \\tat org.apache.cassandra.db.SystemTable.updateLocalTokens(SystemTab \n",
            "my_comment: removeEndpoint should be used instead\n",
            "    [junit] \\tat org.apache.cassandra.db.SystemTable.updateTokens(SystemTable.java:316)\n",
            "    [junit] \\tat org.apache.cassandra.db.SystemTable.updateLocalTokens(SystemTab \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2447\n",
            "issue_type:  Improvement \n",
            "summary:  Remove auto-bootstrap option \n",
            "description:  We already optimize auto-bootstrap to be no-op if there are no non-system tables.\n",
            "comments:  ['While I think the auto_bootstrap token is probably very confusing to users, and simply not exposing it by default is a good idea, I don\\'t think the ability to join the cluster without bootstrapping should be removed entirely.\\n\\nAn example use case of joining the ring without bootstrap is: You have a multi-DC cluster and need to bootstrap new nodes into a DC, but you want to avoid streaming from other DC:s. Provided you know what you\\'re doing and are willing to accept the consequences in terms of consistency, you can do stuff like copy sstables from local nodes manually and bring the node into the ring and trigger AES. Similarly for adding a completely new DC to an existing cluster.\\n\\nI have a vague feeling that it\\'s generally useful in various \"panicy\" types of situations where you want to bypass the normal procedures.\\n\\nMaybe just make the default true always and remove it form the default configuration; or maybe even rename it and revert it\\'s meaning, to something like force_join_without_bootstrap?\\n', 'bq. maybe just make the default true always and remove it form the default configuration\\n\\nGood idea.  Done in r1156837', 'Integrated in Cassandra #1019 (See [https://builds.apache.org/job/Cassandra/1019/])\\n    change auto_bootstrap default to true and remove from example config file\\npatch by jbellis as suggested by Peter Schullerfor CASSANDRA-2447\\n\\njbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156837\\nFiles : \\n* /cassandra/trunk/CHANGES.txt\\n* /cassandra/trunk/conf/cassandra.yaml\\n* /cassandra/trunk/src/java/org/apache/cassandra/config/Config.java\\n', 'I forgot that we ammended bootstrap to wait for SLB.BROADCAST_INTERVAL as well as RING_DELAY.  This is silly; it won\\'t matter if we bootstrap based on current load (as propagated by gossip from last \"broadcast\") or wait another minute for another broadcast.  And if you are bootstrapping before any load has been gossipped? It will pick a random token, but I\\'m okay with that because (1) you shouldn\\'t be letting autobootstrap pick the token in the first place and (2) the workaround is simple: wait until your cluster has been up 60s before bootstrapping more nodes.\\nv2 attached that fixes this and also amends tests to not wait the full ring_delay.\\n', '+1', 'committed'] \n",
            "my_comment: wait until your cluster has been up 60s before bootstrapping more nodes.\n",
            "v2 attached that fixes this and also amends tests to not wait the full ring_delay.\n",
            " +1 committed \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7705\n",
            "issue_type:  Improvement \n",
            "summary:  Safer Resource Management \n",
            "description:  We've had a spate of bugs recently with bad reference counting. these can have potentially dire consequences, generally either randomly deleting data or giving us infinite loops. \n",
            "comments:  ['Linked four related tickets', 'Patch available [here|https://github.com/belliottsmith/cassandra/tree/7705-resourcemgmt]\\n\\nThis patch traps reference leaks as well as double-releasing references. This version only modifies SSTableReader resource management, but it can be rolled out for any heavy-weight objects we manage, i.e. all instances of RefCountedMemory except those stored in SerializingCache.  ', \"I didn't review I just read through because it looked interesting.\\n\\nMaybe not a good fit here, but one of things I found useful in a smart pointer/container was to use conditional compilation to support a debug build where the stack of the allocator, deallocator, and mistaken extra deallocation were all stored in the reference so it was a little easier to debug because errors could log that information. You could store the thread name as well.\\n\\nIf references are not always shared AKA wrapper references does it make sense to have a wrapper report an error if it is released twice?\\n\\nIs using reference queues any better than just using finalization?\\n\\nAstractRefCounted.State.refs appears to be unused?\\n\\nOut of scope for this ticket, but are there resources requiring deallocation that don't use reference counting or that are very high traffic (once or more per request?). \\n\\n\", '+1 for conditional compilation for alloc/de-alloc stacks - would be nice to delete the hack-patches I have lying around to do that locally. :)', 'What do you use for conditional compilation?', \"GCC?\\n\\nIn Java you can't remove fields from a class conditionally. If you don't want dependencies you can write a simple preprocessor in Java.\", \"I have updated the repository with a rebased version, with some improved comments and a debug mode. \\n\\nThis is essentially free given java's object alignment behaviour and run time optimisation (the field doesn't occupy any memory we wouldn't otherwise be occupying, and the relevant statements will be optimised away).\", \"The number of places this would help just keep piling up, so I think there's a good argument to be made for introducing this into 2.1\", 'Noting: there seems to be agreement on IRC that this should be included in 2.1, so I will rebase shortly.', \"I've pushed a rebased-to-2.1 version [here|https://github.com/belliottsmith/cassandra/tree/7705-2.1]\", \"This LGTM for 2.1 inclusion - we have had so many issues related to the ref counting lately we really need this\\n\\nComments;\\n\\n* There is an AbstractRefCounted class in the patch which is not used, guess it was replaced by doing the implementation in the RefCounted interface instead\\n* In SSTableLoader the Ref.sharedRef() is passed to the constructor in SSTableStreamingSections which feels wrong, shouldn't we acquire a new Ref and have SSTSS release that once it is done with the sstable?\\n* Manager.extant - why a Map here? Could we use a Set?\\n* In RefState we directly access the CLQ in RefCountedState, could we encapsulate this and give the methods name better names? Adding 'this' to a 'refs' collection does not tell me much when everything is called ref* :)\\n* In general, I think it would be nicer not putting all the classes inside the RefCounted interface (atleast the public ones), breaking them out into their own would make it a bit easier to follow (but that might just be my personal preference)\\n* A few comments on the methods in RefCounted.* - especially Refs as it is the most publicly visible\\n\\npushed a branch with a few nits fixed here: https://github.com/krummas/cassandra/commits/bes/7705-2.1\", \"I've uploaded a further polished version [here|https://github.com/belliottsmith/cassandra/commits/7705-2.1-x], hopefully addressing all of your nits and concerns, and finishing up your ref -> tryRef refactor.\\n\\nResponding to a couple of the more specific points that I haven't changed:\\n\\nbq. In SSTableLoader the Ref.sharedRef() is passed to the constructor in SSTableStreamingSections which feels wrong, shouldn't we acquire a new Ref and have SSTSS  \n",
            "my_comment: https://github.com/krummas/cassandra/commits/bes/7705-2.1\" \"Ive uploaded a further polished version [here|https://github.com/belliottsmith/cassandra/commits/7705-2.1-x] hopefully addressing all of your nits and concerns and finishing up your ref -> tryRef refactor.\n",
            "\n",
            "Responding to a couple of the more specific points that I havent changed:\n",
            "\n",
            "bq. In SSTableLoader the Ref.sharedRef() is passed to the constructor in SSTableStreamingSections which feels wrong shouldnt we acquire a new Ref and have SSTSS  \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7364\n",
            "issue_type:  Bug \n",
            "summary:  assert error in StorageProxy.submitHint \n",
            "description:  in 2.1-rc1. assert error and hector based client ends with all nodes down message (its single node cluster). I assume that client connection got closed.\n",
            "comments:  ['The bug only affects WTE handling w/ CL.ANY.', \"bikeshedding to keep the equality check out of submitHint -- worried that it could hide bugs silently.  Prefer to force callers to think about what they're doing.\", \"Not sure if I agree, but, it's trivial, so committed anyway. Thanks.\"] \n",
            "my_comment:  The bug only affects WTE handling w/ CL.ANY. \"bikeshedding to keep the equality check out of submitHint -- worried that it could hide bugs silently.  Prefer to force callers to think about what theyre doing.\" \"Not sure if I agree but its trivial so committed anyway. Thanks.\"] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-11944\n",
            "issue_type:  Bug \n",
            "summary:  sstablesInBounds might not actually give all sstables within the bounds due to having start positions moved in sstables \n",
            "description:  Same problem as with CASSANDRA-11886 - if we try to fetch sstablesInBounds for CANONICAL_SSTABLES, we can miss some actually overlapping sstables. In 3.0+ we state which SSTableSet we want when calling the method.\n",
            "comments:  ['https://github.com/krummas/cassandra/commits/marcuse/intervaltreesstableset\\nhttp://cassci.datastax.com/view/Dev/view/krummas/job/krummas-marcuse-intervaltreesstableset-testall/\\nhttp://cassci.datastax.com/view/Dev/view/krummas/job/krummas-marcuse-intervaltreesstableset-dtest/\\n\\npatch to remove the option to pick which SSTableSet you want returned, only LIVE sstables supported now. If you want canonical sstables within bounds you provide an IntervalTree built over those sstables.\\n\\nAlso includes CASSANDRA-11886 so that part might change depending on review in that ticket', '[~benedict] - have bandwidth to review this as well along w/CASSANDRA-11886?', 'Life is very busy right now, but sure...', 'No doubt - my hope was that the context of this would be similar enough to 11886 that the delta would be pretty small to add it on top. Not looking to make a habit of it. :)', 'Habits imply the future, at which date my life will hopefully not be so hectic.  Currently mid-demolition and rebuild of my home, which is eating into my free and non-free time alike.', \"I'm a little confused by the patch and the JIRA comment - I don't see (in this branch) any removal of an option to provide an SSTableSet...\", 'Maybe you were looking at the wrong commit? I have rebased and squashed [here|https://github.com/krummas/cassandra/commits/marcuse/intervaltreesstableset]\\n{code}\\n-    public Collection<SSTableReader> getOverlappingSSTables(SSTableSet sstableSet, Iterable<SSTableReader> sstables)\\n+    public Collection<SSTableReader> getOverlappingSSTables(Iterable<SSTableReader> sstables)\\n{code}', \"Probably - the earlier commits seemed to be for the other ticket.  Thanks.\\n\\nI'll have a proper read of it later, but I would suggest renaming the methods to include the now implicit SSTableSet.LIVE, so it's still minimally front-and-centre when the functionality is used, by perhaps renaming select to selectLive, and sstablesInBounds to, perhaps, inBoundsLive(SSTables)?\", 'pushed a new commit with method renames to the branch above, and triggered new cassci builds', 'ping on this [~benedict]', '+1', 'committed, thanks!'] \n",
            "my_comment:  https://github.com/krummas/cassandra/commits/marcuse/intervaltreesstableset\n",
            "http://cassci.datastax.com/view/Dev/view/krummas/job/krummas-marcuse-intervaltreesstableset-testall/\n",
            "http://cassci.datastax.com/view/Dev/view/krummas/job/krummas-marcuse-intervaltreesstableset-dtest/\n",
            "\n",
            "patch to remove the option to pick which SSTableSet you want returned only LIVE sstables supported now. If you want canonical sstables within bounds you provide an IntervalTree built over those sstables.\n",
            "\n",
            "Also includes CASSANDRA-11886 so that part might change depending on review in that ticket [~benedict] - have bandwidth to review this as well along w/CASSANDRA-11886? Life is very busy right now but sure... No doubt - my hope was that the context of this would be similar enough to 11886 that the delta would be pretty small to add it on top. Not looking to make a habit of it. :) Habits imply the future at which date my life will hopefully not be so hectic.  Currently mid-demolition and rebuild of my home which is eating into my free and non-free time alike. \"Im a little confused by the patch and the JIRA comment - I dont see (in this branch) any removal of an option to provide an SSTableSet...\" Maybe you were looking at the wrong commit? I have rebased and squashed [here|https://github.com/krummas/cassandra/commits/marcuse/intervaltreesstableset]\n",
            "{code}\n",
            "-    public Collection<SSTableReader> getOverlappingSSTables(SSTableSet sstableSet Iterable<SSTableReader> sstables)\n",
            "+    public Collection<SSTableReader> getOverlappingSSTables(Iterable<SSTableReader> sstables)\n",
            "{code} \"Probably - the earlier commits seemed to be for the other ticket.  Thanks.\n",
            "\n",
            "Ill have a proper read of it later but I would suggest renaming the methods to include the now implicit SSTableSet.LIVE so its still minimally front-and-centre when the functionality is used by perhaps renaming select to selectLive and sstablesInBounds to perhaps inBoundsLive(SSTables)?\" pushed a new commit with method renames to the branch above and triggered new cassci builds ping on this [~benedict] +1 committed thanks! \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-17757\n",
            "issue_type:  New Feature \n",
            "summary:  A user should not be able to manually remove ephemeral snapshots \n",
            "description:  in CASSANDRA-16911 we introduced the \"-e\" flag to nodetool listsnaphots which are returning ephemerals as well. An operator might try to remove these snapshots by hand. This should not be possible as these snapshots are there for repair to work on and manual removal breaks it. To be complete, these snapshots are removed as part of repair mechanism automatically, or they are removed on the next reboot upon node' start. They should never be removed by a human. \n",
            "comments:  ['https://github.com/apache/cassandra/pull/1781', 'Hi [~paulo] , I would be delighted to have a review from you. It is rather straightforward patch. I had to parse manifests to see if a snapshot is ephemeral or not in order to skip it from deletion. SnapshotLoader seems to be best suitable for the job, I just accommodated it to my use case - from now on SnapshotLoader is able to list snapshots of some specific keyspace only. This is quite handy as we do not need to load all the snapshots when a user wants to clear snapshots for some keyspace only.', 'Looks mostly good. Added a few minor comments and created [this PR|https://github.com/instaclustr/cassandra/pull/47] to your branch with cosmetic suggestions. Let me know what do you think.', 'https://app.circleci.com/pipelines/github/instaclustr/cassandra/1214/workflows/341a96d4-d7b4-4ba4-b766-7c28465f41cb\\r\\nhttps://github.com/instaclustr/cassandra/tree/CASSANDRA-17757', 'LGTM, submitted CI:\\r\\n* https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1895/', 'I ran one more build here against current trunk https://ci-cassandra.apache.org/job/Cassandra-devbranch/1898/\\r\\n\\r\\nBased on these results I am going to merge it.', 'I have also run 300x circle on added junit test https://app.circleci.com/pipelines/github/instaclustr/cassandra/1232/workflows/bda47ced-ff64-428e-9398-5c293d7c00d8/jobs/4994', 'java 11 precommit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1232/workflows/0ee2b67a-28d3-496e-a372-da234292ec41\\r\\n\\r\\njava 8 precommit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1232/workflows/111cb22a-ae40-481f-8831-5d1af3c668bd'] \n",
            "my_comment:  https://github.com/apache/cassandra/pull/1781 Hi [~paulo]  I would be delighted to have a review from you. It is rather straightforward patch. I had to parse manifests to see if a snapshot is ephemeral or not in order to skip it from deletion. SnapshotLoader seems to be best suitable for the job I just accommodated it to my use case - from now on SnapshotLoader is able to list snapshots of some specific keyspace only. This is quite handy as we do not need to load all the snapshots when a user wants to clear snapshots for some keyspace only. Looks mostly good. Added a few minor comments and created [this PR|https://github.com/instaclustr/cassandra/pull/47] to your branch with cosmetic suggestions. Let me know what do you think. https://app.circleci.com/pipelines/github/instaclustr/cassandra/1214/workflows/341a96d4-d7b4-4ba4-b766-7c28465f41cb\r\n",
            "https://github.com/instaclustr/cassandra/tree/CASSANDRA-17757 LGTM submitted CI:\r\n",
            "* https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1895/ I ran one more build here against current trunk https://ci-cassandra.apache.org/job/Cassandra-devbranch/1898/\r\n",
            "\r\n",
            "Based on these results I am going to merge it. I have also run 300x circle on added junit test https://app.circleci.com/pipelines/github/instaclustr/cassandra/1232/workflows/bda47ced-ff64-428e-9398-5c293d7c00d8/jobs/4994 java 11 precommit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1232/workflows/0ee2b67a-28d3-496e-a372-da234292ec41\r\n",
            "\r\n",
            "java 8 precommit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1232/workflows/111cb22a-ae40-481f-8831-5d1af3c668bd \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3338\n",
            "issue_type:  Bug \n",
            "summary:  Uncompressed sizes are used to estimate space for compaction of compressed sstables \n",
            "description:  We are using the uncompressed data size when estimating if we have enough to compact sstables. This means we can easily refuse compaction when there is clearly enough room to compact. \n",
            "comments:  ['maybe we should rename .length() to uncompressedLength() as well?\\n\\notherwise +1', 'Actually renaming length() to uncompressedLength() proved to be a good idea, as I had missed quite a bunch of places where the onDiskLength should be used. Attached v2.', '+1', 'Committed, thanks'] \n",
            "my_comment:  maybe we should rename .length() to uncompressedLength() as well?\n",
            "\n",
            "otherwise +1 Actually renaming length() to uncompressedLength() proved to be a good idea as I had missed quite a bunch of places where the onDiskLength should be used. Attached v2. +1 Committed thanks \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3489\n",
            "issue_type:  Bug \n",
            "summary:  EncryptionOptions should be instantiated \n",
            "description:  As the title says, otherwise you get an NPE when the options are missing from the yaml.  It's included in my second patch on CASSANDRA-3045 and is a one line fix. \n",
            "comments:  ['There\\'s a bunch of \"if encryption options is null then ignore it\" special cases already, if you\\'re going to instantiate a default instead then let\\'s get rid of those.\\n\\nMay also need to be applied to 0.8 unless aforesaid special cases cover everything.', 'I could only find the special case added the first time we fixed this back in 0.8 for CASSANDRA-3007.  Attached patch removes that and instantiates the default instead.', \"Hmm.  I thought the other place was OTC, but that's going to NPE in the current code base.  So +1 for this patch.\", \"(Checked, and 0.8 OTC does have the null check.  So we're good there.)\", 'Committed.'] \n",
            "my_comment:  There\\s a bunch of \"if encryption options is null then ignore it\" special cases already if you\re going to instantiate a default instead then let\\s get rid of those.\n",
            "\n",
            "May also need to be applied to 0.8 unless aforesaid special cases cover everything. I could only find the special case added the first time we fixed this back in 0.8 for CASSANDRA-3007.  Attached patch removes that and instantiates the default instead. \"Hmm.  I thought the other place was OTC but thats going to NPE in the current code base.  So +1 for this patch.\" \"(Checked and 0.8 OTC does have the null check.  So were good there.)\" Committed. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6747\n",
            "issue_type:  Improvement \n",
            "summary:  MessagingService should handle failures on remote nodes. \n",
            "description:  While going through the code of MessagingService, I discovered that we don't handle callbacks on failure very well. If a Verb Handler on the remote machine throws an exception, it goes right through uncaught exception handler. The machine which triggered the message will keep waiting and will timeout. On timeout, it will so some stuff hard coded in the MS like hints and add to Latency. There is no way in IAsyncCallback to specify that to do on timeouts and also on failures. \n",
            "comments:  ['MS is primarily designed around the needs of mutations and reads, where it\\'s probably not worth distinguishing between failure and timeout since (a) they should both be rare and (b) when the replica does fail completely it turns into a timeout anyway.\\n\\nBut for repair specifically where Prepare can take arbitrarily long (so it\\'s difficult to just pick a timeout and assume, \"if we haven\\'t heard back it must have failed\") then I agree we should make a bigger effort to notify peers of failures.', 'While going through the code of MS, I have found another problem. \\nWhen we send a message through sendRR in MS, we add its callback to an expiring map with a timeout of RPC timeout in most cases. \\nWhen the timeout is hit if the response did not come, the map will remove the callback from the map. \\nSo all late arriving messages will not be able to run the callback.  \\nThe problem with this is that in  ActiveRepairService.prepareForRepair(as given in JIRA description), waiting for 1 hour does not make sense since it will never get the response back once RPC time has passed. \\nAlso snapshot phase in repair will hang forever. \\nThere might be more places where this is a problem. ', 'I am adding a new interface which could be used if we want negative ack back from the remote node or need a callback on timeout. \\n\\nThis new interface is used in these two places. We can use this at other places but I have only updated two places.I can create new JIRA for these if that make more sense. \\nSnapshotTask.java and ActiveRepairService.java\\n\\nThis also fixes the problem in the previous comment for these use cases. ', \"[~kohlisankalp] I like your approach. \\nOne thing you need to change is in SnapshotTask's callback#onFailure, you can't just throw RuntimeException, you have to call task.setException so repair knows there's exception during snapshotting.\", 'Please review v2 with your suggestions. ', 'Thanks [~kohlisankalp], I updated your patch with following:\\n\\n* MessageOut object is immutable and MessageOut#withParameter returns new object, so we have to use that instead of original.\\n* RTE throwed from ActiveRepairService#prepareForRepair has to be catched and notified to client so repair command not to hang.\\n\\nFor remote snapshot fail, the patch certainly catches the error on coordinator side, but it still hangs(marked as TODO in RepairJob#sendTreeRequest). This is handled in CASSANDRA-6455.', 'Looks good. ', 'Committed, thanks!'] \n",
            "my_comment:  MS is primarily designed around the needs of mutations and reads where it\\s probably not worth distinguishing between failure and timeout since (a) they should both be rare and (b) when the replica does fail completely it turns into a timeout anyway.\n",
            "\n",
            "But for repair specifically where Prepare can take arbitrarily long (so it\\s difficult to just pick a timeout and assume \"if we haven\\t heard back it must have failed\") then I agree we should make a bigger effort to notify peers of failures. While going through the code of MS I have found another problem. \n",
            "When we send a message through sendRR in MS we add its callback to an expiring map with a timeout of RPC timeout in most cases. \n",
            "When the timeout is hit if the response did not come the map will remove the callback from the map. \n",
            "So all late arriving messages will not be able to run the callback.  \n",
            "The problem with this is that in  ActiveRepairService.prepareForRepair(as given in JIRA description) waiting for 1 hour does not make sense since it will never get the response back once RPC time has passed. \n",
            "Also snapshot phase in repair will hang forever. \n",
            "There might be more places where this is a problem.  I am adding a new interface which could be used if we want negative ack back from the remote node or need a callback on timeout. \n",
            "\n",
            "This new interface is used in these two places. We can use this at other places but I have only updated two places.I can create new JIRA for these if that make more sense. \n",
            "SnapshotTask.java and ActiveRepairService.java\n",
            "\n",
            "This also fixes the problem in the previous comment for these use cases.  \"[~kohlisankalp] I like your approach. \n",
            "One thing you need to change is in SnapshotTasks callback#onFailure you cant just throw RuntimeException you have to call task.setException so repair knows theres exception during snapshotting.\" Please review v2 with your suggestions.  Thanks [~kohlisankalp] I updated your patch with following:\n",
            "\n",
            "* MessageOut object is immutable and MessageOut#withParameter returns new object so we have to use that instead of original.\n",
            "* RTE throwed from ActiveRepairService#prepareForRepair has to be catched and notified to client so repair command not to hang.\n",
            "\n",
            "For remote snapshot fail the patch certainly catches the error on coordinator side but it still hangs(marked as TODO in RepairJob#sendTreeRequest). This is handled in CASSANDRA-6455. Looks good.  Committed thanks! \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14204\n",
            "issue_type:  Bug \n",
            "summary:  Remove unrepaired SSTables from garbage collection when only_purge_repaired_tombstones is true to avoid AssertionError in nodetool garbagecollect \n",
            "description:  When manually running a garbage collection compaction across a table with unrepaired sstables and only_purge_repaired_tombstones set to true an assertion error is thrown. This is because the unrepaired sstables aren't being removed from the transaction as they are filtered out in filterSSTables().\n",
            "comments:  ['patch LGTM but could probably have a test that will ensure we never include unrepaired sstables when onlyPurgeRepairedTombstones is true.', 'So the patched implementation pretty much follows {{performSSTableRewrite()}}, which looks like the correct way to handle this to me. We could modify {{GcCompactionTest}} a bit to make some of the effects more testable, see [ebd7de7|https://github.com/spodkowinski/cassandra/commit/ebd7de758b48a6f924d60eeecbc615c355c87257].', 'PRs\\r\\n\\r\\ntrunk https://github.com/apache/cassandra/pull/2423\\r\\n4.1 https://github.com/apache/cassandra/pull/2424\\r\\n4.0 https://github.com/apache/cassandra/pull/2425\\r\\n3.11 https://github.com/apache/cassandra/pull/2426\\r\\n\\r\\nbuilds\\r\\n\\r\\ntrunk \\r\\nj11 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/2482/workflows/b8690c93-121b-4ed6-aed7-6e742285ce13\\r\\nj8 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/2482/workflows/20e45719-36cb-4010-95b5-89c5519e91d3 \\r\\n4.1\\r\\nj11 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/2484/workflows/bfa530d2-1ab9-4ce3-9f08-59d41f0bbcac\\r\\nj8 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/2497/workflows/2eb2a057-8b1a-44ad-af12-72c34459b551\\r\\n4.0 \\r\\nj11 pre-commit  https://app.circleci.com/pipelines/github/instaclustr/cassandra/2485/workflows/cb68a9f9-8e52-4d45-843d-24e626ca0402\\r\\nj8 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/2485/workflows/fbf627a1-ed93-42c9-99c8-524e22536891\\r\\n3.11 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2486/workflows/c32e4dc3-e680-4ceb-8218-3bd71c5a5bff', '[~jjirsa] would  you mind to take a look please? This should be quite straightforward.', '[~blambov] would you mind to take a look? I contacted Jeff and he is not working on Cassandra actively at the moment.', 'Thank you [~blambov] for review, would you mind to take a look again? https://github.com/apache/cassandra/pull/2423', 'Branimir +1ed on the PR. Builds are here\\r\\n\\r\\n3.11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2631/workflows/812d7b99-1da3-4cc1-b15e-ac5cb07c8f4e]\\r\\n4.0 j11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2632/workflows/85890046-207a-4508-8c76-fb3e06b76c48]\\r\\n4.0 j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2632/workflows/ae1a70e2-65ff-46e9-a5d1-b885fa372aef]\\r\\n4.1 j11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2633/workflows/9041d37f-a5a0-4346-81ec-0bdcd2189b36]\\r\\n4.1 j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2633/workflows/d55d764d-1661-4b2c-9b2b-964f30775600]\\r\\ntrunk j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2637/workflows/24a86898-bcb3-47d1-9631-a894aac0f12a]\\r\\ntrunk j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2637/workflows/e84fc541-abe8-43ca-9b4b-d38109fe374b]\\r\\n\\r\\n[https://app.circleci.com/pipelines/github/instaclustr/cassandra/2638/workflows/c84de8f7-8e9f-4f41-82d4-7ad3c5892859]\\r\\n[https://app.circleci.com/pipelines/github/instaclustr/cassandra/2638/workflows/38a73d2f-a21d-4bd3-abfc-12f79425ffb3]\\r\\n\\r\\nI am going to merge this.'] \n",
            "my_comment:  patch LGTM but could probably have a test that will ensure we never include unrepaired sstables when onlyPurgeRepairedTombstones is true. So the patched implementation pretty much follows {{performSSTableRewrite()}} which looks like the correct way to handle this to me. We could modify {{GcCompactionTest}} a bit to make some of the effects more testable see [ebd7de7|https://github.com/spodkowinski/cassandra/commit/ebd7de758b48a6f924d60eeecbc615c355c87257]. PRs\r\n",
            "\r\n",
            "trunk https://github.com/apache/cassandra/pull/2423\r\n",
            "4.1 https://github.com/apache/cassandra/pull/2424\r\n",
            "4.0 https://github.com/apache/cassandra/pull/2425\r\n",
            "3.11 https://github.com/apache/cassandra/pull/2426\r\n",
            "\r\n",
            "builds\r\n",
            "\r\n",
            "trunk \r\n",
            "j11 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/2482/workflows/b8690c93-121b-4ed6-aed7-6e742285ce13\r\n",
            "j8 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/2482/workflows/20e45719-36cb-4010-95b5-89c5519e91d3 \r\n",
            "4.1\r\n",
            "j11 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/2484/workflows/bfa530d2-1ab9-4ce3-9f08-59d41f0bbcac\r\n",
            "j8 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/2497/workflows/2eb2a057-8b1a-44ad-af12-72c34459b551\r\n",
            "4.0 \r\n",
            "j11 pre-commit  https://app.circleci.com/pipelines/github/instaclustr/cassandra/2485/workflows/cb68a9f9-8e52-4d45-843d-24e626ca0402\r\n",
            "j8 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/2485/workflows/fbf627a1-ed93-42c9-99c8-524e22536891\r\n",
            "3.11 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2486/workflows/c32e4dc3-e680-4ceb-8218-3bd71c5a5bff [~jjirsa] would  you mind to take a look please? This should be quite straightforward. [~blambov] would you mind to take a look? I contacted Jeff and he is not working on Cassandra actively at the moment. Thank you [~blambov] for review would you mind to take a look again? https://github.com/apache/cassandra/pull/2423 Branimir +1ed on the PR. Builds are here\r\n",
            "\r\n",
            "3.11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2631/workflows/812d7b99-1da3-4cc1-b15e-ac5cb07c8f4e]\r\n",
            "4.0 j11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2632/workflows/85890046-207a-4508-8c76-fb3e06b76c48]\r\n",
            "4.0 j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2632/workflows/ae1a70e2-65ff-46e9-a5d1-b885fa372aef]\r\n",
            "4.1 j11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2633/workflows/9041d37f-a5a0-4346-81ec-0bdcd2189b36]\r\n",
            "4.1 j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2633/workflows/d55d764d-1661-4b2c-9b2b-964f30775600]\r\n",
            "trunk j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2637/workflows/24a86898-bcb3-47d1-9631-a894aac0f12a]\r\n",
            "trunk j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2637/workflows/e84fc541-abe8-43ca-9b4b-d38109fe374b]\r\n",
            "\r\n",
            "[https://app.circleci.com/pipelines/github/instaclustr/cassandra/2638/workflows/c84de8f7-8e9f-4f41-82d4-7ad3c5892859]\r\n",
            "[https://app.circleci.com/pipelines/github/instaclustr/cassandra/2638/workflows/38a73d2f-a21d-4bd3-abfc-12f79425ffb3]\r\n",
            "\r\n",
            "I am going to merge this. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-12988\n",
            "issue_type:  Improvement \n",
            "summary:  make the consistency level for user-level auth reads and writes configurable \n",
            "description:  Most reads for the auth-related tables execute at {{LOCAL_ONE}}. We'd like to make it configurable, with the default still being {{LOCAL_ONE}}. \n",
            "comments:  ['Linked patch allows an operator to set the read and write consistency levels for the auth tables independently, by setting a value in the yaml or calling a JMX {{MXBean}}.\\n\\n||3.X||trunk||\\n|[branch|https://github.com/jasobrown/cassandra/tree/config_auth_reads-3.X]|[branch|https://github.com/jasobrown/cassandra/tree/config_auth_reads-trunk]|\\n|[dtest|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-config_auth_reads-3.X-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-config_auth_reads-trunk-dtest/]|\\n|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-config_auth_reads-3.X-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/jasobrown/job/jasobrown-config_auth_reads-trunk-testall/]|\\n', \"Generally, I think this idea is fine. Since it's a new feature, I fear we have to target 4.0/4.x for this.\\nThe patch breaks {{DatabaseDescriptorRefTest}}, which ensures that daemon components are not started just by accessing lovely {{DatabaseDescriptor}} - for offline tools. {{ConsistencyLevel}}\\xa0is one of the classes that would indirectly start unwanted threads or load unwanted classes. TL;DR I think it's necessary to refactor this a bit.\", '[~jasobrown] is this patch-available?\\nThe code looks good so far. Some thing though:\\n* The MBean should not use {{org.apache.cassandra.db.ConsistencyLevel}} but instead just use a {{String}}. {{DatabaseDescriptor}} should continue to use {{org.apache.cassandra.db.ConsistencyLevel}}\\n', 'Looks patch-available to me (came up in an IRC question, just marking patch-available) ', 'Well, the patch is >2yrs old and probably needs a\\xa0rebase and\\xa0might be a bit late for 4.0.', 'Another 2 years, almost definitely needs a rebase.  Cancelling patch.', 'Have rebased and re-architected implementation here, tests running. Behaves on DatabaseDescriptorRefTest locally.\\r\\n\\r\\n||Item|Link||\\r\\n|JDK8 tests|[Link|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/75/workflows/73c71040-88d4-4165-b7c1-010cbecf845c]|\\r\\n|JDK11 tests|[Link|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/75/workflows/b96bbb92-0eb2-4b10-ae08-7f3956442a13]|\\r\\n|PR|[Link|https://github.com/apache/cassandra/compare/trunk...josh-mckenzie:cassandra-12988?expand=1]|          ', \"Some dtest failures that look related to the slight changes in CL defaults that come along w/the shape of the patch. Bouncing back to in progress while I work through those as I'm not sure how invasive those changes will end up.\\r\\n\\r\\nedit: just saw that you picked this up [~b.lerer@gmail.com]; I'll keep things as they are on the Jira here and ping when I've rooted this out.\", 'This patch as it stands ties read auth CL together at LOCAL_QUORUM, writes at EACH_QUORUM. Previously, we had a kind of hodge-podge of them across various classes:\\r\\n * ConsistencyLevel.LOCAL_ONE for both read and write operations in CassandraAuthorizer\\r\\n * ConsistencyLevel.ONE for batches in there\\r\\n * ConsistencyLevel.QUORUM for checks in CassandraRoleManager + PasswordAuthenticator if the username is \"cassandra\"\\r\\n * ConsistencyLevel.LOCAL_ONE for checks in CassandraRoleManager + PasswordAuthenticator if the username is otherwise\\r\\n\\r\\nRight now there\\'s a handful of dtests failing as they\\'re specifically looking for error messages that match the above expected CL\\'s for various auth failures. At this point I think the right course of action is to tidy up those dtests and also update NEWS.txt with some variation of the above note about the change in consistency levels from essentially ONE to QUORUM.\\r\\n\\r\\nAssuming no disagreement from you [~b.lerer@gmail.com], I\\'ll get on that in the morning.', \"Ok, PR updated and squashed w/NEWS.txt entry, and very trivial relaxation of the python dtests checking for auth failure (removing *which* CL we failed to achieve and instead just confirming we failed to hit the expected CL server side). I think this one's ready to go.\\r\\n\\r\\nLin \n",
            "my_comment: just saw that you picked this up [~b.lerer@gmail.com]; Ill keep things as they are on the Jira here and ping when Ive rooted this out.\" This patch as it stands ties read auth CL together at LOCAL_QUORUM writes at EACH_QUORUM. Previously we had a kind of hodge-podge of them across various classes:\r\n",
            " * ConsistencyLevel.LOCAL_ONE for both read and write operations in CassandraAuthorizer\r\n",
            " * ConsistencyLevel.ONE for batches in there\r\n",
            " * ConsistencyLevel.QUORUM for checks in CassandraRoleManager + PasswordAuthenticator if the username is \"cassandra\"\r\n",
            " * ConsistencyLevel.LOCAL_ONE for checks in CassandraRoleManager + PasswordAuthenticator if the username is otherwise\r\n",
            "\r\n",
            "Right now there\\s a handful of dtests failing as they\re specifically looking for error messages that match the above expected CL\\s for various auth failures. At this point I think the right course of action is to tidy up those dtests and also update NEWS.txt with some variation of the above note about the change in consistency levels from essentially ONE to QUORUM.\r\n",
            "\r\n",
            "Assuming no disagreement from you [~b.lerer@gmail.com] I\\ll get on that in the morning. \"Ok PR updated and squashed w/NEWS.txt entry and very trivial relaxation of the python dtests checking for auth failure (removing *which* CL we failed to achieve and instead just confirming we failed to hit the expected CL server side). I think this ones ready to go.\r\n",
            "\r\n",
            "Lin \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6013\n",
            "issue_type:  Bug \n",
            "summary:  CAS may return false but still commit the insert \n",
            "description:  If a Paxos proposer proposes some value/update and that propose fail, there is no guarantee on whether this value will be accepted or not ultimately. Paxos guarantees that we'll agree on \"a\" value (for a given round in our case), but does not guarantee that the proposer of the agreed upon value will know it.  In particular, if for a given proposal at least one accepter has accepted it but not a quorum does, then that value might (but that's not guaranteed either) be replayed (and committed) by another proposer.\n",
            "comments:  ['bq. if for a given proposal at least one accepter has accepted it but not a quorum does, then that value might (but that\\'s not guaranteed either) be replayed (and committed) by another proposer\\n\\nWhy not have the new leader require a quorum of replicas to say \"I have this unfinished business\" before replaying it?\\n\\n(I\\'m pretty sure I had this logic in originally but you talked me out of it in the name of code simplification.)', 'Okay, so the problem is not the retry per se, but when we have a \"split decision\" on the nodes that reply.  We can reduce the likelihood of that happening by waiting for all known live endpoints if that\\'s required to hear from a majority.\\n\\nIf we still don\\'t hear from a majority, we can return a timeout; it\\'s valid for a transaction to be committed after a timeout.\\n\\nPatch for the above attached.', \"Unfortunately, I think this is a little grimmer than that. The problem is that a proposer shouldn't move on unless the propose was successful (in which case it returns to the client) or it is sure that the propose will *not* be replayed (if it is sure of that, then retrying the proposed value with a newer ballot is safe; the current problem is that we retry with a newer ballot when we're not sure of that). In other words, we should timeout unless we are either successful or all nodes have answered and none have accepted. I'm attaching a v2 doing that (but still tries to timeout as little as possible without compromising correctness).\\n\\nUnfortunately, this mean we'll timeout as soon as a proposer gets a propose reject but at least one acceptor had accepted it, which is not an extremely rare condition even with moderate contention. That being said, the current behavior is plain wrong, so unless someone has a much better idea that is easy to implement, we should probably go ahead with this for now.\\n\", 'It looks to me like both uses of requiredTargets should actually be totalTargets.  v3 attached.', \"I don't follow. getSuccessful/getAcceptCount is supposed to returned how many successful accepts we got. So that's how much time we decremented remainingRequired, i.e. its initial value (requiredTargets) minus it's current value. Similarly, in isFullyRefused, we want to validate that remainingRequired was never decremented (no-one accepted), so we want to compare it's current value with its initial value, requiredTargets (comparing to totalTargets will in fact always fail).\\n\\nI guess the code is more straightforward if we keep the number of accepts instead of the number of remaining accept: attaching v4 with that version (which is equivalent to v2, but with the updated comment of v3).\\n\", '+1', 'Committed, thanks'] \n",
            "my_comment: attaching v4 with that version (which is equivalent to v2 but with the updated comment of v3).\n",
            "\" +1 Committed thanks \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4709\n",
            "issue_type:  Bug \n",
            "summary:  (CQL3) Missing validation for IN queries on column not part of the PK \n",
            "description:  Copy-pasting from the original mail (http://mail-archives.apache.org/mod_mbox/cassandra-user/201209.mbox/%3C20120922185826.GO6205@pslp2%3E):\n",
            "comments:  [\"For now I think we should just refuse the query since this would require secondary indexes to do an OR which they can't do right now. Attaching patch that simply refuse such queries.\", '+1', 'Forgot to mark that one resolved somehow, doing it now.'] \n",
            "my_comment:  [\"For now I think we should just refuse the query since this would require secondary indexes to do an OR which they cant do right now. Attaching patch that simply refuse such queries.\" +1 Forgot to mark that one resolved somehow doing it now. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-12251\n",
            "issue_type:  Bug \n",
            "summary:  Move migration tasks to non-periodic queue, assure flush executor shutdown after non-periodic executor \n",
            "description:  example failure:\n",
            "comments:  [\"This looks to me like in drain/StorageServiceShutdownHook, the schema stage is not shutdown, so if you have a task submitted to that executor that doesn't execute until after the postflush executor has been terminated in drain, you'll hit this exception. This is a C* fix for sure.\", \"Unfortunately, I could not reproduce this locally. However, given the exception it's more or less clear what has happened. It's just much harder to land nodes into the same state. \\n\\nThe tasks executed by {{MigrationManager}} are landing in the {{optional}} tasks of scheduled executor [here|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/MigrationManager.java]. Optional tasks are not being waited for during shutdown or drain. Although even putting the task into the {{nonPeriodicTasks}} executor won't fix the problem since post-flush executor is being shut down before the periodic executor [here|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/StorageService.java#L4251]. \\n\\nIn order to fix the issue, we need to both move the migration task to non-periodic and make sure that shutdown order is correct. Since mutation stage is already shut down, I left commit log shutdown on ints place and only moved the post flush executor shutdown after the periodic tasks.\", '[~ifesdjeen] - I actually need to make a superset of these changes for [CASSANDRA-12260] and [CASSANDRA-12313]. It might make sense to just centralize review there and link this one as a duplicate of [CASSANDRA-12260].', \"[~jkni] I've checked the two other issues. Most likely I'm missing something, but I could not see similar exception there (as regards the duplicate). But we can / should definitely centralise the review.\", \"[~ifesdjeen] - good point, they aren't really duplicates in their symptoms since instead of hitting an exception, those two issues deadlock for a minute. They're the same source though, which is that the ordering of the nonPeriodicTasks/postFlush executors with other parts of the shutdown are not correct. I have a patch that I'll be posting on [CASSANDRA-12260] today which changes the order of executor shutdown. This ticket probably still needs to change the executor for the migration task (I haven't thought this through fully yet).\\n\\nEDIT: I should clarify that I was wrong here - my changes on those issues aren't a superset, just have significant overlap.\", '|[trunk|https://github.com/ifesdjeen/cassandra/tree/12251-upgrade-trunk]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-dtest/]|[testall|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-testall/]|[upgrade|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-upgrade/]|', 'Patch and CI look good to me. Is there a reason you only ran CI on trunk [~ifesdjeen]? I think this should go into 2.2+.', \"You're right. In fact, it all merges cleanly from 2.2 upwards. I've triggered CI as well: \\n\\n|[2.2|https://github.com/ifesdjeen/cassandra/tree/12251-upgrade-2.2]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-2.2-dtest/]|[testall|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-2.2-testall/]|\\n|[3.0|https://github.com/ifesdjeen/cassandra/tree/12251-upgrade-3.0]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-3.0-dtest/]|[testall|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-3.0-testall/]|\\n|[trunk|https://github.com/ifesdjeen/cassandra/tree/12251-upgrade-trunk]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-dtest/]|[testall|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-testall/]|\\n\\n|[upgrade tests|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-upgrade/]|\\n\\nI re-ran upgrade tests  \n",
            "my_comment: \n",
            "\n",
            "|[2.2|https://github.com/ifesdjeen/cassandra/tree/12251-upgrade-2.2]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-2.2-dtest/]|[testall|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-2.2-testall/]|\n",
            "|[3.0|https://github.com/ifesdjeen/cassandra/tree/12251-upgrade-3.0]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-3.0-dtest/]|[testall|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-3.0-testall/]|\n",
            "|[trunk|https://github.com/ifesdjeen/cassandra/tree/12251-upgrade-trunk]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-dtest/]|[testall|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-testall/]|\n",
            "\n",
            "|[upgrade tests|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-upgrade/]|\n",
            "\n",
            "I re-ran upgrade tests  \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1641\n",
            "issue_type:  Bug \n",
            "summary:  auto-guessed memtable sizes are too high \n",
            "description:  I've seen two cases now of the memtable sizes being too large, causing OOMing.  Too-small memtables hurt performance, but too-large hurts worse when you start GC storming. \n",
            "comments:  [\"I'd like to introduce a dependency on number of user CFs, but that's not available until after we've picked a value.  So this cuts the size by half, which feels scientific because on the old default 1GB heap size this gives us memtable throughput of 64MB, which is also the old default.\", '+1', 'committed', 'Integrated in Cassandra #573 (See [https://hudson.apache.org/hudson/job/Cassandra/573/])\\n    reduce automaticallychosen memtablesizes by 50%\\npatch by jbellis; reviewed by brandonwilliams for CASSANDRA-1641\\n'] \n",
            "my_comment:  [\"Id like to introduce a dependency on number of user CFs but thats not available until after weve picked a value.  So this cuts the size by half which feels scientific because on the old default 1GB heap size this gives us memtable throughput of 64MB which is also the old default.\" +1 committed Integrated in Cassandra #573 (See [https://hudson.apache.org/hudson/job/Cassandra/573/])\n",
            "    reduce automaticallychosen memtablesizes by 50%\n",
            "patch by jbellis; reviewed by brandonwilliams for CASSANDRA-1641\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3117\n",
            "issue_type:  Bug \n",
            "summary:  StorageServiceMBean is missing a getCompactionThroughputMbPerSec() method \n",
            "description:  Without a getter, you can assign a new value but not query the existing one (which is strange). \n",
            "comments:  ['+1', 'committed.', 'Integrated in Cassandra-0.8 #310 (See [https://builds.apache.org/job/Cassandra-0.8/310/])\\n    add missing mbean method\\n\\nPatch by eevans; reviewed by jbellis for CASSANDRA-3117\\n\\neevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1164127\\nFiles : \\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageServiceMBean.java\\n'] \n",
            "my_comment: \n",
            "* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java\n",
            "* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageServiceMBean.java\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3997\n",
            "issue_type:  Improvement \n",
            "summary:  Make SerializingCache Memory Pluggable \n",
            "description:  Serializing cache uses native malloc and free by making FM pluggable, users will have a choice of gcc malloc, TCMalloc or JEMalloc as needed. \n",
            "comments:  ['Attached is the test classes used for the test.\\n\\nResults on CentOS:\\n\\n{noformat}\\n[vijay_tcasstest@vijay_tcass-i-a91ee8cd ~]$ /etc/alternatives/jre_1.7.0/bin/java -Djava.library.path=/usr/local/lib/ -cp jna.jar:/apps/nfcassandra_server/lib/*:. com.sun.jna.MallocAllocator 50000 2000000\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   26049380   45638840          0     169116     996172\\n-/+ buffers/cache:   24884092   46804128\\nSwap:            0          0          0\\n**** Starting Test! ****\\nTotal bytes read: 101422934016\\nTime taken: 25407\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   31981924   39706296          0     169116     996312\\n-/+ buffers/cache:   30816496   40871724\\nSwap:            0          0          0\\n**** ending Test!**** \\n[vijay_tcasstest@vijay_tcass-i-a91ee8cd ~]$ export LD_LIBRARY_PATH=/usr/local/lib/\\n[vijay_tcasstest@vijay_tcass-i-a91ee8cd ~]$ /etc/alternatives/jre_1.7.0/bin/java -Djava.library.path=/usr/local/lib/ -cp jna.jar:/apps/nfcassandra_server/lib/*:. com.sun.jna.TCMallocAllocator 50000 2000000\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   26054620   45633600          0     169128     996228\\n-/+ buffers/cache:   24889264   46798956\\nSwap:            0          0          0\\n**** Starting Test! ****\\nTotal bytes read: 101304894464\\nTime taken: 46387\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   28535136   43153084          0     169128     996436\\n-/+ buffers/cache:   27369572   44318648\\nSwap:            0          0          0\\n**** ending Test!**** \\n[vijay_tcasstest@vijay_tcass-i-a91ee8cd ~]$ export LD_LIBRARY_PATH=~/jemalloc-2.2.5/lib/ \\n[vijay_tcasstest@vijay_tcass-i-a91ee8cd ~]$ /etc/alternatives/jre_1.7.0/bin/java -Djava.library.path=~/jemalloc-2.2.5/lib/ -cp jna.jar:/apps/nfcassandra_server/lib/*:. com.sun.jna.JEMallocAllocator 50000 2000000\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   26060604   45627616          0     169128     996300\\n-/+ buffers/cache:   24895176   46793044\\nSwap:            0          0          0\\n**** Starting Test! ****\\nTotal bytes read: 101321734144\\nTime taken: 29937\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   28472436   43215784          0     169128     996440\\n-/+ buffers/cache:   27306868   44381352\\nSwap:            0          0          0\\n**** ending Test!**** \\n[vijay_tcasstest@vijay_tcass-i-a91ee8cd ~]$ \\n\\n{noformat}\\n\\nThe test shows around 4 GB savings. The test was on 101321734144 bytes (101 GB each). The test use CLHM to hold on to the objects and release them when the capacity is reached (5K)', \"I don't understand the goal here.  What is the value in allowing the use of malloc implementations that cause segfaults? :)\", 'Does this preserve the ability to use Unsafe instead of a JNA-backed malloc?', \"Hi Jonathan, The good thing is that it saves us from the Memory Fragmentation which we have seen with the native malloc's after it runs for a prolonge period of time. If the user wants to use a different implementation they can use it. JEMAlloc hasn't segfault in any of my tests I think it is better to use we still need to do more tests per sure.\\n\\n>>> Does this preserve the ability to use Unsafe instead of a JNA-backed malloc?\\nNo, the ticket just makes it pluggable so any other implementation is possible. We dont need to build the *.so/dll's for every environment we come across and the unsafe can be default :)\", 'bq. JEMAlloc hasn\\'t segfault in any of my tests \\n\\nWhat did you mean then by \"both TCMalloc and JEMalloc are kind of single threaded (at-least they crash in my test otherwise)?\"', 'Ohhh sorry for the confusion. \\nJEMAlloc\\'s case: The Malloc/Free should done by ANY one thread at a time. The test had 100 Threads doing malloc/f \n",
            "my_comment: The Malloc/Free should done by ANY one thread at a time. The test had 100 Threads doing malloc/f \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8116\n",
            "issue_type:  Bug \n",
            "summary:  HSHA fails with default rpc_max_threads setting \n",
            "description:  The HSHA server fails with 'Out of heap space' error if the rpc_max_threads is left at its default setting (unlimited) in cassandra.yaml.\n",
            "comments:  ['Committed.', \"Is it guaranteed to OOM?  Can't we just check for that combination and provide a sensible error instead of OOMing and letting the user figure it out?\", \"It's pretty much guaranteed to OOM because the number of handlers per SelectorThread is based on the the max pool size which for the default is Integer.MAX_VALUE. This change happened as part of CASSANDRA-7594.\\n\\nI suppose you could check and throw if the value is Integer.MAX_VALUE but you aren't going to be able to check every value. It also happens when the node is started so is pretty immediate which is why I suggested a doc change rather than a code change.\", 'Just as an FYI - I bisected dtest thrift_hsha_test.ThriftHSHATest.test_6285 failures to the CASSANDRA-7594 commit, and had to increase CCM_MAX_HEAP_SIZE to 4G before the ccm nodes would start successfully without a heap OOM.', 'Setting rpc_max_threads=20 in the thrift_hsha_test test_6285 does appear to keep dtest running with the default ccm heap.', 'bq. I suppose you could check and throw if the value is Integer.MAX_VALUE but you aren\\'t going to be able to check every value.\\n\\n\"unlimited\" is the default value, so I think there\\'s still quite a bit of value in checking for just that.  I\\'ll put together a patch.', '8116-throw-exc-2.0.txt throws a ConfigurationException when hsha is used with unlimited rpc_max_threads.', 'is there any reason to have a default of unlimited?  it just seems like it would be a simple change to have a reasonable default.', '+1 on the latest patch because that will be far clearer to the user than the OOM they get at the moment. ', 'Thanks, committed 8116-throw-exc-2.0.txt as 1b332bc1c02786623e2baf773e9f46af9c04f21f.', \"bq. is there any reason to have a default of unlimited? it just seems like it would be a simple change to have a reasonable default.\\n\\nI'm not sure about the rationale for that default.  Would you mind opening a new ticket to discuss a better default?  I'm not sure if it's something we would want to change in 2.0 or 2.1.\", 'The latest 2.0.x release of Cassandra using hsha with default settings either stalls after a few minutes of operation or crashes.\\n\\nThis does not seem like it should have a priority of \"Minor\". This is a major problem. The longer that 2.0.11 is the \"latest\" version the bigger the problem becomes for new users and existing users that have automation and high levels of trust in minor version upgrades.\\n\\n'] \n",
            "my_comment:  Committed. \"Is it guaranteed to OOM?  Cant we just check for that combination and provide a sensible error instead of OOMing and letting the user figure it out?\" \"Its pretty much guaranteed to OOM because the number of handlers per SelectorThread is based on the the max pool size which for the default is Integer.MAX_VALUE. This change happened as part of CASSANDRA-7594.\n",
            "\n",
            "I suppose you could check and throw if the value is Integer.MAX_VALUE but you arent going to be able to check every value. It also happens when the node is started so is pretty immediate which is why I suggested a doc change rather than a code change.\" Just as an FYI - I bisected dtest thrift_hsha_test.ThriftHSHATest.test_6285 failures to the CASSANDRA-7594 commit and had to increase CCM_MAX_HEAP_SIZE to 4G before the ccm nodes would start successfully without a heap OOM. Setting rpc_max_threads=20 in the thrift_hsha_test test_6285 does appear to keep dtest running with the default ccm heap. bq. I suppose you could check and throw if the value is Integer.MAX_VALUE but you aren\\t going to be able to check every value.\n",
            "\n",
            "\"unlimited\" is the default value so I think there\\s still quite a bit of value in checking for just that.  I\\ll put together a patch. 8116-throw-exc-2.0.txt throws a ConfigurationException when hsha is used with unlimited rpc_max_threads. is there any reason to have a default of unlimited?  it just seems like it would be a simple change to have a reasonable default. +1 on the latest patch because that will be far clearer to the user than the OOM they get at the moment.  Thanks committed 8116-throw-exc-2.0.txt as 1b332bc1c02786623e2baf773e9f46af9c04f21f. \"bq. is there any reason to have a default of unlimited? it just seems like it would be a simple change to have a reasonable default.\n",
            "\n",
            "Im not sure about the rationale for that default.  Would you mind opening a new ticket to discuss a better default?  Im not sure if its something we would want to change in 2.0 or 2.1.\" The latest 2.0.x release of Cassandra using hsha with default settings either stalls after a few minutes of operation or crashes.\n",
            "\n",
            "This does not seem like it should have a priority of \"Minor\". This is a major problem. The longer that 2.0.11 is the \"latest\" version the bigger the problem becomes for new users and existing users that have automation and high levels of trust in minor version upgrades.\n",
            "\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4647\n",
            "issue_type:  Improvement \n",
            "summary:  Rename NodeId to CounterId \n",
            "description:  NodeId is pretty close to the hostId introduced by vnodes, so let's rename NodeId to CounterId to avoid any potential confusion. It's probably a better name anyway. \n",
            "comments:  [\"Patch attached that does the renaming pretty much everywhere. I even edited the comments (at least for those returned by a simple grep). The only thing that still has NodeId is the system table NodeIdInfo, since changing the name would require migrating it's data.\", '+1', 'Committed, thanks'] \n",
            "my_comment:  [\"Patch attached that does the renaming pretty much everywhere. I even edited the comments (at least for those returned by a simple grep). The only thing that still has NodeId is the system table NodeIdInfo since changing the name would require migrating its data.\" +1 Committed thanks \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7745\n",
            "issue_type:  Bug \n",
            "summary:  Background LCS compactions stall with pending compactions remaining \n",
            "description:  We've hit a scenario where background LCS compactions will stall. compactionstats output shows hundreds of pending compactions but none active. The thread dumps show no CompactionExecutor threads running, and no compaction activity is being logged to system.log.  This seems to happen when there are no writes to the node. There are no flushes logged either, and when writes resume, compactions seem to resume as well, but still don't ever get to 0. \n",
            "comments:  ['I think we have concurrency bug in CompactionManager that compactingCF count remains 1 when there is no active compaction.\\n{{submitBackground}} adds compacting CF to compactingCF and {{BackgroundCompactionTask}} removes the same CF after compaction.\\nCurrently this can happen in either order, and when remove happens first, compacting count remains 1.\\n\\nAttaching patch to fix this.\\n\\nedit: Though it should continue to compact if further flushing happens and there is compaction thread available.', '+1', 'Committed, thanks!'] \n",
            "my_comment: Though it should continue to compact if further flushing happens and there is compaction thread available. +1 Committed thanks! \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-17868\n",
            "issue_type:  Improvement \n",
            "summary:  Allow disabling hotness persistence, or tuning of rate limiter \n",
            "description:  The persisting of the sstables hotness when there are 10s of thousands of sstables can have issues keeping up, and the rate limiter is hard coded. Another option may be nice to just completely disable the feature.\n",
            "comments:  ['||Item|Link||\\r\\n|PR|[link|https://github.com/apache/cassandra/pull/1855]|\\r\\n|JDK8 CI|[link|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/295/workflows/c33ed2b0-5063-4707-ba65-075b38dd9361]|\\r\\n|JDK11 CI|[link|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/295/workflows/274b3677-f709-4e07-be40-a6ad4bc97f59]|', '+1\\r\\n\\r\\n(Dropped a couple tiny nits in the PR)', 'The added properties in config / dd have not been added into cassandra.yaml. '] \n",
            "my_comment:  ||Item|Link||\n",
            "|PR|[link|https://github.com/apache/cassandra/pull/1855]|\n",
            "|JDK8 CI|[link|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/295/workflows/c33ed2b0-5063-4707-ba65-075b38dd9361]|\n",
            "|JDK11 CI|[link|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/295/workflows/274b3677-f709-4e07-be40-a6ad4bc97f59]| +1\n",
            "\n",
            "(Dropped a couple tiny nits in the PR) The added properties in config / dd have not been added into cassandra.yaml.  \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-15907\n",
            "issue_type:  Improvement \n",
            "summary:  Operational Improvements & Hardening for Replica Filtering Protection \n",
            "description:  CASSANDRA-8272 uses additional space on the heap to ensure correctness for 2i and filtering queries at consistency levels above ONE/LOCAL_ONE. There are a few things we should follow up on, however, to make life a bit easier for operators and generally de-risk usage:\n",
            "comments:  ['This affects 3.0.x and 3.11.x as well, and {{ReplicaFilteringProtection}} is almost entirely the same between 3.0.x and trunk. It could make sense to make the change in all three versions.\\r\\n\\r\\nCC [~adelapena] [~jwest]', 'CC [~blerer] [~jasonstack]', 'I\\'ve posted [some WIP|https://github.com/apache/cassandra/pull/659] on the raft of minor changes suggested above.\\r\\n\\r\\nI\\'ve also had some offline conversation w/ [~cscotta] and [~adelapena] around how we implement guardrails. One way forward might be to have two thresholds w/ different levels of enforcement (again partly taking inspiration from what we do w/ tombstones). The first threshold would drop a warning in the logs to make it plain that filtering protection is starting to encounter a significant number of potentially stale replica results (\"silent\" replicas) and is therefore keeping many more cached partial results on the heap than we would in the optimal case. The second threshold would be where we start to fail queries.\\r\\n\\r\\nIf we base these on the number of materialized rows, there are at least two ways to quantify things. The first is just to use an absolute threshold for rows per query. The second is to determine an \"expansion factor\" or how many rows we can cache as a factor of the provided query limit or page size (whichever is lower). The former is simpler and probably more intuitive (and would do its job even if a query used an enormous LIMIT), but the second conceptually takes the user\\'s intent around limits and page sizes into account.', 'As discussed with caleb, the memory issue is that potentially outdated rows in the 1st phase of\\xa0replica-filtering-protection(RFP) do not count towards merged counter, so short-read-protect(SRP) can potentially query and cache all data in the query range if only one replica has data.\\r\\n\\r\\nSome ideas to cap memory usage during RFP:\\r\\n * Single phase approach:\\r\\n ** Issue blocking RFP read immediately at {{MergeListener#onMergedRows}} when detecting potential outdated rows.\\r\\n ** This guarantees coordinator will cache at most \"limit * replicas\" num of rows assuming there are no tombstone..\\r\\n ** This should have similar performance as current 2-phase approach, but current approach can be optimized to execute RFP reads in parallel.\\r\\n * two-phase approach with SRP only at 2nd phase:\\r\\n ** the 1st phase is almost the same as current approach: collecting potentially outdated rows, but without SRP.\\r\\n ** in the second phase, issue RFP reads in parallel based on collected rows in 1st phase.\\r\\n *** When parallel RFP reads complete, merge the responses (original + RFP) again using the merger described in previous approach, but only do blocking RFP for rows requested by SRP.\\r\\n ** With this approach, the amount of memory used is the same as single-phase approach. The num of blocking RFP reads from SRP rows are usually small.\\r\\n\\r\\n\\xa0', '[~jasonstack] If the number of stale results is very large (i.e. a \"silent\" replica exists in the vast majority of responses), won\\'t those two approaches result in about the same performance profile? The two-phase approach seems like it would still be exposed to having to make a large number of blocking RFP reads. (Of course, they also should be pretty similar with no stale results, but the single-phase approach would be optimal.)', '{quote}If the number of stale results is very large (i.e. a \"silent\" replica exists in the vast majority of responses), won\\'t those two approaches result in about the same performance profile?\\xa0\\r\\n{quote}\\r\\nthe second approach will execute RFP requests in two places:\\r\\n # at the beginning of 2nd phase, based on the collected outdated rows from 1st phase. These RFP requests can run in parallel and the number can be large.\\r\\n # at merge-listener, for additional rows requested by SRP. These RFP requests have to run in\\xa0serial, but the number is usually small.\\r\\n\\r\\n\\xa0', \"{quote}\\r\\nthe second approach will execute RFP requests in two plac \n",
            "my_comment: collecting potentially outdated rows but without SRP.\n",
            " ** in the second phase issue RFP reads in parallel based on collected rows in 1st phase.\n",
            " *** When parallel RFP reads complete merge the responses (original + RFP) again using the merger described in previous approach but only do blocking RFP for rows requested by SRP.\n",
            " ** With this approach the amount of memory used is the same as single-phase approach. The num of blocking RFP reads from SRP rows are usually small.\n",
            "\n",
            "\\xa0 [~jasonstack] If the number of stale results is very large (i.e. a \"silent\" replica exists in the vast majority of responses) won\\t those two approaches result in about the same performance profile? The two-phase approach seems like it would still be exposed to having to make a large number of blocking RFP reads. (Of course they also should be pretty similar with no stale results but the single-phase approach would be optimal.) {quote}If the number of stale results is very large (i.e. a \"silent\" replica exists in the vast majority of responses) won\\t those two approaches result in about the same performance profile?\\xa0\n",
            "{quote}\n",
            "the second approach will execute RFP requests in two places:\n",
            " # at the beginning of 2nd phase based on the collected outdated rows from 1st phase. These RFP requests can run in parallel and the number can be large.\n",
            " # at merge-listener for additional rows requested by SRP. These RFP requests have to run in\\xa0serial but the number is usually small.\n",
            "\n",
            "\\xa0 \"{quote}\n",
            "the second approach will execute RFP requests in two plac \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2412\n",
            "issue_type:  Task \n",
            "summary:  Upgrade to thrift 0.6 \n",
            "description: None\n",
            "comments: None\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14010\n",
            "issue_type:  Bug \n",
            "summary:  Fix SStable ordering by max timestamp in SinglePartitionReadCommand \n",
            "description:  We have a test environment were we drop and create keyspaces and tables several times within a short time frame. Since upgrading from 3.11.0 to 3.11.1, we are seeing a lot of create statements failing. See the logs below:\n",
            "comments:  [\"I had a look at this and assumed that org.apache.cassandra.schema.SchemaKeyspace#fetchKeyspaceParams() was just not getting any rows returned when it queried the system_schema.keyspaces table. In fact in my testing it was getting a row returned but it only contained the primary key and null's for both other columns. I didn't dig too deep into this but it doesn't seem like it should happen, it's probably worth someone with a more intimate knowledge of the read path taking a look. Also on my machine I could only trigger this exception with multiple clients looping CREATE/DROP commands and it was still relatively rare.  \\r\\n\", \"I just saw this on some tests today as well.  The issue seems to be that the drop is happening concurrently with tables being initialized:\\r\\n\\r\\n{quote}\\r\\n2017-11-13T14:29:40.566922871Z INFO Initializing my_keyspace.schema_version\\r\\n2017-11-13T14:29:42.719380089Z INFO Drop Keyspace 'my_keyspace'\\r\\n2017-11-13T14:29:43.124510221Z INFO Create new Keyspace: \\r\\n{quote}\", '| patch  |  test  | dtest|\\r\\n| [3.0 |https://github.com/apache/cassandra/compare/cassandra-3.0...jasonstack:CASSANDRA-14010-3.0?expand=1 ] |\\r\\n| [3.11 |https://github.com/apache/cassandra/compare/cassandra-3.11...jasonstack:CASSANDRA-14010-3.11?expand=1 ] |\\r\\n| [trunk | https://github.com/apache/cassandra/compare/trunk...jasonstack:CASANDRA-14010-trunk?expand=1] |\\r\\n\\r\\n\\r\\nIt turns out that the query in {{fetchKeyspaceParams()}} gets incomplete data from memtable.\\r\\n\\r\\n{code}\\r\\nprocess:\\r\\n  0. drop ks with ts1 \\r\\n  1. apply create ks mutation with t2 (t2>t1)\\r\\n  2. flush memtables including \"system_schema.keyspaces\" table\\r\\n  3. select keyspace_name from \"system_schema.keyspaces\" table in {{fetchKeyspaceOnly()}} causing \"defragmenting\" (at the end of SPRC.queryMemtableAndSSTablesInTimestampOrder()) to insert the selected data into memtable\\r\\n  4. select * from \"system_schema.keyspaces\" table in {{fetchKeyspaceParams()}} getting incomplete data(row with liveness of t2 and deletion of t1, no regular columns) from memtable. First sstable\\'s maxtimestamp is smaller than memtable data\\'s deletion time(drop ks time, t1) because sstables are sorted by maxTS in ascending order and other newer sstables are skipped...\\r\\n\\r\\nThe correct order is descending to eliminate older sstables.\\r\\n{code}\\r\\n\\r\\nThe patch is to make sure sstables are compared with max-timestamp in descending order...\\r\\n\\r\\nThe reason that it only happened on 3.11 is related to {{queriedColumn in ColumnFilter}} and value skipping added in 3.x.  (a bit complex...)\\r\\n\\r\\nWhen no non-pk column is selected, the {{queried}} columns in ColumnFilter.builder will be initialized as empty, thus when processing the query in #3, unselected columns(eg. durable_wirtes, replication) are skipped in Cell.Serializer: helper.canSkipValue().\\r\\n\\r\\nBut in trunk, due to CASSANDRA-7396, when no non-pk column is selected, the {{queried}} columns in ColumnFilter.builder will be initialized as null, thus unselected columns are not skipped, later put into memtable. (lost the benefit of value skipping)', 'If we ignore the complexity of defragmenting, columnfilter, etc... It can be reproduced easily:\\r\\n{code:title=reproduce}\\r\\n        createTable(\"CREATE TABLE %s (k1 int, v1 int, v2 int, PRIMARY KEY (k1))\");\\r\\n        ColumnFamilyStore cfs = ColumnFamilyStore.getIfExists(keyspace(), currentTable());\\r\\n        cfs.disableAutoCompaction();\\r\\n\\r\\n        execute(\"INSERT INTO %s(k1,v1,v2) VALUES(1,1,1)  USING TIMESTAMP 5\");\\r\\n        cfs.forceBlockingFlush();\\r\\n\\r\\n        execute(\"INSERT INTO %s(k1,v1,v2) VALUES(1,1,2)  USING TIMESTAMP 8\");\\r\\n        cfs.forceBlockingFlush();\\r\\n\\r\\n        execute(\"INSERT INTO %s(k1) VALUES(1)  USING TIMESTAMP 7\");\\r\\n        // deletion 6 shadow sstable-1 with ts=5 ...\\r\\n        execute(\"DELETE FROM %s USING TIMESTAMP 6 WHERE k1 = 1\");\\r\\n\\r\\n        assertRows(execute(\"SELECT * FROM %s WHERE k1=1\"), row(1, 1, 2));\\r\\n{code}\\r\\n', 'CI looks good. \n",
            "my_comment: helper.canSkipValue().\n",
            "\n",
            "But in trunk due to CASSANDRA-7396 when no non-pk column is selected the {{queried}} columns in ColumnFilter.builder will be initialized as null thus unselected columns are not skipped later put into memtable. (lost the benefit of value skipping) If we ignore the complexity of defragmenting columnfilter etc... It can be reproduced easily:\n",
            "{code:title=reproduce}\n",
            "        createTable(\"CREATE TABLE %s (k1 int v1 int v2 int PRIMARY KEY (k1))\");\n",
            "        ColumnFamilyStore cfs = ColumnFamilyStore.getIfExists(keyspace() currentTable());\n",
            "        cfs.disableAutoCompaction();\n",
            "\n",
            "        execute(\"INSERT INTO %s(k1v1v2) VALUES(111)  USING TIMESTAMP 5\");\n",
            "        cfs.forceBlockingFlush();\n",
            "\n",
            "        execute(\"INSERT INTO %s(k1v1v2) VALUES(112)  USING TIMESTAMP 8\");\n",
            "        cfs.forceBlockingFlush();\n",
            "\n",
            "        execute(\"INSERT INTO %s(k1) VALUES(1)  USING TIMESTAMP 7\");\n",
            "        // deletion 6 shadow sstable-1 with ts=5 ...\n",
            "        execute(\"DELETE FROM %s USING TIMESTAMP 6 WHERE k1 = 1\");\n",
            "\n",
            "        assertRows(execute(\"SELECT * FROM %s WHERE k1=1\") row(1 1 2));\n",
            "{code}\n",
            " CI looks good. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9631\n",
            "issue_type:  Bug \n",
            "summary:  Unnecessary required filtering for query on indexed clustering key \n",
            "description:  Let's create and populate a simple table composed of one partition key {{a}}, two clustering keys {{b}} & {{c}}, and one secondary index on a standard column {{e}}:\n",
            "comments:  ['Is there a chance that CASSANDRA-8418 introduced this bug ?', '/cc [~blerer]', \"It's actually not a bug. The new index you've introduced might be used by Cassandra (whether it is or not depends on some internal metrics on your data set), and filtering will be involved if that index is used as the primary index. And so requiring {{ALLOW FILTERING}} is correct.\\n\\nNow, using the index on {{b}} for that query is not smart and C* should always stick to the index on {{e}} for that query as it will be faster and won't require filtering. It's just not what happens in 2.1. So we can definitively improve this. That said, a (very) quick glance at 2.2/trunk seems to suggest that this improvement is actually made there. So [~blerer], can you double check if that is the case? If so, we can close this (as duplicate of CASSANDRA-7981?). Otherwise, let's look at improving it.\", \"{quote}Now, using the index on b for that query is not smart and C* should always stick to the index on e for that query as it will be faster and won't require filtering. It's just not what happens in 2.1.{quote}\\n\\nI had a look at the code of 2.1 and it seems that it used to ignore the index on {{b}} for this specific case. I broke that behaviour when I implemented CASSANDRA-8275. The 2.2/trunk behaviour was not affected by my patch.\\n\\nI will provide some patches to fix the problem in 2.0 and 2.1 and add some extra unit tests to all the versions.\", 'Thanks [~blerer] for this quick update ! :)', 'The patches for 2.0 and 2.1 fix the problem and add extra unit tests to validate the behaviour.\\n\\nThe patch for 2.2 just add extra unit tests.\\n\\nThe results of the tests are bellow:\\n* [unit tests for 2.0|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.0-testall/lastCompletedBuild/testReport/]\\n* [dtests for 2.0|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.0-dtest/lastCompletedBuild/testReport/]\\n* [unit tests for 2.1|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.1-testall/lastCompletedBuild/testReport/]\\n* [dtests for 2.1|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.1-dtest/lastCompletedBuild/testReport/]\\n* [unit tests for 2.2|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.2-testall/lastCompletedBuild/testReport/]\\n* [dtests for 2.2|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.2-dtest/lastCompletedBuild/testReport/]', '[~thobbs] could you review?', '+1, committed to 2.0 as {{f2db756abd135cc6ca4cf657d29fb2601764d50f}} and merged to 2.1, 2.2, and trunk.  Thanks!'] \n",
            "my_comment:  Is there a chance that CASSANDRA-8418 introduced this bug ? /cc [~blerer] \"Its actually not a bug. The new index youve introduced might be used by Cassandra (whether it is or not depends on some internal metrics on your data set) and filtering will be involved if that index is used as the primary index. And so requiring {{ALLOW FILTERING}} is correct.\n",
            "\n",
            "Now using the index on {{b}} for that query is not smart and C* should always stick to the index on {{e}} for that query as it will be faster and wont require filtering. Its just not what happens in 2.1. So we can definitively improve this. That said a (very) quick glance at 2.2/trunk seems to suggest that this improvement is actually made there. So [~blerer] can you double check if that is the case? If so we can close this (as duplicate of CASSANDRA-7981?). Otherwise lets look at improving it.\" \"{quote}Now using the index on b for that query is not smart and C* should always stick to the index on e for that query as it will be faster and wont require filtering. Its just not what happens in 2.1.{quote}\n",
            "\n",
            "I had a look at the code of 2.1 and it seems that it used to ignore the index on {{b}} for this specific case. I broke that behaviour when I implemented CASSANDRA-8275. The 2.2/trunk behaviour was not affected by my patch.\n",
            "\n",
            "I will provide some patches to fix the problem in 2.0 and 2.1 and add some extra unit tests to all the versions.\" Thanks [~blerer] for this quick update ! :) The patches for 2.0 and 2.1 fix the problem and add extra unit tests to validate the behaviour.\n",
            "\n",
            "The patch for 2.2 just add extra unit tests.\n",
            "\n",
            "The results of the tests are bellow:\n",
            "* [unit tests for 2.0|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.0-testall/lastCompletedBuild/testReport/]\n",
            "* [dtests for 2.0|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.0-dtest/lastCompletedBuild/testReport/]\n",
            "* [unit tests for 2.1|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.1-testall/lastCompletedBuild/testReport/]\n",
            "* [dtests for 2.1|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.1-dtest/lastCompletedBuild/testReport/]\n",
            "* [unit tests for 2.2|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.2-testall/lastCompletedBuild/testReport/]\n",
            "* [dtests for 2.2|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.2-dtest/lastCompletedBuild/testReport/] [~thobbs] could you review? +1 committed to 2.0 as {{f2db756abd135cc6ca4cf657d29fb2601764d50f}} and merged to 2.1 2.2 and trunk.  Thanks! \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5935\n",
            "issue_type:  New Feature \n",
            "summary:  Allow disabling slab allocation \n",
            "description:  Arena allocation (CASSANDRA-2252) dramatically improves garbage collection performance, but it can limit the number of tables that can be practically defined in the schema since you have a minimum of 1MB of heap per table.\n",
            "comments:  ['Note that I\\'ve deliberately omitted this setting from cassandra.yaml; see above on \"almost certainly a Bad Idea.\"', 'v2 fixes constructor visibility', 'lgtm. ', 'Committed to 1.2.10 and 2.0.1 (but not 2.0.0).'] \n",
            "my_comment:  Note that I\\ve deliberately omitted this setting from cassandra.yaml; see above on \"almost certainly a Bad Idea.\" v2 fixes constructor visibility lgtm.  Committed to 1.2.10 and 2.0.1 (but not 2.0.0). \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-17921\n",
            "issue_type:  Bug \n",
            "summary:  Harden JMX by resolving beanshooter issues \n",
            "description:  Fix JMX security vulnerabilities\n",
            "comments:  ['Committed as [b2660bcf78ccf08a3a0ae39a8c9ffb397efef9ff|https://github.com/apache/cassandra/commit/b2660bcf78ccf08a3a0ae39a8c9ffb397efef9ff].'] \n",
            "my_comment:  Committed as [b2660bcf78ccf08a3a0ae39a8c9ffb397efef9ff|https://github.com/apache/cassandra/commit/b2660bcf78ccf08a3a0ae39a8c9ffb397efef9ff]. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-18105\n",
            "issue_type:  Bug \n",
            "summary:  TRUNCATED data come back after a restart or upgrade \n",
            "description:  When we use the TRUNCATE command to delete all data in the table, the deleted data come back after a node restart or upgrade. This problem happens at the latest releases (2.2.19, 3.0.28, or 4.0.7)\n",
            "comments:  ['Does this only manifest when there is an index present?', '[~maedhroz] Thanks for the reply. Yes. The index is necessary to trigger it.\\xa0', 'for 4.0.7 I tried ，but failed. I can not reproduce it ', '[~maxwellguo]\\xa0Could you try 3.0.28 or 2.2.19? I just reproduced at those two versions.', \"This doesn't reproduce against 3.0 head.  I'm not going to hunt down java7 to attempt 2.2 since I don't have any reason to believe it will reproduce there either.\", \"Thanks so much for the reply! [~brandon.williams] [~maxwellguo] . I am sorry for one mistake I made about the reproduce procedure. To reproduce it at 3.0.28 or 4.0.7, we need to kill the Cassandra process after the flush. But for 2.2.19, we can perform a normal shutdown by {{{}bin/nodetool stopdaemon{}}}. Here's the detailed log on how I reproduced it.\\r\\n\\r\\n*To reproduce it at 4.0.7 or 3.0.28* (Both Java8), after flushing the node, I stop Cassandra process by {{{}pgrep -f cassandra | xargs kill -9{}}}. Then I directly restart the Cassandra. The deleted data shows again. Here's the full log I got to reproduce it at 4.0.7 release.\\r\\n{code:java}\\r\\n➜ \\xa0apache-cassandra-4.0.7 bin/cassandra\\r\\n➜ \\xa0apache-cassandra-4.0.7 CompilerOracle: dontinline org/apache/cassandra/db/Columns$Serializer.deserializeLargeSubset (Lorg/apache/cassandra/io/util/DataInputPlus;Lorg/apache/cassandra/db/Columns;I)Lorg/apache/cassandra/db/Columns;\\r\\n......\\r\\n\\r\\n➜ \\xa0apache-cassandra-4.0.7 bin/cqlsh\\r\\nConnected to Test Cluster at 127.0.0.1:9042\\r\\n[cqlsh 6.0.0 | Cassandra 4.0.7 | CQL spec 3.4.5 | Native protocol v5]\\r\\nUse HELP for help.\\r\\ncqlsh> CREATE KEYSPACE IF NOT EXISTS ks WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\\r\\ncqlsh> CREATE TABLE \\xa0ks.tb (c3 TEXT,c4 TEXT,c2 INT,c1 TEXT, PRIMARY KEY (c1, c2, c3 ));\\r\\ncqlsh> INSERT INTO ks.tb (c3, c1, c2) VALUES ('val1','val2',1);\\r\\ncqlsh> CREATE INDEX IF NOT EXISTS tb ON ks.tb ( c3);\\r\\ncqlsh> TRUNCATE TABLE ks.tb;\\r\\ncqlsh> DROP INDEX IF EXISTS ks.tb;\\xa0\\r\\ncqlsh> SELECT c2 FROM ks.tb;\\xa0\\r\\n\\xa0c2\\r\\n----\\r\\n\\r\\n(0 rows)\\r\\n➜ \\xa0apache-cassandra-4.0.7 bin/nodetool flush\\r\\n➜ \\xa0apache-cassandra-4.0.7 pgrep -f cassandra | xargs kill -9\\r\\n➜ \\xa0apache-cassandra-4.0.7 bin/cassandra\\r\\n➜ \\xa0apache-cassandra-4.0.7 CompilerOracle: dontinline org/apache/cassandra/db/Columns$Serializer.deserializeLargeSubset (Lorg/apache/cassandra/io/util/DataInputPlus;Lorg/apache/cassandra/db/Columns;I)Lorg/apache/cassandra/db/Columns;\\r\\n......\\r\\n\\r\\n➜ \\xa0apache-cassandra-4.0.7 bin/cqlsh\\r\\nConnected to Test Cluster at 127.0.0.1:9042\\r\\n[cqlsh 6.0.0 | Cassandra 4.0.7 | CQL spec 3.4.5 | Native protocol v5]\\r\\nUse HELP for help.\\r\\ncqlsh> SELECT c2 FROM ks.tb;\\xa0\\r\\n\\xa0c2\\r\\n----\\r\\n\\xa0 1\\r\\n\\r\\n(1 rows) {code}\\r\\n\\xa0\\r\\n\\r\\n*To reproduce it at 2.2.19* (Java8). We can stop the Cassandra daemon normally to trigger it. Here's the full log I got for 2.2.19.\\r\\n{code:java}\\r\\n➜ \\xa0apache-cassandra-2.2.19 bin/cassandra\\r\\n➜ \\xa0apache-cassandra-2.2.19 OpenJDK 64-Bit Server VM warning: Cannot open file bin/../logs/gc.log due to No such file or directoryCompilerOracle: inline org/apache/cassandra/db/AbstractNativeCell.compareTo (Lorg/apache/cassandra/db/composites/Composite;)I\\r\\n......\\r\\n\\r\\n➜ \\xa0apache-cassandra-2.2.19 bin/cqlsh\\r\\nConnected to Test Cluster at 127.0.0.1:9042.\\r\\n[cqlsh 5.0.1 | Cassandra 2.2.19 | CQL spec 3.3.1 | Native protocol v4]\\r\\nUse HELP for help.\\r\\ncqlsh> CREATE KEYSPACE IF NOT EXISTS ks WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\\r\\ncqlsh> CREATE TABLE \\xa0ks.tb (c3 TEXT,c4 TEXT,c2 INT,c1 TEXT, PRIMARY KEY (c1, c2, c3 ));\\r\\ncqlsh> INSERT INTO ks.tb (c3, c1, c2) VALUES ('val1','val2',1);\\r\\ncqlsh> CREATE INDEX IF NOT EXISTS tb ON ks.tb ( c3);\\r\\ncqlsh> TRUNCATE TABLE ks.tb;\\r\\ncqlsh> DROP INDEX IF EXISTS ks.tb;\\xa0\\r\\ncqlsh> SELECT c2 FROM ks.tb;\\r\\n c2\\r\\n----\\r\\n\\r\\n(0 rows)\\r\\n\\r\\n➜\\xa0 apache-cassandra-2.2.19 bin/nodetool -h ::FFFF:127.0.0.1 flush\\xa0 \\xa0 \\x \n",
            "my_comment: 1 };\n",
            "cqlsh> CREATE TABLE \\xa0ks.tb (c3 TEXTc4 TEXTc2 INTc1 TEXT PRIMARY KEY (c1 c2 c3 ));\n",
            "cqlsh> INSERT INTO ks.tb (c3 c1 c2) VALUES (val1val21);\n",
            "cqlsh> CREATE INDEX IF NOT EXISTS tb ON ks.tb ( c3);\n",
            "cqlsh> TRUNCATE TABLE ks.tb;\n",
            "cqlsh> DROP INDEX IF EXISTS ks.tb;\\xa0\n",
            "cqlsh> SELECT c2 FROM ks.tb;\n",
            " c2\n",
            "----\n",
            "\n",
            "(0 rows)\n",
            "\n",
            "➜\\xa0 apache-cassandra-2.2.19 bin/nodetool -h ::FFFF:127.0.0.1 flush\\xa0 \\xa0 \\x \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1406\n",
            "issue_type:  Bug \n",
            "summary:  Dropping column families doesn't clean up secondary indexes \n",
            "description: None\n",
            "comments: None\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-16780\n",
            "issue_type:  Improvement \n",
            "summary:  Log when writing many tombstones to a partition \n",
            "description:  Log when writing many tombstones to a partition like we do when writing a large partition \n",
            "comments:  ['https://github.com/krummas/cassandra/commits/marcuse/16780\\r\\n\\r\\nhttps://app.circleci.com/pipelines/github/krummas/cassandra?branch=marcuse%2F16780', 'I think we should add this to the yaml with comments explaining it too.', 'yep, good point, pushed a fix', '+1', \"I don't think we can technically put this in 4.0.x unless it's considered a bug, unfortunately.\", 'committed, thanks'] \n",
            "my_comment:  https://github.com/krummas/cassandra/commits/marcuse/16780\n",
            "\n",
            "https://app.circleci.com/pipelines/github/krummas/cassandra?branch=marcuse%2F16780 I think we should add this to the yaml with comments explaining it too. yep good point pushed a fix +1 \"I dont think we can technically put this in 4.0.x unless its considered a bug unfortunately.\" committed thanks \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3743\n",
            "issue_type:  Improvement \n",
            "summary:  Lower memory consumption used by index sampling \n",
            "description:  currently j.o.a.c.io.sstable.indexsummary is implemented as ArrayList of KeyPosition (RowPosition key, long offset)i propose to change it to:\n",
            "comments:  ['Its all about nuking KeyPosition class', 'This is a good idea.  Are you going to take a stab at it, Radim?', 'i will load cassandra into Eclipse and check how many times is IndexSummary referenced', 'I am working on it now', 'patch is against-1.0. This was expected to go into 1.0-branch. Its small change KeyPosition was referenced by just one other class', '+1 except coding style:\\n\\n- you should use 4 spaces instead of tab\\n- always place { and } on new line', 'Are you going to fix these codestyle errors? its just few lines', 'Please submit an updated patch against 1.1, Radim.', 'Wondering if we can use DecoratedKey[] instead of ArrayList in the attached patch... Just my 2 cents (I noticed this while doing another patch).', 'has java builtin binary search operating on array[] ?', \"There's Arrays.binarySearch.  But do we always know how many positions we have before performing the sampling?  If not you could just use AL.trimToSize at the end to avoid wasted space.  With that the difference b/t AL and array are negligible here.\", 'new patch attached lgtm.', \"Really?  I'm getting 7 of 11 hunks failing to apply against current trunk.\", 'I thought patch is for 1.0 branch.\\nIs it going to trunk?', 'Yes, we should really be keeping 1.0 for bugfixes at this point.  1.1 freeze is in less than a week.', '>>> But do we always know how many positions we have before performing the sampling?\\nI think we do while doing the flush... not a big deal i just noticed long position[] hence was wondering why not DecoratedKeys[]\\n\\n>>> With that the difference b/t AL and array are negligible here.\\nAgreed!', 'i have no plans to use 1.1, lets close ticket.', \"Rebased Radim's patch for trunk.\", 'Slight update attached to use positions.trimToSize instead of leaving an \"orphan\" arraylist around.  As mentioned above, the size difference is negligible.\\n\\nQuestion: is the getKeySamples transform still necessary, or can we just cast the keys collection?\\n\\nOr could we just make it a List of DK in the first place instead of RP?', 'bq. is the getKeySamples transform still necessary, or can we just cast the keys collection? Or could we just make it a List of DK in the first place instead of RP?\\n\\nI think it is better just make a list of DK since IndexSummary.addEntry accepts only DK. Attached v2.', 'committed, thanks!'] \n",
            "my_comment: is the getKeySamples transform still necessary or can we just cast the keys collection?\n",
            "\n",
            "Or could we just make it a List of DK in the first place instead of RP? bq. is the getKeySamples transform still necessary or can we just cast the keys collection? Or could we just make it a List of DK in the first place instead of RP?\n",
            "\n",
            "I think it is better just make a list of DK since IndexSummary.addEntry accepts only DK. Attached v2. committed thanks! \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3545\n",
            "issue_type:  Improvement \n",
            "summary:  Fix very low Secondary Index performance \n",
            "description:  While performing index search + value filtering over large Index Row ( ~100k keys per index value) with chunks (size of 512-1024 keys) search time is about 8-12 seconds, which is very very low.\n",
            "comments:  ['Screen from profiler', 'Patch for 1) solution, that is using fast algorithm for search element in sorted collection with much less compare usage.\\n\\nActually this solution can improve any ColumnSlice resolving.', 'That sounds like a good place to start. Thanks for the analysis!', 'New version of patch with fix of bugs and clear code.', 'Patch is ready, please review.\\n\\nI have check search over index with 70k keys.\\nResult: search is 3 times faster, profiling show no more bottleneck on MD5 hash calculation.', \"Wow.  That's a really clever workaround to working on a generic Collection iterator, that we happen to know is sorted.  But it's also kind of complicated, and does a bunch of copying to a temporary collection that I'd prefer to avoid.\\n\\nWhat if we added a getSortedColumns(byte[] startWith) overload to AbstractColumnContainer + ISortedColumns?  Then each ISortedColumns implementation could implement that the straightforward way (for ArrayBacked, with Collections.binarySearch + subList; for the NavigableMap based ones, with tailMap).\", \"Yep, a little bit complicated. That is why it is fully covered by JUnits and comments.\\nCobertura reports: \\nClasses in this File\\tLine Coverage\\tBranch Coverage\\tComplexity\\nCollectionSearchUtil\\t91%\\t51/56    81%\\t36/44      6,5\\n\\n\\nThere is NO bunch of copying to a temporary collection. There only one array of size sqrt(2*N), allocated once and coping is linear (same as iterating).\\n\\nLet's analyze resources required for this search on N size collection:\\n\\nMemory usage:                                  == sqrt(2*N)\\nArray copping:                                 <= N \\nIteration ( next() execution):                 <= N\\nCompare (with MD5/Column comparator):          <= sqrt(2*N)\\n\\nI have solution (see patch 1) that perform iterating instead of allocation array, but it will require O(N^2) iterating in worst case.\\n\\nIn second patch memory usage is trade of for only one passage with iterator. Iteration can be slow, so array is much better. \\n\\nYou can check that for million columns search (pretty big row) it will be array with length: ~1440. Not too much for such huge search I think . \\nIn case of 10k columns row it will be only 144 length array, with is pretty few.\\n\\n\\n\\nAbout getSortedColumns(byte[] startWith): \\nYes, it is another good solution. binarySearch is little bit faster in case you have indexed access to underling Columns (like List or Array).\\n\\nBut there is still one disadvantage: My patch is solving this problem and changes only few code lines\\nand this solution requires much more code changes. Lot's of code changes - low release stability. Sorry for sharing pain in JIRA tickets but 1.0.3 seems to be last stable release :(\\n\\nAs a compromise I can suggest to apply this patch and add ticket for feature to cleanup code, move to binary search and new API in ISortedColumns.\\n\\n\\n\", \"bq. As a compromise I can suggest to apply this patch and add ticket for feature to cleanup code, move to binary search and new API in ISortedColumns.\\n\\nI'm afraid that's typically not how we work.  If we see a clearly better approach during review, which this qualifies as, then we'll wait for that before committing.  Especially when the goal is to get into a stable branch.\", \"I agree with Jonathan than interning this inside the column family feels cleaner (and is more efficient). Attaching patch to do that (actually 2 patch, the second one does some cleaning of the comparator being given to lots of methods that don't care about it or can get it by other means). The patches are against trunk since I don't think we should push that into a stable release (independently of the actual implementation).\\n\\nNote that this only applies to memtable, so this has probably much more impact on small benchmarks (where you insert and get immediately) than it will have in real life (it's still an improvement, don't get me wrong).\\n\\nFor the rest:\\nbq. 2) Don't calculate MD5 hash for startKey every time. It' \n",
            "my_comment: My patch is solving this problem and changes only few code lines\n",
            "and this solution requires much more code changes. Lots of code changes - low release stability. Sorry for sharing pain in JIRA tickets but 1.0.3 seems to be last stable release :(\n",
            "\n",
            "As a compromise I can suggest to apply this patch and add ticket for feature to cleanup code move to binary search and new API in ISortedColumns.\n",
            "\n",
            "\n",
            "\" \"bq. As a compromise I can suggest to apply this patch and add ticket for feature to cleanup code move to binary search and new API in ISortedColumns.\n",
            "\n",
            "Im afraid thats typically not how we work.  If we see a clearly better approach during review which this qualifies as then well wait for that before committing.  Especially when the goal is to get into a stable branch.\" \"I agree with Jonathan than interning this inside the column family feels cleaner (and is more efficient). Attaching patch to do that (actually 2 patch the second one does some cleaning of the comparator being given to lots of methods that dont care about it or can get it by other means). The patches are against trunk since I dont think we should push that into a stable release (independently of the actual implementation).\n",
            "\n",
            "Note that this only applies to memtable so this has probably much more impact on small benchmarks (where you insert and get immediately) than it will have in real life (its still an improvement dont get me wrong).\n",
            "\n",
            "For the rest:\n",
            "bq. 2) Dont calculate MD5 hash for startKey every time. It \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5250\n",
            "issue_type:  Improvement \n",
            "summary:  Improve LeveledScanner work estimation \n",
            "description:  See https://issues.apache.org/jira/browse/CASSANDRA-5222?focusedCommentId=13577420&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13577420 \n",
            "comments:  ['filters out sstables not intersecting the range given\\n\\nI might have totally misunderstood the comment in CASSANDRA-5222 though. \\n\\nIt also \"solves\" CASSANDRA-5249 in another way, i left that code in there though since the same amount of work needs to be done.', \"I would say, go ahead and rip out EmptyCompactionScanner.\\n\\nI was also hoping that we could get a better estimate for sstables where the range we're scanning is only part of the sstable.  One way would be to do lookups against our in-memory IndexSummary for the start and end of the range, and assume that 1/10 of the rows in the summary correspond to 1/10 of the size on disk.  Obviously not 100% accurate but a lot better than throwing the entire sstable size in each time.\", 'does a better job estimating work\\n\\nalso removes EmptyCompactionScanner and changes SSTableBoundedScanner constructor back to taking a Range (felt a bit cleaner)', 'My OCD wanted to get rid of the == null test. :)', 'looks like you missed a negation:\\nif (intersecting.isEmpty())\\n\\nv4 attached with the fix', 'Thanks, committed.'] \n",
            "my_comment:  filters out sstables not intersecting the range given\n",
            "\n",
            "I might have totally misunderstood the comment in CASSANDRA-5222 though. \n",
            "\n",
            "It also \"solves\" CASSANDRA-5249 in another way i left that code in there though since the same amount of work needs to be done. \"I would say go ahead and rip out EmptyCompactionScanner.\n",
            "\n",
            "I was also hoping that we could get a better estimate for sstables where the range were scanning is only part of the sstable.  One way would be to do lookups against our in-memory IndexSummary for the start and end of the range and assume that 1/10 of the rows in the summary correspond to 1/10 of the size on disk.  Obviously not 100% accurate but a lot better than throwing the entire sstable size in each time.\" does a better job estimating work\n",
            "\n",
            "also removes EmptyCompactionScanner and changes SSTableBoundedScanner constructor back to taking a Range (felt a bit cleaner) My OCD wanted to get rid of the == null test. :) looks like you missed a negation:\n",
            "if (intersecting.isEmpty())\n",
            "\n",
            "v4 attached with the fix Thanks committed. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14141\n",
            "issue_type:  Improvement \n",
            "summary:  Enable CDC unittest \n",
            "description:  Follow up for CASSANDRA-14066\n",
            "comments:  ['Here is the patch, please review:\\r\\n| Branch | uTest |\\r\\n| [14141-3.11|https://github.com/cooldoger/cassandra/tree/14141-3.11] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14141-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14141-3.11] |\\r\\n| [14141-trunk|https://github.com/cooldoger/cassandra/tree/14141-trunk] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14141-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14141-trunk] |\\r\\n\\r\\nTested with both {{$ ant test}} and {{$ ant test-cdc}}.', '+1. committed as sha {{5133526733f7ff24062acb5aa57fcfff050ac424}}\\r\\n\\r\\nThanks, Jay!', \"Just a quick note: I wouldn't remove or stop running the {{test-cdc}} target on ci. The reason I kept these completely separate was to fully exercise the rest of the testing infrastructure with cdc enabled to confirm no regression / data loss in the CommitLog from the changed cdc code. Running the unit tests specific to cdc on a regular {{ant test}} run is fine as a smoke test but shouldn't serve as a replacement for running the entire test suite against the changed logic.\", \"bq. Just a quick note: I wouldn't remove or stop running the test-cdc target on ci. The reason I kept these completely separate was to fully exercise the rest of the testing infrastructure with cdc enabled to confirm no regression / data loss in the CommitLog from the changed cdc code. Running the unit tests specific to cdc on a regular ant test run is fine as a smoke test but shouldn't serve as a replacement for running the entire test suite against the changed logic.\\r\\n+1\", 'Yup, 100% agree, [~JoshuaMcKenzie] .', \"This doesn't work in CircleCI as of when it was merged here 0ac5a17d3aac49f678f8b1ec4c934c8a0e7367ab\\r\\n\\r\\n{noformat}\\r\\n    [junit] Testcase: testRetainLinkOnDiscardCDC(org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest):\\tCaused an ERROR\\r\\n    [junit] Rejecting mutation to keyspace cql_test_keyspace. Free up space in build/test/cassandra/cdc_raw:0 by processing CDC logs.\\r\\n    [junit] org.apache.cassandra.exceptions.CDCWriteException: Rejecting mutation to keyspace cql_test_keyspace. Free up space in build/test/cassandra/cdc_raw:0 by processing CDC logs.\\r\\n    [junit] \\tat org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDC.throwIfForbidden(CommitLogSegmentManagerCDC.java:136)\\r\\n    [junit] \\tat org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDC.allocate(CommitLogSegmentManagerCDC.java:108)\\r\\n    [junit] \\tat org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:272)\\r\\n    [junit] \\tat org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:603)\\r\\n    [junit] \\tat org.apache.cassandra.db.Keyspace.apply(Keyspace.java:480)\\r\\n    [junit] \\tat org.apache.cassandra.db.Mutation.apply(Mutation.java:194)\\r\\n    [junit] \\tat org.apache.cassandra.db.Mutation.apply(Mutation.java:199)\\r\\n    [junit] \\tat org.apache.cassandra.db.Mutation.apply(Mutation.java:208)\\r\\n    [junit] \\tat org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest.testRetainLinkOnDiscardCDC(CommitLogSegmentManagerCDCTest.java:256)\\r\\n    [junit]\\r\\n    [junit]\\r\\n    [junit] Testcase: testCompletedFlag(org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest):\\tFAILED\\r\\n    [junit] Index file not written: build/test/cassandra/cdc_raw:0/CommitLog-7-1517000670440_cdc.idx\\r\\n    [junit] junit.framework.AssertionFailedError: Index file not written: build/test/cassandra/cdc_raw:0/CommitLog-7-1517000670440_cdc.idx\\r\\n    [junit] \\tat org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest.testCompletedFlag(CommitLogSegmentManagerCDCTest.java:210)\\r\\n{noformat}\\r\\n\\r\\nI'm going to try it with the circle.yml on trunk just to see what is up. I am wondering if this test requires more disk space to pass? \", \"I think this may be disk space related. We suddenly lost access to the larger volume in CircleCI that is supposed to be mounted in the container but it's no longer appear \n",
            "my_comment: build/test/cassandra/cdc_raw:0/CommitLog-7-1517000670440_cdc.idx\n",
            "    [junit] \\tat org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest.testCompletedFlag(CommitLogSegmentManagerCDCTest.java:210)\n",
            "{noformat}\n",
            "\n",
            "Im going to try it with the circle.yml on trunk just to see what is up. I am wondering if this test requires more disk space to pass? \" \"I think this may be disk space related. We suddenly lost access to the larger volume in CircleCI that is supposed to be mounted in the container but its no longer appear \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-18219\n",
            "issue_type:  Improvement \n",
            "summary:  Warning message on aggregation queries doesn't specify table name or aggregate type \n",
            "description:  The existing aggregation query warning messages in cassandra.log from SelectStatement.java:\n",
            "comments:  ['Thank you for reporting this! I will take a closer look if we could propagate the information you need. What you mean under the \"type of aggregation\"? \\r\\n\\r\\nI am not sure what slow query logger is. I think you meant \"no spamming logger\", we can probably print this in debug via that logger if you find it useful.', '[~bschoeni] this is what I have. (1)\\r\\n\\r\\nI am little bit confused by emitting warning in CQL(SH) but we are logging it on debug level in the logs. I would say these levels should be probably equal but at the same time there is not anything like \"debug level\" in CQLSH so this might be probably just fine.\\r\\n\\r\\n[~blerer] what do you think?\\r\\n\\r\\n(1) https://github.com/apache/cassandra/pull/2128', 'https://github.com/apache/cassandra/pull/2128/files', '[~smiklosovic] Thanks for the lightning fast fix! Yes, by DEBUG level I was referring to the NoSpamLogger which writes to debug.log.\\r\\n{quote}{{INFO [ScheduledTasks:1] 2016-08-10 16:56:50,209 NoSpamLogger.java:91 - Some operations were slow, details available at debug level (debug.log)}}\\r\\n{quote}\\r\\nThis issue is only about cassandra.log and debug.log, not CQLSH.\\xa0\\xa0To be most useful, the logger would log the CQL query text, not just repeat what WARN already logs. I.e. in <...> and probably without the query execution time:\\r\\n{quote}DEBUG [ScheduledTasks:1] 2016-08-10 16:56:50,211 MonitoringTask.java:173 - 3 operations were slow in the last 4998 msecs:\\r\\n<SELECT * FROM ks.test2 LIMIT 5000>, time 3026 msec - slow timeout 30 msec\\r\\n<SELECT * FROM ks.test2 WHERE id = 1 LIMIT 5000>, was slow 2 times: avg/min/max 330/325/335 msec - slow timeout 30 msec\\r\\n<SELECT * FROM ks.test2 WHERE token(id) > 0 LIMIT 5000>, time 1449 msec - slow timeout 30 msec\\r\\n{quote}', 'The idea behind the warn is to send the warning back to the client performing the query. Of course they can choose to ignore it which is why we also log the message to the logs.\\r\\nAdding the table name make total sense from the log point of view.\\r\\n[~bschoeni] I imagine that if you want to find the problematic query your intention is to ask the user to stop running that query. Would it not make sense at this point to had some Guardrails to block those queries rather than introducing more logging?', 'Yes you are right we probably do not need that in cql warn, this warning is propagated to drivers anyway (as far as I know) and users executing queries know what query they executed so we do not need to repeat the information about table and keyspace because they are the ones executing the query and they know it. So I will remove modifications about cqlsh warn.\\r\\n\\r\\nThe current PR (1) logs this via NoSpamLogger:\\r\\n{code:java}\\r\\nDEBUG [Native-Transport-Requests-1] 2023-02-02 11:23:47,086 NoSpamLogger.java:105 - Aggregation query used without partition key on table ks.tb, aggregation type: AGGREGATE_EVERYTHING, query: <SELECT c1, c2 FROM ks.tb>\\r\\n{code}\\r\\nI am working on the fix because it should write \"max(c1), max(c2)\" ....', '[~blerer]\\xa0 we could do guardrails as well, sure. By default it would be on warn and if turned on it would fail the query. We have done similar logic in CASSANDRA-18042.\\r\\n\\r\\n[~bschoeni] would be guardrail viable option for\\xa0 you? Do we want to emit warnings or failures or we just enable / disable these queries?\\r\\n\\r\\nThe PR for simply logging it is here:\\r\\n\\r\\n[https://github.com/apache/cassandra/pull/2128/files]\\r\\n\\r\\n\\xa0I will wait until there is a decision whether a guardrail is not better option before implementing that instead.', \"[~blerer] [~smiklosovic] both application code and ad hoc queries can generate these WARN messages.\\xa0 Ad hoc may be something like 'select count *'.\\xa0 We wouldn't want to block those and, in any event, if they're too slow they'll time out.\\xa0\\r\\n\\r\\nThe situations where the existing warning is a problem is with application code.\\xa0 There may be multiple application teams using the same database, and when the aggregation warn m \n",
            "my_comment: <SELECT c1 c2 FROM ks.tb>\n",
            "{code}\n",
            "I am working on the fix because it should write \"max(c1) max(c2)\" .... [~blerer]\\xa0 we could do guardrails as well sure. By default it would be on warn and if turned on it would fail the query. We have done similar logic in CASSANDRA-18042.\n",
            "\n",
            "[~bschoeni] would be guardrail viable option for\\xa0 you? Do we want to emit warnings or failures or we just enable / disable these queries?\n",
            "\n",
            "The PR for simply logging it is here:\n",
            "\n",
            "[https://github.com/apache/cassandra/pull/2128/files]\n",
            "\n",
            "\\xa0I will wait until there is a decision whether a guardrail is not better option before implementing that instead. \"[~blerer] [~smiklosovic] both application code and ad hoc queries can generate these WARN messages.\\xa0 Ad hoc may be something like select count *.\\xa0 We wouldnt want to block those and in any event if theyre too slow theyll time out.\\xa0\n",
            "\n",
            "The situations where the existing warning is a problem is with application code.\\xa0 There may be multiple application teams using the same database and when the aggregation warn m \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1007\n",
            "issue_type:  Improvement \n",
            "summary:  Make memtable flush thresholds per-CF instead of global \n",
            "description:  This is particularly useful in the scenario where you have a few CFs with a high volume of overwrite operations; increasing the memtable size/op count means that you can do the overwrite in memory before it ever hits disk.  Once on disk compaction is much more work for the system.\n",
            "comments:  ['Working on this.', 'Still working on this, Todd?', \"Yeah, got distracted with some things. I'll try and have it finished this weekend. Just have to rebase what I have with trunk\", 'does it make sense to still have a global default which is used when the flush thresholds are not specified in the CF (i.e. *optionally* set them per CF)?', 'Yes, but it should be a sane default (see CASSANDRA-1469) not an extra layer of configurability.', 'CHANGES:\\n\\nM       conf/cassandra.yaml\\n- Moves memtable settings into cf by demonstration.\\n\\nM       src/java/org/apache/cassandra/db/Table.java\\nM       src/java/org/apache/cassandra/db/ColumnFamilyStore.java\\nM       src/java/org/apache/cassandra/db/Memtable.java\\n- All the places where we previously asked for the global flushtime/size/ops now ask for it on a per-CF basis.\\n\\nM       src/java/org/apache/cassandra/config/CFMetaData.java\\n- The bulk of the changes. This adds the new field, handles all its constructors, into and out of avro/thrift, validation, and defaulting.\\n\\nM       src/java/org/apache/cassandra/config/DatabaseDescriptor.java\\nM       src/java/org/apache/cassandra/config/Config.java\\nM       src/java/org/apache/cassandra/config/RawColumnFamily.java\\nM       src/java/org/apache/cassandra/config/Converter.java\\n- Changed the way we read in the YAML, and updated the converter to handle 0.6 -> 0.7 transitions.\\n\\nM       src/java/org/apache/cassandra/thrift/CassandraServer.java\\nM       src/java/org/apache/cassandra/avro/CassandraServer.java\\n- Updated add/update CF to add new fields and validate them appropriately.\\n\\nM       interface/cassandra.genavro\\nM       interface/thrift/gen-java/org/apache/cassandra/thrift/Constants.java\\nM       interface/thrift/gen-java/org/apache/cassandra/thrift/CfDef.java\\nM       interface/cassandra.thrift\\n- Adds three new fields to the CfDef, and adds the generated bindings.', \"Nope, tests are now crashing and burning.\\n\\nEDIT1:\\ntest/conf/cassandra.yaml was still using global memtable settings, so that was the first error.\\nNow it's complaining that KSMetaData.deflate() isn't defined, which is broken.\", 'Tests fail to compile:\\n\\n    [javac] Compiling 80 source files to /srv/cassandra/build/test/classes\\n    [javac] /srv/cassandra/test/unit/org/apache/cassandra/db/DefsTest.java:686: cannot find symbol\\n    [javac] symbol  : constructor CFMetaData(java.lang.String,java.lang.String,org.apache.cassandra.db.ColumnFamilyType,org.apache.cassandra.db.marshal.UTF8Type,<nulltype>,java.lang.String,int,boolean,double,int,int,org.apache.cassandra.db.marshal.BytesType,int,int,java.util.Map<byte[],org.apache.cassandra.config.ColumnDefinition>)\\n    [javac] location: class org.apache.cassandra.config.CFMetaData\\n    [javac]         return new CFMetaData(ks,\\n    [javac]                ^\\n\\n', 'Updated but still broken.', \"Apparantly it's only broken for me.\", 'Updated the pretty print to not crash on usage. Now uses commons-lang ToStringBuilder instead.', 'Committed.', 'Integrated in Cassandra #567 (See [https://hudson.apache.org/hudson/job/Cassandra/567/])\\n    Make memtable flush thresholds per-CF instead of global.  Patch by Jon Hermes, reviewed by brandonwilliams for CASSANDRA-1007\\n'] \n",
            "my_comment: class org.apache.cassandra.config.CFMetaData\n",
            "    [javac]         return new CFMetaData(ks\n",
            "    [javac]                ^\n",
            "\n",
            " Updated but still broken. \"Apparantly its only broken for me.\" Updated the pretty print to not crash on usage. Now uses commons-lang ToStringBuilder instead. Committed. Integrated in Cassandra #567 (See [https://hudson.apache.org/hudson/job/Cassandra/567/])\n",
            "    Make memtable flush thresholds per-CF instead of global.  Patch by Jon Hermes reviewed by brandonwilliams for CASSANDRA-1007\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1843\n",
            "issue_type:  Bug \n",
            "summary:  Indexes: CF MBeans for automatic indexes are never unregistered when they are deleted. \n",
            "description:  Add, delete, and add the same index and you should get a stacktrace to this effect:\n",
            "comments:  ['Someone already wrote this method and just forgot to call it.\\nWhoops.', 'Committed.', 'updated CHANGES', 'Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])\\n    ', 'Integrated in Cassandra #625 (See [https://hudson.apache.org/hudson/job/Cassandra/625/])\\n    '] \n",
            "my_comment:  Someone already wrote this method and just forgot to call it.\n",
            "Whoops. Committed. updated CHANGES Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])\n",
            "     Integrated in Cassandra #625 (See [https://hudson.apache.org/hudson/job/Cassandra/625/])\n",
            "     \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-34\n",
            "issue_type:  Bug \n",
            "summary:  Hinted handoff rows never get deleted \n",
            "description:  from the list: \"after the hints are delivered, the hinted keys are deleted from the hinted CF only, but not from the application CF.\"\n",
            "comments:  ['Sending out hinted data needs correct RowMutation support.', 'Attach a fix. About the patch.\\n\\n1. Make sendMessage blocking.\\n2. Delete the rows in application CF after hinted data is sent. To do that, we need to collect the largest timestamp among columns in a CF and then delete the CF with the largest timestamp.\\n3. When a column is in a deleted CF and their timestamps are the same, the rule is that the deleted CF wins. This rule is needed for step 2 above. Change CFStore.removeDeleted according to this rule.', 'Patch v2. Added some description on hinted data gets delivered.', 'any idea what the purpose of this code in runHints is?  why flush if nothing changed?\\n\\n            \\tif(hintedColumnFamily == null)\\n            \\t{\\n                    columnFamilyStore_.forceFlush();\\n            \\t\\treturn;\\n            \\t}\\n', 'also, now that we have range queries, it seems that a normal CF would be a better fit for this than a single super CF with keys as supercolumns.  i guess that is a separate issue.', \"committed the sendMessage fix.  I've reworked the rest substantially in two parts.\\n\\n01 is just refactoring / cleanup.\\n\\n02 includes your fix to deleteKey, and also:\\n A make hint generation include a real timestamp so we can do meaningful deletes\\n B call removeDeleted on the data we read locally to purge tombstones\\n C because of (B) any supercolumn w/o subcolumns simply won't exist so we know we can skip re-deleting the endpoint data.  so deleteKey becomes deleteHintedData.\\n D because deleted data is not immediately purged, increased the scheduled interval fro 20min to 60 to reduce the load of scanning the hint CF.\\n\\n(for another ticket: since all this is purely local data and not subject to read repair we should find a way to GC it immediately post-delete.)\\n\\nhow does this look to you?\", 'Integrated in Cassandra #57 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/57/])\\n    make sendMessage only return true after ack by recipient.\\npatch by Jun Rao; reviewed by jbellis for \\n', 'created CASSANDRA-128 for improvments beyond the scope of 0.3', \"Looked at the new patch. Here are some comments.\\n\\n1. Move the comments above sendMessage to the beginning of class.\\n\\n2. There is compilation error in MinorCompactionManager because of the removed HintedHandOff class.\\n\\n3. Your new code deletes a column using the same timestamp as the column to be deleted. This is a more fundamental question. If a column has a non-delete entry and a deleted entry with the same timestamp, which one wins? I don't think that we can rely on the ordering of the insertion/deletion. This is because the insertion and the deletion can end up in different SSTables. During compaction, FileStruct is sorted only by keys. Therefore, columns from different SSTables with the same key can come in arbitrary order. One solution is to modify CF.addColumn such that a deletion always wins when the column timestamp is the same. Not sure if there is any other implications, though.\\n\", \"4. It's probably worthwhile to make intervalInMins_ in HHM configurable.\", 'patch to make tombstones have higher precedence than non- for same timestamp in Column (and SuperColumn, to be consistent w/ C and CF)', \"(incorporated changes for comments (1) and (2) into my patchset, not bothering resubmitting those unless you really want 'em)\", 'noted comment (4) on CASSANDRA-128', 'Comments for the new patch.\\n1. In CFStore.removeDeleted(), we should add comments to explain how we resolve the conflicts among CF, SC, and C when the timestamps are the same. As time goes, we are likely to forget those decisions that we have made.\\n\\nOther than that, the patch looks fine to me.\\n', 'done and committed.', 'Integrated in Cassandra #63 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/63/])\\n    '] \n",
            "my_comment: since all this is purely local data and not subject to read repair we should find a way to GC it immediately post-delete.)\n",
            "\n",
            "how does this look to you?\" Integrated in Cassandra #57 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/57/])\n",
            "    make sendMessage only return true after ack by recipient.\n",
            "patch by Jun Rao; reviewed by jbellis for \n",
            " created CASSANDRA-128 for improvments beyond the scope of 0.3 \"Looked at the new patch. Here are some comments.\n",
            "\n",
            "1. Move the comments above sendMessage to the beginning of class.\n",
            "\n",
            "2. There is compilation error in MinorCompactionManager because of the removed HintedHandOff class.\n",
            "\n",
            "3. Your new code deletes a column using the same timestamp as the column to be deleted. This is a more fundamental question. If a column has a non-delete entry and a deleted entry with the same timestamp which one wins? I dont think that we can rely on the ordering of the insertion/deletion. This is because the insertion and the deletion can end up in different SSTables. During compaction FileStruct is sorted only by keys. Therefore columns from different SSTables with the same key can come in arbitrary order. One solution is to modify CF.addColumn such that a deletion always wins when the column timestamp is the same. Not sure if there is any other implications though.\n",
            "\" \"4. Its probably worthwhile to make intervalInMins_ in HHM configurable.\" patch to make tombstones have higher precedence than non- for same timestamp in Column (and SuperColumn to be consistent w/ C and CF) \"(incorporated changes for comments (1) and (2) into my patchset not bothering resubmitting those unless you really want em)\" noted comment (4) on CASSANDRA-128 Comments for the new patch.\n",
            "1. In CFStore.removeDeleted() we should add comments to explain how we resolve the conflicts among CF SC and C when the timestamps are the same. As time goes we are likely to forget those decisions that we have made.\n",
            "\n",
            "Other than that the patch looks fine to me.\n",
            " done and committed. Integrated in Cassandra #63 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/63/])\n",
            "     \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2296\n",
            "issue_type:  Bug \n",
            "summary:  Scrub resulting in \"bloom filter claims to be longer than entire row size\" error \n",
            "description:  Doing a scrub on a node which I upgraded from 0.7.1 (was previously 0.6.8) to 0.7.3. Getting this error multiple times:\n",
            "comments:  [\"With debug logging turned on it looks like this:\\n\\n{noformat}\\n[lots of rows around 119 bytes long]\\nDEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 559) row 337a306f615f666c38756a is 119 bytes\\nDEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 588) Index doublecheck: row 337a306f615f666c38756a is 119 bytes\\nDEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 550) Reading row at 44385\\nDEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 559) row 34306536785f666f666b65 is 0 bytes\\nDEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,252 CompactionManager.java (line 588) Index doublecheck: row 34306536785f666f666b65 is 0 bytes\\n WARN [CompactionExecutor:1] 2011-03-08 20:34:12,253 CompactionManager.java (line 606) Non-fatal error reading row (stacktrace follows)\\njava.io.IOError: java.io.EOFException: bloom filter claims to be 734305 bytes, longer than entire row size 0\\n        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:125)\\n        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:597)\\n        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:57)\\n        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:196)\\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\\n        at java.lang.Thread.run(Thread.java:680)\\nCaused by: java.io.EOFException: bloom filter claims to be 734305 bytes, longer than entire row size 0\\n        at org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:113)\\n        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:95)\\n        ... 8 more\\n WARN [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 632) Row is unreadable; skipping to next\\nDEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 550) Reading row at 44406\\nDEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 559) row 34616465655f66707a6178 is 119 bytes\\nDEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 588) Index doublecheck: row 34616465655f66707a6178 is 119 bytes\\n[lots more rows around 119 bytes]\\n{noformat}\\n\\nIn other words: there's an row that's empty except for the key, which is causing the problem because we're not supposed to write rows like that.  I checked with a hex editor and that's what it looks like.\\n\\nThe good news is that scrub is correctly skipping it and recovering everything else fine.\\n\\nThe bad news is we have (or possibly, had) a bug that was causing those empty rows to be written.\", 'added asserts to catch zero-length rows in r1079650', 'Got the following error while restarting *after* I ran the scrub on that same node:\\n\\n{code}\\nERROR [CompactionExecutor:1] 2011-03-08 19:54:48,023 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]\\njava.io.IOError: java.io.EOFException\\n        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:117)\\n        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:67)\\n        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:179)\\n        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:144)\\n        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:136)\\n        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTable \n",
            "my_comment: java.io.EOFException\n",
            "        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:117)\n",
            "        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:67)\n",
            "        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:179)\n",
            "        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:144)\n",
            "        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:136)\n",
            "        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTable \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-10363\n",
            "issue_type:  Bug \n",
            "summary:  NullPointerException returned with select ttl(value), IN, ORDER BY and paging off \n",
            "description:  Running this query with paging off returns a NullPointerException:\n",
            "comments:  ['The patch for 2.1 is [here|https://github.com/apache/cassandra/compare/trunk...blerer:10363-2.1] and the patch for 2.2 is [here|https://github.com/apache/cassandra/compare/trunk...blerer:10363-2.2].\\n\\nThe problem comes from the fact that the column index used by the {{Comparator}} performing the sort is the one of the fetched columns and not the one of the {{ResultSet}} columns. \\nIn the case where no {{Selector}} is used the two indexes are the same. It is not always the case when some {{Selectors}} are used.\\n\\nThe patches make sure than the index used to sort the columns are the proper ones.\\nThey also fix {{UntypedResultSet}} which was providing access to the columns used for ordering. \\n\\n* The unit test results for 2.1 are [here|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-10363-2.1-testall/4/]\\n* The dtest results for 2.1 are [here|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-10363-2.1-dtest/4/]\\n* The unit test results for 2.2 are [here|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-10363-2.2-testall/2/]\\n* The dtest results for 2.2 are [here|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-10363-2.2-dtest/2/]\\n* The unit test results for 3.0 are [here|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-10363-3.0-testall/3/]\\n* The dtest results for 3.0 are [here|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-10363-3.0-dtest/3/]', '+1. My only nit is the naming of {{Selector::isReturningColumn}}, which seems a little clunky. Do you think {{isSimpleSelectorFactory}} would work there, or is it too specific? It would be more consistent with the existing methods, {{isAggregateSelectorFactory/isWriteTimeSelectorFactory/isTTLSelectorFactory}}. ', \"I've attached a patch backporting this to 2.0, not for actually committing but so those unable to upgrade just yet can patch their own systems if necessary. The test changes the expectations for a few scenarios from the 2.1+ version because CASSANDRA-4911 isn't in 2.0 & so {{ORDER BY}} can only contain columns in the selection.\\n\\n[branch|https://github.com/beobal/cassandra/tree/10363-2.0], [testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10363-2.0-testall/], [dtests|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10363-2.0-dtest/] (test runs pending)\\n\\nEdit: there are a few dtest failures in the run above, but checking these against 2.0 there aren't any new failures.\", 'Thanks for the 2.0 patch it looks good to me.', 'Committed in 2.1 at f587397c9c41c1a68b4e46fc16bad8d48c975e4d and merged in 2.2, 3.0 and trunk', 'This should probably get a CHANGES.txt entry.', 'Sorry, I forgot it. \\nI pushed the entry in 2.1 at 86583af4ca0eac34725136adee3143f9b14b75b4 and merged it into 2.2, 3.0 and trunk'] \n",
            "my_comment: there are a few dtest failures in the run above but checking these against 2.0 there arent any new failures.\" Thanks for the 2.0 patch it looks good to me. Committed in 2.1 at f587397c9c41c1a68b4e46fc16bad8d48c975e4d and merged in 2.2 3.0 and trunk This should probably get a CHANGES.txt entry. Sorry I forgot it. \n",
            "I pushed the entry in 2.1 at 86583af4ca0eac34725136adee3143f9b14b75b4 and merged it into 2.2 3.0 and trunk \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2107\n",
            "issue_type:  Improvement \n",
            "summary:  MessageDigests are created in several places, centralize the creation and error handling \n",
            "description:  MessageDigest.getInstance(\"SomeAlg\") throws NoSuchAlgorithm exception (a checked exception).  This is annoying as it causes everyone that uses standard algs like MD5 to surround their code in try/catch.  We should concentrate the creation in one method that doesn't raise an exception (i.e. catches NoSuchAlgorithm and raises a RuntimeException) just to clean the code up a little.\n",
            "comments:  ['attached patch puts all MessageDigest creation into util class.', \"Let's use http://guava-libraries.googlecode.com/svn-history/r2/trunk/javadoc/com/google/common/io/MessageDigestAlgorithm.html\", 'The above link is from SVN revision 2 on Sep 15, 2009 \"initial code dump\".  It doesn\\'t look like MessageDigestAlgorithm (or any equivalent) exists in Guava (even in r2 despite the fact that it has javadoc for it).', 'rebased', \"bq. It doesn't look like MessageDigestAlgorithm (or any equivalent) exists in Guava\\n\\nBummer.\\n\\nFBUtilities is fine, but let's refactor to use the threadlocal instead of going back to using the synchronized md5 directly.\", \"I thought about that, but couldn't convince myself that it was safe to do without looking at all the invocations closely.  In particular, if any thread inadvertently passed the result of the thread local to another thread (e.g. via putting it in a WrappedRunnable and passing it to another stage).  \\n\\nThe subtle difference between:\\n\\n{code}\\nx = getThreadLocal(); \\nstage.submit(new Runnable(){x.doSomething()});\\n{code}\\n\\nand \\n\\n{code}\\nRunnable r = new Runnable(){getThreadLocal().doSomething()}\\nstage.submit(r);\\n{code}\\n\\nworried me because there would be no exceptions or errors, just incorrect results.  I was also concerned that some implementations of digests might return the same underlying byte array (as opposed to a copy) it used to compute the hash when .digest() was called.\\n\\nthat being said, attached patch uses thread local instance.\", 'rebased + updated GuidGenerator to use the FBUtilities threadlocal + committed', 'Integrated in Cassandra-0.7 #296 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/296/])\\n    centralize MessageDigest creation and use threadlocals for MD5s\\npatch by mdennis; reviewed by jbellis for CASSANDRA-2107\\n'] \n",
            "my_comment:  attached patch puts all MessageDigest creation into util class. \"Lets use http://guava-libraries.googlecode.com/svn-history/r2/trunk/javadoc/com/google/common/io/MessageDigestAlgorithm.html\" The above link is from SVN revision 2 on Sep 15 2009 \"initial code dump\".  It doesn\\t look like MessageDigestAlgorithm (or any equivalent) exists in Guava (even in r2 despite the fact that it has javadoc for it). rebased \"bq. It doesnt look like MessageDigestAlgorithm (or any equivalent) exists in Guava\n",
            "\n",
            "Bummer.\n",
            "\n",
            "FBUtilities is fine but lets refactor to use the threadlocal instead of going back to using the synchronized md5 directly.\" \"I thought about that but couldnt convince myself that it was safe to do without looking at all the invocations closely.  In particular if any thread inadvertently passed the result of the thread local to another thread (e.g. via putting it in a WrappedRunnable and passing it to another stage).  \n",
            "\n",
            "The subtle difference between:\n",
            "\n",
            "{code}\n",
            "x = getThreadLocal(); \n",
            "stage.submit(new Runnable(){x.doSomething()});\n",
            "{code}\n",
            "\n",
            "and \n",
            "\n",
            "{code}\n",
            "Runnable r = new Runnable(){getThreadLocal().doSomething()}\n",
            "stage.submit(r);\n",
            "{code}\n",
            "\n",
            "worried me because there would be no exceptions or errors just incorrect results.  I was also concerned that some implementations of digests might return the same underlying byte array (as opposed to a copy) it used to compute the hash when .digest() was called.\n",
            "\n",
            "that being said attached patch uses thread local instance.\" rebased + updated GuidGenerator to use the FBUtilities threadlocal + committed Integrated in Cassandra-0.7 #296 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/296/])\n",
            "    centralize MessageDigest creation and use threadlocals for MD5s\n",
            "patch by mdennis; reviewed by jbellis for CASSANDRA-2107\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1321\n",
            "issue_type:  Improvement \n",
            "summary:  loadSchemaFromYaml should push migrations to cluster. \n",
            "description: None\n",
            "comments: None\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-16283\n",
            "issue_type:  Bug \n",
            "summary:  Incorrect output in \"nodetool status -r\" \n",
            "description:  nodetool status -r not working well on C* 4,\n",
            "comments:  ['See {{ToolRunner}} on how to test tooling #collaborating', 'Fixed via PR https://github.com/apache/cassandra/pull/845', 'Thank you [~wolfenhaut] :)', 'Thanks [~wolfenhaut]. Would you consider adding a test and moving it to \"Ready for Revidew\"?', 'Sure, and thanks!', 'Thanks for taking a look at this, and for the patch. I reviewed and left a few comments on the PR.\\r\\n\\r\\nWhile looking at this I think I noticed one other potential flaw around looking things up by endpoint (unrelated the resolved host lookup logic). It seems like the tool is never showing anything definitive for the \"Owns (effective)\" column. Resolved or not, the table always just produces \\'?\\'.', \"Did review it. Mostly aligned to what Adam mentioned + a minor. The only real concern is whether we tackle the 'Owns' issue here or on a new ticket.\", \"I personally don't see a reason to split out a new ticket, but I don't feel too strongly about it. To me it fits the description, and it's closely related in code. I can take a look at revising the patch if [~wolfenhaut] doesn't care to.\", \"[~wolfenhaut] Just checking to see if you're going to have time to get back to this review? If not, I have no problem carrying the work forward. Just wanted to check in first.\", '[~aholmber]\\r\\n\\r\\nBeen a little busy, feel free to run with this if you want...\\r\\n\\r\\nThanks for letting me help out!\\r\\n\\r\\n--scott\\xa0', 'Will do. Thanks for getting back on the matter, and thanks for getting it this far.', 'Updated branch with PR here:\\r\\nhttps://github.com/aholmberg/cassandra/pull/38\\r\\n\\r\\ntl;dr there are several maps in Status that were broken depending on whether host is resolved (-r) or port printing was enabled (-pp), and some were using different forms of string conversion. Now everything is normalized to always using the form \"<ip>:<port>\" for command internal maps. Port printing and DNS resolution only apply to printing the host string for table output.\\r\\n\\r\\nCI running:\\r\\nhttps://app.circleci.com/pipelines/github/aholmberg/cassandra?branch=CASSANDRA-16283', \"I see that the port is not fixed for unit tests, and that there is a jvm-dtest that codified the question marks. I'll address those test issues tomorrow.\", 'Thank you, [~aholmber]. Please let me know when you are done, I will be happy to review the patch.', \"The branch is updated. Full CI run finished revealing an in-jvm test I overlooked. Another limited run is still running on that tweak, but I think it's ready for review. Thanks in advance.\", 'Jenkins run pushed [here | [https://jenkins-cm4.apache.org/job/Cassandra-devbranch/333/#showFailuresLink].]\\r\\n\\r\\nReview in progress, thanks :)\\xa0', 'The patch looks good to me, I left just a few small comments [here|https://github.com/ekaterinadimitrova2/cassandra/commit/9e79a336bf5348ac6fae59dac7ffa60eb4c29bae]\\xa0- I created a new branch with squashed commits for myself while reviewing.\\r\\n\\r\\nI believe the two main things are CHANGES.txt entry is missing and the last in-jvm test [failing|https://jenkins-cm4.apache.org/job/Cassandra-devbranch/333/]\\xa0to be fixed.\\xa0', 'Thanks for the review. I pushed updates. I think CI looks good as well.', \"Thanks both [~wolfenhaut] and [~aholmber] for the patch!\\r\\nLGTM +1\\r\\nCircleCI has unrelated failures\\r\\n[~brandon.williams] can you review it as a second committer, please?\\r\\nI believe Berenguer's concerns were also addressed. We need only to move the CHANGES.txt update to the top on commit. :-)  \", \"+1\\r\\n\\r\\nbq. We need only to move the CHANGES.txt update to the top on commit.\\r\\n\\r\\nHistorically we've left it out of the patch, it's up to the committer to handle, and it often ends up conflicting too when in a patch.\", 'Patch rebased and squashed [here|https://github.com/ekaterinadimitrova2/cassandra/commit/f99faca3e62dad3c748673e306433ce242a31b92]\\r\\nCommit pending on final Jenkins run in progress [here|https://jenkins-cm4.apache.org/job/Cassandra-devbranch/352/]', 'Patch committed [here| https://github.com/apache/cassandra/ \n",
            "my_comment:  See {{ToolRunner}} on how to test tooling #collaborating Fixed via PR https://github.com/apache/cassandra/pull/845 Thank you [~wolfenhaut] :) Thanks [~wolfenhaut]. Would you consider adding a test and moving it to \"Ready for Revidew\"? Sure and thanks! Thanks for taking a look at this and for the patch. I reviewed and left a few comments on the PR.\n",
            "\n",
            "While looking at this I think I noticed one other potential flaw around looking things up by endpoint (unrelated the resolved host lookup logic). It seems like the tool is never showing anything definitive for the \"Owns (effective)\" column. Resolved or not the table always just produces \\?\\. \"Did review it. Mostly aligned to what Adam mentioned + a minor. The only real concern is whether we tackle the Owns issue here or on a new ticket.\" \"I personally dont see a reason to split out a new ticket but I dont feel too strongly about it. To me it fits the description and its closely related in code. I can take a look at revising the patch if [~wolfenhaut] doesnt care to.\" \"[~wolfenhaut] Just checking to see if youre going to have time to get back to this review? If not I have no problem carrying the work forward. Just wanted to check in first.\" [~aholmber]\n",
            "\n",
            "Been a little busy feel free to run with this if you want...\n",
            "\n",
            "Thanks for letting me help out!\n",
            "\n",
            "--scott\\xa0 Will do. Thanks for getting back on the matter and thanks for getting it this far. Updated branch with PR here:\n",
            "https://github.com/aholmberg/cassandra/pull/38\n",
            "\n",
            "tl;dr there are several maps in Status that were broken depending on whether host is resolved (-r) or port printing was enabled (-pp) and some were using different forms of string conversion. Now everything is normalized to always using the form \"<ip>:<port>\" for command internal maps. Port printing and DNS resolution only apply to printing the host string for table output.\n",
            "\n",
            "CI running:\n",
            "https://app.circleci.com/pipelines/github/aholmberg/cassandra?branch=CASSANDRA-16283 \"I see that the port is not fixed for unit tests and that there is a jvm-dtest that codified the question marks. Ill address those test issues tomorrow.\" Thank you [~aholmber]. Please let me know when you are done I will be happy to review the patch. \"The branch is updated. Full CI run finished revealing an in-jvm test I overlooked. Another limited run is still running on that tweak but I think its ready for review. Thanks in advance.\" Jenkins run pushed [here | [https://jenkins-cm4.apache.org/job/Cassandra-devbranch/333/#showFailuresLink].]\n",
            "\n",
            "Review in progress thanks :)\\xa0 The patch looks good to me I left just a few small comments [here|https://github.com/ekaterinadimitrova2/cassandra/commit/9e79a336bf5348ac6fae59dac7ffa60eb4c29bae]\\xa0- I created a new branch with squashed commits for myself while reviewing.\n",
            "\n",
            "I believe the two main things are CHANGES.txt entry is missing and the last in-jvm test [failing|https://jenkins-cm4.apache.org/job/Cassandra-devbranch/333/]\\xa0to be fixed.\\xa0 Thanks for the review. I pushed updates. I think CI looks good as well. \"Thanks both [~wolfenhaut] and [~aholmber] for the patch!\n",
            "LGTM +1\n",
            "CircleCI has unrelated failures\n",
            "[~brandon.williams] can you review it as a second committer please?\n",
            "I believe Berenguers concerns were also addressed. We need only to move the CHANGES.txt update to the top on commit. :-)  \" \"+1\n",
            "\n",
            "bq. We need only to move the CHANGES.txt update to the top on commit.\n",
            "\n",
            "Historically weve left it out of the patch its up to the committer to handle and it often ends up conflicting too when in a patch.\" Patch rebased and squashed [here|https://github.com/ekaterinadimitrova2/cassandra/commit/f99faca3e62dad3c748673e306433ce242a31b92]\n",
            "Commit pending on final Jenkins run in progress [here|https://jenkins-cm4.apache.org/job/Cassandra-devbranch/352/] Patch committed [here| https://github.com/apache/cassandra/ \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5918\n",
            "issue_type:  Improvement \n",
            "summary:  Remove CQL2 entirely from Cassandra 3.0 \n",
            "description:  CQL2 is officially no longer worked on since 1.2. cqlsh no longer supports CQL2 as of Cassandra 2.0.\n",
            "comments:  ['Added the deprecation warning to 2.0 in 5fe46e145adfe54a1fc4521fd274833e3bce4ac2.', 'If we are going to remove it as a breaking change, this should be 3.0. There are still people in production using CQL2. If 2.x issues a warning, then we can give them until 3.x to migrate away. ', 'That\\'s basically what 2.2 is, \"not the next major release, but the one after that.\"', \"If it takes calling it 3.0 to make you happy, that's doable. :)\", 'Your concern for my happiness is heart warming. heh\\n\\nWhatever it is that isn\\'t \"we just took away functionality inside 2.x\" will be just fine.', \"FWIW, this sounds like a reasonable idea to me. It is also encouraging as a model for deprecating features which some small set of people might be using now but which no one should start using because they're unsupported/unmaintained.\", 'Pushed the commit to https://github.com/iamaleksey/cassandra/commits/5918.\\n\\nNot sure what to do with thirft definitions (besides obviously bumping the major). Throw IRE for all the CQL2 methods (as in the commit), or get rid of them altogether, erasing the last and only traces of CQL2 existence (other than NEWS and CHANGES).', \"I'd say it's marginally more user-friendly to throw IRE than to rely on the Thrift server to respond with whatever its no-such-method exception is.\", 'Left them alone (throwing exceptions instead of removing entirely). That, and regenerated thrift (tiny change). Pushed -f to the same branch.', 'Ship it!', 'Committed, thanks.'] \n",
            "my_comment:  Added the deprecation warning to 2.0 in 5fe46e145adfe54a1fc4521fd274833e3bce4ac2. If we are going to remove it as a breaking change this should be 3.0. There are still people in production using CQL2. If 2.x issues a warning then we can give them until 3.x to migrate away.  That\\s basically what 2.2 is \"not the next major release but the one after that.\" \"If it takes calling it 3.0 to make you happy thats doable. :)\" Your concern for my happiness is heart warming. heh\n",
            "\n",
            "Whatever it is that isn\\t \"we just took away functionality inside 2.x\" will be just fine. \"FWIW this sounds like a reasonable idea to me. It is also encouraging as a model for deprecating features which some small set of people might be using now but which no one should start using because theyre unsupported/unmaintained.\" Pushed the commit to https://github.com/iamaleksey/cassandra/commits/5918.\n",
            "\n",
            "Not sure what to do with thirft definitions (besides obviously bumping the major). Throw IRE for all the CQL2 methods (as in the commit) or get rid of them altogether erasing the last and only traces of CQL2 existence (other than NEWS and CHANGES). \"Id say its marginally more user-friendly to throw IRE than to rely on the Thrift server to respond with whatever its no-such-method exception is.\" Left them alone (throwing exceptions instead of removing entirely). That and regenerated thrift (tiny change). Pushed -f to the same branch. Ship it! Committed thanks. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-399\n",
            "issue_type:  Bug \n",
            "summary:  Consisteny Level of ZERO blocks for ack on Commit Log \n",
            "description:  If consistency level is set to ZERO and endpoint is local, clients must wait for a write to the commit log. We need to remove this special case, and just send through MessagingService.getMessagingInstance().sendOneWay. \n",
            "comments:  ['I remember we put this in to get limited read-your-writes consistency for a session. (Log + rm.apply()  before ack.)\\nSee CASSANDRA-132.\\n\\nThere are cases when this is not the right thing to do (eg skip logging during a load operation), but for a normal client, read-your-writes is way easier to deal with than pure eventual consistency. \\n\\n\\n\\n\\n\\n', \"Committed before I saw Sandeep's objection.  But insert -- the method that handles ConsistencyLevel.ZERO -- is the wrong place to do this.  If you want any blocking you need to use ONE or higher, that's how it's supposed to work.\", '(which goes to the separate insertBlocking method.)', 'to clarify: I am +1 on adding a CASSANDRA-132 special case for insertBlocking, but -1 on having it in insert.  My bad for not catching that in review originally.  Should we re-open 132?', 'I agree that I think this special case should live in insertBlocking. ', \"I'm +1 on the patch, but we should re-open 132. \\nCurrent insertBlocking=1 is not the same as session level read-your-writes.\\nI guess we could just special-case this write for insertBlocking=1. I'd rather not introduce a new ConsistencyLevel.\\n\\n\\nConsistencyLevel.ZERO should help stress test MessagingService... I remember with the old codepath writes would get queued up in the MessagingService and we ended up with a few lost writes. Should be interesting to see if those problems pop up again.\\n\\n\\n\\n\", 'Integrated in Cassandra #182 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/182/])\\n    r/m special case of local destination when writing with ConsistencyLevel.ZERO, since it causes blocking for commitlog.  (MessagingService still optimizes out the network write/read.)  Patch by Chris Goffinet; reviewed by jbellis for \\n'] \n",
            "my_comment: I am +1 on adding a CASSANDRA-132 special case for insertBlocking but -1 on having it in insert.  My bad for not catching that in review originally.  Should we re-open 132? I agree that I think this special case should live in insertBlocking.  \"Im +1 on the patch but we should re-open 132. \n",
            "Current insertBlocking=1 is not the same as session level read-your-writes.\n",
            "I guess we could just special-case this write for insertBlocking=1. Id rather not introduce a new ConsistencyLevel.\n",
            "\n",
            "\n",
            "ConsistencyLevel.ZERO should help stress test MessagingService... I remember with the old codepath writes would get queued up in the MessagingService and we ended up with a few lost writes. Should be interesting to see if those problems pop up again.\n",
            "\n",
            "\n",
            "\n",
            "\" Integrated in Cassandra #182 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/182/])\n",
            "    r/m special case of local destination when writing with ConsistencyLevel.ZERO since it causes blocking for commitlog.  (MessagingService still optimizes out the network write/read.)  Patch by Chris Goffinet; reviewed by jbellis for \n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-527\n",
            "issue_type:  Improvement \n",
            "summary:  clean up gossip notifications to the rest of the system \n",
            "description: None\n",
            "comments: None\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6962\n",
            "issue_type:  Improvement \n",
            "summary:  examine shortening path length post-5202 \n",
            "description:  From CASSANDRA-5202 discussion:\n",
            "comments:  [\"It's actually not clear to me that the benefit of a shorter path length is worth giving up the redundancy that makes it easy to manually toss sstables around.  But manually pushing sstables is a lot less necessary in 2014 than it was 4 years ago...\", \"I'm also not sure I'm a fan of completely removing any indication of the table name in the file name: feels pretty error prone. What about keeping the keyspace/table name as now (for the sake of making it easy to not mix sstables by mistake), but limit them to say 10 characters each (just for the file name), truncating the name if necessary?\", 'bq. limit them to say 10 characters each (just for the file name), truncating the name if necessary?\\n\\nWe can truncate name to fit within os path limit adaptively with some calculation.\\n\\nHow about completely omit Keyspace name but keep ColumnFamily name and adaptively adjust(truncate) its name?', 'This turns out to be a bit complex than I first thought because secondary index CFs are flushing to the same directory. :(\\nAny ideas?', \"I'm inclined to say we should leave it alone for 2.1.  If Windows users start yelling for it in 3.0 then we can address then.  Lots of other stuff to work on in the meantime.\", \"Patch attached with unit test.\\nI ended up the following naming convention/directory structure:\\n\\n{code}\\n/var/lib/cassandra/data/ks/cf-a85fc210cb1011e3a15f9d25721dbb44\\n    /.idx\\n        ka-1-Data.db\\n        ...\\n    /snapshots\\n        /my_snapshot\\n            /.idx\\n                ka-1-Data.db\\n                ...\\n            ka-1-Data.db\\n            ...\\n    /backups\\n        /.idx\\n            ka-1-Data.db\\n            ...\\n        ka-1-Data.db\\n        ...\\n    ka-1-Data.db\\n    ...\\n    ks-cf-jb-123-Data.db    (Older version can co exist)\\n    ks-cf.idx-jb-2-Data.db\\n{code}\\n\\nHighlight:\\n\\n* Keyspace/ColumnFamily name is omitted from filename.\\n* If it is 'temporary' file, then file name would be 'tmp-ka-1-Data.db'.\\n* Each secondary index has its own SSTable directory whose name starts with '.'. This is to distinguish from snapshots/backup directories.\\n* Older version of SSTable file can co-exist along with new directory structure.\", \"Still not a fan of removing the keyspace/table name from the filename. Imo we should go with Jonathan's suggestion above, leave this alone until people actually start yelling cause I don't remember a single user reporting that path lengths was a blocker, so I'd rather not change the directory layout once again (which might break user scripts) until we have practical evidence it's a problem..\", 'WDYT [~JoshuaMcKenzie]?  How likely is this to be a problem for Windows users?', 'CASSANDRA-4110 and the limitations in Schema.java provide us some protection but there\\'s really nothing to stop users nesting their cassandra data 250 characters deep in a path and having things blow up on them regardless of what length we limit ourselves to.\\n\\nOn snapshots we\\'ll be using 204 chars worst-case (48 KS, 48 CF, *2 each, 9 for \"snapshots\", 3 for slashes) so that doesn\\'t leave us a lot of breathing room on path for data_file_directories.  Maybe lowering the NAME_LENGTH in Schema.java would be appropriate given CASSANDRA-7136?  Do we have a lot of users rolling out 40+ char KS and CF names in general, much less on Windows?', 'We had a lot of people hit issues the first time we lowered the max name lengths in schemas', \"Looks like it was lowered across the board and not on a per-platform basis.  I can see a file-path limitation on linux being a surprise but it's part of the ecosystem people are used to with windows.\", 'bq. Looks like it was lowered across the board and not on a per-platform basis.\\n\\nYeah, nobody *really* needs keyspace names that long; not worth explaining why your linux snapshot broke when you moved to windows if we can avoid it.\\n\\nbq. On snapshots we\\'ll be using 204 chars worst-case (48 KS, 48 CF, *2 each, 9 for \"snapshots\", 3 for slashes) so that doesn\\'t leave us a lot of breat \n",
            "my_comment: feels pretty error prone. What about keeping the keyspace/table name as now (for the sake of making it easy to not mix sstables by mistake) but limit them to say 10 characters each (just for the file name) truncating the name if necessary?\" bq. limit them to say 10 characters each (just for the file name) truncating the name if necessary?\n",
            "\n",
            "We can truncate name to fit within os path limit adaptively with some calculation.\n",
            "\n",
            "How about completely omit Keyspace name but keep ColumnFamily name and adaptively adjust(truncate) its name? This turns out to be a bit complex than I first thought because secondary index CFs are flushing to the same directory. :(\n",
            "Any ideas? \"Im inclined to say we should leave it alone for 2.1.  If Windows users start yelling for it in 3.0 then we can address then.  Lots of other stuff to work on in the meantime.\" \"Patch attached with unit test.\n",
            "I ended up the following naming convention/directory structure:\n",
            "\n",
            "{code}\n",
            "/var/lib/cassandra/data/ks/cf-a85fc210cb1011e3a15f9d25721dbb44\n",
            "    /.idx\n",
            "        ka-1-Data.db\n",
            "        ...\n",
            "    /snapshots\n",
            "        /my_snapshot\n",
            "            /.idx\n",
            "                ka-1-Data.db\n",
            "                ...\n",
            "            ka-1-Data.db\n",
            "            ...\n",
            "    /backups\n",
            "        /.idx\n",
            "            ka-1-Data.db\n",
            "            ...\n",
            "        ka-1-Data.db\n",
            "        ...\n",
            "    ka-1-Data.db\n",
            "    ...\n",
            "    ks-cf-jb-123-Data.db    (Older version can co exist)\n",
            "    ks-cf.idx-jb-2-Data.db\n",
            "{code}\n",
            "\n",
            "Highlight:\n",
            "\n",
            "* Keyspace/ColumnFamily name is omitted from filename.\n",
            "* If it is temporary file then file name would be tmp-ka-1-Data.db.\n",
            "* Each secondary index has its own SSTable directory whose name starts with .. This is to distinguish from snapshots/backup directories.\n",
            "* Older version of SSTable file can co-exist along with new directory structure.\" \"Still not a fan of removing the keyspace/table name from the filename. Imo we should go with Jonathans suggestion above leave this alone until people actually start yelling cause I dont remember a single user reporting that path lengths was a blocker so Id rather not change the directory layout once again (which might break user scripts) until we have practical evidence its a problem..\" WDYT [~JoshuaMcKenzie]?  How likely is this to be a problem for Windows users? CASSANDRA-4110 and the limitations in Schema.java provide us some protection but there\\s really nothing to stop users nesting their cassandra data 250 characters deep in a path and having things blow up on them regardless of what length we limit ourselves to.\n",
            "\n",
            "On snapshots we\\ll be using 204 chars worst-case (48 KS 48 CF *2 each 9 for \"snapshots\" 3 for slashes) so that doesn\\t leave us a lot of breathing room on path for data_file_directories.  Maybe lowering the NAME_LENGTH in Schema.java would be appropriate given CASSANDRA-7136?  Do we have a lot of users rolling out 40+ char KS and CF names in general much less on Windows? We had a lot of people hit issues the first time we lowered the max name lengths in schemas \"Looks like it was lowered across the board and not on a per-platform basis.  I can see a file-path limitation on linux being a surprise but its part of the ecosystem people are used to with windows.\" bq. Looks like it was lowered across the board and not on a per-platform basis.\n",
            "\n",
            "Yeah nobody *really* needs keyspace names that long; not worth explaining why your linux snapshot broke when you moved to windows if we can avoid it.\n",
            "\n",
            "bq. On snapshots we\\ll be using 204 chars worst-case (48 KS 48 CF *2 each 9 for \"snapshots\" 3 for slashes) so that doesn\\t leave us a lot of breat \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-880\n",
            "issue_type:  Task \n",
            "summary:  add \"drain\" command \n",
            "description:  \"drain\" will flush and not accept more writes, leaving the commitlog empty on restart.\n",
            "comments:  ['(that is, this is a jmx command to StorageServiceMBean, not a thrift command.)', \"We don't want to leave the ring, we want to be in the ring but down (or HH will not get a chance to work).  Stopping Gossiper from doing its thing is probably the best way to do this.\\n\\nRefusing to take new connections is ok but unnecessary, the important thing is not to (a) read any more commands from existing connections and (b) not start the flush until any in-progress commands are done.  MS.shutdown does the former but not the latter.  \\n\\n(executor.shutdownNow doesn't stop currently-executing threads dead in their tracks, which is a good thing.  I think adding awaitTermination would do the trick.)\", '0001 looks at first glance like this keeps a Future for each replayed mutation until the end of recovery.  That seems like a good way to OOM yourself... ?\\n\\n+1 on 0002.', 'Right on the OOM.  Modified to use a counter instead.\\n\\nMaybe the WrappedRunnable should be replaced with a Runnable and the decrement should happen in a finally.  But if there is an IOException during the Table.apply(), we probably have far worse problems than non-recovery.', 'IMO IOE on recover should be allowed to propagate up and kill it.', 'pretty sure logwriter.close calls sync(); if so that addition is unnecessary.\\n\\n+1 the rest.', \"Close doesn't call sync. I added it when I discovered that the delete was failing.  nio being stupid? \", \"Hmm.  SSTW has this\\n\\n        dataFile.close(); // calls force\\n\\nbut as near as I can tell that has never actually been true of BRAF.\\n\\nare there any places where we *don't* want to sync a BRAF we've been writing to, on close?\\n\\nnew sstable: yes\\n\\nCLS: yes\\n\\nanything else?  most uses of BRAF are read-only, I think.\", \"I didn't realize that was a cassandra class.  Yes, close should definitely call sync.  I can't think of any reason why not.  In cases of read-only sync is a noop anyway.\", 'sync calls flush, so you can r/m the extra flush() in close.  +1 w/ that.', \"The flush was there before, and sync isn't guaranteed to call flush.  (The semantic differences of dirty_ and syncNeeded_ aren't very clear to me atm.)\", 'syncneeded is true if writes have been done since last sync.\\n\\ndirty is true if writes have been done since last flush.\\n\\nyou can have syncneeded w/o dirty (if flush is called separately) but not vice versa.  so if you are syncing in close, a separate flush is redundant.', 'looks like flush is only called now by close + sync so the distinction can be removed.', '(committed by gdusbabek to 0.6 and trunk)'] \n",
            "my_comment: yes\n",
            "\n",
            "anything else?  most uses of BRAF are read-only I think.\" \"I didnt realize that was a cassandra class.  Yes close should definitely call sync.  I cant think of any reason why not.  In cases of read-only sync is a noop anyway.\" sync calls flush so you can r/m the extra flush() in close.  +1 w/ that. \"The flush was there before and sync isnt guaranteed to call flush.  (The semantic differences of dirty_ and syncNeeded_ arent very clear to me atm.)\" syncneeded is true if writes have been done since last sync.\n",
            "\n",
            "dirty is true if writes have been done since last flush.\n",
            "\n",
            "you can have syncneeded w/o dirty (if flush is called separately) but not vice versa.  so if you are syncing in close a separate flush is redundant. looks like flush is only called now by close + sync so the distinction can be removed. (committed by gdusbabek to 0.6 and trunk) \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-173\n",
            "issue_type:  Improvement \n",
            "summary:  add getPendingTasks to CFSMBean \n",
            "description:  need to add an atomicint and inc/decr it whenever we acquire memtableLock \n",
            "comments:  ['rebased patch as 0001-CASSANDRA-173-added-CFS-pending-tasks-JMX-attr.txt', 'patch 001-CASSANDRA-173-added-CFS-pending-tasks-JMX-attr.txt looks good to me\\n\\n-Arin', 'committed', 'Integrated in Cassandra #132 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/132/])\\n    added CFS pending tasks JMX attribute\\n\\nPatch by eevans; reviewed by Arin Sarkissian for \\n'] \n",
            "my_comment:  rebased patch as 0001-CASSANDRA-173-added-CFS-pending-tasks-JMX-attr.txt patch 001-CASSANDRA-173-added-CFS-pending-tasks-JMX-attr.txt looks good to me\n",
            "\n",
            "-Arin committed Integrated in Cassandra #132 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/132/])\n",
            "    added CFS pending tasks JMX attribute\n",
            "\n",
            "Patch by eevans; reviewed by Arin Sarkissian for \n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6495\n",
            "issue_type:  Bug \n",
            "summary:  LOCAL_SERIAL  use QUORUM consistency level to validate expected columns \n",
            "description:  If CAS is done at LOCAL_SERIAL consistency level, only the nodes from the local data center should be involved. \n",
            "comments:  ['[~jbellis]\\nFor this should we use LOCAL_QUORAM for LOCAL_SERIAL or use the consistency level of commit. I think adding a third CL for this will be too confusing. So I think we can use CL of commit for validating columns.  ', 'Pretty sure we *must* use LOCAL_QUORUM. LOCAL_SERIAL should still provided serializability within the local data-center and this require doing a LOCAL_QUORUM. Using the CL of the commit would be incorrect in most case.', 'I think that \"cas\" CL of SERIAL -> read at Q, LOCAL_SERIAL -> LQ.', 'OK. Let me do that', 'committed the StorageProxy change.\\n\\nLeaving the AbstractPaxosCallback timeout alone; the intention is that CasContentionTimeout is for \"the replicas are responding normally, but I couldn\\'t get a ballot accepted within X seconds because I\\'m competing with other transactions.\"  So using WriteTimeout for a response to a single prepare or propose is correct.'] \n",
            "my_comment:  [~jbellis]\n",
            "For this should we use LOCAL_QUORAM for LOCAL_SERIAL or use the consistency level of commit. I think adding a third CL for this will be too confusing. So I think we can use CL of commit for validating columns.   Pretty sure we *must* use LOCAL_QUORUM. LOCAL_SERIAL should still provided serializability within the local data-center and this require doing a LOCAL_QUORUM. Using the CL of the commit would be incorrect in most case. I think that \"cas\" CL of SERIAL -> read at Q LOCAL_SERIAL -> LQ. OK. Let me do that committed the StorageProxy change.\n",
            "\n",
            "Leaving the AbstractPaxosCallback timeout alone; the intention is that CasContentionTimeout is for \"the replicas are responding normally but I couldn\\t get a ballot accepted within X seconds because I\\m competing with other transactions.\"  So using WriteTimeout for a response to a single prepare or propose is correct. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-298\n",
            "issue_type:  Task \n",
            "summary:  check for common PEBCAKs in startup \n",
            "description:  listenaddress=0.0.0.0 is one\n",
            "comments:  ['http://en.wikipedia.org/wiki/Pebcak', \"adds checks for listenaddress.\\n\\nno suggestions for other checks have been forthcoming so let's put this ticket out of its misery.\", 'Looks good. +1', 'committed', 'Integrated in Cassandra #264 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/264/])\\n    check for listenaddress misconfiguration\\npatch by jbellis; reviewed by gdusbabek for \\n'] \n",
            "my_comment:  http://en.wikipedia.org/wiki/Pebcak \"adds checks for listenaddress.\n",
            "\n",
            "no suggestions for other checks have been forthcoming so lets put this ticket out of its misery.\" Looks good. +1 committed Integrated in Cassandra #264 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/264/])\n",
            "    check for listenaddress misconfiguration\n",
            "patch by jbellis; reviewed by gdusbabek for \n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3047\n",
            "issue_type:  Improvement \n",
            "summary:  implementations of IPartitioner.describeOwnership() are not DC aware \n",
            "description:  see http://www.mail-archive.com/user@cassandra.apache.org/msg16375.html\n",
            "comments:  [\"What if we split up nodetool output per-DC?  For NTS it would be much better and for SS/ONTS it wouldn't make things worse...\", \"Hi jonathan\\n\\nI'm not sure I completely understand your suggestion.\\n\\nIf I understand correctly the problem is that probe.effectiveOwnership(keyspace) is calculating ring assignment percentages on multiple ring scenarios as if it was a single ring, correct?\\n\\nIf these values are incorrectly calculated how would separating the output per dc help?\", \"You'd need to perform the computation per-DC as well as the output.\", \"I'm thinking about producing slightly different output for different strategies. It's a bit difficult to understand the output without knowing the strategy/replication factor and if it if is dc aware or not.\\n\\nsomething like changing:\\n{code}\\nAddress         DC          Rack        Status State   Load            Effective-Ownership Token                                       \\n                                                                                           148873535527910577765226390751398592512     \\n127.0.0.1       DC1         RAC1        Up     Normal  49.76 KB        16.66%             0                                           \\n127.0.0.2       DC1         RAC1        Up     Normal  40 KB           16.66%              21267647932558653966460912964485513216      \\n127.0.0.3       DC1         RAC2        Up     Normal  63.45 KB        16.66%             42535295865117307932921825928971026432      \\n127.0.0.4       DC1         RAC2        Up     Normal  49.76 KB        16.66%              63802943797675961899382738893456539648      \\n127.0.0.5       DC2         RAC1        Up     Normal  49.76 KB        16.66%             85070591730234615865843651857942052864      \\n127.0.0.6       DC2         RAC1        Up     Normal  40 KB           16.66%              106338239662793269832304564822427566080    \\n{code}\\n\\ninto \\n\\n{code}\\nKeyspace: myKeyspace  DataCenter aware? yes Strategy: NetworkTopologyStragey\\n\\nDC     Replicas Address      Rack    Status State   Load        Effective-Ownership          Token                                       \\n                                                                                                \\nDC1    1        127.0.0.1    RAC1    Up     Normal  40 KB       100.0%                     0                                           \\nDC2    1        127.0.0.2    RAC1    Up     Normal  40 KB       50.0%                      0\\nDC2    -        127.0.0.3    RAC2    Up     Normal  40 KB       50.0%                      8...\\nDC3    2        127.0.0.4    RAC1    Up     Normal  40 KB       50.0%                      0\\nDC3    -        127.0.0.5    RAC1    Up     Normal  40 KB       50.0%                      4...\\nDC3    -        127.0.0.6    RAC2    Up     Normal  40 KB       50.0%                      8...\\nDC3    -        127.0.0.7    RAC2    Up     Normal  40 KB       50.0%                      12...\\n\\nTotal Replicas: 4      \\n{code}\\n\\nwhat do you think?\\n\", \"I like where you're going here.  Maybe split the DCs out even more explicitly?\\n\\n{noformat}\\nKeyspace: myKeyspace\\nReplication Strategy: NetworkTopologyStrategy\\n\\n\\nDC1\\n===\\nReplicas: 1\\n\\nAddress      Rack    Status State   Load        Effective-Ownership          Token                                       \\n127.0.0.1    RAC1    Up     Normal  40 KB       100.0%                     0\\n\\n\\nDC2\\n===\\nReplicas: 1\\n\\nAddress      Rack    Status State   Load        Effective-Ownership          Token                                       \\n127.0.0.2    RAC1    Up     Normal  40 KB       50.0%                      0\\n127.0.0.3    RAC2    Up     Normal  40 KB       50.0%                      8...\\n\\n\\nDC3\\n===\\nReplicas: 2\\n\\nAddress      Rack    Status State   Load        Effective-Ownership          Token                                       \\n127.0.0.4    RAC1    Up     Normal  40 KB       50.0%                      0\\n127.0.0.5    RAC1    Up     Normal  40 KB       50.0%  \n",
            "my_comment: 2\n",
            "\n",
            "Address      Rack    Status State   Load        Effective-Ownership          Token                                       \n",
            "127.0.0.4    RAC1    Up     Normal  40 KB       50.0%                      0\n",
            "127.0.0.5    RAC1    Up     Normal  40 KB       50.0%  \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-16077\n",
            "issue_type:  Improvement \n",
            "summary:  Only allow strings to be passed to JMX authentication \n",
            "description:  It doesn't make sense to allow other object types. \n",
            "comments:  ['CI:\\r\\n* trunk\\r\\n** https://ci-cassandra.apache.org/job/Cassandra-devbranch/285/\\r\\n* 3.11\\r\\n** https://ci-cassandra.apache.org/job/Cassandra-devbranch/284/\\r\\n* 3.0\\r\\n** https://ci-cassandra.apache.org/job/Cassandra-devbranch/289/\\r\\n* 2.2\\r\\n** https://ci-cassandra.apache.org/job/Cassandra-devbranch/288/', 'Committed as [63f4da90c3c51d230c535265786dbc7a33c1ace9|https://github.com/apache/cassandra/commit/63f4da90c3c51d230c535265786dbc7a33c1ace9]'] \n",
            "my_comment:  CI:\n",
            "* trunk\n",
            "** https://ci-cassandra.apache.org/job/Cassandra-devbranch/285/\n",
            "* 3.11\n",
            "** https://ci-cassandra.apache.org/job/Cassandra-devbranch/284/\n",
            "* 3.0\n",
            "** https://ci-cassandra.apache.org/job/Cassandra-devbranch/289/\n",
            "* 2.2\n",
            "** https://ci-cassandra.apache.org/job/Cassandra-devbranch/288/ Committed as [63f4da90c3c51d230c535265786dbc7a33c1ace9|https://github.com/apache/cassandra/commit/63f4da90c3c51d230c535265786dbc7a33c1ace9] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5015\n",
            "issue_type:  Improvement \n",
            "summary:  move bloom_filter_fp_chance to compaction options \n",
            "description:  This setting doesn't take affect until data is recompacted, so should be moved into compaction options.\n",
            "comments:  ['bq. Alternatively, we could do what index_interval does, and rebuild it on startup if changed.\\n\\nCan you elaborate?', \"I believe Jonathan's referring to how in SSTableReader.load we recompute the index summary if we can't read it from disk. In fact, SSTableReader.load already takes a boolean to decide if it should rebuild the bloom filter, so it seems we only need to pass true if we detect the bloom_filter_fp_chance has changed. Though that later detection may require that we save the bffc in the SSTable metadata. \", 'https://github.com/iamaleksey/cassandra/compare/5015', \"Should probably assume that if bffpc is not in metadata, then it's correct (unless it's an ancient strings-in-bloom-filter file).\\n\\nOtherwise LGTM.\", \"bq. Should probably assume that if bffpc is not in metadata, then it's correct (unless it's an ancient strings-in-bloom-filter file).\\n\\nCommitted with this change. Thanks.\"] \n",
            "my_comment:  bq. Alternatively we could do what index_interval does and rebuild it on startup if changed.\n",
            "\n",
            "Can you elaborate? \"I believe Jonathans referring to how in SSTableReader.load we recompute the index summary if we cant read it from disk. In fact SSTableReader.load already takes a boolean to decide if it should rebuild the bloom filter so it seems we only need to pass true if we detect the bloom_filter_fp_chance has changed. Though that later detection may require that we save the bffc in the SSTable metadata. \" https://github.com/iamaleksey/cassandra/compare/5015 \"Should probably assume that if bffpc is not in metadata then its correct (unless its an ancient strings-in-bloom-filter file).\n",
            "\n",
            "Otherwise LGTM.\" \"bq. Should probably assume that if bffpc is not in metadata then its correct (unless its an ancient strings-in-bloom-filter file).\n",
            "\n",
            "Committed with this change. Thanks.\"] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-13396\n",
            "issue_type:  Bug \n",
            "summary:  Cassandra 3.10: ClassCastException in ThreadAwareSecurityManager \n",
            "description:  https://www.mail-archive.com/user@cassandra.apache.org/msg51603.html \n",
            "comments:  ['https://github.com/apache/cassandra/compare/trunk...edwardcapriolo:CASSANDRA-13396?expand=1', 'I\\'m strongly -1 on this change.\\n\\nThis change will cause weird and hard to catch follow-up issues (see the discussions and issues around that piece code), which _cannot_ be caught by neither unit nor dtests because it\\'s an unsupported setup. We do not support embedding C* in a container (i.e. a JVM not controlled \"by us\"). IMO, supporting C* in such an environment will cause other issues. Technically, it\\'s not a major bug - changed it to wish.', 'How come everyone in Cassandra\\'s first reaction is to -1 everything? \\n\\nThe entire model of apache is \"I have an itch to scratch\". This person WANTS to run Cassandra in a container it is an \"itch\". The immediate opposition position should not be \"BUT DON\\'T SCRATCH THAT ITCH\", because I say so.\\n\\n', \"@[~snazy]: OK but what if the cassandra daemon is not embedded anywhere but is simply running with a classpath containing several slf4j bindings?\\nIt will still crash, right?\\n\\n@[~appodictic]: please don't over-react (and don't hijack my question), it's an open discussion ;-)\", 'LOL I just posted this tweet yesterday.\\n\\nhttps://twitter.com/edwardcapriolo/status/847484593041100800\\n\\nWhat comedy cassandra is. No one even bothers to say \"how can we work together?\" or \"how can we wrote the code to make all users happy\" They just instantly drop a -1 on things. lol', 'So funny that i litteraly wake up, go out of my way to fix an issue for someone, and even though everyone is Cassandra is too busy to reply to emails and help people they are Johnny on the spot to jump on Jira and -1 code.', '\"Open discussions\" in cassandra always start with the concept of \"its not my idea so -1\" which is the exact opposite of \"scratch an itch\". \\n\\n\"We do not support embedding C* in a container\"\\n\\nReally? who says? where is it said? Who is \"we\"?', \"[~apassiou], right, it's true for any other slf4j binding. \\nReason is stuff like CASSANDRA-12535 and CASSANDRA-13173, which are hard to figure out and even harder to ensure its functionality in unit tests. That's why I'm against such a change. We cannot foresee the consequences, because we have not tested other bindings. Even further, the performance implications of using another logger implementation are not determined. Believe me, it's not blindly shooting something down - I had a hard time to fix this issue and do not like to see it happen again. BTW: It's late in the afternoon over here, so it's not a too quick reaction early in the morning.\", '{quote}\\nReason is stuff like CASSANDRA-12535 and CASSANDRA-13173, which are hard to figure out and even harder to ensure its functionality in unit tests. \\n{quote}\\n\\nSo because someone made bugs in the past, which are \"hard to figure out\" and you can not \"foresee the consequences\" . Is this back to the future part 4?\\n\\nPlease verify your claim of \"not supporting containers\" before finding other reasons to not like the idea of fixing an obvious problem.\\n\\n\\n', 'So strange:\\n\\nNo such statement about supporting containers seems to exist.\\n[edward@jackintosh cassandra]$ find . -type f | xargs grep containers\\n./src/java/org/apache/cassandra/db/ColumnFamilyStore.java:     * thread safety.  All we do is wipe the sstable containers clean, while leaving the actual\\n./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:        private final List<TokenTreeBuilder> containers = new ArrayList<>();\\n./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:                containers.add(keys);\\n./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:            if (containers.size() > 0)\\n./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:                for (TokenTreeBuilder tokens : containers)\\n./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:            containers.clear();\\n./conf/jvm.options:# This helps prevent soft faults in con \n",
            "my_comment:            containers.clear();\n",
            "./conf/jvm.options:# This helps prevent soft faults in con \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5137\n",
            "issue_type:  Bug \n",
            "summary:  Make sure SSTables left over from compaction get deleted and logged \n",
            "description:  When opening ColumnFamily, cassandra checks SSTable files' ancestors and skips loading already compacted ones. Those files are expected to be deleted, but currently that never happens.\n",
            "comments:  ['We need to mark skipping SSTable as compacted to be removed.', 'Hmm.\\n\\nThis patch is correct as far as it goes but I think the existing assumption is broken: that if we have any sstable with ancestor X, then X is safe to delete.\\n\\nSpecifically, LCS will create multiple sstables from a given set of ancestors, so unless we know that we finished the compaction (and finished writing all the resulting descendant sstables), we could lose data if we delete the ancestors themselves.\\n\\nOne possible fix:\\n\\n# Add a flag to SSTM for \"this was the final sstable in the compaction\"\\n# When we scan sstables, we can delete ancestors if we find that marker in any of the descendants\\n# Otherwise, we should delete the *descendants* and leave the ancestors alone (so we don\\'t doublecount data for counters)', \"I think you're right.\\n\\nA fourth pseudo-solution could be to wait the end of the compaction to rename all the newly created writers (i.e remove the tmp markers). It's not bulletproof though as I don't think we can rename multiple files atomically but just wanted to mention it.\\n\\nMaybe at least for 1.1, 3. is the best/simplest option. On the longer term, maybe 1. is better.\", 'You\\'re right, you don\\'t actually need a marker since if compaction completes the next step is to remove the ancestors.  I think \"if ancestors are still alive, assume compaction didn\\'t finish and delete the descendants\" is good enough.', 'bq. I think \"if ancestors are still alive, assume compaction didn\\'t finish and delete the descendants\" is good enough.\\n\\nyeah agreed.', \"Hum wait, it only works if we have all ancestors though. What if just one ancestor don't get deleted for some reason (or only some of the SSTableDeletingTask have executed before a crash)? It's easy enough to check that we have *all* ancestors, but what if we don't? We're back to square one :(\", \"You're right.  Guess we need a compaction-finished flag after all.\\n\\nInstead of storing it in sstable metadata, maybe we could store it in system.local the way we do with truncation information.  Unfortunately 1.1 doesn't support Maps so we'd be doing two separate implementations for 1.1 and 1.2.\\n\\nShould we just say that for 1.1 we'll retain all sstables (counter users will get overcounts, everyone else just gets extra compaction work) and fix it better in 1.2?\", 'Something like this...\\n\\n{code}\\ncreate table compaction_log (\\n  id uuid primary key,\\n  inputs set<int>,\\n  outputs set<int>\\n);\\n{code}\\n\\nWhen we start a compaction, we add it to the log.  When we finish, we remove it.  If we restart and compaction_log is not empty, we remove any sstables from the outputs set.', \"bq. Should we just say that for 1.1 we'll retain all sstables\\n\\nFor 1.1, I'd suggest doing my fourth pseudo-solution above, i.e. moving the renaming of newly created writes at the end of the compaction (it's trivial). Then at startup, we could indeed retain all sstables for normal CF, but for counter we would keep removing the predecessors as we do now. It wouldn't totally fix the risk of losing counters, but it would make it very unlikely (you'd need to fail exactly in the middle of the bulk renaming a newly create sstable writers), while just retaining all sstables would make it very easy to have overcounts.\\n\\nFor 1.2, you compaction_log solution does seem reasonable.\", \"V2 implements Sylvain's idea that renames written SSTables at the end of compaction.\\n\\nFor 1.2, let's open different issue for Jonathan's suggestion.\", \"The code of v2 looks alright, but let's also disable the filtering in ColumnFamilyStore.ctor for non-counter CFs so we take zero chance of losing data (and since reusing an already compacted sstable is a bit inefficient but harmless).\\n\\nbq. For 1.2, let's open different issue for Jonathan's suggestion\\n\\nAgreed.\", 'Attached v3 that also changes the filtering part only for counter CF.', \"+1 (though do commit your v1 along the way, no way in keeping sstable we're not going to use, even if it's only fo \n",
            "my_comment: that if we have any sstable with ancestor X then X is safe to delete.\n",
            "\n",
            "Specifically LCS will create multiple sstables from a given set of ancestors so unless we know that we finished the compaction (and finished writing all the resulting descendant sstables) we could lose data if we delete the ancestors themselves.\n",
            "\n",
            "One possible fix:\n",
            "\n",
            "# Add a flag to SSTM for \"this was the final sstable in the compaction\"\n",
            "# When we scan sstables we can delete ancestors if we find that marker in any of the descendants\n",
            "# Otherwise we should delete the *descendants* and leave the ancestors alone (so we don\\t doublecount data for counters) \"I think youre right.\n",
            "\n",
            "A fourth pseudo-solution could be to wait the end of the compaction to rename all the newly created writers (i.e remove the tmp markers). Its not bulletproof though as I dont think we can rename multiple files atomically but just wanted to mention it.\n",
            "\n",
            "e right you don\\t actually need a marker since if compaction completes the next step is to remove the ancestors.  I think \"if ancestors are still alive assume compaction didn\\t finish and delete the descendants\" is good enough. bq. I think \"if ancestors are still alive assume compaction didn\\t finish and delete the descendants\" is good enough.\n",
            "\n",
            "yeah agreed. \"Hum wait it only works if we have all ancestors though. What if just one ancestor dont get deleted for some reason (or only some of the SSTableDeletingTask have executed before a crash)? Its easy enough to check that we have *all* ancestors but what if we dont? Were back to square one :(\" \"Youre right.  Guess we need a compaction-finished flag after all.\n",
            "\n",
            "Instead of storing it in sstable metadata maybe we could store it in system.local the way we do with truncation information.  Unfortunately 1.1 doesnt support Maps so wed be doing two separate implementations for 1.1 and 1.2.\n",
            "\n",
            "Should we just say that for 1.1 well retain all sstables (counter users will get overcounts everyone else just gets extra compaction work) and fix it better in 1.2?\" Something like this...\n",
            "\n",
            "{code}\n",
            "create table compaction_log (\n",
            "  id uuid primary key\n",
            "  inputs set<int>\n",
            "  outputs set<int>\n",
            ");\n",
            "{code}\n",
            "\n",
            "When we start a compaction we add it to the log.  When we finish we remove it.  If we restart and compaction_log is not empty we remove any sstables from the outputs set. \"bq. Should we just say that for 1.1 well retain all sstables\n",
            "\n",
            "For 1.1 Id suggest doing my fourth pseudo-solution above i.e. moving the renaming of newly created writes at the end of the compaction (its trivial). Then at startup we could indeed retain all sstables for normal CF but for counter we would keep removing the predecessors as we do now. It wouldnt totally fix the risk of losing counters but it would make it very unlikely (youd need to fail exactly in the middle of the bulk renaming a newly create sstable writers) while just retaining all sstables would make it very easy to have overcounts.\n",
            "\n",
            "For 1.2 you compaction_log solution does seem reasonable.\" \"V2 implements Sylvains idea that renames written SSTables at the end of compaction.\n",
            "\n",
            "For 1.2 lets open different issue for Jonathans suggestion.\" \"The code of v2 looks alright but lets also disable the filtering in ColumnFamilyStore.ctor for non-counter CFs so we take zero chance of losing data (and since reusing an already compacted sstable is a bit inefficient but harmless).\n",
            "\n",
            "bq. For 1.2 lets open different issue for Jonathans suggestion\n",
            "\n",
            "Agreed.\" Attached v3 that also changes the filtering part only for counter CF. \"+1 (though do commit your v1 along the way no way in keeping sstable were not going to use even if its only fo \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5407\n",
            "issue_type:  Bug \n",
            "summary:  Repair exception when getPositionsForRanges returns empty iterator \n",
            "description:  CASSANDRA-5250 broke repair, this re-adds the code from CASSANDRA-5249 \n",
            "comments:  ['What is causing the breakage?  Is it possible to add a test that exposes the problem?', \"my fix in CASSANDRA-5250 just fixed it for LCS (tests if intersecting sstables is empty in LeveledScanner), and this re-broke it for STCS\\n\\ni'll try to write a unit test for this\", 'adds a unit test that would have found the bug', 'LGTM, committed'] \n",
            "my_comment:  What is causing the breakage?  Is it possible to add a test that exposes the problem? \"my fix in CASSANDRA-5250 just fixed it for LCS (tests if intersecting sstables is empty in LeveledScanner) and this re-broke it for STCS\n",
            "\n",
            "ill try to write a unit test for this\" adds a unit test that would have found the bug LGTM committed \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2006\n",
            "issue_type:  Improvement \n",
            "summary:  Serverwide caps on memtable thresholds \n",
            "description:  By storing global operation and throughput thresholds, we could eliminate the \"many small memtables\" problem caused by having many CFs. The global threshold would be set in the config file, to allow different classes of servers to have different values configured.\n",
            "comments:  ['The way I see it - and it becomes even more necessary for a multi-tenant configuration - there should be three completely separate levels of configuration.\\n\\n- Column family configuration is based on data and usage characteristics of column families in your application.\\n- Keyspace configuration enables the modularization of your application (and in a multi-tenant environment, you can assign a keyspace to a tenant)\\n- Server configuration is based on the specific hardware limitations of the server.\\n\\nServer configuration takes priority over keyspace configuration which takes priority over application configuration.\\n\\nLooking at if from the inverse perspective:\\n\\n- CF configuration tunes access to your CFs\\n- Keyspace configuration protects one module of your application from problems in the other modules\\n- Server configuration protects the server as a whole from going beyond the limits of its hardware ', 'I guess that what my suggestion means, in practice, is that \"the memtable in the system that was using the largest fraction of it\\'s local threshold would be flushed\" would be applied when a keyspace threshold is exceeded, rather than when a system threshold is exceeded.\\n\\nWhen a server threshold is exceeded, you would first look for the keyspace that is using the largest fraction of its threshold, then flush the memtable in that keyspace that is using the largest fraction of its local threshold.', 'HBase accomplishes this by keeping a threshold of heap usage for Memtables, and flushing the largest when the threshold is crossed: similar to the safety threshold that jbellis added recently.', 'Patch that optionally creates a global heap usage threshold and tries to keep total memtable size under that.\\n\\nThe two main points of interest are Memtable.updateLiveRatio and MeteredFlusher.\\n\\nMeteredFlusher is what checks memory usage (once per second) and kicks of the flushes.  Note that naively flushing when we hit the threshold is wrong, since you can have multiple memtables in-flight during the flush process.  To address this, we track inactive but unflushed memtables and include those in our total. We also aggressively flush any memtable that reaches the level of \"if my entire flush pipeline were full of memtables of this size, how big could I allow them to be.\"\\n\\nSince counting each object\\'s size is far too slow to be useful directly, we compute the ratio of serialized size to memory size in the background, and update that periodically; That is what updateLiveRatio does.  MeteredFlusher then bases its work on actual serialized size, multiplied by this ratio.\\n\\nOne last note: the config code is a little messy because we want to leave behavior unchanged (i.e.: only use old per-CF thresholds) if the setting is absent as it would be for an upgrader. But, we want a setting to allow \"pick a reasonable default based on heap usage;\" hence the distinction b/t null (off) and -1 (autocompute).\\n\\nI tested by creating the stress schema, then modifying the per-CF settings to be multiple TB, so only the new global flusher affects things.  Then I created half a GB of commitlog files to reply -- CL replay hammers it much harder than even stress.java.\\n\\nIt was successful in preventing OOM (or even the \"emergency flushing\" at 85% of heap) but heap usage as reported by CMS was consistently about 25% higher than what MeteredFlusher thought it should be. It may be that we can fudge factor this; otherwise, tuning by watching CMS vs estimated size and adjusting the setting manually to compensate, is still much easier than the status quo of per-CF tuning.\\n\\nTo experiment, I recommend also patching the log4j settings as follows:\\n\\n{noformat}\\nIndex: conf/log4j-server.properties\\n===================================================================\\n--- conf/log4j-server.properties\\t(revision 1085010)\\n+++ conf/log4j-server.properties\\t(working copy)\\n@@ -35,7 +35,8 @@\\n log4j.appender.R.File=/var/log/cassandra/system.log\\n \\n # Application  \n",
            "my_comment: conf/log4j-server.properties\n",
            "===================================================================\n",
            "--- conf/log4j-server.properties\\t(revision 1085010)\n",
            "+++ conf/log4j-server.properties\\t(working copy)\n",
            "@@ -357 +358 @@\n",
            " log4j.appender.R.File=/var/log/cassandra/system.log\n",
            " \n",
            " # Application  \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-957\n",
            "issue_type:  Improvement \n",
            "summary:  convenience workflow for replacing dead node \n",
            "description:  Replacing a dead node with a new one is a common operation, but \"nodetool removetoken\" followed by bootstrap is inefficient (re-replicating data first to the remaining nodes, then to the new one) and manually bootstrapping to a token \"just less than\" the old one's, followed by \"nodetool removetoken\" is slightly painful and prone to manual errors.\n",
            "comments:  ['It would be easier if a node could start without joining the ring: CASSANDRA-526.', 'Found when testing using RF=2 that there is a chance that the local node could be included in the workMap, and be the first node ordered by proximity. workMap uses the first node from the list as a source. This would cause sending Bootstrap stream sessions to itself.', 'What happens if I bring up a node with the same ip and bootstrap on but forget the replace option? It looks like it will try to bootstrap to an auto picked token. Am I reading that right?\\n\\nWhat happens if I accidentally give the wrong token with the replace option? If I accidentally give the token for a live node will it try to bootstrap to the same position?', \"Good observation. I'll work on covering both use cases.\", 'Chris, are you still working on this?', 'yes. ill post an update next week with an updated patchset.', 'I am going to defer this to anyone else who would like to pick up this ticket. I just do not have the spare time to focus on this.', 'Thanks Chris, I will give it a try...', \"Adding support for replacing token.... This also supports replacement with the same IP. Reworked Hints to be based on Token instead of IP's.... The 3rd part of the patch also makes the hints be delivered to the host (currently seems like it is not delivered at all... Let me know if you want to move this to a different ticket).\", 'Reported to 2496, Testing it took longer than expected time...', 'Seems like CASSANDRA-2928 fixes the hints issue... so we can ignore 0003 in this ticket.', 'Rebased.... with a better way to look for operator error.', \"So a few questions:\\n\\n* In Gossiper.doStatusCheck() you made it ignore any state that is for the local endpoint and is not a dead state. Shouldn't it just always ignore any state about the local endpoint though? Basically what it was doing previously?\\n* Basically the same question about Gossiper.applyStateLocally() the loop continues if the state is for the local node and the state is dead. Why would we want to apply a live local state?\\n* Does the hibernate state need the true/false value? Seems like all we care about is that it is set at all. Looks like we we are starting up right now we automatically go into a hibernate state, then we go into a bootstrap state afterwards if the specified a replace token. Seems like we shouldn't set a state at all until we know we are doing one of replace/bootstrap/just joining.\\n* It looks like right now you could specify a replace token that isn't part of the cluster. If that happens we should throw an exception and tell the user to do the normal bootstrap process.\\n* Why use the last gossip time to determine if the node we are replacing is alive? Why not just check gossip to see if the ring thinks it is alive?\\n* We should update the the message for the exception that is thrown when you try to bootstrap to an existing token. It should indicate either remove the dead node or follow this replacement process.\\n* I'm not sure why we are calling updateNormalToken() in the StorageService.bootstrap() method when it's a token replacement.\\n* A little bit of doc on this would be good, maybe in cassandra.yaml? Just on how to pass the argument to the startup process.\\n\\nI also need to dive into the hint stuff a little bit more, I'm less familiar with that code.\\n\", \"The hint rework looks good. The only comment I have there is that it would be nice if the logging statements for sending hints creating hints indicated the ip as well as the token. Even though it's stored by token it would be nice to immediately see the ip in the log without having to look it up.\\n\\nI'm also unsure about the reasoning behind the last patch. Why increase the initial sleep in joinTokenRing?\", \"* In Gossiper.doStatusCheck() you made it ignore any state that is for the local endpoint and is not a dead state. Shouldn't it just always ignore any state about the local endpoint though? Basically what it was doing previously?\\n* Basically the sa \n",
            "my_comment: CASSANDRA-526. Found when testing using RF=2 that there is a chance that the local node could be included in the workMap and be the first node ordered by proximity. workMap uses the first node from the list as a source. This would cause sending Bootstrap stream sessions to itself. What happens if I bring up a node with the same ip and bootstrap on but forget the replace option? It looks like it will try to bootstrap to an auto picked token. Am I reading that right?\n",
            "\n",
            "What happens if I accidentally give the wrong token with the replace option? If I accidentally give the token for a live node will it try to bootstrap to the same position? \"Good observation. Ill work on covering both use cases.\" Chris are you still working on this? yes. ill post an update next week with an updated patchset. I am going to defer this to anyone else who would like to pick up this ticket. I just do not have the spare time to focus on this. Thanks Chris I will give it a try... \"Adding support for replacing token.... This also supports replacement with the same IP. Reworked Hints to be based on Token instead of IPs.... The 3rd part of the patch also makes the hints be delivered to the host (currently seems like it is not delivered at all... Let me know if you want to move this to a different ticket).\" Reported to 2496 Testing it took longer than expected time... Seems like CASSANDRA-2928 fixes the hints issue... so we can ignore 0003 in this ticket. Rebased.... with a better way to look for operator error. \"So a few questions:\n",
            "\n",
            "* In Gossiper.doStatusCheck() you made it ignore any state that is for the local endpoint and is not a dead state. Shouldnt it just always ignore any state about the local endpoint though? Basically what it was doing previously?\n",
            "* Basically the same question about Gossiper.applyStateLocally() the loop continues if the state is for the local node and the state is dead. Why would we want to apply a live local state?\n",
            "* Does the hibernate state need the true/false value? Seems like all we care about is that it is set at all. Looks like we we are starting up right now we automatically go into a hibernate state then we go into a bootstrap state afterwards if the specified a replace token. Seems like we shouldnt set a state at all until we know we are doing one of replace/bootstrap/just joining.\n",
            "* It looks like right now you could specify a replace token that isnt part of the cluster. If that happens we should throw an exception and tell the user to do the normal bootstrap process.\n",
            "* Why use the last gossip time to determine if the node we are replacing is alive? Why not just check gossip to see if the ring thinks it is alive?\n",
            "* We should update the the message for the exception that is thrown when you try to bootstrap to an existing token. It should indicate either remove the dead node or follow this replacement process.\n",
            "* Im not sure why we are calling updateNormalToken() in the StorageService.bootstrap() method when its a token replacement.\n",
            "* A little bit of doc on this would be good maybe in cassandra.yaml? Just on how to pass the argument to the startup process.\n",
            "\n",
            "I also need to dive into the hint stuff a little bit more Im less familiar with that code.\n",
            "\" \"The hint rework looks good. The only comment I have there is that it would be nice if the logging statements for sending hints creating hints indicated the ip as well as the token. Even though its stored by token it would be nice to immediately see the ip in the log without having to look it up.\n",
            "\n",
            "Im also unsure about the reasoning behind the last patch. Why increase the initial sleep in joinTokenRing?\" \"* In Gossiper.doStatusCheck() you made it ignore any state that is for the local endpoint and is not a dead state. Shouldnt it just always ignore any state about the local endpoint though? Basically what it was doing previously?\n",
            "* Basically the sa \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2950\n",
            "issue_type:  Bug \n",
            "summary:  Data from truncated CF reappears after server restart \n",
            "description:  * Configure 3 node cluster\n",
            "comments:  [\"This is a general issue with all CF's. updating bug.\", 'The other permutation of this bug looked like, assuming write with CL.Q:\\n* Insert 50 (3 nodes up)\\n* truncate CF (3 nodes up)\\n* Insert 1 (3 nodes up)\\n* Bring node3 down\\n* Delete 1  (2 nodes up)\\n* Bring up node3 and run repair\\n* Take down node1 and node2.\\n* Query node3 with CL.ONE: list Standard1;  --- 30 rows returned\\n\\nNot sure, but this looked suspicious in my logs:\\n{code}\\n INFO 01:19:45,616 Streaming to /50.57.114.45\\n INFO 01:19:45,689 Finished streaming session 698609583499991 from /50.57.107.176\\n INFO 01:19:45,690 Finished streaming session 698609609994154 from /50.57.114.45\\n INFO 01:19:46,501 Finished streaming repair with /50.57.114.45 for (0,56713727820156410577229101238628035242]: 0 oustanding to complete session\\n INFO 01:19:46,531 Compacted to /var/lib/cassandra/data/Keyspace1/Standard1-tmp-g-106-Data.db.  16,646,523 to 16,646,352 (~99% of original) bytes for 30 keys.  Time: 1,509ms.\\n INFO 01:19:46,930 Finished streaming repair with /50.57.107.176 for (113427455640312821154458202477256070484,0]: 1 oustanding to complete session\\n INFO 01:19:47,619 Finished streaming repair with /50.57.114.45 for (113427455640312821154458202477256070484,0]: 0 oustanding to complete session\\n INFO 01:19:48,232 Finished streaming repair with /50.57.107.176 for (56713727820156410577229101238628035242,113427455640312821154458202477256070484]: 1 oustanding to complete session\\n INFO 01:19:48,856 Finished streaming repair with /50.57.114.45 for (56713727820156410577229101238628035242,113427455640312821154458202477256070484]: 0 oustanding to complete session\\n{code}', \"Currently, truncate does:\\n* force a flush\\n* record the time\\n* delete any sstables older than the time\\n\\nThis isn't quite enough if the machine crashes shortly afterward, however, since there can be mutations present in the commitlog that were previously truncated and are now resurrected by CL replay.\\n\\nOne thing we could do is record the truncate time for the CF in the system ks and then ignore mutations older than that, however this would require time synchronization between the client and the server to be accurate.\\n\", 'but we record CL \"context\" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.\\n\\nchecked and we do wait for flush to complete in truncate.', 'bq. but we record CL \"context\" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.\\n\\nI think there\\'s something wrong with that, then:\\n\\n{noformat}\\n INFO 21:25:15,274 Replaying /var/lib/cassandra/commitlog/CommitLog-1312924388053.log\\nDEBUG 21:25:15,290 Replaying /var/lib/cassandra/commitlog/CommitLog-1312924388053.log starting at 0\\nDEBUG 21:25:15,291 Reading mutation at 0\\nDEBUG 21:25:15,295 replaying mutation for system.4c: {ColumnFamily(LocationInfo [47656e65726174696f6e:false:4@1312924388140000,])}\\nDEBUG 21:25:15,321 Reading mutation at 89\\nDEBUG 21:25:15,322 replaying mutation for system.426f6f747374726170: {ColumnFamily(LocationInfo [42:false:1@1312924388203,])}\\nDEBUG 21:25:15,322 Reading mutation at 174\\nDEBUG 21:25:15,322 replaying mutation for system.4c: {ColumnFamily(LocationInfo [546f6b656e:false:16@1312924388204,])}\\nDEBUG 21:25:15,322 Reading mutation at 270\\nDEBUG 21:25:15,324 replaying mutation for Keyspace1.3030: {ColumnFamily(Standard1 [C0:false:34@1312924813259,C1:false:34@1312924813260,C2:false:34@1312924813260,C3:false:34@1312924813260,C4:false:34@1312924813260,])}\\n{noformat}\\n\\nThe last entry there is the first of many errant mutations.', 'Ah, CASSANDRA-2419 keeps on giving...\\n\\nbq. but we record CL \"context\" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.\\n\\nThe obvious problem with this is that the point of truncate is to blow away such sstables...  Patch attached.  Comment explains the core fix:\\n\\n{noformat}\\n// Bonus complication: sin \n",
            "my_comment: sin \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2592\n",
            "issue_type:  Bug \n",
            "summary:  CQL greater-than and less-than operators (> and <) result in key ranges that are inclusive of the terms \n",
            "description:  This affects range queries against keys, but not index queries.\n",
            "comments:  ['I think this patch makes it so if I say \"LIMIT 10\" I might only get 8 back because one result got chopped off.  QP will need to request more than LIMIT to give back the right number.\\n\\nCan you add a test for this?', \"added a check which will prevent removing a first/last row if it wasn't start/end key. Tests for LIMIT added.\", \"Some issues around LIMIT. I've added more tests in v3 to demonstrate (but not fix) the problem.\", 'v3 with your tests included and insured to pass.', 'committed w/ minor changes', 'Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])\\n    ', '\\nThis blows up on queries that return no results.\\n\\n{noformat}\\njava.lang.IndexOutOfBoundsException: Index: 0, Size: 0\\n\\tat java.util.ArrayList.RangeCheck(ArrayList.java:547)\\n\\tat java.util.ArrayList.get(ArrayList.java:322)\\n\\tat org.apache.cassandra.cql.QueryProcessor.multiRangeSlice(QueryProcessor.java:194)\\n\\tat org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:534)\\n\\tat org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1131)\\n\\tat org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)\\n\\tat org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)\\n\\tat org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\\n\\tat java.lang.Thread.run(Thread.java:662)\\n{noformat}', 'I think Pavel\\'s patch is more correct since the result could come back with exactly one row equal to the start key on a \"KEY > X AND KEY < Y\" query.  Then we want to remove the extra row on the first check, w/o erroring out on the second.', \"you're right; +1.  committed.\", 'Integrated in Cassandra-0.8 #119 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/119/])\\n    properly handle empty result set\\n\\nPatch by Pavel Yaskevich; reviewed by eevans for CASSANDRA-2592\\n\\neevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1125622\\nFiles : \\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java\\n'] \n",
            "my_comment: \n",
            "* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4802\n",
            "issue_type:  Bug \n",
            "summary:  Regular startup log has confusing \"Bootstrap/Replace/Move completed!\" without boostrap, replace, or move \n",
            "description:  A regular startup completes successfully, but it has a confusing message the end of the startup:\n",
            "comments:  ['How about just saying:\\nBootstrap completed! Now serving reads.\\n\\n? Do we need any additional information?', \"I think the point is that we should not print it if we didn't actually bootstrap, and we should be able to distinguish between bootstrap/replace/move.\", 'Move doesnt use the same code anymore, replace uses this but there are other log info explaining that....\\n\\nIf Bootstrap is a wrong word then how about: Startup completed?\\n(I am still looking for an abstract word :))', 'Bootstrap means something specifically with cassandra in that you think some data has streamed in.\\n\\nI think \"Startup completed\" would be great.\\n\\nIf there IS a bootstrap/replace/move then I think the message ought to specify which has happened and that it\\'s ready now (if it\\'s easy to do) :)', 'Committed https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blobdiff;f=src/java/org/apache/cassandra/service/StorageService.java;h=8de0bd24632c89ea1b41c952ee6ec2db58808894;hp=7d92fbe0ff15c8c686a93425f4fccca49b921c0b;hb=d525cf969c042b21a9375446f5449ee82d7d1484;hpb=7e937b3d1308c0774e4b0366b6e66b14af1dd5f6\\n\\nLet me know if you need more info, i will reopen this ticket.', \"This isn't quite what I had in mind.  It's not a semantic issue, it's a logical issue.  We should clearly indicate the operation that was actually performed, which after a quick glance at the code means we need to store this state somewhere to do so.\", 'Had a discussion with Brandon offline, \\nThere is enough information in the logs to show the operation was Bootstrap vs Repair vs Startup, so closing the ticket for now.'] \n",
            "my_comment: Startup completed?\n",
            "(I am still looking for an abstract word :)) Bootstrap means something specifically with cassandra in that you think some data has streamed in.\n",
            "\n",
            "I think \"Startup completed\" would be great.\n",
            "\n",
            "If there IS a bootstrap/replace/move then I think the message ought to specify which has happened and that it\\s ready now (if it\\s easy to do) :) Committed https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blobdiff;f=src/java/org/apache/cassandra/service/StorageService.java;h=8de0bd24632c89ea1b41c952ee6ec2db58808894;hp=7d92fbe0ff15c8c686a93425f4fccca49b921c0b;hb=d525cf969c042b21a9375446f5449ee82d7d1484;hpb=7e937b3d1308c0774e4b0366b6e66b14af1dd5f6\n",
            "\n",
            "Let me know if you need more info i will reopen this ticket. \"This isnt quite what I had in mind.  Its not a semantic issue its a logical issue.  We should clearly indicate the operation that was actually performed which after a quick glance at the code means we need to store this state somewhere to do so.\" Had a discussion with Brandon offline \n",
            "There is enough information in the logs to show the operation was Bootstrap vs Repair vs Startup so closing the ticket for now. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8414\n",
            "issue_type:  Improvement \n",
            "summary:  Avoid loops over array backed iterators that call iter.remove() \n",
            "description:  I noticed from sampling that sometimes compaction spends almost all of its time in iter.remove() in ColumnFamilyStore.removeDeletedStandard. It turns out that the cf object is using ArrayBackedSortedColumns, so deletes are from an ArrayList. If the majority of your columns are GCable tombstones then this is O(n^2). The data structure should be changed or a copy made to avoid this. \n",
            "comments:  [\"I've edited the title because it's not quite that compaction is O(n^2), but that certain operations within a partition are. It's also not limited to just that specific method. The best solution is probably to introduce a special deletion iterator on which a call to remove() simply sets a corresponding bit to 1; once we exhaust the iterator we commit the deletes in one pass.\", '/cc [~slebresne]', \"Actually Richard's issue is with 1.2 and 2.0.\\n\\nI'm not sure how much of an issue in practice it really is for compaction in 2.1, w/ only LazilyCompactedRow there, and PreCompactedRow gone.\", \"An attempt to implement a BatchRemovalIterator on the 2.0 branch. I've also tested that it indeed is much faster. For non-removals, it should be the same thing.\", 'Do you have time to review, [~rlow]?', 'Yes, I can review this week.', 'We should integrate this for 2.1 also, since this behaviour is exhibited still, just not in compaction. In 2.1 we should use System.arraycopy and removed.nextSetBit though, as the performance will be improved, particularly for sparse removes.', \"Good point. I've attached a new patch containing code using removed.nextSetBit and Collections.copy. Should be easy now to change for 2.1.\", \"Nice backporting of the better approach.\\n\\nI've uploaded a tweaked version, the goal of which was just to clean up the variable names (and switch to a while loop) so it's more obvious what's happening. But while at it I also added use of nextClearBit in tandem with nextSetBit, as it's a minor tweak but gives better behaviour with runs of adjacent removes.\\n\\nI haven't properly reviewed otherwise, but it might be worth introducing this to CFS.removeDroppedColumns() and SliceQueryFilter.trim(), \", \"Thanks for writing the patch! A few \n",
            "my_comment:  [\"Ive edited the title because its not quite that compaction is O(n^2) but that certain operations within a partition are. Its also not limited to just that specific method. The best solution is probably to introduce a special deletion iterator on which a call to remove() simply sets a corresponding bit to 1; once we exhaust the iterator we commit the deletes in one pass.\" /cc [~slebresne] \"Actually Richards issue is with 1.2 and 2.0.\n",
            "\n",
            "Im not sure how much of an issue in practice it really is for compaction in 2.1 w/ only LazilyCompactedRow there and PreCompactedRow gone.\" \"An attempt to implement a BatchRemovalIterator on the 2.0 branch. Ive also tested that it indeed is much faster. For non-removals it should be the same thing.\" Do you have time to review [~rlow]? Yes I can review this week. We should integrate this for 2.1 also since this behaviour is exhibited still just not in compaction. In 2.1 we should use System.arraycopy and removed.nextSetBit though as the performance will be improved particularly for sparse removes. \"Good point. Ive attached a new patch containing code using removed.nextSetBit and Collections.copy. Should be easy now to change for 2.1.\" \"Nice backporting of the better approach.\n",
            "\n",
            "Ive uploaded a tweaked version the goal of which was just to clean up the variable names (and switch to a while loop) so its more obvious whats happening. But while at it I also added use of nextClearBit in tandem with nextSetBit as its a minor tweak but gives better behaviour with runs of adjacent removes.\n",
            "\n",
            "I havent properly reviewed otherwise but it might be worth introducing this to CFS.removeDroppedColumns() and SliceQueryFilter.trim() \" \"Thanks for writing the patch! A few \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4416\n",
            "issue_type:  Improvement \n",
            "summary:  Include metadata for system keyspace itself in schema_* tables \n",
            "description:  The `system.schema_keyspaces`, `system.schema_columnfamilies`, and `system.schema_columns` virtual tables allow clients to query schema and layout information through CQL. This will be invaluable when users start to make more use of the CQL-only protocol (CASSANDRA-2478), since there will be no other way to determine certain information about available columnfamilies, keyspaces, or show metadata about them.\n",
            "comments:  ['Not sure what a good solution here is, since these are defined programatically (and can and do change).', \"I suppose we could artificially write them at startup. At each startup we could erase everything (relating to the system keyspace) to be sure we don't have old info and then dump the new info.\", \"Tagging version 1.2 since the metdata isn't even specified internally until CASSANDRA-4018.\", 'This is more important now, after CASSANDRA-4377, since as far as I can tell there is no way at all for a client or user to be able to see all the tables in the system and system_traces keyspaces.', \"Patch attached implementing Sylvain's suggestion to delete out and write anew the schema entries for hardcoded tables.\\n\\nAlso removes SSTableExport restriction that you can only run it if non-system tables have been defined which is kind of nonsensical.\", \"nit: in SystemTable.finishStartup, the code currently tries to delete the keyspaces, columnsfamilies and columns table from the system_trace keyspace too. It's harmless but useless so we might want to add a 'if' or move the deletion outside the 'for'.\\n\\n+1 otherwise.\", \"Hmm, I still don't see it...  we run\\n\\n{code}\\nDELETE FROM system.{schema_keyspaces, schema_columnfamilies, schema_columns} WHERE keyspace_name = '{system, system_traces}'\\n{code}\\n\\nso system_traces is the partition key of the data being removed, not the keyspace we're DELETEing from.\", \"Oh right, I shouldn't review tickets before my second coffee. +1.\", 'committed', \"This patch causes Cassandra to not start if there is at least one keyspace besides the system ones.\\n\\nRun create keyspace test with replication = {'class':'SimpleStrategy', 'replication_factor':1}; from cqlsh, stop cassandra, start cassandra, and you'll observe the following stacktrace:\\n\\njava.lang.RuntimeException: Attempting to load already loaded column family system.batchlog\\n\\tat org.apache.cassandra.config.Schema.load(Schema.java:396)\\n\\tat org.apache.cassandra.config.Schema.load(Schema.java:112)\\n\\tat org.apache.cassandra.config.Schema.load(Schema.java:97)\\n\\tat org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:564)\\n\\tat org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:214)\\n\\tat org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:393)\\n\\tat org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:436)\\nException encountered during startup: Attempting to load already loaded column family system.batchlog\\n\\nIn fact you don't even need to create a new keyspace - just start/stop cassandra a couple times.\\n\", \"The problem being, when we load the table from the system table, the system ones have already been loaded and shouldn't be loaded a second time. Attaching a patch that ignore the system keyspaces when reading schema tables (we only write those for client sake, we never need them internally, so let just do as if they weren't there).\", '+1', 'Alright, fix committed.'] \n",
            "my_comment: Attempting to load already loaded column family system.batchlog\n",
            "\n",
            "In fact you dont even need to create a new keyspace - just start/stop cassandra a couple times.\n",
            "\" \"The problem being when we load the table from the system table the system ones have already been loaded and shouldnt be loaded a second time. Attaching a patch that ignore the system keyspaces when reading schema tables (we only write those for client sake we never need them internally so let just do as if they werent there).\" +1 Alright fix committed. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9636\n",
            "issue_type:  Bug \n",
            "summary:  Duplicate columns in selection causes AssertionError \n",
            "description:  Prior to CASSANDRA-9532, unaliased duplicate fields in a selection would be silently ignored. Now, they trigger a server side exception and an unfriendly error response, which we should clean up. Duplicate columns *with* aliases are not affected.\n",
            "comments:  [\"I'm seeing the same error als on a query not containing duplicates in 2.0.16:\\nSELECT writetime(valid) as wtv, unixtimestampof(now()) as ts, ip, valid, usr, db, impersonate, test FROM session WHERE id = 'FVAIFWJVUWXUPNJVGDGSUDXKVTWKCI'\\n\\nThe table definition being:\\nCREATE TABLE session (\\n  id text,\\n  db text,\\n  impersonate int,\\n  ip set<inet>,\\n  login_token text,\\n  oauth text,\\n  test boolean,\\n  usr text,\\n  valid boolean,\\n  PRIMARY KEY ((id))\\n) WITH\\n  bloom_filter_fp_chance=0.010000 AND\\n  caching='ALL' AND\\n  comment='' AND\\n  dclocal_read_repair_chance=0.000000 AND\\n  gc_grace_seconds=864000 AND\\n  index_interval=128 AND\\n  read_repair_chance=0.100000 AND\\n  replicate_on_write='true' AND\\n  populate_io_cache_on_flush='false' AND\\n  default_time_to_live=0 AND\\n  speculative_retry='99.0PERCENTILE' AND\\n  memtable_flush_period_in_ms=0 AND\\n  compaction={'class': 'LeveledCompactionStrategy'} AND\\n  compression={'sstable_compression': 'LZ4Compressor'};\\n\\n2.0.15 works fine with the above query\", 'I even got the error with a simple \"ping\" query in cqlsh.\\n\\n2.1.7:\\n{code}\\nConnected to Cluster Fucked at 127.0.0.1:9042.\\n[cqlsh 5.0.1 | Cassandra 2.1.7 | CQL spec 3.2.0 | Native protocol v3]\\nUse HELP for help.\\nuser@cqlsh> select now() from system.local;\\nServerError: <ErrorMessage code=0000 [Server error] message=\"java.lang.AssertionError\">\\n{code}\\n\\n2.1.6:\\n{code}\\nConnected to Cluster-o-Nuts at 127.0.0.1:9042.\\n[cqlsh 5.0.1 | Cassandra 2.1.6 | CQL spec 3.2.0 | Native protocol v3]\\nUse HELP for help.\\nuser@cqlsh> select now() from system.local;\\n\\n now()\\n--------------------------------------\\n 577708f0-1ba9-11e5-85d0-df82a948a83c\\n\\n(1 rows)\\n{code}', \"In both cases mentioned in the comments, the issue is that the Selection contains a result column which doesn't map to any underlying column (as both use the no-arg {{now()}} function). This was overlooked in the unit tests, but the unit testing wasn't quite rigorous enough anyway as it only verified the mappings collected for a given query matched expectations. It needs to go further and actually execute the query to ensure that the resultset can be properly constructed from the mappings. I've made the necessary changes to the tests & pushed a fix for the no-arg function case ([2.0|https://github.com/beobal/cassandra/tree/9636-2.0], [2.1|https://github.com/beobal/cassandra/tree/9636-2.1]). I should note that this particular problem doesn't affect 2.2.\\n\\nRegarding the original problem regarding duplicates in the selection, my characterisation of the pre-9532 behaviour was slightly off, so for the sake of clarity:\\n\\n||Branch||pre-9532 behaviour||post-9532 behaviour||\\n|2.0|duplicates are included in results|AssertionError & error response|\\n|2.1|duplicates are collated|AssertionError & error response|\\n|2.2|AssertionError & error response|duplicates are collated|\\n \\nThe branches I've linked also revert 2.0 & 2.1 to their original behaviours.\", 'I noticed an issue with {{count(*)}} queries. \\nUp to 2.2, the count function was implemented in a different way than the other functions. It was some form of hack in {{SelectStatement}}. Due to that the mapping returned is wrong for this function.\\n\\nTo be on the safe side, I think it will be good to add some tests for duplicate function calls and for 2.2 some tests with aggregations. \\n', \"Thanks, you're right the mapping for {{count}} queries were definitely wrong for 2.0 & 2.1.\\nAlso, there were some issues on 2.2 with selecting the same column with multiple distinct aliases & with selecting duplicate unaliased functions.\\n\\nI've rebased & pushed fixes for 2.0 -> trunk; CI is running now\\n\\n[2.0|https://github.com/beobal/cassandra/tree/9636-2.0] [utest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.0-testall/] [dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.0-dtest/]\\n[2.1|https://github.com/beobal/cassandra/tree/9636-2.1] [utest|http://cassci.datastax.com/view/Dev/view/beoba \n",
            "my_comment: <ErrorMessage code=0000 [Server error] message=\"java.lang.AssertionError\">\n",
            "{code}\n",
            "\n",
            "2.1.6:\n",
            "{code}\n",
            "Connected to Cluster-o-Nuts at 127.0.0.1:9042.\n",
            "[cqlsh 5.0.1 | Cassandra 2.1.6 | CQL spec 3.2.0 | Native protocol v3]\n",
            "Use HELP for help.\n",
            "user@cqlsh> select now() from system.local;\n",
            "\n",
            " now()\n",
            "--------------------------------------\n",
            " 577708f0-1ba9-11e5-85d0-df82a948a83c\n",
            "\n",
            "(1 rows)\n",
            "{code} \"In both cases mentioned in the comments the issue is that the Selection contains a result column which doesnt map to any underlying column (as both use the no-arg {{now()}} function). This was overlooked in the unit tests but the unit testing wasnt quite rigorous enough anyway as it only verified the mappings collected for a given query matched expectations. It needs to go further and actually execute the query to ensure that the resultset can be properly constructed from the mappings. Ive made the necessary changes to the tests & pushed a fix for the no-arg function case ([2.0|https://github.com/beobal/cassandra/tree/9636-2.0] [2.1|https://github.com/beobal/cassandra/tree/9636-2.1]). I should note that this particular problem doesnt affect 2.2.\n",
            "\n",
            "Regarding the original problem regarding duplicates in the selection my characterisation of the pre-9532 behaviour was slightly off so for the sake of clarity:\n",
            "\n",
            "||Branch||pre-9532 behaviour||post-9532 behaviour||\n",
            "|2.0|duplicates are included in results|AssertionError & error response|\n",
            "|2.1|duplicates are collated|AssertionError & error response|\n",
            "|2.2|AssertionError & error response|duplicates are collated|\n",
            " \n",
            "The branches Ive linked also revert 2.0 & 2.1 to their original behaviours.\" I noticed an issue with {{count(*)}} queries. \n",
            "Up to 2.2 the count function was implemented in a different way than the other functions. It was some form of hack in {{SelectStatement}}. Due to that the mapping returned is wrong for this function.\n",
            "\n",
            "To be on the safe side I think it will be good to add some tests for duplicate function calls and for 2.2 some tests with aggregations. \n",
            " \"Thanks youre right the mapping for {{count}} queries were definitely wrong for 2.0 & 2.1.\n",
            "Also there were some issues on 2.2 with selecting the same column with multiple distinct aliases & with selecting duplicate unaliased functions.\n",
            "\n",
            "Ive rebased & pushed fixes for 2.0 -> trunk; CI is running now\n",
            "\n",
            "[2.0|https://github.com/beobal/cassandra/tree/9636-2.0] [utest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.0-testall/] [dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.0-dtest/]\n",
            "[2.1|https://github.com/beobal/cassandra/tree/9636-2.1] [utest|http://cassci.datastax.com/view/Dev/view/beoba \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-124\n",
            "issue_type:  Bug \n",
            "summary:  NullPointerException in consistency manager after a failed node rejoins \n",
            "description:  ERROR [CONSISTENCY-MANAGER:2] 2009-04-30 18:22:38,946 DebuggableThreadPoolExecutor.java (line 89) Error in ThreadPoolExecutor\n",
            "comments:  [\"Shouldn't ConsistencyManager() constructor contain the following line?\\nthis.replicas_ = replicas_;\", \"FWIW this is not a regression, it's only showing up in the logs now after I added a change that logs exceptions from our executors instead of ignoring them.\", \"Oops, I take it back.  It's a regression from CASSANDRA-95.  nk11 is right, the constructor got broken.\", 'Integrated in Cassandra #59 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/59/])\\n    do not leave variables uninitialized in ConsistencyManager constructor.  fixes regression from #95.  patch by jbellis for \\n', 'looks fixed in my testing'] \n",
            "my_comment:  [\"Shouldnt ConsistencyManager() constructor contain the following line?\n",
            "this.replicas_ = replicas_;\" \"FWIW this is not a regression its only showing up in the logs now after I added a change that logs exceptions from our executors instead of ignoring them.\" \"Oops I take it back.  Its a regression from CASSANDRA-95.  nk11 is right the constructor got broken.\" Integrated in Cassandra #59 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/59/])\n",
            "    do not leave variables uninitialized in ConsistencyManager constructor.  fixes regression from #95.  patch by jbellis for \n",
            " looks fixed in my testing \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4249\n",
            "issue_type:  Bug \n",
            "summary:  LOGGING: Info log is not displaying number of rows read from saved cache at startup \n",
            "description:  As part of commit with revision c9270f4e info logging for number of rows read from saved cache is not working. \n",
            "comments:  ['This is a very small change.', 'committed, thanks!'] \n",
            "my_comment:  This is a very small change. committed thanks! \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14705\n",
            "issue_type:  Improvement \n",
            "summary:  ReplicaLayout follow-up \n",
            "description:  Clarify the new {{ReplicaLayout}} code, separating it into ReplicaPlan (for what we want to do) and {{ReplicaLayout}} (for what we know about the cluster), with well defined semantics (and comments in the rare cases those semantics are weird)\n",
            "comments:  ['[trunk|https://github.com/belliottsmith/cassandra/tree/14705]  [CI|https://circleci.com/workflow-run/4f121d12-86b5-48c6-a8e2-b78597ef47c2]', 'I\\'ve tried to split the patch into digestible chunks.  \\r\\n\\r\\nThe initial two commits are just to get most of the changes that touch a lot of files out of the way, to reduce the cognitive burden of review.  They consist of only a handful of IDE refactors (and a fairly safe find/replace of replicaLayout->replicaPlan).  You can probably ignore these commits almost entirely.\\r\\n\\r\\nThe \"main refactor\" commit has the guts of the work.\\r\\n* Introduce {{ReplicaPlan}}, made up of\\r\\n** two {{ReplicaLayout}}, for the {{live}} and {{liveAndDown}} replicas (maybe we will want to lazily build a separate {{downOnly}} in future, but I\\'ve kept it simple for now)\\r\\n** two {{Endpoints}}, containing the candidate replicas and those we intend to contact; there are now comments clearly defining what the contents of each are in any given scenario.  It\\'s probably the case there were some bugs introduced wrt totalEndpoints before this, but I didn\\'t waste time corroborating.\\r\\n* There are now separate {{ReplicaPlan}} and {{ReplicaLayout}} for each of read/write and token/range, and we enforce only the correct type be provided to each recipient.  {{ReplicaLayout}} only supports {{natural()}} for read operations, so it is impossible to invoke {{pending()}} here, or think you have {{pending()}} nodes when you do not.  <*Open question*: should we disable {{all()}} for read queries, and only support it for writes?  Haven\\'t tried, and might get a little ugly, but might be conceptually cleaner>\\r\\n* {{ReplicaCollection}}\\r\\n** I have caved and implemented a {{count}} method, because there is none in {{Iterables}}.   I\\'m actually wavering on the review feedback I gave to [~aweisberg]\\'s original version of this patch (to not implement all the {{Iterables}} method equivalents), for two reasons: we now use these _extensively_, and we also have only one implementation, so it could be very cheap to implement e.g. a {{filterLazily}} and {{\\\\{any,all,none\\\\}Match}}\\r\\n** I have removed {{select()}} because the correct logic did not benefit from this abstraction.  I have, however, renamed the {{keep()}} method in {{Endpoints}} that re-orders the contents to {{select()}}, to make clear the distinction of semantics.\\r\\n\\r\\nThen there are a few follow up commits to either fix bugs or perform some follow up clean ups.\\r\\n\\r\\nLooking forward to your feedback.', 'FYI, my fix for {{maybeSendAdditionalReads}} is incomplete, I will follow up with it on Monday, but it will be very minor.', \"[~ifesdjeen] that branch you linked to in your PR is the wrong one, it's 14705\", '[~aweisberg] sorry, it was right until branch was renamed and switched)\\r\\n\\r\\nThank you for the patch. Some comments, all rather minor:\\r\\n\\r\\n  * we could rename {{liveOnly}} to {{live}} unless \"only\" here bears \\r\\n  * {{allUncontactedCandidates}} could be just {{uncontactedCandidates}} \\r\\n  * {{ForTokenWrite}} can be just {{ForWrite}} since we\\'ve removed {{ForRangeWrite}}, similarly in {{forTokenWriteLiveAndDown}} and {{forTokenWrite}}\\r\\n  * we can avoid iterating in {{ReplicaPlan#forWrite}} checking for {{isTransient}} and short-circuit it for non-transient keyspace with {{CL#hasTransient}}.\\r\\n * in {{EndpointsFor(Range|Token)}} we have checks for range/token so they matched for concatenated ranges. Should we apply same checks when constructing layouts?\\r\\n  * I know it was same before the patch and we can do it in a separate one, but {{isSufficientReplicasForRead}} and {{assureSufficientReplicas}} share the logic which potentially makes it prone to modifications just in one place and not the other. Looks like we could combine the two. In same methods, naming is either liveReplicas or allLive for same argument. \\r\\n  * {{assureSufficientReplicas}} might need word \"live\" in it\\r\\n  * {{ReplicaCount}} can now use the new {{count}} method\\r\\n  * {{Rep \n",
            "my_comment: we now use these _extensively_ and we also have only one implementation so it could be very cheap to implement e.g. a {{filterLazily}} and {{\\\\{anyallnone\\\\}Match}}\n",
            "** I have removed {{select()}} because the correct logic did not benefit from this abstraction.  I have however renamed the {{keep()}} method in {{Endpoints}} that re-orders the contents to {{select()}} to make clear the distinction of semantics.\n",
            "\n",
            "Then there are a few follow up commits to either fix bugs or perform some follow up clean ups.\n",
            "\n",
            "Looking forward to your feedback. FYI my fix for {{maybeSendAdditionalReads}} is incomplete I will follow up with it on Monday but it will be very minor. \"[~ifesdjeen] that branch you linked to in your PR is the wrong one its 14705\" [~aweisberg] sorry it was right until branch was renamed and switched)\n",
            "\n",
            "Thank you for the patch. Some comments all rather minor:\n",
            "\n",
            "  * we could rename {{liveOnly}} to {{live}} unless \"only\" here bears \n",
            "  * {{allUncontactedCandidates}} could be just {{uncontactedCandidates}} \n",
            "  * {{ForTokenWrite}} can be just {{ForWrite}} since we\\ve removed {{ForRangeWrite}} similarly in {{forTokenWriteLiveAndDown}} and {{forTokenWrite}}\n",
            "  * we can avoid iterating in {{ReplicaPlan#forWrite}} checking for {{isTransient}} and short-circuit it for non-transient keyspace with {{CL#hasTransient}}.\n",
            " * in {{EndpointsFor(Range|Token)}} we have checks for range/token so they matched for concatenated ranges. Should we apply same checks when constructing layouts?\n",
            "  * I know it was same before the patch and we can do it in a separate one but {{isSufficientReplicasForRead}} and {{assureSufficientReplicas}} share the logic which potentially makes it prone to modifications just in one place and not the other. Looks like we could combine the two. In same methods naming is either liveReplicas or allLive for same argument. \n",
            "  * {{assureSufficientReplicas}} might need word \"live\" in it\n",
            "  * {{ReplicaCount}} can now use the new {{count}} method\n",
            "  * {{Rep \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1343\n",
            "issue_type:  Bug \n",
            "summary:  gossip throws IllegalStateException \n",
            "description:  when starting a second node, gossip throws IllegalStateException when KS with RF>1 defined on an existing 1-node cluster.\n",
            "comments:  [\"/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/bin/java -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:54219,suspend=y,server=n -ea -Xms128M -Xmx1G -XX:TargetSurvivorRatio=90 -XX:+AggressiveOpts -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+HeapDumpOnOutOfMemoryError -XX:SurvivorRatio=128 -XX:MaxTenuringThreshold=0 -Dcom.sun.management.jmxremote.port=8081 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcassandra -Dcassandra-foreground=yes -Dlog4j.configuration=log4j-server.properties -Dmx4jport=9081 -Dfile.encoding=MacRoman -classpath /System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/deploy.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/dt.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/javaws.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/jce.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/management-agent.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/plugin.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/sa-jdi.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/alt-rt.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/charsets.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/classes.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/dt.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/jce.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/jconsole.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/jsse.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/laf.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/management-agent.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/ui.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/apple_provider.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/dnsns.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/localedata.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/sunjce_provider.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/sunpkcs11.jar:/Users/gary.dusbabek/codes/apache/git-trunk/out/production/conf1:/Users/gary.dusbabek/codes/apache/git-trunk/out/production/core:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jackson-mapper-asl-1.4.0.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/log4j-1.2.14.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/antlr-3.1.3.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/clhm-production.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-cli-1.1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jetty-6.1.21.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jackson-core-asl-1.4.0.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/slf4j-log4j12-1.5.8.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/hadoop-core-0.20.1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jug-2.0.0.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/slf4j-api-1.5.8.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jetty-util-6.1.21.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-collections-3.2.1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jline-0.9.94.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/servlet-api-2.5-20081211.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/high-scale-lib.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-codec-1.2.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-lang-2.4.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/libthrift-r959516.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/json-simple-1.1.jar:/Users/gary.dusbabek/codes/apache/ \n",
            "my_comment:  [\"/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/bin/java -agentlib:jdwp=transport=dt_socketaddress=127.0.0.1:54219suspend=yserver=n -ea -Xms128M -Xmx1G -XX:TargetSurvivorRatio=90 -XX:+AggressiveOpts -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+HeapDumpOnOutOfMemoryError -XX:SurvivorRatio=128 -XX:MaxTenuringThreshold=0 -Dcom.sun.management.jmxremote.port=8081 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcassandra -Dcassandra-foreground=yes -Dlog4j.configuration=log4j-server.properties -Dmx4jport=9081 -Dfile.encoding=MacRoman -classpath /System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/deploy.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/dt.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/javaws.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/jce.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/management-agent.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/plugin.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/sa-jdi.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/alt-rt.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/charsets.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/classes.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/dt.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/jce.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/jconsole.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/jsse.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/laf.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/management-agent.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/ui.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/apple_provider.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/dnsns.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/localedata.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/sunjce_provider.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/sunpkcs11.jar:/Users/gary.dusbabek/codes/apache/git-trunk/out/production/conf1:/Users/gary.dusbabek/codes/apache/git-trunk/out/production/core:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jackson-mapper-asl-1.4.0.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/log4j-1.2.14.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/antlr-3.1.3.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/clhm-production.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-cli-1.1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jetty-6.1.21.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jackson-core-asl-1.4.0.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/slf4j-log4j12-1.5.8.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/hadoop-core-0.20.1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jug-2.0.0.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/slf4j-api-1.5.8.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jetty-util-6.1.21.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-collections-3.2.1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jline-0.9.94.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/servlet-api-2.5-20081211.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/high-scale-lib.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-codec-1.2.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-lang-2.4.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/libthrift-r959516.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/json-simple-1.1.jar:/Users/gary.dusbabek/codes/apache/ \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3931\n",
            "issue_type:  Bug \n",
            "summary:  gossipers notion of schema differs from reality as reported by the nodes in question \n",
            "description:  On a 1.1 cluster we happened to notice that {{nodetool gossipinfo | grep SCHEMA}} reported disagreement:\n",
            "comments:  [\"I forgot to mention that {{describe_ring}} is using a piece of code that actually sends messages to other nodes asking for their schema, while {{gossipinfo}} is giving the information contained in the Gossiper's endpoint state map.\\n\\nGiven that the schema seems to *actually* be propagated, I suspect this is only a gossiping propagation bug but that is not confirmed.\", \"Hmm, does hinted handoff work in this state?  I ask because we've had this problem before and addressed it there:\\n\\n{code}\\n        waited = 0;\\n        // then wait for the correct schema version.\\n        // usually we use DD.getDefsVersion, which checks the local schema uuid as stored in the system table.\\n        // here we check the one in gossip instead; this serves as a canary to warn us if we introduce a bug that\\n        // causes the two to diverge (see CASSANDRA-2946)\\n{code}\", 'Patch to update gossip when merging remote versions.', 'v2 replaces all the one-off calls of passiveAnnounce by introducing updateVersionAndAnnounce in Schema.', \"+1 with nit: change in Migration is unnecessary because passiveAnnounce get's called as part of Migration.announce() routine so we don't need to change apply() behavior.\", 'Committed w/Migration.apply change removed.', \"@Brandon For the record, we had HH turned off so I don't know.\", 'FYI the fixes for this issue introduced issue CASSANDRA-5025.'] \n",
            "my_comment: change in Migration is unnecessary because passiveAnnounce gets called as part of Migration.announce() routine so we dont need to change apply() behavior.\" Committed w/Migration.apply change removed. \"@Brandon For the record we had HH turned off so I dont know.\" FYI the fixes for this issue introduced issue CASSANDRA-5025. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-13272\n",
            "issue_type:  Bug \n",
            "summary:  \"nodetool bootstrap resume\" does not exit \n",
            "description:  I have a script that calls \"nodetool bootstrap resume\" after a failed join (in my environment some streams sometimes fail due to mis-tuning of stream bandwidth settings). However, if the streams fail again, nodetool won't exit.\n",
            "comments:  ['{noformat}\\ndiff --git a/src/java/org/apache/cassandra/service/StorageService.java b/src/java/org/apache/cassandra/service/StorageService.java\\nindex 9bc046f..d7c1aa5 100644\\n--- a/src/java/org/apache/cassandra/service/StorageService.java\\n+++ b/src/java/org/apache/cassandra/service/StorageService.java\\n@@ -1287,8 +1287,9 @@ public class StorageService extends NotificationBroadcasterSupport implements IE\\n                 @Override\\n                 public void onFailure(Throwable e)\\n                 {\\n-                    String message = \"Error during bootstrap: \" + e.getCause().getMessage();\\n-                    logger.error(message, e.getCause());\\n+                    Throwable cause = Throwables.getRootCause(e);\\n+                    String message = \"Error during bootstrap: \" + cause.getMessage();\\n+                    logger.error(message, cause);\\n                     progressSupport.progress(\"bootstrap\", new ProgressEvent(ProgressEventType.ERROR, 1, 1, message));\\n                     progressSupport.progress(\"bootstrap\", new ProgressEvent(ProgressEventType.COMPLETE, 1, 1, \"Resume bootstrap complete\"));\\n                 }\\ndiff --git a/src/java/org/apache/cassandra/utils/Throwables.java b/src/java/org/apache/cassandra/utils/Throwables.java\\nindex 5ad9686..30fc9f4 100644\\n--- a/src/java/org/apache/cassandra/utils/Throwables.java\\n+++ b/src/java/org/apache/cassandra/utils/Throwables.java\\n@@ -30,6 +30,18 @@ import org.apache.cassandra.io.FSWriteError;\\n\\n public final class Throwables\\n {\\n+    public static Throwable getRootCause(Throwable t)\\n+    {\\n+        Throwable cause = t.getCause();\\n+        if (cause == null) {\\n+            return t;\\n+        }\\n+        while (cause.getCause() != null) {\\n+            cause = cause.getCause();\\n+        }\\n+        return cause;\\n+    }\\n+\\n     public enum FileOpType { READ, WRITE }\\n\\n     public interface DiscreteAction<E extends Exception>\\n{noformat}', 'Thanks for the patch.\\n\\nI guess, based on the patch, that the problem came from the fact that {{e.getCause().getMessage()}} was causing a {{NPE}}?\\n\\nRegarding the patch, could you add some javadoc and a unit test for {{Throwables::getRootCause}}.\\nCould you also attach the patch as a file?', 'I just realize that your patch change the current behavior. The cause should be the cause of the Exception or the Exception itself not the root cause of the all chain.', 'I had a look at the sources and I am not sure to understand the purpose of the original code. Everywhere else in {{StorageService}} the full chain of Exceptions is being logged.\\n\\nIf the wrapping Exception was created without a message it will automatically use the message of the wrapped exception. Due to that, we should probably use:\\n{code}\\n                @Override\\n                public void onFailure(Throwable e)\\n                {\\n                    String message = \"Error during bootstrap: \" + e.getMessage();\\n                    logger.error(message, e);\\n                    progressSupport.progress(\"bootstrap\", new ProgressEvent(ProgressEventType.ERROR, 1, 1, message));\\n                    progressSupport.progress(\"bootstrap\", new ProgressEvent(ProgressEventType.COMPLETE, 1, 1, \"Resume bootstrap complete\"));\\n                }\\n{code}\\n\\n[~yukim] Do you remember why you used the exception cause instead of the exception?', \"I think at the time I thought {{Throwable e}} is usually {{ExecutionException}} so I wanted to get what caused that execution error.\\nI'm not sure what's causing its cause to be null, but it looks it can happen.\\nI think we need proper null handling there.\", 'bq. I think at the time I thought {{Throwable e}} is usually {{ExecutionException}} so I wanted to get what caused that execution error.\\nIn this case, we should probably retrieve the cause only if {{e}} is an {{ExecutionException}} and the cause is not {{null}}. It should never be the case if {{e}} is an {{ExecutionException}} but better be safe than sorry.\\n\\n[~W \n",
            "my_comment: \" + e.getMessage();\n",
            "                    logger.error(message e);\n",
            "                    progressSupport.progress(\"bootstrap\" new ProgressEvent(ProgressEventType.ERROR 1 1 message));\n",
            "                    progressSupport.progress(\"bootstrap\" new ProgressEvent(ProgressEventType.COMPLETE 1 1 \"Resume bootstrap complete\"));\n",
            "                }\n",
            "{code}\n",
            "\n",
            "[~yukim] Do you remember why you used the exception cause instead of the exception? \"I think at the time I thought {{Throwable e}} is usually {{ExecutionException}} so I wanted to get what caused that execution error.\n",
            "Im not sure whats causing its cause to be null but it looks it can happen.\n",
            "I think we need proper null handling there.\" bq. I think at the time I thought {{Throwable e}} is usually {{ExecutionException}} so I wanted to get what caused that execution error.\n",
            "In this case we should probably retrieve the cause only if {{e}} is an {{ExecutionException}} and the cause is not {{null}}. It should never be the case if {{e}} is an {{ExecutionException}} but better be safe than sorry.\n",
            "\n",
            "[~W \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-414\n",
            "issue_type:  Improvement \n",
            "summary:  remove sstableLock \n",
            "description: None\n",
            "comments: None\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4221\n",
            "issue_type:  Bug \n",
            "summary:  Error while deleting a columnfamily that is being compacted. \n",
            "description:  The following dtest command produces an error:\n",
            "comments:  ['This one seems to be caused by the same problem as CASSANDRA-4230.', \"Maybe, but I'm skeptical -- 4230 is complaining about a file existing when it shouldn't, while this one says a file doesn't exist that should :)\", 'Patch adds a try to stop all running compactions on given Keyspace or ColumnFamily before running a drop command. I have tried the test you have in the description and it ran without failures.', 'That takes us back to the Bad Old Days pre-CASSANDRA-3116, though.  We should be able to fix w/o resorting to A Big Lock.', 'For the KS or CF drop this seems necessary to try to wait until all running compactions finish otherwise it would end up in errors like one in the description, also other operations - create, update - are not affected by this.', 'The idea from 3116 was:\\n\\n- Drop will only delete sstables not actively being compacted\\n- post-compaction, we check if the CF was dropped, and if so we delete the sstables then', \"I don't know which one is better tho because if compaction fails for some reason which that scenario, wouldn't that mean that all SSTables that were left behind are staying until somebody manually deletes them (or restart would drop them)? We would have to add complexity to the schema merge just to handle that case as well as on the local side...\", \"The other way would be to 'mark a CF for delete' and return to the user right way (making CF invisible to users), sending the drop request to the others where they would apply the same thing (try to stop all compactions running, wait until they are done) and drop.\", \"bq. wouldn't that mean that all SSTables that were left behind are staying until somebody manually deletes them (or restart would drop them)? \\n\\nWe already clean up partially-written sstables after compaction failure, I don't see why we couldn't use similar logic here.\", \"The problem I see that that is we need to do a snapshot before start dropping or deleting any CF files so it's probably better to make that drop option 'deferred' until running compactions are stopped so we have a persistent view of the files we would have to operate upon.\", 'DataTracker already makes sstable changes atomic, though.  At any time you can snapshot with that and get a consistent view.', 'Tyler, can you still reproduce after the recent schema fixes on the 1.1 branch?', 'Yes, the error just happened again for me. I did a fresh pull on branch branch cassandra-1.1.', \"Interesting, I can't reproduce it myself. Can you please run it with logging patch attached (and enabled DEBUG logging) and attach debug log from your C* node to this task, so I can check that is happening inside of DataTracker in your case?... \", 'This was after applying both patches to the cassandra-1.1 branch, and setting logging to DEBUG.', 'Somehow that server.log did not have the debug info. Looking into it now.', 'Debug is enabled now; It looks like CCM overwrites the log level. Only the logging patch was applied in this run. ', 'I see debug information I added right now, but there is no IOError in that log described in this task...', \"So after some experimentation, the problem is only happening when the log level is set to INFO, but it doesn't happen at DEBUG. Gotta love these ones! I modified the logging patch to do logging.info() rather then logging.debug(), and the problem still happens, so at least you can see those debug messages. I hope this is enough to go on.\", \"Hah, now I know what is causing it - it's not a drop problem, the situation is triggered when you re-create ColumnFamily right after drop +(before all SSTables were actually deleted by background task)+ so it reads up all SSTables in the directory back to system and tries to compact them simultaneously with them being deleted in the background. That is why we warn people to *avoid* making any modifications to the active CFs otherwise it could lead to the strange situations like this one.\", 'Would this be fixed by CASSANDRA-3794 then, since old and new CF will have diff \n",
            "my_comment:  This one seems to be caused by the same problem as CASSANDRA-4230. \"Maybe but Im skeptical -- 4230 is complaining about a file existing when it shouldnt while this one says a file doesnt exist that should :)\" Patch adds a try to stop all running compactions on given Keyspace or ColumnFamily before running a drop command. I have tried the test you have in the description and it ran without failures. That takes us back to the Bad Old Days pre-CASSANDRA-3116 though.  We should be able to fix w/o resorting to A Big Lock. For the KS or CF drop this seems necessary to try to wait until all running compactions finish otherwise it would end up in errors like one in the description also other operations - create update - are not affected by this. The idea from 3116 was:\n",
            "\n",
            "- Drop will only delete sstables not actively being compacted\n",
            "- post-compaction we check if the CF was dropped and if so we delete the sstables then \"I dont know which one is better tho because if compaction fails for some reason which that scenario wouldnt that mean that all SSTables that were left behind are staying until somebody manually deletes them (or restart would drop them)? We would have to add complexity to the schema merge just to handle that case as well as on the local side...\" \"The other way would be to mark a CF for delete and return to the user right way (making CF invisible to users) sending the drop request to the others where they would apply the same thing (try to stop all compactions running wait until they are done) and drop.\" \"bq. wouldnt that mean that all SSTables that were left behind are staying until somebody manually deletes them (or restart would drop them)? \n",
            "\n",
            "We already clean up partially-written sstables after compaction failure I dont see why we couldnt use similar logic here.\" \"The problem I see that that is we need to do a snapshot before start dropping or deleting any CF files so its probably better to make that drop option deferred until running compactions are stopped so we have a persistent view of the files we would have to operate upon.\" DataTracker already makes sstable changes atomic though.  At any time you can snapshot with that and get a consistent view. Tyler can you still reproduce after the recent schema fixes on the 1.1 branch? Yes the error just happened again for me. I did a fresh pull on branch branch cassandra-1.1. \"Interesting I cant reproduce it myself. Can you please run it with logging patch attached (and enabled DEBUG logging) and attach debug log from your C* node to this task so I can check that is happening inside of DataTracker in your case?... \" This was after applying both patches to the cassandra-1.1 branch and setting logging to DEBUG. Somehow that server.log did not have the debug info. Looking into it now. Debug is enabled now; It looks like CCM overwrites the log level. Only the logging patch was applied in this run.  I see debug information I added right now but there is no IOError in that log described in this task... \"So after some experimentation the problem is only happening when the log level is set to INFO but it doesnt happen at DEBUG. Gotta love these ones! I modified the logging patch to do logging.info() rather then logging.debug() and the problem still happens so at least you can see those debug messages. I hope this is enough to go on.\" \"Hah now I know what is causing it - its not a drop problem the situation is triggered when you re-create ColumnFamily right after drop +(before all SSTables were actually deleted by background task)+ so it reads up all SSTables in the directory back to system and tries to compact them simultaneously with them being deleted in the background. That is why we warn people to *avoid* making any modifications to the active CFs otherwise it could lead to the strange situations like this one.\" Would this be fixed by CASSANDRA-3794 then since old and new CF will have diff \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-447\n",
            "issue_type:  New Feature \n",
            "summary:  Ability to temporary set minimum and maximum compaction threshold \n",
            "description:  We need the ability to temporary set minimum and maximum compaction threshold.  This is needed so that we can turn off compaction during BMT.\n",
            "comments:  ['Added the ability set/get max/min compaction threshold in MBean interface\\nUpdated nodeprobe with these new commands.', \"let's make a CompactionManagerMBean instead of echoing calls from SS to MCM.  SS is starting to get cluttered.\\n\\nalso,\\n\\n - make the variables non-static so we don't need two versions of the getter and setter methods.  (this is ok since MCM is a singleton)\\n - follow Cassandra brace placement convention\\n\", 'Created new MinorCompactionManagerMBean, removed SS wrappings, and updated NodeProbe\\nMade static variables regular instance variables in MCM.\\n\\n', 'committed.  also renamed MCM -> CM.', 'Integrated in Cassandra #199 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/199/])\\n    rename MinorCompactionManager -> CompactionManager.  patch by jbellis for \\nadd mbean to get/set compaction thresholds.  patch by Sammy Yu; reviewed by jbellis for \\n'] \n",
            "my_comment:  Added the ability set/get max/min compaction threshold in MBean interface\n",
            "Updated nodeprobe with these new commands. \"lets make a CompactionManagerMBean instead of echoing calls from SS to MCM.  SS is starting to get cluttered.\n",
            "\n",
            "also\n",
            "\n",
            " - make the variables non-static so we dont need two versions of the getter and setter methods.  (this is ok since MCM is a singleton)\n",
            " - follow Cassandra brace placement convention\n",
            "\" Created new MinorCompactionManagerMBean removed SS wrappings and updated NodeProbe\n",
            "Made static variables regular instance variables in MCM.\n",
            "\n",
            " committed.  also renamed MCM -> CM. Integrated in Cassandra #199 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/199/])\n",
            "    rename MinorCompactionManager -> CompactionManager.  patch by jbellis for \n",
            "add mbean to get/set compaction thresholds.  patch by Sammy Yu; reviewed by jbellis for \n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5923\n",
            "issue_type:  Improvement \n",
            "summary:  Upgrade Apache Thrift to 0.9.1 \n",
            "description:  Upgrades the Apache Thrift version from 0.9.0 to 0.9.1 which include over 190 bug fixes and additions since the previous release. This will also upgrade commons-lang from 2.6 to 3.1 as it is a dependency for Thrift. \n",
            "comments:  ['Does it update the bundled python thrift for cqlsh as well?', 'Hi Aleksey, it does not. I do not know why the design decision was made to use an internal only version when the packages are available via pypi. I would recommend that Thrift should be listed as a dependency in setup.py rather than using the thrift-python-internal-only-0.7.0.zip unless there are modifications to the internal only version, which I am unaware of. ', \"There are no modifications AFAIK. You should just bundle it the same way it is now, but with 0.9.1 instead. It's bundled so that you can start using cqlsh immediately without installing anything (that and cql-internal-only).\", \"+1\\n\\nI'm attaching an update that includes the updates to the python lib.\\n\\nAlso, the rowmutation change is also required in trunk; including in this patch.\", \"The patch doesn't apply for me (you want to base it on the tip of cassandra-2.0.0 branch if it's go go into 2.0 and not 2.0.1).\", \"I've cleaned up the patch to be based off of  cassandra-2.0 (not 2.0.0 as that ship has sailed).\\n\\nI've also updated references to o.a.commons.lang to lang3. The only issue I had was that NotImplementedException has been remove; I've replaced that usage with UnsupportedOperationException ([related commons issue|https://issues.apache.org/jira/browse/LANG-769]).\", \"Nope, you've bundled it all wrong - and broke cqlsh.\\n\\nUnzip both the old and the new thrift-internal-only-x.zip and compare the structure.\", 'Fixed; zip file follows previous one.', '+1 and committed, thanks.'] \n",
            "my_comment:  Does it update the bundled python thrift for cqlsh as well? Hi Aleksey it does not. I do not know why the design decision was made to use an internal only version when the packages are available via pypi. I would recommend that Thrift should be listed as a dependency in setup.py rather than using the thrift-python-internal-only-0.7.0.zip unless there are modifications to the internal only version which I am unaware of.  \"There are no modifications AFAIK. You should just bundle it the same way it is now but with 0.9.1 instead. Its bundled so that you can start using cqlsh immediately without installing anything (that and cql-internal-only).\" \"+1\n",
            "\n",
            "Im attaching an update that includes the updates to the python lib.\n",
            "\n",
            "Also the rowmutation change is also required in trunk; including in this patch.\" \"The patch doesnt apply for me (you want to base it on the tip of cassandra-2.0.0 branch if its go go into 2.0 and not 2.0.1).\" \"Ive cleaned up the patch to be based off of  cassandra-2.0 (not 2.0.0 as that ship has sailed).\n",
            "\n",
            "Ive also updated references to o.a.commons.lang to lang3. The only issue I had was that NotImplementedException has been remove; Ive replaced that usage with UnsupportedOperationException ([related commons issue|https://issues.apache.org/jira/browse/LANG-769]).\" \"Nope youve bundled it all wrong - and broke cqlsh.\n",
            "\n",
            "Unzip both the old and the new thrift-internal-only-x.zip and compare the structure.\" Fixed; zip file follows previous one. +1 and committed thanks. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-18320\n",
            "issue_type:  Bug \n",
            "summary:  Incompatible file system thrown while running Simulator \n",
            "description:  {code}\n",
            "comments:  ['It says that it can not recursively remove a directory because it was not empty. This is happening after Cluster (as in dtest jvm api) is being closed.\\r\\n\\r\\nI wonder how that is related to what we did in CASSANDRA-18294. There are two ways of removing stuff, \"Java way\" (whatever that means) and calling \"rm\" command in *nix by means of CASSANDRA-17427 which adds this CassandraRelevantProperty:\\r\\n\\r\\n{code}\\r\\n    /** When enabled, recursive directory deletion will be executed using a unix command `rm -rf` instead of traversing\\r\\n     * and removing individual files. This is now used only tests, but eventually we will make it true by default.*/\\r\\n    USE_NIX_RECURSIVE_DELETE(\"cassandra.use_nix_recursive_delete\"),\\r\\n{code}\\r\\n\\r\\nWhen I set this to true before that cleanup logic in simulator kicks in, it will just remove it without any error.\\r\\n\\r\\nhttps://app.circleci.com/pipelines/github/instaclustr/cassandra/1980/workflows/9cbd06eb-4333-485c-ad38-5986cd4870c6\\r\\nhttps://github.com/instaclustr/cassandra/commit/d7c44651d63a75ca017414b172538bd9246e4b98\\r\\n\\r\\nThis property is set only in IDEA test run config but you see that there is the intention to set it by default to true in tests without setting any flag.\\r\\n\\r\\nhttps://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/util/PathUtils.java#L391', 'agree with [~smiklosovic] here, this isn\\'t a simulator issue but a jvm-dtest Cluster.close issue\\r\\n\\r\\n{code}\\r\\n\\tat org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1047)\\r\\n\\tat org.apache.cassandra.simulator.ClusterSimulation.close(ClusterSimulation.java:816)\\r\\n\\tat org.apache.cassandra.simulator.SimulationRunner$Run.run(SimulationRunner.java:370)\\r\\n\\tat org.apache.cassandra.simulator.SimulationRunner$BasicCommand.run(SimulationRunner.java:345)\\r\\n\\tat org.apache.cassandra.simulator.paxos.PaxosSimulationRunner.main(PaxosSimulationRunner.java:148)\\r\\n{code}\\r\\n\\r\\nSince we got a \"Caused by: java.nio.file.DirectoryNotEmptyException: /cassandra/node1/commitlog\" we then rethrow the error, but this looks like we have a close bug...\\r\\n\\r\\n{code}\\r\\n@Override\\r\\n    public void close()\\r\\n    {\\r\\n        FBUtilities.waitOnFutures(instances.stream()\\r\\n                                           .filter(i -> !i.isShutdown())\\r\\n                                           .map(IInstance::shutdown)\\r\\n                                           .collect(Collectors.toList()),\\r\\n                                  1L, TimeUnit.MINUTES);\\r\\n\\r\\n        instances.clear();\\r\\n        instanceMap.clear();\\r\\n        PathUtils.setDeletionListener(ignore -> {});\\r\\n        // Make sure to only delete directory when threads are stopped\\r\\n        if (Files.exists(root))\\r\\n            PathUtils.deleteRecursive(root);\\r\\n        Thread.setDefaultUncaughtExceptionHandler(previousHandler);\\r\\n        previousHandler = null;\\r\\n        checkAndResetUncaughtExceptions();\\r\\n        //checkForThreadLeaks();\\r\\n        //withThreadLeakCheck(futures);\\r\\n    }\\r\\n{code}\\r\\n\\r\\nAll our close futures were success, yet commit log touched a file while we were deleting... so we have a concurrency issue with PathUtils.deleteRecursive(root);\\r\\n\\r\\nbq. This property is set only in IDEA test run config but you see that there is the intention to set it by default to true in tests without setting any flag.\\r\\n\\r\\nIts hard for me to test this as the issue looks to be a concurrency issue, so if `rm -rd` fails when the dir adds a file AFTER the dentry was loaded by rm I can\\'t say... but I don\\'t think changing the implementation actually fixes this bug, it looks like commit log said it was fully closed when it wasn\\'t, so we have concurrent file creation while we are deleting...\\r\\n', 'Yes I agree, [~dcapwell], it seems like CommitLog \"leaks\". deleteRecursive should just delete that stuff and no new files should be created while it is doing its job. It seems like somebody wrote a file while it was about \n",
            "my_comment: /cassandra/node1/commitlog\" we then rethrow the error but this looks like we have a close bug...\n",
            "\n",
            "{code}\n",
            "@Override\n",
            "    public void close()\n",
            "    {\n",
            "        FBUtilities.waitOnFutures(instances.stream()\n",
            "                                           .filter(i -> !i.isShutdown())\n",
            "                                           .map(IInstance::shutdown)\n",
            "                                           .collect(Collectors.toList())\n",
            "                                  1L TimeUnit.MINUTES);\n",
            "\n",
            "        instances.clear();\n",
            "        instanceMap.clear();\n",
            "        PathUtils.setDeletionListener(ignore -> {});\n",
            "        // Make sure to only delete directory when threads are stopped\n",
            "        if (Files.exists(root))\n",
            "            PathUtils.deleteRecursive(root);\n",
            "        Thread.setDefaultUncaughtExceptionHandler(previousHandler);\n",
            "        previousHandler = null;\n",
            "        checkAndResetUncaughtExceptions();\n",
            "        //checkForThreadLeaks();\n",
            "        //withThreadLeakCheck(futures);\n",
            "    }\n",
            "{code}\n",
            "\n",
            "All our close futures were success yet commit log touched a file while we were deleting... so we have a concurrency issue with PathUtils.deleteRecursive(root);\n",
            "\n",
            "bq. This property is set only in IDEA test run config but you see that there is the intention to set it by default to true in tests without setting any flag.\n",
            "\n",
            "Its hard for me to test this as the issue looks to be a concurrency issue so if `rm -rd` fails when the dir adds a file AFTER the dentry was loaded by rm I can\\t say... but I don\\t think changing the implementation actually fixes this bug it looks like commit log said it was fully closed when it wasn\\t so we have concurrent file creation while we are deleting...\n",
            " Yes I agree [~dcapwell] it seems like CommitLog \"leaks\". deleteRecursive should just delete that stuff and no new files should be created while it is doing its job. It seems like somebody wrote a file while it was about \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6689\n",
            "issue_type:  New Feature \n",
            "summary:  Partially Off Heap Memtables \n",
            "description:  Move the contents of ByteBuffers off-heap for records written to a memtable.\n",
            "comments:  [\"I've uploaded a patch [here|https://github.com/belliottsmith/cassandra/tree/offheap.merge]\\n\\nIn the simplest terms this boils down to a new off heap allocator. However it is much more involved than that, as we need to now manage the life cycle of the memory in question.\\n\\nI won't go into excessive detail here, because I have tried to do so as much as possible in the code itself. For full details see the comments in OffHeapCleaner, RefAction/Referrer, OffHeapAllocator, OffHeapRegion.\\n\\nThe basic thrust of the approach is that we guard accesses to the memory through the OpOrder synchronisation primitive introduced with the CASSANDRA-5549. However to prevent stalls and make management of the memory easier, we have some further stages to the lifecycle which are much more GC-like. A corollary is that we also effectively perform GC of the data in the memtable, so that space that is no longer needed as a result of overwritten records can be made available again without first flushing the memtable.\\n\", \"Assigning [~krummas] as reviewer so I don't become the bottleneck this time. :)\\n\\n[~xedin] and [~jasobrown] might also be interested.\", 'I can jump in if [~krummas] is busy.', \"Thanks, [~jbellis], I'm already in the watcher list :)\", \"[~jasobrown] [~xedin] I wouldn't mind more sets of eyes on this patch, so, if you have time, please take a look!\", 'There are some natural boundaries if you want to share the burden. Everything inside of utils.concurrent is pretty isolated from everything outside, so could easily be vetted independently.\\n\\nAlso, the utilisation of Referrer/RefAction is probably going to be a painstaking thing to vet (that\\'s what makes the majority of small touches outside of the main changes), and quite independent of their declarations. We just need to be certain we always use the correct type of RefAction, and never let one disappear somewhere - OutboundTCPConnection and native transport writing are the two danger areas here (also Memtable flushing needs a bit of care, but is definitely less scary).\\n\\nThe most difficult thing to review is going to be the main body of work inside of utils.memory, however. This is pretty hardcore lock-free stuff, and the thing we\\'re looking for is _unintended_ race conditions (there are lots of intended races) - in particular pay attention to the way in which we now asynchronously manage the \"subpool\" and \"suballocator\" (ledgers of how much we\\'ve allocated / claimed / are reclaiming), and obviously most importantly that we never accidentally overwrite data that is being read elsewhere. This should all hopefully be very clearly documented both at the level of abstraction and the individual points where interesting / dangerous things happen. But try to figure it out for yourself as well, in case I and my tests missed something. I will be doing further tests in the near future, but I much prefer to catch things by eye if possible.\\n\\nAlways feel free to throw up a \"this bit isn\\'t well explained\" flag and I\\'ll try to improve it. I want this stuff to be as clearly self documenting as possible.\\n\\n', \"Pushed a slightly revised version [here|https://github.com/belliottsmith/cassandra/tree/offheap1], as didn't override a couple of methods in the Cell heirarchy. It's also merged with latest trunk.\", \"Overall this is a very complicated/involved change that we really need to test/benchmark (I can't make it go faster than HeapSlabPool)/figure out if we want it in. That being said, it looks very solid.\\n\\nThe biggest risks are, I guess (and where I spent most of my time when reviewing), that we start corrupting/freeing used/leaking offheap memory, but as far as I can tell, the patch avoids that. We should do long-running tests to verify it as well.\\n\\nIt would probably have been nice to split this patch up in smaller pieces, say doing the GC in a separate ticket and only discarding entire regions on flush here, but I understand it would be difficult to separate the things.\\n\\nA commit on  \n",
            "my_comment:  [\"Ive uploaded a patch [here|https://github.com/belliottsmith/cassandra/tree/offheap.merge]\n",
            "\n",
            "In the simplest terms this boils down to a new off heap allocator. However it is much more involved than that as we need to now manage the life cycle of the memory in question.\n",
            "\n",
            "I wont go into excessive detail here because I have tried to do so as much as possible in the code itself. For full details see the comments in OffHeapCleaner RefAction/Referrer OffHeapAllocator OffHeapRegion.\n",
            "\n",
            "The basic thrust of the approach is that we guard accesses to the memory through the OpOrder synchronisation primitive introduced with the CASSANDRA-5549. However to prevent stalls and make management of the memory easier we have some further stages to the lifecycle which are much more GC-like. A corollary is that we also effectively perform GC of the data in the memtable so that space that is no longer needed as a result of overwritten records can be made available again without first flushing the memtable.\n",
            "\" \"Assigning [~krummas] as reviewer so I dont become the bottleneck this time. :)\n",
            "\n",
            "[~xedin] and [~jasobrown] might also be interested.\" I can jump in if [~krummas] is busy. \"Thanks [~jbellis] Im already in the watcher list :)\" \"[~jasobrown] [~xedin] I wouldnt mind more sets of eyes on this patch so if you have time please take a look!\" There are some natural boundaries if you want to share the burden. Everything inside of utils.concurrent is pretty isolated from everything outside so could easily be vetted independently.\n",
            "\n",
            "Also the utilisation of Referrer/RefAction is probably going to be a painstaking thing to vet (that\\s what makes the majority of small touches outside of the main changes) and quite independent of their declarations. We just need to be certain we always use the correct type of RefAction and never let one disappear somewhere - OutboundTCPConnection and native transport writing are the two danger areas here (also Memtable flushing needs a bit of care but is definitely less scary).\n",
            "\n",
            "e looking for is _unintended_ race conditions (there are lots of intended races) - in particular pay attention to the way in which we now asynchronously manage the \"subpool\" and \"suballocator\" (ledgers of how much we\\ve allocated / claimed / are reclaiming) and obviously most importantly that we never accidentally overwrite data that is being read elsewhere. This should all hopefully be very clearly documented both at the level of abstraction and the individual points where interesting / dangerous things happen. But try to figure it out for yourself as well in case I and my tests missed something. I will be doing further tests in the near future but I much prefer to catch things by eye if possible.\n",
            "\n",
            "Always feel free to throw up a \"this bit isn\\t well explained\" flag and I\\ll try to improve it. I want this stuff to be as clearly self documenting as possible.\n",
            "\n",
            " \"Pushed a slightly revised version [here|https://github.com/belliottsmith/cassandra/tree/offheap1] as didnt override a couple of methods in the Cell heirarchy. Its also merged with latest trunk.\" \"Overall this is a very complicated/involved change that we really need to test/benchmark (I cant make it go faster than HeapSlabPool)/figure out if we want it in. That being said it looks very solid.\n",
            "\n",
            "The biggest risks are I guess (and where I spent most of my time when reviewing) that we start corrupting/freeing used/leaking offheap memory but as far as I can tell the patch avoids that. We should do long-running tests to verify it as well.\n",
            "\n",
            "It would probably have been nice to split this patch up in smaller pieces say doing the GC in a separate ticket and only discarding entire regions on flush here but I understand it would be difficult to separate the things.\n",
            "\n",
            "A commit on  \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14523\n",
            "issue_type:  New Feature \n",
            "summary:  Thread pool stats virtual table \n",
            "description:  Expose the thread pools like in status logger/tpstats. Additionally be nice to include the scheduled executor pools that are currently unmonitored.\n",
            "comments:  ['GitHub user clohfink opened a pull request:\\n\\n    https://github.com/apache/cassandra/pull/237\\n\\n    Virtual table of thread pool stats for CASSANDRA-14523\\n\\n    \\n\\nYou can merge this pull request into a Git repository by running:\\n\\n    $ git pull https://github.com/clohfink/cassandra virtual_tpstats\\n\\nAlternatively you can review and apply these changes as the patch at:\\n\\n    https://github.com/apache/cassandra/pull/237.patch\\n\\nTo close this pull request, make a commit to your master/trunk branch\\nwith (at least) the following in the commit message:\\n\\n    This closes #237\\n    \\n----\\ncommit 3d1d75e014b6262a4c37e9eb607559825a945d4d\\nAuthor: Chris Lohfink <clohfink@...>\\nDate:   2018-06-14T06:45:06Z\\n\\n    wip\\n\\ncommit 7b6762f9ddb4afcec458134dd32dbb46cccfcc3e\\nAuthor: Chris Lohfink <clohfink@...>\\nDate:   2018-06-14T19:09:26Z\\n\\n    wip\\n\\n----\\n', 'Thanks for the patch.\\r\\n\\r\\nI think we should take advantages of the virtual table patches to clean a bit the Metric classes. I pushed a commit [here|https://github.com/apache/cassandra/commit/7e8ec2f22c8d58351e72f550f811108fdc71a0a7] to remove the duplicated code in {{ThreadPoolMetrics}} and {{SEPMetrics}}.\\r\\n\\r\\nOne common problem to expose the metrics through virtual tables when the {{DataSet}} is build on demand is to be able to retrieve the set of metrics. Due to that the current patch has to fetch the metric values through {{JMX}} and change the visibility of some thread pools. To avoid that problem, we could keep a concurrent {{DataSet}} in memory and force the metric classes to add the column values to the tables. I have pushed a patch to add a concurrent {{DataSet}} [here|https://github.com/apache/cassandra/commit/98ff447ec580bf42cc7141a4a484f79ef7f664a9] and one to use it in {{ThreadPoolMetrics}} [here|https://github.com/apache/cassandra/commit/960563fdc865c9c06b66c4f29b161ffbb3d7213d]. I had too change a bit the {{DataSet}} API to be able to add the concurrent {{DataSet}} but the new API for building the {{DataSet}} can be used for both type of {{DataSets}}.\\r\\n\\r\\nAdding the metrics for the {{ScheduledThreadPool}} is a good idea. I just wonder if we should not simply add some real metrics for them. We could do that by replacing the {{ScheduledThreadPoolExecutor}} from {{SSTableReader}} by a {{DebuggableScheduledThreadPoolExecutor}} and by having the {{DebuggableScheduledThreadPoolExecutor}} class creating a metric for itself. The advantage of that approach is that it will also be possible to monitor those thread pools through {{nodetool}} and {{JMX}}. Ideally, I think it should be done in another ticket.\\r\\n\\r\\nI noticed that the patch reformat the name of the pools for exposing them in the virtual table. We should probably keep the original name as it is a bit confusing if other tools and logs display a different name.\\r\\n\\r\\nIt is not directly related to this patch but more a general remark about Virtual Tables: I wonder if we should not use a {{LocalPartitioner}} for them. Keeping the partition ordered by their partition key natural order is in my opinion an advantage for the virtual tables. ([~iamaleksey] do you see any problem with that?)\\r\\n\\r\\nI pushed some commits [here|https://github.com/apache/cassandra/compare/trunk...blerer:14523-trunk-review] to cover all the things I mentioned.\\r\\n\\r\\n\\xa0What do you think [~cnlwsu]?', \"If going with the dynamic virtual table i think that can be generic and creating the MetricTable shouldn't be necessary. Instead maybe a DynamicVirtualTable or something that does what the current MetricTable implementation is, then people can just build them inline without creating a parent class for each.\\r\\n\\r\\nbq.  replacing the ScheduledThreadPoolExecutor...\\r\\n\\r\\nAgreed. We can just remove all the scheduled timer executors and pull off in different ticket then to register metrics for tpstats etc as well. We wouldnt want to put it into DebuggableScheduledThreadPoolExecutor just like we dont put the metrics in DebuggableT \n",
            "my_comment: I wonder if we should not use a {{LocalPartitioner}} for them. Keeping the partition ordered by their partition key natural order is in my opinion an advantage for the virtual tables. ([~iamaleksey] do you see any problem with that?)\n",
            "\n",
            "I pushed some commits [here|https://github.com/apache/cassandra/compare/trunk...blerer:14523-trunk-review] to cover all the things I mentioned.\n",
            "\n",
            "\\xa0What do you think [~cnlwsu]? \"If going with the dynamic virtual table i think that can be generic and creating the MetricTable shouldnt be necessary. Instead maybe a DynamicVirtualTable or something that does what the current MetricTable implementation is then people can just build them inline without creating a parent class for each.\n",
            "\n",
            "bq.  replacing the ScheduledThreadPoolExecutor...\n",
            "\n",
            "Agreed. We can just remove all the scheduled timer executors and pull off in different ticket then to register metrics for tpstats etc as well. We wouldnt want to put it into DebuggableScheduledThreadPoolExecutor just like we dont put the metrics in DebuggableT \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3762\n",
            "issue_type:  Improvement \n",
            "summary:  AutoSaving KeyCache and System load time improvements. \n",
            "description:  CASSANDRA-2392 saves the index summary to the disk... but when we have saved cache we will still scan through the index to get the data out.\n",
            "comments:  ['This patch will read the keys from the cache and sort it, so the index scan can be less impact full on the MMaped index. \\n\\nTest (Not an extensive test but from my laptop):\\n100 Keys to be loaded into cache.\\n\\n7G data file and 15M index (long type keys)\\nbefore the patch:\\n/var/log/cassandra/system.log:DEBUG [SSTableBatchOpen:4] 2012-01-23 15:10:40,825 SSTableReader.java (line 196) INDEX LOAD TIME for /var/lib/cassandra/data/Keyspace2/Standard3/Keyspace2-Standard3-hc-2777: 850 ms.\\nafter this patch:\\n/var/log/cassandra/system.log:DEBUG [SSTableBatchOpen:4] 2012-01-23 15:10:59,128 SSTableReader.java (line 196) INDEX LOAD TIME for /var/lib/cassandra/data/Keyspace2/Standard3/Keyspace2-Standard3-hc-2777: 177 ms.', \"With this patch we trade whole sequential primary_index read for random I/O with SSTableReader.getPosition() only for amount saved keys. Can you extend key cache, let's make it 75% of the keys, and run your test again? I think the closer key cache size will get to actual number of keys the worse will performance get...\", \"I dont think it is as bad as it looks.... We aren't doing a lot of random IO because with this patch Keys are sorted and we will read the same blocks often and if it is mmapped it will get the most advantage. Also most of the work load, the keys will not be from the same SST's and 75% of the keys falling into a SST is not that common IMO (If they do they have a bigger problem because all their reads are going to be loger and longer) the load time increases if we have a lot of data in the disk.\\nI got around 180ms for 3K keys and thats far is the memory in my laptop :)\\n\\nThe other option is to redesign keycache and save the Index location when we store the keys and then look it up and to fault fill the data which are not in the cache via (getPosition).... Makes sense?\", \"I mention this because the problem in the original ticket was with rolling restarts taking too much time on index summary computation (read going though whole PrimaryIndex for every SSTable out there), so imagine situation when you have few hundreds of SSTables each with key cache in the different parts of the primary index this means if you go with getPosition() calls you will have a lot of random I/O (meaning you will have to seek deeper and deeper into the primary index file which means slower data access even in mmap mode) on each of those and I'm not sure if it's really better than reading primary index sequentially especially knowing that you have already read all of the index/data positions from the Summary component. I propose you do the test with many SSTables and compare system load times (don't forget to drop page cache between tests with `sync; echo 3 > /proc/sys/vm/drop_caches`).\\n\\nBy the way, I forgot to ask you if you dropped page cache before running second test? if you didn't that would pretty much explain such a dramatic improvement in the load time...\", '>>> By the way, I forgot to ask you if you dropped page cache before running second test?\\nNo it is basically a average of multiple runs.... :) Without any additional writes... start and immediatly stop and compare the logs... both tests are the same. Again it is on my laptop.', 'bq. No it is basically a average of multiple runs....  Without any additional writes... start and immediatly stop and compare the logs... both tests are the same. Again it is on my laptop.\\n\\nWhich means the more you run the more data you get cached which affects the results, I would suggest you to drop cache every time you run each of the tests to get cleaner load time values when any I/O is involved.', 'That applies to the sequential index scans too (the variables are same both are in-memory it is just 15MB index :) )... looks like i have to do more extensive test than this one but i am not sure how much is an optimal number i will try loading the keycache over night with stress tool.', \"Bigger test with much spread out data showed some mixed results.... Tested on M24XL nodes in AWS,  \n",
            "my_comment: 177 ms. \"With this patch we trade whole sequential primary_index read for random I/O with SSTableReader.getPosition() only for amount saved keys. Can you extend key cache lets make it 75% of the keys and run your test again? I think the closer key cache size will get to actual number of keys the worse will performance get...\" \"I dont think it is as bad as it looks.... We arent doing a lot of random IO because with this patch Keys are sorted and we will read the same blocks often and if it is mmapped it will get the most advantage. Also most of the work load the keys will not be from the same SSTs and 75% of the keys falling into a SST is not that common IMO (If they do they have a bigger problem because all their reads are going to be loger and longer) the load time increases if we have a lot of data in the disk.\n",
            "I got around 180ms for 3K keys and thats far is the memory in my laptop :)\n",
            "\n",
            "The other option is to redesign keycache and save the Index location when we store the keys and then look it up and to fault fill the data which are not in the cache via (getPosition).... Makes sense?\" \"I mention this because the problem in the original ticket was with rolling restarts taking too much time on index summary computation (read going though whole PrimaryIndex for every SSTable out there) so imagine situation when you have few hundreds of SSTables each with key cache in the different parts of the primary index this means if you go with getPosition() calls you will have a lot of random I/O (meaning you will have to seek deeper and deeper into the primary index file which means slower data access even in mmap mode) on each of those and Im not sure if its really better than reading primary index sequentially especially knowing that you have already read all of the index/data positions from the Summary component. I propose you do the test with many SSTables and compare system load times (dont forget to drop page cache between tests with `sync; echo 3 > /proc/sys/vm/drop_caches`).\n",
            "\n",
            "By the way I forgot to ask you if you dropped page cache before running second test? if you didnt that would pretty much explain such a dramatic improvement in the load time...\" >>> By the way I forgot to ask you if you dropped page cache before running second test?\n",
            "No it is basically a average of multiple runs.... :) Without any additional writes... start and immediatly stop and compare the logs... both tests are the same. Again it is on my laptop. bq. No it is basically a average of multiple runs....  Without any additional writes... start and immediatly stop and compare the logs... both tests are the same. Again it is on my laptop.\n",
            "\n",
            "Which means the more you run the more data you get cached which affects the results I would suggest you to drop cache every time you run each of the tests to get cleaner load time values when any I/O is involved. That applies to the sequential index scans too (the variables are same both are in-memory it is just 15MB index :) )... looks like i have to do more extensive test than this one but i am not sure how much is an optimal number i will try loading the keycache over night with stress tool. \"Bigger test with much spread out data showed some mixed results.... Tested on M24XL nodes in AWS  \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14058\n",
            "issue_type:  Improvement \n",
            "summary:  Refactor read executor and response resolver, abstract read repair \n",
            "description:  CASSANDRA-10726 is stuck right now because the state of {{AbstractReadExecutor}} and {{DataResolver}} make it difficult to cleanly implement. It also looks like some additional read repair strategies might be added. This goal of this ticket is to clean up the structure of some of the read path components to make CASSANDRA-10726 doable, and additional read repair strategies possible. \n",
            "comments:  ['I have an initial approach here https://github.com/bdeggleston/cassandra/tree/14058 that:\\r\\n* adds a {{ReadRepair}} interface, and 2 implementations: noop and blocking\\r\\n* Move read repair and short read implementation details out of DataResolver\\r\\n* Added a {{service.reads}} package <- any thoughts on moving the other read related classes from {{service}} in there?\\r\\n* removes DigestMismatchException', 'This looks good to me in general;\\r\\n\\r\\nA few comments/questions/bike sheds/nits;\\r\\n* {{Row/PartitionIteratorMergeListener}} has a dependency on {{BlockingReadRepair}} - guess this should be {{ReadRepair}} instead? Or, if the {{R/PIML}} is intended to be {{BRR}} specific, we should perhaps make them inner classes there?\\r\\n* For the {{HintedReadRepair}} (CASSANDRA-10726) we can share most of the code from {{BlockingReadRepair}} - we should probably break that out in an abstract class (but that should be done in 10726)\\r\\n* Not a huge fan that {{ReadRepair}} has {{DigestResolver}}-specific methods - but I have no real improvement suggestion here - either {{ReadRepair}} has {{DigestResolver}}-specific logic or {{DigestResolver}} has read repair logic.\\r\\n* The comment on top of {{ShortReadPartitionsProtection}} should probably go to {{ShortReadProtection}} as that is the natural start point of SRP\\r\\n* {{BlockingReadRepair.PartitionRepair}} could be a static class by passing in the size of {{endpoints}} to the constructor\\r\\n* The trace message on line 434 in {{AbstractReadExecutor}} should still log the partition key (maybe move the trace message back to DigestResolver#responsesMatch)\\r\\n* In {{AsyncOneResponse}} we could remove {{synchronized}} on {{response(..)}} (nice refactoring of that class btw)\\r\\n* {{get()}} in {{AsyncOneResponse}} should probably have {{@Override}}\\r\\n* {{DigestResolver}} parameter in {{backgroundDigestRepair}} is unused\\r\\n\\r\\nI wrote up a sloppy non-tested version of HintedReadRepair to get a feeling for the abstractions [here|https://github.com/krummas/cassandra/commits/blake/14058-hint] - it contains the comments above as well (in a single big commit, sorry)', '\\xa0\\r\\nbq. Row/PartitionIteratorMergeListener has a dependency on BlockingReadRepair - guess this should be ReadRepair instead? Or, if the R/PIML is intended to be BRR specific, we should perhaps make them inner classes there?\\r\\n\\xa0\\r\\nThey’re intended to be BlockingReadRepair specific, since the DigestReadRepair doesn’t do any merging. Given their size though (RIML in particular), I’d rather not make them inner classes. Maybe moving them into repair.blocking sub-package would be the way to go?\\r\\n\\xa0\\r\\nbq. Not a huge fan that ReadRepair has DigestResolver-specific methods - but I have no real improvement suggestion here - either ReadRepair has DigestResolver-specific logic or DigestResolver has read repair logic.\\r\\n\\xa0\\r\\nI think I intended that as sort of a shim that would go away after CASSANDRA-10726. Once the strategy is configurable, there will probably be a factory class or something attached to the table metadata that can handle that background repair case.\\r\\n\\r\\nrebased on current trunk and pushed up with review fixes [here|https://github.com/bdeggleston/cassandra/tree/14058-v2]\\r\\n\\xa0', '+1, lgtm', \"This is +1'd, is it ready to commit?\\r\\n\", \"I'd rebased after CASSANDRA-7544 was committed and saw some dtest failures. It should be committed soon, I just haven't had enough contiguous free time to get everything buttoned up.\", 'committed to trunk as\\xa039807ba48ed2e02223014fbf47dce21d4124b380', 'Noticed a couple regressions when merging up CASSANDRA-14330:\\r\\n1. {{DataResolver}} no longer uses {{cassandra.drop_oversized_readrepair_mutations}} prop - and {{DROP_OVERSIZED_READ_REPAIR_MUTATIONS}} constant is now unused, and the feature is missing.\\r\\n2. {{RowIteratorMergeListener}} re-thrown {{AssertionError}} no longer includes the responses. This should be restored, as without it debugging RR issues is an even worse, p \n",
            "my_comment: noop and blocking\n",
            "* Move read repair and short read implementation details out of DataResolver\n",
            "* Added a {{service.reads}} package <- any thoughts on moving the other read related classes from {{service}} in there?\n",
            "* removes DigestMismatchException This looks good to me in general;\n",
            "\n",
            "A few comments/questions/bike sheds/nits;\n",
            "* {{Row/PartitionIteratorMergeListener}} has a dependency on {{BlockingReadRepair}} - guess this should be {{ReadRepair}} instead? Or if the {{R/PIML}} is intended to be {{BRR}} specific we should perhaps make them inner classes there?\n",
            "* For the {{HintedReadRepair}} (CASSANDRA-10726) we can share most of the code from {{BlockingReadRepair}} - we should probably break that out in an abstract class (but that should be done in 10726)\n",
            "* Not a huge fan that {{ReadRepair}} has {{DigestResolver}}-specific methods - but I have no real improvement suggestion here - either {{ReadRepair}} has {{DigestResolver}}-specific logic or {{DigestResolver}} has read repair logic.\n",
            "* The comment on top of {{ShortReadPartitionsProtection}} should probably go to {{ShortReadProtection}} as that is the natural start point of SRP\n",
            "* {{BlockingReadRepair.PartitionRepair}} could be a static class by passing in the size of {{endpoints}} to the constructor\n",
            "* The trace message on line 434 in {{AbstractReadExecutor}} should still log the partition key (maybe move the trace message back to DigestResolver#responsesMatch)\n",
            "* In {{AsyncOneResponse}} we could remove {{synchronized}} on {{response(..)}} (nice refactoring of that class btw)\n",
            "* {{get()}} in {{AsyncOneResponse}} should probably have {{@Override}}\n",
            "* {{DigestResolver}} parameter in {{backgroundDigestRepair}} is unused\n",
            "\n",
            "I wrote up a sloppy non-tested version of HintedReadRepair to get a feeling for the abstractions [here|https://github.com/krummas/cassandra/commits/blake/14058-hint] - it contains the comments above as well (in a single big commit sorry) \\xa0\n",
            "bq. Row/PartitionIteratorMergeListener has a dependency on BlockingReadRepair - guess this should be ReadRepair instead? Or if the R/PIML is intended to be BRR specific we should perhaps make them inner classes there?\n",
            "\\xa0\n",
            "They’re intended to be BlockingReadRepair specific since the DigestReadRepair doesn’t do any merging. Given their size though (RIML in particular) I’d rather not make them inner classes. Maybe moving them into repair.blocking sub-package would be the way to go?\n",
            "\\xa0\n",
            "bq. Not a huge fan that ReadRepair has DigestResolver-specific methods - but I have no real improvement suggestion here - either ReadRepair has DigestResolver-specific logic or DigestResolver has read repair logic.\n",
            "\\xa0\n",
            "I think I intended that as sort of a shim that would go away after CASSANDRA-10726. Once the strategy is configurable there will probably be a factory class or something attached to the table metadata that can handle that background repair case.\n",
            "\n",
            "rebased on current trunk and pushed up with review fixes [here|https://github.com/bdeggleston/cassandra/tree/14058-v2]\n",
            "\\xa0 +1 lgtm \"This is +1d is it ready to commit?\n",
            "\" \"Id rebased after CASSANDRA-7544 was committed and saw some dtest failures. It should be committed soon I just havent had enough contiguous free time to get everything buttoned up.\" committed to trunk as\\xa039807ba48ed2e02223014fbf47dce21d4124b380 Noticed a couple regressions when merging up CASSANDRA-14330:\n",
            "1. {{DataResolver}} no longer uses {{cassandra.drop_oversized_readrepair_mutations}} prop - and {{DROP_OVERSIZED_READ_REPAIR_MUTATIONS}} constant is now unused and the feature is missing.\n",
            "2. {{RowIteratorMergeListener}} re-thrown {{AssertionError}} no longer includes the responses. This should be restored as without it debugging RR issues is an even worse p \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-11540\n",
            "issue_type:  Bug \n",
            "summary:  The JVM should exit if jmx fails to bind \n",
            "description:  If you are already running a cassandra instance, but for some reason try to start another one, this happens:\n",
            "comments:  [\"We could exit on IOException (it's {{BindException}} most of time, although since it's wrapped into {{ExportException}}, we'd have to extract the cause)\\n\\n|| |2.2|3.0|trunk|\\n|code|[2.2|https://github.com/ifesdjeen/cassandra/tree/11540-2.2]|[3.0|https://github.com/ifesdjeen/cassandra/tree/11540-3.0]|[trunk|https://github.com/ifesdjeen/cassandra/tree/11540-trunk]|\\n|dtest|[2.2|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-11540-2.2-dtest/]|[3.0|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-11540-3.0-dtest/]|[trunk|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-11540-trunk-dtest/]|\\n|utest|[2.2|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-11540-2.2-testall/]|[3.0|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-11540-3.0-testall/]|[trunk|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-11540-trunk-testall/]|\\n\\nI'm re-running the tests for both branches just in case.\", 'CI looks good, so committed to 2.2 in {{93c5bc616e21ffa7f31266ad095ca374f2ba73a4}} and merged to 3.0/3.7/trunk.', 'From dtests, {{restart_node_localhost_test}} is already reported was introduced before the patch: [11702|https://issues.apache.org/jira/browse/CASSANDRA-11702] (although it looked related. also, passes locally) same with {{test_upgrade_index_summary}} [11127|https://issues.apache.org/jira/browse/CASSANDRA-11127] and {{upgrade_with_wide_partition_reversed_test}} [11663|https://issues.apache.org/jira/browse/CASSANDRA-11663]. The rest of them also look unrelated, although I ran them just in case. {{testJsonThreadSafety}} is failing for me quite often, so I filed the issue, too: [11712|https://issues.apache.org/jira/browse/CASSANDRA-11712].', 'oh :) thank you [~beobal] :) looks like I got late with my comment about tests )', 'Just a quick note on behavior after this commit, as we have found in cstar_perf (trunk will not start at all, currently).\\n\\nIf a user happens to set an environment variable {{JVM_OPTS=\"$JVM_OPTS -Dcom.sun.management.jmxremote.port=7199 ...\"}} startup fails. There\\'s no other Cassandra running and nothing was listening to 7199 prior to startup.\\n\\nI believe that commit 7b0c716 introduced for CASSANDRA-10091 does something a little wonky with regards to JVM_OPTS. The previous commit, ad7e36b, does not throw an error when JVM_OPTS are passed in the user env.\\n\\nIf this hard failure is working as intended, please let me know!\\n\\nRepro on trunk HEAD:\\n{noformat}\\n(trunk)mshuler@mana:~/git/cassandra$ sudo netstat -atunp | grep 7199\\n(trunk)mshuler@mana:~/git/cassandra$\\n(trunk)mshuler@mana:~/git/cassandra$ export JVM_OPTS=\"-Dcom.sun.management.jmxremote.port=7199 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Djava.rmi.server.hostname=127.0.0.1\"\\n(trunk)mshuler@mana:~/git/cassandra$ \\n(trunk)mshuler@mana:~/git/cassandra$ cassandra -f\\n<...>\\nINFO  22:00:18 Not submitting build tasks for views in keyspace system as storage service is not initialized\\nERROR 22:00:18 Port already in use: 7199; nested exception is: \\n        java.net.BindException: Address already in use\\njava.net.BindException: Address already in use\\n        at java.net.PlainSocketImpl.socketBind(Native Method) ~[na:1.8.0_92]\\n        at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:387) ~[na:1.8.0_92]\\n        at java.net.ServerSocket.bind(ServerSocket.java:375) ~[na:1.8.0_92]\\n        at java.net.ServerSocket.<init>(ServerSocket.java:237) ~[na:1.8.0_92]\\n        at javax.net.DefaultServerSocketFactory.createServerSocket(ServerSocketFactory.java:231) ~[na:1.8.0_92]\\n        at org.apache.cassandra.utils.RMIServerSocketFactoryImpl.createServerSocket(RMIServerSocketFactoryImpl.java:21) ~[main/:na]\\n        at sun.rmi.transport.tcp.TCPEndpoint.newServerSocket(TCPEndpoint.java:666) ~[na:1.8.0_92]\\n        at sun.rmi.transport.tcp.TCPTransport.listen(TCPTransport.java:330) ~[na:1.8.0_92]\\n         \n",
            "my_comment: Address already in use\n",
            "        at java.net.PlainSocketImpl.socketBind(Native Method) ~[na:1.8.0_92]\n",
            "        at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:387) ~[na:1.8.0_92]\n",
            "        at java.net.ServerSocket.bind(ServerSocket.java:375) ~[na:1.8.0_92]\n",
            "        at java.net.ServerSocket.<init>(ServerSocket.java:237) ~[na:1.8.0_92]\n",
            "        at javax.net.DefaultServerSocketFactory.createServerSocket(ServerSocketFactory.java:231) ~[na:1.8.0_92]\n",
            "        at org.apache.cassandra.utils.RMIServerSocketFactoryImpl.createServerSocket(RMIServerSocketFactoryImpl.java:21) ~[main/:na]\n",
            "        at sun.rmi.transport.tcp.TCPEndpoint.newServerSocket(TCPEndpoint.java:666) ~[na:1.8.0_92]\n",
            "        at sun.rmi.transport.tcp.TCPTransport.listen(TCPTransport.java:330) ~[na:1.8.0_92]\n",
            "         \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9880\n",
            "issue_type:  Bug \n",
            "summary:  ScrubTest.testScrubOutOfOrder should generate test file on the fly \n",
            "description:  ScrubTest#testScrubOutOfOrder is failing on trunk due to the serialization format change from pre-generated out-of-order SSTable.\n",
            "comments:  [\"Patch attached as link.\\nLet's see what cassci says.\", 'testall: http://cassci.datastax.com/job/yukim-9880-testall/lastBuild/testReport/\\n\\nScrubTest passed.', '[~Stefania] to review', 'The code is +1 but we need to rebase. There are also a couple of unused imports.', 'Force pushed rebased version to: https://github.com/yukim/cassandra/tree/9880\\n\\nCan you check the code again?\\nScrubTest still passes, but new API is not yet familiar to me.\\n', 'I am not an expert with the new API either but it looks correct, so +1. \\n\\nI created a [pull request|https://github.com/yukim/cassandra/pull/1] with some, mostly unrelated, suggestions:\\n\\n- keepOriginals in scrubber is not used\\n- there is a BAD RELEASE error in sstable.validate(), if you are not comfortable committing this just open another ticket, exception details below\\n- there is a slightly more compact API taken from fillCF().\\n\\n{code}\\nERROR 23:46:59 BAD RELEASE: attempted to release a reference (org.apache.cassandra.utils.concurrent.Ref$State@59662a93) that has already been released\\nERROR 23:46:59 Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@59662a93:\\nThread[main,5,main]\\n  at java.lang.Thread.getStackTrace(Thread.java:1552)\\n  at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:218)\\n  at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:148)\\n  at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:70)\\n  at org.apache.cassandra.io.sstable.format.SSTableReader.<init>(SSTableReader.java:215)\\n  at org.apache.cassandra.io.sstable.format.big.BigTableReader.<init>(BigTableReader.java:57)\\n  at org.apache.cassandra.io.sstable.format.big.BigFormat$ReaderFactory.open(BigFormat.java:103)\\n  at org.apache.cassandra.io.sstable.format.SSTableReader.internalOpen(SSTableReader.java:600)\\n  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:464)\\n  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:362)\\n  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:357)\\n  at org.apache.cassandra.db.ScrubTest.testScrubOutOfOrder(ScrubTest.java:349)\\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n  at java.lang.reflect.Method.invoke(Method.java:497)\\n  at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\\n  at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\\n  at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\\n  at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\\n  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\\n  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\\n  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\\n  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)\\n  at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)\\n  at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)\\n  at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)\\n  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\\n  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\\n  at org.junit.runners.ParentRunner.run(ParentRunner.java:220)\\n  at org.junit.runner.JUnitCore.run(JUnitCore.java:159)\\n  at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)\\n  at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)\\n  at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)\\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n  at  \n",
            "my_comment: attempted to release a reference (org.apache.cassandra.utils.concurrent.Ref$State@59662a93) that has already been released\n",
            "ERROR 23:46:59 Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@59662a93:\n",
            "Thread[main5main]\n",
            "  at java.lang.Thread.getStackTrace(Thread.java:1552)\n",
            "  at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:218)\n",
            "  at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:148)\n",
            "  at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:70)\n",
            "  at org.apache.cassandra.io.sstable.format.SSTableReader.<init>(SSTableReader.java:215)\n",
            "  at org.apache.cassandra.io.sstable.format.big.BigTableReader.<init>(BigTableReader.java:57)\n",
            "  at org.apache.cassandra.io.sstable.format.big.BigFormat$ReaderFactory.open(BigFormat.java:103)\n",
            "  at org.apache.cassandra.io.sstable.format.SSTableReader.internalOpen(SSTableReader.java:600)\n",
            "  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:464)\n",
            "  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:362)\n",
            "  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:357)\n",
            "  at org.apache.cassandra.db.ScrubTest.testScrubOutOfOrder(ScrubTest.java:349)\n",
            "  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "  at java.lang.reflect.Method.invoke(Method.java:497)\n",
            "  at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n",
            "  at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n",
            "  at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n",
            "  at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n",
            "  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n",
            "  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n",
            "  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n",
            "  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)\n",
            "  at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)\n",
            "  at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)\n",
            "  at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)\n",
            "  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n",
            "  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n",
            "  at org.junit.runners.ParentRunner.run(ParentRunner.java:220)\n",
            "  at org.junit.runner.JUnitCore.run(JUnitCore.java:159)\n",
            "  at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)\n",
            "  at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)\n",
            "  at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)\n",
            "  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "  at  \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-512\n",
            "issue_type:  Bug \n",
            "summary:  regression prevents recognizing local reads \n",
            "description: None\n",
            "comments: None\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-12186\n",
            "issue_type:  Improvement \n",
            "summary:  anticompaction log message doesn't include the parent repair session id \n",
            "description:  It appears that even though incremental repair is now enabled by default post C*-3.0 (which means at the end of each repair session, there is an anti-compaction step that needs to be executed), we don't include the parent repair session UUID in the log message of the anti-compaction log entries. This makes observing all activities related to an incremental repair session to be more difficult. See the following:\n",
            "comments:  ['I created a small patch for this issue, it just adds the parent repair session UUID to the \"Started\" and \"Completed\" log entry. The example above with my patch would be:\\n\\n{noformat}\\nDEBUG [AntiEntropyStage:1] 2016-07-13 01:57:30,956  RepairMessageVerbHandler.java:149 - Got anticompaction request AnticompactionRequest{parentRepairSession=27103de0-489d-11e6-a6d6-cd06faa0aaa2} org.apache.cassandra.repair.messages.AnticompactionRequest@34449ff4\\n<...>\\n<snip>\\n<...>\\nINFO  [CompactionExecutor:5] 2016-07-13 02:07:47,512  CompactionManager.java:511 - Starting anticompaction for trivial_ks.weitest on 1/[BigTableReader(path=\\'/var/lib/cassandra/data/trivial_ks/weitest-538b07d1489b11e6a9ef61c6ff848952/mb-1-big-Data.db\\')] sstables, parentRepairSession=27103de0-489d-11e6-a6d6-cd06faa0aaa2\\nINFO  [CompactionExecutor:5] 2016-07-13 02:07:47,513  CompactionManager.java:540 - SSTable BigTableReader(path=\\'/var/lib/cassandra/data/trivial_ks/weitest-538b07d1489b11e6a9ef61c6ff848952/mb-1-big-Data.db\\') fully contained in range (-9223372036854775808,-9223372036854775808], mutating repairedAt instead of anticompacting\\nINFO  [CompactionExecutor:5] 2016-07-13 02:07:47,570  CompactionManager.java:578 - Completed anticompaction successfully, parentRepairSession=27103de0-489d-11e6-a6d6-cd06faa0aaa2\\n{noformat}\\n\\n', \"This looks good, but how about using the same {{\\\\[repair #b14328a9-dcbc-4bc3-b1d2-47ea48256757\\\\]}} prefix so it's consistent with the rest of repair logging? If you agree, could you provide another patch with this change? You can probably include this prefix in other anti-compaction messages as well.\\n\\nAlso, I think we can/should include this on 3.0 as well, since this is not invasive and will help troubleshooting. Can you provide a 3.0, trunk, and 4.0 patches prepared for commit (add CHANGES.TXT entry and commit message according to [these guidelines|http://cassandra.apache.org/doc/latest/development/patches.html]) ?\", 'I have changed so I use the {{\\\\[repair #b14328a9-dcbc-4bc3-b1d2-47ea48256757\\\\]}} prefix and added that to the other anti-compaction logging. I have put the patches on github: [3.0|https://github.com/tommystendahl/cassandra/tree/cassandra-12186-30], [3.X|https://github.com/tommystendahl/cassandra/tree/cassandra-12186-3x] and [trunk|https://github.com/tommystendahl/cassandra/tree/cassandra-12186-trunk].', 'LGTM, thanks! Submitted CI to make sure this will not break any tests and will commit if everything looks all right:\\n\\n||trunk||\\n|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-12186]|\\n|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12186-testall/lastCompletedBuild/testReport/]|\\n|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12186-dtest/lastCompletedBuild/testReport/]|', 'Committed as 2256778726319fb76b6d85c4a47a957116c78147 on 3.0 and merged up. Thanks!'] \n",
            "my_comment: [3.0|https://github.com/tommystendahl/cassandra/tree/cassandra-12186-30] [3.X|https://github.com/tommystendahl/cassandra/tree/cassandra-12186-3x] and [trunk|https://github.com/tommystendahl/cassandra/tree/cassandra-12186-trunk]. LGTM thanks! Submitted CI to make sure this will not break any tests and will commit if everything looks all right:\n",
            "\n",
            "||trunk||\n",
            "|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-12186]|\n",
            "|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12186-testall/lastCompletedBuild/testReport/]|\n",
            "|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12186-dtest/lastCompletedBuild/testReport/]| Committed as 2256778726319fb76b6d85c4a47a957116c78147 on 3.0 and merged up. Thanks! \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-17870\n",
            "issue_type:  Improvement \n",
            "summary:  nodetool/rebuild: Add flag to exclude nodes from local datacenter \n",
            "description:  During expansion by Dc, when we issue nodetool/rebuild from new dc to rebuild the data from other DCs. If src-dc is not passed explicitly, then C* tries to rebuild the data from the same (new dc) dc. \n",
            "comments:  ['Can you send a PR instead? It can be created on [https://github.com/apache/cassandra]\\r\\n\\r\\nWhy prefer excluding the local DC instead of setting a source DC?\\xa0', \"{quote}\\r\\nWhy prefer excluding the local DC instead of setting a source DC?\\r\\n{quote}\\r\\n\\r\\n[~yifanc] This would be for the case where you are building out a new DC, and your local [new] DC nodes do not have data yet. There's already a way to specify a *{{src-dc-name}}* (which can be the local DC) [see|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/tools/nodetool/Rebuild.java#L30].\", '[~yifanc]\\xa0I created a PR as you had asked https://github.com/apache/cassandra/pull/1931', '+1 on the patch. Thanks for addressing all my comments ', 'Another +1 from me as well (non-committer +1 :( )', 'Circle CI run: [https://app.circleci.com/pipelines/github/sarankk/cassandra?branch=fix-nodetool-rebuild]', '+1, rerunning cci [here|https://app.circleci.com/pipelines/github/krummas/cassandra/840/workflows/aa597ffc-610d-4cc6-a4a4-cbe831cb16f9] - will commit once that looks good', 'and committed, thanks\\r\\n\\r\\none known test failure: https://issues.apache.org/jira/browse/CASSANDRA-17708'] \n",
            "my_comment: https://issues.apache.org/jira/browse/CASSANDRA-17708 \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-15379\n",
            "issue_type:  Improvement \n",
            "summary:  Flush with fast compressors by default \n",
            "description:  [~josnyder] and I have been testing out CASSANDRA-14482 (Zstd compression) on some of our most dense clusters and have been observing close to 50% reduction in footprint with Zstd on some of our workloads! Unfortunately though we have been running into an issue where the flush might take so long (Zstd is slower to compress than LZ4) that we can actually block the next flush and cause instability.\n",
            "comments:  [\"Makes sense.  We keep adding things to 4.0, but this seems to me to bundle along with other config cleanups, and is super minor.\\r\\n\\r\\nI'm not certain about the idea of putting default parameters in the yaml, though, as this is a feature we'll have to maintain despite making very little sense.  We've talked about introducing global and per-Keyspace defaults for tables, and I wonder if we should depend on that here.\", 'Yeah I understand it\\'s a bit late in the 4.0 cut, but I think we\\'ll have users running into this trying out the new Zstd compressor rather quickly (we ran into it on the first cluster we dropped it on).\\r\\n\\r\\nIf you\\'re not a fan of the defaults in yaml (I agree it\\'s not great) to reduce scope I could keep the change internal to C* entirely by adding a default method on\\xa0{{ICompressor}} such as:\\r\\n{noformat}\\r\\ndefault Set<String> unsuitableUseHints() {\\r\\n  return Collections.emptySet();\\r\\n}{noformat}\\r\\nThen the ZstdCompressor would yield a set with the string \"flush\" or something and the flush code path would just use the default compressor in that case.\\r\\n\\r\\n[~benedict] what do you think about this alternative?\\r\\n\\r\\n\\xa0', 'I am +1 on this idea. [~jolynch] would be happy to help review this.', 'In principle this seems reasonable to me, though since this is a temporary measure, and anyway because {{String}} are a bad way to communicate intent (particularly with a specific concept like \"unsuitability\"), perhaps just something like:\\r\\n\\r\\n{code}\\r\\ndefault boolean useOnMemtableFlush() { return true; }\\r\\n{code}\\r\\n\\r\\nOr alternatively\\r\\n{code}\\r\\ndefault ICompressor useOnMemtableFlush() { return this; }\\r\\n{code}', \"Alright, I made it so that Zstd, Deflate and LZ4HC (which compresses extremely slowly) now flush with LZ4 (fast) controlled via an EnumSet. Since I'm changing the ICompressor interface I figured it is more maintainable this way than having a somewhat arbitrary boolean switch.\\r\\n\\r\\nI also took the opportunity to add some more tests and improve the documentation as well. I tried to add some helpful documentation to help people pick compressors (I hear a lot of confusion about why we have Snappy and Deflate still around, so I tried to clarify in the documentation). I'll squash after review comments are integrated.\\r\\n\\r\\n||trunk||\\r\\n|[branch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-15379]|\\r\\n|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-15379.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-15379]|\\r\\n\\r\\nThe one failing unit test appears to be org.apache.cassandra.config.DatabaseDescriptorRefTest, which I thought was supposed to be fixed as part of CASSANDRA-15371, I'll double check tomorrow.\", \"What is your rationale for an {{EnumSet}} being more maintainable than a\\xa0member function? \\xa0As far as I understand we explicitly intend to retire this functionality, so planning for future uses seems counterproductive to me.\\r\\n\\r\\nIf we're adding per-table config for this, why are we blanket changing the behaviour for all relevant compressors? \\xa0This may well be surprising to users, and also seems to make the per-table config superfluous (or at least, only useful to restore the probably-assumed behaviour of using the same compressor for both flush and compaction)\\r\\n\\r\\n\\xa0\", 'My rationale for the {{EnumSet}} over a boolean member function is:\\r\\n # Versus the boolean function idea it doesn\\'t break the ICompressor abstraction and let compressors know that flushes exist. As in, it is very easy for an ICompressor author to claim to be good at {{FAST_COMPRESSION}} but probably can\\'t make the call if that should be used in flushes or other situations. I could have a {{isFastCompressor}} boolean function but given that {{ICompressor}} is a public API interface I think sets of capabilities will be more maintainable than a collection of boolean functions going forwards, especia \n",
            "my_comment:  [\"Makes sense.  We keep adding things to 4.0 but this seems to me to bundle along with other config cleanups and is super minor.\n",
            "\n",
            "Im not certain about the idea of putting default parameters in the yaml though as this is a feature well have to maintain despite making very little sense.  Weve talked about introducing global and per-Keyspace defaults for tables and I wonder if we should depend on that here.\" Yeah I understand it\\s a bit late in the 4.0 cut but I think we\\ll have users running into this trying out the new Zstd compressor rather quickly (we ran into it on the first cluster we dropped it on).\n",
            "\n",
            "e not a fan of the defaults in yaml (I agree it\\s not great) to reduce scope I could keep the change internal to C* entirely by adding a default method on\\xa0{{ICompressor}} such as:\n",
            "{noformat}\n",
            "default Set<String> unsuitableUseHints() {\n",
            "  return Collections.emptySet();\n",
            "}{noformat}\n",
            "Then the ZstdCompressor would yield a set with the string \"flush\" or something and the flush code path would just use the default compressor in that case.\n",
            "\n",
            "[~benedict] what do you think about this alternative?\n",
            "\n",
            "\\xa0 I am +1 on this idea. [~jolynch] would be happy to help review this. In principle this seems reasonable to me though since this is a temporary measure and anyway because {{String}} are a bad way to communicate intent (particularly with a specific concept like \"unsuitability\") perhaps just something like:\n",
            "\n",
            "{code}\n",
            "default boolean useOnMemtableFlush() { return true; }\n",
            "{code}\n",
            "\n",
            "Or alternatively\n",
            "{code}\n",
            "default ICompressor useOnMemtableFlush() { return this; }\n",
            "{code} \"Alright I made it so that Zstd Deflate and LZ4HC (which compresses extremely slowly) now flush with LZ4 (fast) controlled via an EnumSet. Since Im changing the ICompressor interface I figured it is more maintainable this way than having a somewhat arbitrary boolean switch.\n",
            "\n",
            "I also took the opportunity to add some more tests and improve the documentation as well. I tried to add some helpful documentation to help people pick compressors (I hear a lot of confusion about why we have Snappy and Deflate still around so I tried to clarify in the documentation). Ill squash after review comments are integrated.\n",
            "\n",
            "||trunk||\n",
            "|[branch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-15379]|\n",
            "|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-15379.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-15379]|\n",
            "\n",
            "The one failing unit test appears to be org.apache.cassandra.config.DatabaseDescriptorRefTest which I thought was supposed to be fixed as part of CASSANDRA-15371 Ill double check tomorrow.\" \"What is your rationale for an {{EnumSet}} being more maintainable than a\\xa0member function? \\xa0As far as I understand we explicitly intend to retire this functionality so planning for future uses seems counterproductive to me.\n",
            "\n",
            "If were adding per-table config for this why are we blanket changing the behaviour for all relevant compressors? \\xa0This may well be surprising to users and also seems to make the per-table config superfluous (or at least only useful to restore the probably-assumed behaviour of using the same compressor for both flush and compaction)\n",
            "\n",
            "\\xa0\" My rationale for the {{EnumSet}} over a boolean member function is:\n",
            " # Versus the boolean function idea it doesn\\t break the ICompressor abstraction and let compressors know that flushes exist. As in it is very easy for an ICompressor author to claim to be good at {{FAST_COMPRESSION}} but probably can\\t make the call if that should be used in flushes or other situations. I could have a {{isFastCompressor}} boolean function but given that {{ICompressor}} is a public API interface I think sets of capabilities will be more maintainable than a collection of boolean functions going forwards especia \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6706\n",
            "issue_type:  Bug \n",
            "summary:  Duplicate rows returned when in clause has repeated values \n",
            "description:  If a value is repeated within an IN clause then repeated rows are returned for  the repeats:\n",
            "comments:  [\"That is kind of the intended behavior. Is it the best behavior? I don't know, though I'm not sure it matters much in practice tbh. But when there is an IN, we do order the resulting rows following the order of the values in the IN (unless there is an explicit ordering that takes precedence of course) which kind of suggest we consider the IN values as a list rather than a set, and from that perspective, it's probably not entirely crazy to return duplicate results in that case. In particular, if you use a prepared marker for an IN, the server will expect a list, not a set for the values (and changing now would really break users). It's easy enough to avoid the duplication client side if you don't want duplicates.\\n\\nDon't get me wrong, I'm not saying not returning duplicate in that case would be inferior, but rather that I don't see a big problem with the current behavior and so that I'd rather not introduce a breaking change, even a small one, for no good reason.\", 'If I were performing a calculation summing the results then the answer would be wrong. \\nI suppose it is arguable that I should know better than to put duplicates in an in clause but this bit me when I was generating a query by aggregating selection parameters from separate sources.\\nMy background is much more extensive in relational databases and in Postgres (to take my preferred example) I would only get back one row for this query which in terms of correctness is absolutely what I would expect.', \"[~slebresne] Should this be closed as Won't Fix or Not a Problem, or left open?\", 'IRC discussion shows a desire to fix this in 3.0, but only to warn in 2.1.', 'No DBA or developer familiar with SQL will expect this behavior. 100% of them will find it unexpected, and many will conclude it is a bug.\\n\\nI understand CQL is not SQL, but people expect similar semantics.', 'This behavior has apparently already been changed in trunk during the {{SelectStatement}} refactoring CASSANDRA-7981.', 'The patch for 2.1 make sure that Cassandra will log a warning the first time a user execute a query containing an IN restriction with duplicate values on the primary key.\\n\\nThe patch for trunk add unit tests to check the behavior and fix the {{CHANGES.txt}} files.', '[~snazy] can you review?', 'Sure :)', '+1 on the 2.1 patch. It triggers the warning once for the example in the ticket description.\\n\\nThe trunk patch should have tests for duplicate IN values on the partition key.\\nIt checks on duplicate IN values on the clustering key (even 2.1 does not return duplicates on CK duplicates).\\nYou can simply add other assertions like {{SELECT * FROM %s WHERE k1 IN (?, ?) AND k2 = ?}} or {{SELECT * FROM %s WHERE k1 IN (?, ?) AND k2 IN (?, ?)}}.', 'Adds the missing tests.', '+1\\n\\ncommitted as 0c2eaa9 (2.1) + 732986b (trunk, merge-commit)'] \n",
            "my_comment:  [\"That is kind of the intended behavior. Is it the best behavior? I dont know though Im not sure it matters much in practice tbh. But when there is an IN we do order the resulting rows following the order of the values in the IN (unless there is an explicit ordering that takes precedence of course) which kind of suggest we consider the IN values as a list rather than a set and from that perspective its probably not entirely crazy to return duplicate results in that case. In particular if you use a prepared marker for an IN the server will expect a list not a set for the values (and changing now would really break users). Its easy enough to avoid the duplication client side if you dont want duplicates.\n",
            "\n",
            "Dont get me wrong Im not saying not returning duplicate in that case would be inferior but rather that I dont see a big problem with the current behavior and so that Id rather not introduce a breaking change even a small one for no good reason.\" If I were performing a calculation summing the results then the answer would be wrong. \n",
            "I suppose it is arguable that I should know better than to put duplicates in an in clause but this bit me when I was generating a query by aggregating selection parameters from separate sources.\n",
            "My background is much more extensive in relational databases and in Postgres (to take my preferred example) I would only get back one row for this query which in terms of correctness is absolutely what I would expect. \"[~slebresne] Should this be closed as Wont Fix or Not a Problem or left open?\" IRC discussion shows a desire to fix this in 3.0 but only to warn in 2.1. No DBA or developer familiar with SQL will expect this behavior. 100% of them will find it unexpected and many will conclude it is a bug.\n",
            "\n",
            "I understand CQL is not SQL but people expect similar semantics. This behavior has apparently already been changed in trunk during the {{SelectStatement}} refactoring CASSANDRA-7981. The patch for 2.1 make sure that Cassandra will log a warning the first time a user execute a query containing an IN restriction with duplicate values on the primary key.\n",
            "\n",
            "The patch for trunk add unit tests to check the behavior and fix the {{CHANGES.txt}} files. [~snazy] can you review? Sure :) +1 on the 2.1 patch. It triggers the warning once for the example in the ticket description.\n",
            "\n",
            "The trunk patch should have tests for duplicate IN values on the partition key.\n",
            "It checks on duplicate IN values on the clustering key (even 2.1 does not return duplicates on CK duplicates).\n",
            "You can simply add other assertions like {{SELECT * FROM %s WHERE k1 IN (? ?) AND k2 = ?}} or {{SELECT * FROM %s WHERE k1 IN (? ?) AND k2 IN (? ?)}}. Adds the missing tests. +1\n",
            "\n",
            "committed as 0c2eaa9 (2.1) + 732986b (trunk merge-commit) \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3708\n",
            "issue_type:  Sub-task \n",
            "summary:  Support \"composite prefix\" tombstones \n",
            "description: None\n",
            "comments: None\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-16545\n",
            "issue_type:  Bug \n",
            "summary:  Cluster topology change may produce false unavailable for queries \n",
            "description:  When the coordinator processes a query, it first gets the {{ReplicationStrategy}} (RS) from the keyspace to decide the peers to contact. Again, it gets the RS to perform the liveness check for the requested CL. \n",
            "comments:  ['PR: https://github.com/apache/cassandra/pull/954\\r\\nCI: https://app.circleci.com/pipelines/github/yifan-c/cassandra?branch=CASSANDRA-16545%2Ftrunk\\r\\n\\r\\nThe patch is largely a refactor to pass the same {{ReplicationStrategy}} object to build replicaLayout, replicaPlan and CL liveness validation. \\r\\nA test is added to prove that the false unavailable can be thrown when creating the replicaPlan. (in the [first commit|https://github.com/apache/cassandra/pull/954/commits/8d921c5d311c6e97d1f757af64a2e65a84b419ef])\\r\\nThe [second commit|https://github.com/apache/cassandra/pull/954/commits/1b935280e09869736f334f67a72ed778ccfcdec7] makes sure the same RS object is used for peer selection and CL liveness check to avoid race. \\r\\nHowever, {{blockFor}} calculation can still use a different RS object, leading to that the coordinator blocks for a different condition as it originally calculated for. The rest 2 commits address the problem. \\r\\n\\r\\nThe highlights of the patch:\\r\\n* ReplicaLayout and ReplicaPlan now keep a reference to the replication strategy snapshot. The snapshot is now used for peer selection, liveness validation and blockFor calculation. \\r\\n* The usage of Keyspace to validate CL liveness is fully eliminated to avoid potential race. It uses replication strategy instead. \\r\\n\\r\\ncc: [~aleksey][~cnlwsu]', '[~yifanc] overall the approach looks good to me, I have left some comments in the PR. The CI failures in {{ReadRepairTest}} can be fixed [this way|https://github.com/apache/cassandra/pull/954#discussion_r609865746]. I have also run the new {{AssureSufficientLiveNodesTest}} (which is a nice test) is our internal test multiplexer some few hundred times.', 'The last changes look good to me, +1 assuming CI looks good. I have started a ci-cassandra round [here|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/634/pipeline].', '\\r\\nThat CI run looked good, with the exception of the [dtest-upgrade|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest-upgrade/202/] which fell over for an unrelated reason. \\r\\n\\r\\nJust that job has been restarted here: [!https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest-upgrade/209/badge/icon!|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest-upgrade/209/]\\r\\n\\r\\n\\r\\n\\r\\n', '+1', 'Starting commit\\r\\n\\r\\nCI Results:\\r\\n||Branch||Source||Circle CI||Jenkins||\\r\\n|trunk|[branch|https://github.com/yifan-c/cassandra/tree/commit_remote_branch/CASSANDRA-16545-trunk-A70927C8-3771-4980-809D-C36119B6B351]|[build|https://app.circleci.com/pipelines/github/yifan-c/cassandra?branch=commit_remote_branch%2FCASSANDRA-16545-trunk-A70927C8-3771-4980-809D-C36119B6B351]|[build|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/658/pipeline]|\\r\\n\\r\\nCI and Jenkins has a few unrelated failures.\\xa0', 'Committed to trunk as [b915688|https://github.com/apache/cassandra/commit/b915688ea878aaa284f5cedeb799c5f797c4d824]'] \n",
            "my_comment: [!https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest-upgrade/209/badge/icon!|https://ci-cassandra.apache.org/job/Cassandra-devbranch-dtest-upgrade/209/]\n",
            "\n",
            "\n",
            "\n",
            " +1 Starting commit\n",
            "\n",
            "CI Results:\n",
            "||Branch||Source||Circle CI||Jenkins||\n",
            "|trunk|[branch|https://github.com/yifan-c/cassandra/tree/commit_remote_branch/CASSANDRA-16545-trunk-A70927C8-3771-4980-809D-C36119B6B351]|[build|https://app.circleci.com/pipelines/github/yifan-c/cassandra?branch=commit_remote_branch%2FCASSANDRA-16545-trunk-A70927C8-3771-4980-809D-C36119B6B351]|[build|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/658/pipeline]|\n",
            "\n",
            "CI and Jenkins has a few unrelated failures.\\xa0 Committed to trunk as [b915688|https://github.com/apache/cassandra/commit/b915688ea878aaa284f5cedeb799c5f797c4d824] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7983\n",
            "issue_type:  Bug \n",
            "summary:  nodetool repair triggers OOM \n",
            "description:  Customer has a 3 node cluster with 500Mb data on each node\n",
            "comments:  ['system logs from other 2 nodes', 'We got the same problem on Cassandra 2.0.10. I\\'ve traced it to a bug in StorageService#createRepairRangeFrom which gets stuck in an infinite loop allocating memory. This happens when you try to repair (using -st and -et) the very \"first\" range in the ring and the lowest token in the ring is not the minimum token for that partitioner. The problem is the following lines:\\n\\n{code}\\nToken previous = tokenMetadata.getPredecessor(TokenMetadata.firstToken(tokenMetadata.sortedTokens(), parsedEndToken));\\nwhile (parsedBeginToken.compareTo(previous) < 0)\\n  ...\\n  previous = tokenMetadata.getPredecessor(previous);\\n}\\n{code}\\n\\nprevious will never become less than parsedBeginToken.\\n\\nThis bug was introduced with CASSANDRA-7317.\\n', 'Good detective work, [~yarin].', \"I've attached a patch. I ended up rewriting createRepairRangeFrom to make it simpler and added some unit tests.\", 'Committed, thanks!'] \n",
            "my_comment:  system logs from other 2 nodes We got the same problem on Cassandra 2.0.10. I\\ve traced it to a bug in StorageService#createRepairRangeFrom which gets stuck in an infinite loop allocating memory. This happens when you try to repair (using -st and -et) the very \"first\" range in the ring and the lowest token in the ring is not the minimum token for that partitioner. The problem is the following lines:\n",
            "\n",
            "{code}\n",
            "Token previous = tokenMetadata.getPredecessor(TokenMetadata.firstToken(tokenMetadata.sortedTokens() parsedEndToken));\n",
            "while (parsedBeginToken.compareTo(previous) < 0)\n",
            "  ...\n",
            "  previous = tokenMetadata.getPredecessor(previous);\n",
            "}\n",
            "{code}\n",
            "\n",
            "previous will never become less than parsedBeginToken.\n",
            "\n",
            "This bug was introduced with CASSANDRA-7317.\n",
            " Good detective work [~yarin]. \"Ive attached a patch. I ended up rewriting createRepairRangeFrom to make it simpler and added some unit tests.\" Committed thanks! \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-10164\n",
            "issue_type:  Bug \n",
            "summary:  Re-apply MV updates on commitlog replay \n",
            "description:  If a node crashes between the Commit log update and local memtable update of the materialized view the node replica could lose MV data.  This is really only an issue for RF=1 since the other replicas will likely apply successfully.\n",
            "comments:  [\"[patch|https://github.com/tjake/cassandra/tree/10164]\\n[tests|http://cassci.datastax.com/job/tjake-10164-testall/1/]\\n[dtests|http://cassci.datastax.com/job/tjake-10164-dtest/1/]\\n\\n*  I cleaned up the areas that were trying to catch write timeouts from submitMV since that isn't possible as it's all done locally now and actual view updates are async.\\n\\n* Added logic to make MV updates from CL replay not write to the commit log (since we always flush after CL replay anyway)\\n\\n*  Added logic to avoid CL when updating mutations from a streamed sstable (it will flush before transaction is complete)\\n\\n* Found/Fixed a little bug in the builder that would not build > 128 rows in a partition.  I'll add a test for this...\", 'Added wide partition builder test', '+1', 'committed'] \n",
            "my_comment:  [\"[patch|https://github.com/tjake/cassandra/tree/10164]\n",
            "[tests|http://cassci.datastax.com/job/tjake-10164-testall/1/]\n",
            "[dtests|http://cassci.datastax.com/job/tjake-10164-dtest/1/]\n",
            "\n",
            "*  I cleaned up the areas that were trying to catch write timeouts from submitMV since that isnt possible as its all done locally now and actual view updates are async.\n",
            "\n",
            "* Added logic to make MV updates from CL replay not write to the commit log (since we always flush after CL replay anyway)\n",
            "\n",
            "*  Added logic to avoid CL when updating mutations from a streamed sstable (it will flush before transaction is complete)\n",
            "\n",
            "* Found/Fixed a little bug in the builder that would not build > 128 rows in a partition.  Ill add a test for this...\" Added wide partition builder test +1 committed \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9411\n",
            "issue_type:  Bug \n",
            "summary:  Bound statement executions fail after adding a collection-type column \n",
            "description:  After adding a collection-type column to an existing table, executions of statements that are already prepared result in server error (error code 0), with the error message {{java.lang.ArrayIndexOutOfBoundsException}}.\n",
            "comments:  ['Server Stack trace, C* 2.0.11: \\n{code}\\nERROR [Native-Transport-Requests:53] 2015-05-21 16:49:24,657 ErrorMessage.java (line 230) Unexpected exception during request\\njava.lang.ArrayIndexOutOfBoundsException: 2\\n\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap.add(ColumnGroupMap.java:48)\\n\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap.access$200(ColumnGroupMap.java:32)\\n\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap$Builder.add(ColumnGroupMap.java:140)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.processColumnFamily(SelectStatement.java:1176)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:1079)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:285)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:241)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:65)\\n\\tat org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:158)\\n\\tat org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:309)\\n\\tat org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:132)\\n\\tat org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:321)\\n\\tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\\n\\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\\n\\tat org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\\n\\tat org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)\\n\\tat org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\\n\\tat java.lang.Thread.run(Thread.java:745)\\n{code}', 'Server stack trace using C* 2.0.13:\\n{code}\\nERROR [Native-Transport-Requests:53] 2015-05-21 17:04:10,840 ErrorMessage.java (line 231) Unexpected exception during request\\njava.lang.ArrayIndexOutOfBoundsException: 2\\n\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap.add(ColumnGroupMap.java:51)\\n\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap.access$200(ColumnGroupMap.java:35)\\n\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap$Builder.add(ColumnGroupMap.java:167)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.processColumnFamily(SelectStatement.java:1237)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:1123)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:286)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:242)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:64)\\n\\tat org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:158)\\n\\tat org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:309)\\n\\tat org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:132)\\n\\tat org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:321)\\n\\tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\\n\\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\\n\\tat org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\\n\\tat org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)\\n\\tat org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)\\n\\tat jav \n",
            "my_comment: 2\n",
            "\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap.add(ColumnGroupMap.java:51)\n",
            "\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap.access$200(ColumnGroupMap.java:35)\n",
            "\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap$Builder.add(ColumnGroupMap.java:167)\n",
            "\\tat org.apache.cassandra.cql3.statements.SelectStatement.processColumnFamily(SelectStatement.java:1237)\n",
            "\\tat org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:1123)\n",
            "\\tat org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:286)\n",
            "\\tat org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:242)\n",
            "\\tat org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:64)\n",
            "\\tat org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:158)\n",
            "\\tat org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:309)\n",
            "\\tat org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:132)\n",
            "\\tat org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:321)\n",
            "\\tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n",
            "\\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n",
            "\\tat org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n",
            "\\tat org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)\n",
            "\\tat org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)\n",
            "\\tat jav \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3577\n",
            "issue_type:  Bug \n",
            "summary:  TimeoutException When using QuorumEach or ALL consistency on Multi-DC \n",
            "description:  Currently we have \n",
            "comments:  ['All we need to do is forward with the original id, no?  Patch attached to do that.', '(Patch is against 0.8.)', 'But when the Co-Ordinator receives the response with the message ID the message is already removed because ResponseVerbHandler does\\nMessagingService.instance().removeRegisteredCallback(id);\\nWe wont have the ID there.\\n\\n\\n\\n', \"You're right, we switched to using unique message IDs per target in CASSANDRA-2058 so that we can track timeouts for the dynamic snitch, so my patch won't work.\\n\\nI agree that pre-generating extra IDs on the coordinator is the easiest fix, and also that we should just disable this behavior in 0.8 (which was the case until CASSANDRA-3472 anyway).\", 'removing mutation optimization for .8, i will work on the update to 1.1 shortly. Thanks!', 'committed .8 patch w/ comment pointing to this issue', 'Integrated in Cassandra-0.8 #409 (See [https://builds.apache.org/job/Cassandra-0.8/409/])\\n    remove nonlocal DC write optimization\\npatch by Vijay; reviewed by jbellis for CASSANDRA-3577\\n\\njbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1210902\\nFiles : \\n* /cassandra/branches/cassandra-0.8/CHANGES.txt\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java\\n', 'Testing took some additional time, This patch is on 1.1 with an updated MessagingService.version to handle both older version and new version mutations.', \"Patch doesn't apply to latest trunk for me, can you rebase?\", 'Sorry, Rebased to the the trunk. Thanks!', 'v3 attached.  Some cleanup of StorageProxy, switches to FastBAIS, and does a version check on the receiving side as well as the sending (since we do have released versions in the wild sending out \"bad\" FORWARD_HEADERs).', '+1 Thanks!', 'committed', 'Integrated in Cassandra #1249 (See [https://builds.apache.org/job/Cassandra/1249/])\\n    multi-dc replication optimization supporting CL > ONE\\npatch by Vijay and jbellis for CASSANDRA-3577\\n\\njbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1212088\\nFiles : \\n* /cassandra/trunk/CHANGES.txt\\n* /cassandra/trunk/src/java/org/apache/cassandra/db/RowMutationVerbHandler.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java\\n', 'This can actually cause the more subtle problem of CASSANDRA-3585: Node A (DC1) sends a write to node B (DC2), which forwards to node C (DC2).  Node C replies to node A with the message ID it received from node B.  If the message generation on A and B is far enough apart, then A will not have a callback for the reply and all you will see happen is the write timeout (at CL > ONE).  But if A *does* have a callback (for a different operation) waiting, then A will try to apply the mutation response to that callback, which (if the callback is for a read) will result in the error see in that ticket.'] \n",
            "my_comment: Node A (DC1) sends a write to node B (DC2) which forwards to node C (DC2).  Node C replies to node A with the message ID it received from node B.  If the message generation on A and B is far enough apart then A will not have a callback for the reply and all you will see happen is the write timeout (at CL > ONE).  But if A *does* have a callback (for a different operation) waiting then A will try to apply the mutation response to that callback which (if the callback is for a read) will result in the error see in that ticket. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-18153\n",
            "issue_type:  Bug \n",
            "summary:  Memtable being flushed without hostId in version \"me\" and newer during CommitLogReplay \n",
            "description:  On ticket CASSANDRA-16619 some files were changed to allow Cassandra to store HostID in the new \"me\" SSTable version.\n",
            "comments:  ['CC [~jlewandowski]\\xa0', 'Storage service is not yet initiated during CommitLogReplay and MetadataCollection calls StorageService.instance.getLocalHostUUID.\\r\\n\\r\\n[https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/metadata/MetadataCollector.java#L127]\\r\\n\\r\\n{{public MetadataCollector(ClusteringComparator comparator)}}\\r\\n{{\\xa0 \\xa0 {}}\\r\\n{{\\xa0 \\xa0 \\xa0 \\xa0 this(comparator, StorageService.instance.getLocalHostUUID());}}\\r\\n{{\\xa0 \\xa0 }}}\\r\\n\\r\\n\\xa0\\r\\n\\r\\nStorageService.instance.getLocalHostUUID() will return null and all Memtables will be flushed without this information.\\r\\n\\r\\nNext time cassandra starts and these SStables are present, this test will fail because originatingHostId is null:\\r\\n\\r\\n[https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogReplayer.java#L337]\\r\\n\\r\\n{{\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 UUID originatingHostId = reader.getSSTableMetadata().originatingHostId;}}\\r\\n{{\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 if (originatingHostId != null && originatingHostId.equals(localhostId))}}\\r\\n{{\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 builder.addAll(reader.getSSTableMetadata().commitLogIntervals);}}\\r\\n{{\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 else}}\\r\\n{{\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 skippedSSTables.add(reader.getFilename());}}\\r\\n\\r\\n\\xa0\\r\\n\\r\\nI thought of implementing it using same strategy as CommitLog.java\\r\\n\\r\\n[https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLog.java#L211]\\r\\n\\r\\n{{\\xa0 \\xa0 \\xa0 \\xa0 Optional.ofNullable(StorageService.instance.getLocalHostUUID()).orElseGet(SystemKeyspace::getLocalHostId);}}', \"In fact there are some SYSTEM.LOCAL's memtables flushed before Storage service got initialized.\\xa0\\r\\n\\r\\nThese SSTables also have missing Host UUID information.\", 'https://github.com/apache/cassandra/pull/2096/files', \"Hi [~abonacin], to comment on your last comment: _In fact there are some SYSTEM.LOCAL's memtables flushed before Storage service got initialized.  These SSTables also have missing Host UUID information._\\r\\n\\r\\nDo you think this is something which could be included in this patch? Or does your patch solves this already?\", \"Hi. It's just another scenario where we have this issue but this patch will address both.\\r\\n\\r\\n\\xa0\\r\\n\\r\\nAlways we have a memtable being flushed before StorageService initialization, SSTable are created without HostUUID.\\r\\n\\r\\nSSTables are flushed during CommigLogReplay (Keyspaces with mutations on CommitLog). But specifically for system.local, we also have flushes before \\xa0StorageService initialization.\\r\\n\\r\\nThis Patch will correct HostUUID returned by MetadataCollector before StorageService initialization.\\r\\n\\r\\n\\xa0\", \"Hi [~abonacin], thank you for working on this bug.\\r\\n\\r\\nI'm wondering if is there any case where just calling {{SystemKeyspace.getLocalHostId}} would be incorrect? All in all, could you add an assertion on that the ultimately obtained host id is not null?\\r\\n\\r\\nAnother thing is that complaining about missing host id for local strategy sstables is somehow dumb :/\\r\\n\", '[~abonacin] I have added a reproducer here.\\r\\n\\r\\nWhen I run the test, it will print this:\\r\\n\\r\\n{code}\\r\\n/tmp/dtests4687378434417513292/node1/data0/system/local-7ad54392bcdd35a684174e047860b377/nb-2-big -> 00000000-0000-4000-8000-000000000001\\r\\n/tmp/dtests4687378434417513292/node1/data0/system/local-7ad54392bcdd35a684174e047860b377/nb-1-big -> null\\r\\nlocal host id -> 00000000-0000-4000-8000-000000000001\\r\\n{code}\\r\\n\\r\\nSo you see that for the first SSTable of system.local, it will still print null, only the second table is correctly set on originating id so your fix is actually not fixing it?\\r\\n\\r\\nIf we do this:\\r\\n\\r\\n{code}\\r\\n    public MetadataCollector(ClusteringComparator comparator)\\r\\n    {\\r\\n        this(comparator, Optional.ofNullable(StorageService.instance.getLocalHostUUID()).orElseGet(SystemKeyspace::getLocalHostId));\\r\\ \n",
            "my_comment: _In fact there are some SYSTEM.LOCALs memtables flushed before Storage service got initialized.  These SSTables also have missing Host UUID information._\n",
            "\n",
            "Do you think this is something which could be included in this patch? Or does your patch solves this already?\" \"Hi. Its just another scenario where we have this issue but this patch will address both.\n",
            "\n",
            "\\xa0\n",
            "\n",
            "Always we have a memtable being flushed before StorageService initialization SSTable are created without HostUUID.\n",
            "\n",
            "SSTables are flushed during CommigLogReplay (Keyspaces with mutations on CommitLog). But specifically for system.local we also have flushes before \\xa0StorageService initialization.\n",
            "\n",
            "This Patch will correct HostUUID returned by MetadataCollector before StorageService initialization.\n",
            "\n",
            "\\xa0\" \"Hi [~abonacin] thank you for working on this bug.\n",
            "\n",
            "Im wondering if is there any case where just calling {{SystemKeyspace.getLocalHostId}} would be incorrect? All in all could you add an assertion on that the ultimately obtained host id is not null?\n",
            "\n",
            "Another thing is that complaining about missing host id for local strategy sstables is somehow dumb :/\n",
            "\" [~abonacin] I have added a reproducer here.\n",
            "\n",
            "When I run the test it will print this:\n",
            "\n",
            "{code}\n",
            "/tmp/dtests4687378434417513292/node1/data0/system/local-7ad54392bcdd35a684174e047860b377/nb-2-big -> 00000000-0000-4000-8000-000000000001\n",
            "/tmp/dtests4687378434417513292/node1/data0/system/local-7ad54392bcdd35a684174e047860b377/nb-1-big -> null\n",
            "local host id -> 00000000-0000-4000-8000-000000000001\n",
            "{code}\n",
            "\n",
            "So you see that for the first SSTable of system.local it will still print null only the second table is correctly set on originating id so your fix is actually not fixing it?\n",
            "\n",
            "If we do this:\n",
            "\n",
            "{code}\n",
            "    public MetadataCollector(ClusteringComparator comparator)\n",
            "    {\n",
            "\\ \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4308\n",
            "issue_type:  Improvement \n",
            "summary:  Promote the use of IFilter for internal commands \n",
            "description:  All read commands (the IReadCommand) work on slice and names filters, but none of them uses the SliceQueryFilter and NamesQueryFilter classes (RangeSliceCommand uses SlicePrediate from thrift and {SliceFrom, SliceByNames}ReadReadCommand interns the arguments).\n",
            "comments:  ['Attaching patch that reuse the IFilter for command serialization, which will make it easier to add new field to them if needed.\\n\\nThis also remove some uses of thrift internally as a bonus.', 'LGTM.\\n\\nnit: You can now replace QueryFilter.getFilter(SlicePredicate, AbstractType) which is only used in deprecated IndexScanCommand and ColumnFamilyStoreTest,  with newly added ThriftValidation.asIFilter.', 'Committed (with nit fixed). Thanks.'] \n",
            "my_comment: You can now replace QueryFilter.getFilter(SlicePredicate AbstractType) which is only used in deprecated IndexScanCommand and ColumnFamilyStoreTest  with newly added ThriftValidation.asIFilter. Committed (with nit fixed). Thanks. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8746\n",
            "issue_type:  Bug \n",
            "summary:  SSTableReader.cloneWithNewStart can drop too much page cache for compressed files \n",
            "description: None\n",
            "comments: None\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-17509\n",
            "issue_type:  New Feature \n",
            "summary:  Add Guardrail to disable GROUP BY functionality \n",
            "description:  GROUP BY can be expensive and troublesome on large tables. We should have a guardrail to disable this in clusters where we don't want users to have this functionality. \n",
            "comments:  ['[PR|https://github.com/apache/cassandra/pull/1543]\\r\\n[JDK8 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/199/workflows/94525b46-3917-428f-bf96-f8671b1a7ac3]\\r\\n[JDK11 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/199/workflows/df58d46d-cf06-4f00-869c-0f1a8752f1c6]', 'Patch LGTM, I left small comments mostly about \"is\" vs \"get\" in MBeans/Interfaces... I see that we use \"get\" already, but this is actually incorrect... so think we should also fix (100% cool not in this patch, I accept consistency here over correctness)', 'SelectStatement#prepare(boolean forView) is called from View#getSelectStatement() but once that code path is executed, SelectStatement#prepare(ClientState state) is not called. I would say there is a need to propagate ClientState to SelectStatement#prepare(boolean forView)', '[~smiklosovic] I might be wrong but I think that {{View#getSelectStatement()}} is only used by internal queries to get the query used on the {{CREATE VIEW}} query to populate the MV table from the base table contents. I think that query never supports {{{}GROUP BY{}}}. The path for querying MVs seems to work as expected:\\r\\n{code:java}\\r\\n@Test\\r\\npublic void checkView() throws Throwable\\r\\n{\\r\\n    setGuardrail(false);\\r\\n    createTable( \"CREATE TABLE %s(pk int, ck int, v int, PRIMARY KEY(pk, ck))\");\\r\\n    String viewName = createView(\"CREATE MATERIALIZED VIEW %s AS \" +\\r\\n                                 \"SELECT * FROM %s WHERE pk IS NOT null and ck IS NOT null \" +\\r\\n                                 \"PRIMARY KEY(ck, pk)\");\\r\\n    String viewQuery = \"SELECT * FROM \" + viewName + \" WHERE ck=0 GROUP BY pk\";\\r\\n    assertFails(viewQuery, \"GROUP BY functionality is not allowed\");\\r\\n    testExcludedUsers(() -> viewQuery);\\r\\n} {code}', 'yes, I think you are right, thanks for testing that. Please add that test there.', \"Added the couple extra tests and tweaked the query so we didn't need to differentiate warning type on query results and force pushed.\\r\\n\\r\\nWe good to go here [~adelapena]?\", 'latest changes LGTM', 'Looks good to me, we only need a rebase fixing the (trivial) conflicts with the recently added guardrails and a final CI round.', 'Had a rebase w/clean run last Friday; went ahead and did one final one this morning and CI kicked off.\\r\\n\\r\\n[JDK8 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851]\\r\\n[JDK11 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/abb10877-f285-49b4-9d44-fb852fb8a584]', \"The failures at\\xa0[testMetricsCleanupOnDrop|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851/jobs/1977] and [testConnectionsAreRejectedWithInvalidConfig|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851/jobs/1983] in the runs above don't seem related to the changes, so I think the results look good. Those tests failures don't appear on [Butler|https://butler.cassandra.apache.org/#/ci/upstream/compare/Cassandra-trunk/trunk] though, and I haven't found tickets for them, so we should probably create tickets for them.\", \"bq.  we should probably create tickets for them.\\r\\nAgree; I ran a clean (i.e. vanilla trunk) circle run earlier today as a reference and am planning on getting together a focused effort to get us to stable green there in the run up to the freeze. I'll make sure failures from here make it to that effort.\", \"I just saw testMetricsCleanupOnDrop in my runs, was there a ticket opened or should I open? (wasn't sure whether there is not some umbrella ticket or anything that I am missing)\", \"I forgot about that failure, and I can't find any ticket for it. The failure can be reproduced on the CircleCI's multiplexer for both 4.1 and trunk, although I think we haven't seen it yet on Jenkins. I have created CASSANDRA-17658 for fixing it.\"] \n",
            "my_comment:  [PR|https://github.com/apache/cassandra/pull/1543]\n",
            "[JDK8 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/199/workflows/94525b46-3917-428f-bf96-f8671b1a7ac3]\n",
            "[JDK11 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/199/workflows/df58d46d-cf06-4f00-869c-0f1a8752f1c6] Patch LGTM I left small comments mostly about \"is\" vs \"get\" in MBeans/Interfaces... I see that we use \"get\" already but this is actually incorrect... so think we should also fix (100% cool not in this patch I accept consistency here over correctness) SelectStatement#prepare(boolean forView) is called from View#getSelectStatement() but once that code path is executed SelectStatement#prepare(ClientState state) is not called. I would say there is a need to propagate ClientState to SelectStatement#prepare(boolean forView) [~smiklosovic] I might be wrong but I think that {{View#getSelectStatement()}} is only used by internal queries to get the query used on the {{CREATE VIEW}} query to populate the MV table from the base table contents. I think that query never supports {{{}GROUP BY{}}}. The path for querying MVs seems to work as expected:\n",
            "{code:java}\n",
            "@Test\n",
            "public void checkView() throws Throwable\n",
            "{\n",
            "    setGuardrail(false);\n",
            "    createTable( \"CREATE TABLE %s(pk int ck int v int PRIMARY KEY(pk ck))\");\n",
            "    String viewName = createView(\"CREATE MATERIALIZED VIEW %s AS \" +\n",
            "                                 \"SELECT * FROM %s WHERE pk IS NOT null and ck IS NOT null \" +\n",
            "                                 \"PRIMARY KEY(ck pk)\");\n",
            "    String viewQuery = \"SELECT * FROM \" + viewName + \" WHERE ck=0 GROUP BY pk\";\n",
            "    assertFails(viewQuery \"GROUP BY functionality is not allowed\");\n",
            "    testExcludedUsers(() -> viewQuery);\n",
            "} {code} yes I think you are right thanks for testing that. Please add that test there. \"Added the couple extra tests and tweaked the query so we didnt need to differentiate warning type on query results and force pushed.\n",
            "\n",
            "We good to go here [~adelapena]?\" latest changes LGTM Looks good to me we only need a rebase fixing the (trivial) conflicts with the recently added guardrails and a final CI round. Had a rebase w/clean run last Friday; went ahead and did one final one this morning and CI kicked off.\n",
            "\n",
            "[JDK8 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851]\n",
            "[JDK11 CI|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/abb10877-f285-49b4-9d44-fb852fb8a584] \"The failures at\\xa0[testMetricsCleanupOnDrop|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851/jobs/1977] and [testConnectionsAreRejectedWithInvalidConfig|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/211/workflows/2cbb5465-a970-440b-a502-06e380ce6851/jobs/1983] in the runs above dont seem related to the changes so I think the results look good. Those tests failures dont appear on [Butler|https://butler.cassandra.apache.org/#/ci/upstream/compare/Cassandra-trunk/trunk] though and I havent found tickets for them so we should probably create tickets for them.\" \"bq.  we should probably create tickets for them.\n",
            "Agree; I ran a clean (i.e. vanilla trunk) circle run earlier today as a reference and am planning on getting together a focused effort to get us to stable green there in the run up to the freeze. Ill make sure failures from here make it to that effort.\" \"I just saw testMetricsCleanupOnDrop in my runs was there a ticket opened or should I open? (wasnt sure whether there is not some umbrella ticket or anything that I am missing)\" \"I forgot about that failure and I cant find any ticket for it. The failure can be reproduced on the CircleCIs multiplexer for both 4.1 and trunk although I think we havent seen it yet on Jenkins. I have created CASSANDRA-17658 for fixing it.\"] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-758\n",
            "issue_type:  New Feature \n",
            "summary:  support wrapped range queries \n",
            "description:  we want to support scanning from KeyX to KeyA where X > A.  (Thus over the alphabet this would include X Y Z A.)  this is important to allow hadoop to scan each key in the ring exactly once. \n",
            "comments:  ['add wrapped range support + test', '+1 Looks good to me.', 'committed', 'Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])\\n    '] \n",
            "my_comment:  add wrapped range support + test +1 Looks good to me. committed Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])\n",
            "     \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8386\n",
            "issue_type:  Bug \n",
            "summary:  Make sure we release references to sstables after incremental repair \n",
            "description:  We don't release references to all sstables after anticompaction. If they are not anticompacted or are contained fully within the repaired range, we never release the reference.\n",
            "comments:  ['+1', \"I'm going to test this patch, this could fix CASSANDRA-8366\", 'committed, thanks', 'This patch doesn\\'t seem to fix the storage size issue of CASSANDRA-8366. However, I\\'m getting many errors \"Repair session.... Sync failed between ...\". So I think It\\'s better to wait the patch of CASSANDRA-8316 and give it another try.'] \n",
            "my_comment:  +1 \"Im going to test this patch this could fix CASSANDRA-8366\" committed thanks This patch doesn\\t seem to fix the storage size issue of CASSANDRA-8366. However I\\m getting many errors \"Repair session.... Sync failed between ...\". So I think It\\s better to wait the patch of CASSANDRA-8316 and give it another try. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6753\n",
            "issue_type:  Bug \n",
            "summary:  Cassandra2.1~beta1 Stall at Boot \n",
            "description:  I was trying out the new release for several perf. improvements that I am very interested in. After upgrading my cassandra from 2.0.5 to the beta version, cassandra is stalled while init the column families.\n",
            "comments:  [\"Some obvious questions:\\n- Do you see any other errors?\\n- The 0 and -858993472 correspond to the used() and nextClean part of that method, respectively, correct? What about the limit and the cleanThreshold? What do they say?\\n- This is consistent, every time you start?\\n\\nThis is definitely not normal, and is almost certainly a bug, but it shouldn't ever stop Cassandra from starting. So, I wonder if there is a strange interaction going on with some other problem, which may be easier to track down if we can figure out if there is another such problem.\\n\\nCould you attach the output from jstacking the process?\\n\\nThe easiest possibility to explain this is that somehow the memtable_cleanup_threshold is negative. We don't actually check this on startup, which is an oversight. The fact that the value for nextClean is exactly \\\\-0.4 * 2Gb has me suspicious - with an 8Gb heap, we would default to a 2Gb limit, and default cleanup_threshold is 0.4. Is it possible you accidentally added a '\\\\-' prefix to the line in the config file? Unlikely, I know, but it would explain it instantly :-)\\n\\n\", 'aha! good call!\\n\\n{code:title=src/java/org/apache/cassandra/utils/memory/Pool.java#needsCleaning}\\nused(0) >= nextClean(-858993472) && updateNextClean(true) && cleanerThread(true) -- limit(-2147483648), cleanThreshold(0.400000)\\n{code}\\n\\nIt seems to be working after:\\n{code:title=src/java/org/apache/cassandra/config/DatabaseDescriptor.java:1385}\\n(long) conf.memtable_total_space_in_mb << 20\\n{code}\\n\\nAnd failed with an exception (doh!)... but it is unrelated to this ticket.', 'Uploaded a simple patch to both correct the overflow and prevent provision of bad cleanup thresholds', 'committed'] \n",
            "my_comment:  [\"Some obvious questions:\n",
            "- Do you see any other errors?\n",
            "- The 0 and -858993472 correspond to the used() and nextClean part of that method respectively correct? What about the limit and the cleanThreshold? What do they say?\n",
            "- This is consistent every time you start?\n",
            "\n",
            "This is definitely not normal and is almost certainly a bug but it shouldnt ever stop Cassandra from starting. So I wonder if there is a strange interaction going on with some other problem which may be easier to track down if we can figure out if there is another such problem.\n",
            "\n",
            "Could you attach the output from jstacking the process?\n",
            "\n",
            "The easiest possibility to explain this is that somehow the memtable_cleanup_threshold is negative. We dont actually check this on startup which is an oversight. The fact that the value for nextClean is exactly \\\\-0.4 * 2Gb has me suspicious - with an 8Gb heap we would default to a 2Gb limit and default cleanup_threshold is 0.4. Is it possible you accidentally added a \\\\- prefix to the line in the config file? Unlikely I know but it would explain it instantly :-)\n",
            "\n",
            "\" aha! good call!\n",
            "\n",
            "{code:title=src/java/org/apache/cassandra/utils/memory/Pool.java#needsCleaning}\n",
            "used(0) >= nextClean(-858993472) && updateNextClean(true) && cleanerThread(true) -- limit(-2147483648) cleanThreshold(0.400000)\n",
            "{code}\n",
            "\n",
            "It seems to be working after:\n",
            "{code:title=src/java/org/apache/cassandra/config/DatabaseDescriptor.java:1385}\n",
            "(long) conf.memtable_total_space_in_mb << 20\n",
            "{code}\n",
            "\n",
            "And failed with an exception (doh!)... but it is unrelated to this ticket. Uploaded a simple patch to both correct the overflow and prevent provision of bad cleanup thresholds committed \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5856\n",
            "issue_type:  Bug \n",
            "summary:  AE in ArrayBackedSortedColumns \n",
            "description:  {noformat}\n",
            "comments:  [\"First guess: something's getting confused about reversed-ness.  Is that part of the test in question?\", 'CASSANDRA-5762 is probably causing this indirectly by forcing more code to go through the slice path, rather than introducing a bug in collation directly.', \"bq. First guess: something's getting confused about reversed-ness\\n\\nSecond guess: there's a bug in ISR's code for reversed fetches (CASSANDRA-5712).\\n\\nMight need to make it print out the cells in collectReducedColumns to see...\", \"If there's a bug in ISR it's probably older than 5712.  {{prefetched}} makes my head hurt.\", 'Patch to get more information from the assert.\\n\\n(NB: the existing error message indicates that this is NOT a reversed slice.  So, beats the hell out of me how this could be erroring out.  Hence, the need for more information.)', \"I'll get to it today/tomorrow. Yeah. 5762 is very unlikely to be the cause of it.\", 'Two of the patched assertion failures:\\n\\n{noformat}\\njava.lang.AssertionError: Added cell val31254: does not sort as the last; contents are val2960::false:0@1376407099059000,val2960:value:false:4@1376407099059000,val31254::false:0@1376407110826001,val31254:value:false:4@1376407110826001, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)\\n{noformat}\\n\\n{noformat}\\njava.lang.AssertionError: Added cell val54806: does not sort as the last; contents are val22917::false:0@1376408295872003,val22917:value:false:4@1376408295872003,val54806::false:0@1376408305568001,val54806:value:false:4@1376408305568001, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type\\n{noformat}\\n', \"Here's the most concise one I've seen:\\n\\n{noformat}\\njava.lang.AssertionError: Added cell val11599: does not sort as the last; contents are val11599::false:0@1376409359730001,val11599:value:false:4@1376409359730001, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)\\n{noformat}\", \"So, this is a lot simpler than the test makes it look.  It's caused simple by asking for the same column by name twice:\\n\\n{noformat}\\ncqlsh> create keyspace foo WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\\ncqlsh> use foo;\\ncqlsh:foo> create table bar (row varchar, name varchar, value int, PRIMARY KEY (row, name));\\ncqlsh:foo> update bar set value = 1 WHERE row = 'baz' AND name = 'qux';\\ncqlsh:foo> select value from bar where row='baz' AND name in ('qux', 'qux');\\nRequest did not complete within rpc_timeout.\\n{noformat}\\n\\nResults in:\\n{noformat}\\njava.lang.AssertionError: Added cell qux: does not sort as the last; contents are qux::false:0@1376590034567000,qux:value:false:4@1376590034567000, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)\\n{noformat}\", '+1\\n\\nNit: can we make buildBound return Collection to avoid the extra copy?', \"Committed, thanks.\\n\\nbq. Nit: can we make buildBound return Collection to avoid the extra copy?\\n\\nNo, unfortunately we can't. Well, we kinda can, but we'd then have to create an extra iterator for getKeyBound (won't be able to just .get(0)) and two extra iterators in makeFilter (again, won't be able to just .get(i) - and we need to iterate over startBounds and endBounds simultaneously to build the ColumnSlices).\"] \n",
            "my_comment: can we make buildBound return Collection to avoid the extra copy?\n",
            "\n",
            "No unfortunately we cant. Well we kinda can but wed then have to create an extra iterator for getKeyBound (wont be able to just .get(0)) and two extra iterators in makeFilter (again wont be able to just .get(i) - and we need to iterate over startBounds and endBounds simultaneously to build the ColumnSlices).\"] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3032\n",
            "issue_type:  Task \n",
            "summary:  clean up KSMetadata, CFMetadata \n",
            "description:  There are too many conversion methods between Thrift and Avro and Native, which is a potential source of bugs. \n",
            "comments:  ['Standardizes on to{Avro,Thrift} and from{Avro,Thrift} instance methods.  redundant methods are removed.', 'updated to fix regression in CFMD.fromThrift that was causing CliTest to fail', 'Needs rebase', 'rebased', 'Integrated in Cassandra-0.8 #293 (See [https://builds.apache.org/job/Cassandra-0.8/293/])\\n    Fix NPE in describe_ring with a mixed cluster.\\nPatch by brandonwilliams, reviewed by jbellis for CASSANDRA-3032\\n\\nbrandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1161189\\nFiles : \\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java\\n', 'Committed with minor changes and fixes + CFMetaDataTest', 'Integrated in Cassandra #1045 (See [https://builds.apache.org/job/Cassandra/1045/])\\n    Clean up KSMetadata, CFMetadata from unnecessary Thrift<->Avro conversion methods\\npatch by Jonathan Ellis and Pavel Yaskevich; reviewed by Pavel Yaskevich for CASSANDRA-3032\\n\\nxedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1161230\\nFiles : \\n* /cassandra/trunk/test/unit/org/apache/cassandra/config/DatabaseDescriptorTest.java\\n* /cassandra/trunk/test/unit/org/apache/cassandra/config/ColumnDefinitionTest.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/config/ColumnDefinition.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/cql/DropIndexStatement.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/UpdateColumnFamily.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/Migration.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/cql/QueryProcessor.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/AddKeyspace.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/thrift/CassandraServer.java\\n* /cassandra/trunk/test/unit/org/apache/cassandra/thrift/ThriftValidationTest.java\\n* /cassandra/trunk/CHANGES.txt\\n* /cassandra/trunk/src/java/org/apache/cassandra/db/DefsTable.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/cql/AlterTableStatement.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/config/KSMetaData.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/UpdateKeyspace.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/AddColumnFamily.java\\n* /cassandra/trunk/test/unit/org/apache/cassandra/config/CFMetaDataTest.java\\n* /cassandra/trunk/test/unit/org/apache/cassandra/db/DefsTest.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/config/CFMetaData.java\\n* /cassandra/trunk/test/unit/org/apache/cassandra/SchemaLoader.java\\n'] \n",
            "my_comment: \n",
            "* /cassandra/trunk/test/unit/org/apache/cassandra/config/DatabaseDescriptorTest.java\n",
            "* /cassandra/trunk/test/unit/org/apache/cassandra/config/ColumnDefinitionTest.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/config/ColumnDefinition.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/cql/DropIndexStatement.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/UpdateColumnFamily.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/Migration.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/cql/QueryProcessor.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/AddKeyspace.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/thrift/CassandraServer.java\n",
            "* /cassandra/trunk/test/unit/org/apache/cassandra/thrift/ThriftValidationTest.java\n",
            "* /cassandra/trunk/CHANGES.txt\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/db/DefsTable.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/cql/AlterTableStatement.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/config/KSMetaData.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/UpdateKeyspace.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/AddColumnFamily.java\n",
            "* /cassandra/trunk/test/unit/org/apache/cassandra/config/CFMetaDataTest.java\n",
            "* /cassandra/trunk/test/unit/org/apache/cassandra/db/DefsTest.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/config/CFMetaData.java\n",
            "* /cassandra/trunk/test/unit/org/apache/cassandra/SchemaLoader.java\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1367\n",
            "issue_type:  Improvement \n",
            "summary:  Upgrade to Thrift 0.5.0 \n",
            "description:  There's finally a new thrift release out. This gives us a chance to standardize on a release instead of just a revision of thrift trunk.\n",
            "comments:  ['would we be able to keep using a stable release or would things like THRIFT-831 make us decide to go back to revision specific versions?\\n\\nmaybe it depends on whether thrift releases are frequent enough.', \"If it's a serious issue I assume they'll release a 0.3.1 to fix it, otherwise we'll catch it in the next release.\", \"It's not a fix, it's a huge performance increase.\\n\\nIMO having THRIFT-830 and THRIFT-831 would be much more useful than being on 0.3 exactly (which affects clients not at all).\", 'Thrift 0.4 is out now which includes the performance improvements, so we could definitely use that.', '... thrift 0.5 now.', \"This is my first pass at this issue. \\n\\nThe patch does not include the thrift 0.5 jar, so you need to remove the current jar and replace with 0.5 jar\\n\\nAll tests pass except for TestMutations.test_dynamic_indexes_with_system_update_cf\\n\\nI'd appreciate if someone with fresh eyes could look at this.\\n\\n-Jake\", 'Just as an fyi, nirmal just got THRIFT-106 committed for version 0.6 which enables Java client SSL.  Just for those interested.  There will always be feature X enabled in revision Y.  I guess if thrift is coming out with regular versions now, we can just upgrade when version Z comes out (instead of doing builds of revision Y).', 'Removed the configuration changes that should not have been in first patch.', 'Added missing files', 'Fixed so patch will apply cleanly', 'patch has class named CopyOfByteArrayToken\\n\\nas Stu mentioned, BBUtil equals and hashcode are unnecessary\\n\\nByteArrayToken is broken (discards offset and limit information) and should be unnecessary in the first place, if something in BytesToken is causing problems then we should fix that instead (but this is not the cause of the test failure)', 'MurmurHash is broken, it should be using data.get(index) (it also shouldn\\'t take a separate length value since BB knows its length).\\n\\nthe use of position() scares me, it\\'s a bug waiting to happen, e.g. in FBUtilities.hash\\n\\nblock.position()+block.arrayOffset()\\n\\nall of these these should all just be block.arrayOffset().\\n\\nSimilarly remaining() scares me.  We don\\'t \"use up\" our ByteBuffers on purpose except in very unusual cases (e.g. your BBUtil.getLong), all? of these should be limit - offset instead.\\n\\nI _suspect_ that there is a bug from position/remaining causing the test failure: it\\'s building the index on the test rows and being rejected at the row-level bloom filter saying \"this row doesn\\'t exist\" which is completely bogus.\\n\\nstyle: space after commas and between operators please.\\n\\nLet\\'s fix the above and see where that gets us.', \"Ok, I can fix these.  \\n\\nByteArrayToken is needed for MerkleTree serialization.  ByteBuffer isn't Serializable so I convert ByteToken to ByteArrayToken.\\n\\nWe have to use position() as arrayOffset() is almost always zero.  I'm following the logic here http://blog.rapleaf.com/dev/2010/10/19/striving-for-zero-copies-with-thrift-0.5/\\n\\n\", 'i _think_ limit - offset is what capacity() gives you.', 'if thrift is creating multiple bytebuffers from the same byte[] with the same offset it is broken.  look at HeapByteBuffer.slice, the idea is the offset should be the start of the valid bytes in the buffer.  position is mutable by the relative get methods but offset is not.  or hell, look at BB equals/hashcode.', \"bq. ByteArrayToken is needed for MerkleTree serialization. ByteBuffer isn't Serializable so I convert ByteToken to ByteArrayToken\\n\\nMerkleTree doesn't have any member Token fields that I can see...?\", 'RowHash class', 'bq. i think limit - offset is what capacity() gives you. \\n\\nturns out this is wrong.\\n\\nwhat we really want is capacity - offset.  limit is some weird-ass inverse mark.  Buffer doc explains,\\n\\nbq. 0 <= mark <= position <= limit <= capacity ... A read-only buffer does not allow its content to be changed, but its mark, position, and limit values are mutable.', \"... ByteBuffer.wrap cleared things up for me: you're right, we do \n",
            "my_comment: youre right we do \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3223\n",
            "issue_type:  Bug \n",
            "summary:  probably don't need to do full copy to row cache after un-mmap() change \n",
            "description:  3179  changes from directly using the bytebuffer from mmap(), to copying that buffer,\n",
            "comments:  [\"I think you're right.  Care to submit a patch to remove the extra copy?\", 'sure\\n\\n', 'do not need to deep copy column value bytebuffer into row cache, now that we already do this copy in JIRA 3179', 'committed, thanks!'] \n",
            "my_comment:  [\"I think youre right.  Care to submit a patch to remove the extra copy?\" sure\n",
            "\n",
            " do not need to deep copy column value bytebuffer into row cache now that we already do this copy in JIRA 3179 committed thanks! \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-810\n",
            "issue_type:  Improvement \n",
            "summary:  Do away with the streaming directory \n",
            "description:  It muddies the approaches we use when deducing keyspace, cf, etc. from paths. \n",
            "comments:  [\"didn't have any adverse affects when streaming.\", 'Files to be streamed should probably stay marked as temporary so that they get cleaned up automatically if the process dies.', '+1', 'r912620.', 'Integrated in Cassandra #364 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/364/])\\n    No longer use a streaming directory. Patch by Gary Dusbabek and Stu Hood, reviewed by same.  .\\n'] \n",
            "my_comment:  [\"didnt have any adverse affects when streaming.\" Files to be streamed should probably stay marked as temporary so that they get cleaned up automatically if the process dies. +1 r912620. Integrated in Cassandra #364 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/364/])\n",
            "    No longer use a streaming directory. Patch by Gary Dusbabek and Stu Hood reviewed by same.  .\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-18058\n",
            "issue_type:  New Feature \n",
            "summary:  In-memory index and query path \n",
            "description:  An in-memory index using the in-memory trie structure introduced with CASSANDRA-17240 along with a query path implementation to perform index queries from the in-memory index. \n",
            "comments:  ['The github PR for this ticket is here:\\xa0\\r\\n\\r\\n[https://github.com/maedhroz/cassandra/pull/6]\\r\\n\\r\\nThe PR is built against the CASSANDRA-16052 feature branch maintained by [~maedhroz].\\xa0', 'It seems that there is a build error due to {{checkstyle}} detecting some unused imports:\\r\\n{code:java}\\r\\ncheckstyle:\\r\\n    [mkdir] Created dir: /Users/adelapena/src/cassandra/trunk/build/checkstyle\\r\\n[checkstyle] Running Checkstyle 8.40 on 2206 files\\r\\n[checkstyle] [ERROR] /Users/adelapena/src/cassandra/trunk/src/java/org/apache/cassandra/db/PartitionRangeReadCommand.java:51:8: Unused import - org.apache.cassandra.schema.IndexMetadata. [UnusedImports]\\r\\n[checkstyle] [ERROR] /Users/adelapena/src/cassandra/trunk/src/java/org/apache/cassandra/db/TableWriteHandler.java:22:8: Unused import - org.apache.cassandra.index.transactions.UpdateTransaction. [UnusedImports]\\r\\n[checkstyle] [ERROR] /Users/adelapena/src/cassandra/trunk/src/java/org/apache/cassandra/index/sai/StorageAttachedIndex.java:80:8: Unused import - org.apache.cassandra.service.StorageService. [UnusedImports]\\r\\n[checkstyle] [ERROR] /Users/adelapena/src/cassandra/trunk/src/java/org/apache/cassandra/index/sasi/plan/SASIIndexSearcher.java:22:8: Unused import - org.apache.cassandra.config.DatabaseDescriptor. [UnusedImports]\\r\\n[checkstyle] [ERROR] /Users/adelapena/src/cassandra/trunk/src/java/org/apache/cassandra/index/sasi/plan/SASIIndexSearcher.java:30:8: Unused import - org.apache.cassandra.exceptions.RequestTimeoutException. [UnusedImports]\\r\\n[checkstyle] [ERROR] /Users/adelapena/src/cassandra/trunk/src/java/org/apache/cassandra/index/sasi/plan/SASIIndexSearcher.java:35:15: Unused import - java.util.concurrent.TimeUnit.MILLISECONDS. [UnusedImports]\\r\\n[checkstyle] [ERROR] /Users/adelapena/src/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java:59:8: Unused import - org.apache.cassandra.db.rows.SerializationHelper. [UnusedImports]\\r\\n{code}\\r\\nThat prevents us from running the patch on CI, that uses {{checkstyle}} on the root build jobs.', 'Apart from StorageAttachedIndex the rest of these classes are not touched by my patch. Is it appropriate for me to correct these on my patch?\\r\\n\\r\\nEDIT: Ignore this, I have fixed all the imports on failing classes on my patch to allow JDK8 CI builds to run.', \"Thanks for addressing the import issues. After that, there is another failure on {{{}eclipse-warnings{}}}:\\r\\n{code:java}\\r\\neclipse-warnings:\\r\\n    [mkdir] Created dir: /Users/adelapena/src/cassandra/trunk/build/ecj\\r\\n     [echo] Running Eclipse Code Analysis.  Output logged to /Users/adelapena/src/cassandra/trunk/build/ecj/eclipse_compiler_checks.txt\\r\\n     [java] ----------\\r\\n     [java] 1. ERROR in /Users/adelapena/src/cassandra/trunk/src/java/org/apache/cassandra/index/sai/plan/StorageAttachedIndexSearcher.java (at line 195)\\r\\n     [java] \\treturn iterator != null\\r\\n     [java]                    ? iteratePartition(iterator)\\r\\n     [java]                    : endOfData();\\r\\n     [java] \\t^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\r\\n     [java] Potential resource leak: 'iterator' may not be closed at this location\\r\\n     [java] ----------\\r\\n     [java] 1 problem (1 error)\\r\\n{code}\\r\\nDid you manage to build without problems or run CI? CI build job uses these Ant targets:\\r\\n{code:java}\\r\\nant clean realclean jar eclipse-warnings{code}\", \"Sorry, I was not aware of the exact CI build job and hadn't run the eclipse_warnings. I have now addressed above issue as well and have a CI job running that has passed the JDK8 build phase.\", '[~adelapena] Please note that I have pushed a commit to help resolve some of the dtest failures. The SecondaryIndexManager was attempting to propagate index states when the Gossiper was not enabled. This resulted in exceptions being added to the logs which were being picked up as test failures.\\xa0', \"Thanks, I'll take a look at the changes. \n",
            "my_comment: iterator may not be closed at this location\n",
            "     [java] ----------\n",
            "     [java] 1 problem (1 error)\n",
            "{code}\n",
            "Did you manage to build without problems or run CI? CI build job uses these Ant targets:\n",
            "{code:java}\n",
            "ant clean realclean jar eclipse-warnings{code}\" \"Sorry I was not aware of the exact CI build job and hadnt run the eclipse_warnings. I have now addressed above issue as well and have a CI job running that has passed the JDK8 build phase.\" [~adelapena] Please note that I have pushed a commit to help resolve some of the dtest failures. The SecondaryIndexManager was attempting to propagate index states when the Gossiper was not enabled. This resulted in exceptions being added to the logs which were being picked up as test failures.\\xa0 \"Thanks Ill take a look at the changes. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6047\n",
            "issue_type:  Bug \n",
            "summary:  Memory leak when using snapshot repairs \n",
            "description:  Running nodetool repair repeatedly with the -snapshot parameter results in a native memory leak. The JVM process will take up more and more physical memory until it is killed by the Linux OOM killer.\n",
            "comments:  ['Patch attached to let SSTableReader implement Closeable and do clean up at #close. Validation compaction against snapshot calls #close at the end of validation.', '+1', 'Committed.'] \n",
            "my_comment:  Patch attached to let SSTableReader implement Closeable and do clean up at #close. Validation compaction against snapshot calls #close at the end of validation. +1 Committed. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2858\n",
            "issue_type:  Improvement \n",
            "summary:  make request dropping more accurate \n",
            "description:  Based on the discussion in CASSANDRA-2819, we can make the bookkeeping for request times more accurate. \n",
            "comments:  ['There are a few issues here:\\n * The originally mentioned issue of not having an accurate timeout on the server side\\n  ** Add a creation time on the client side? Set the remaining time immediately before serialization?\\n * The ExpiringMap of callbacks on the client side always waits until the RPC timeout to expire messages\\n  ** The Callback could eagerly expire itself from the map when it is is satisfied \\n * A client thread calling get() on a callback has a timeout independent from the timeout for the Callback in the ExpiringMap: they can become disconnected\\n  ** One way to solve that would be to change the IAsyncCallback interface to be more like a Future, such that it is notified when it expires, and could kill anyone blocking on get() with a TimeoutException', 'Regarding to the first bullet point, why not introducing the creation time in the message header, so that whenever we need to know the remaining time we could just compare the current time with it?', \"For 3rd bullet, would it be good enough to add a 'cancel' method to IAsyncCallback so that we it got expired in ExpiringMap, cancel will be called which will unblock the get() method and possibly throwing TimeoutException?\", 'bq. The ExpiringMap of callbacks on the client side always waits until the RPC timeout to expire messages\\n\\nNot so, ResponseVerbHandler calls removeRegisteredCallback.\\n\\nbq. A client thread calling get() on a callback has a timeout independent from the timeout for the Callback in the ExpiringMap\\n\\nThat would be nice to clean up in a separate ticket.\\n\\nbq. The originally mentioned issue of not having an accurate timeout on the server side\\n\\nThis is the most important part to address.  Patch attached.', '+1 \\nSide Note (Can be ignored): Maybe just sending the Integer for timeout instead of long will save us some bandwidth :)', \"bq. Maybe just sending the Integer for timeout instead of long will save us some bandwidth\\n\\nWhat did you have in mind?  milliseconds gets too large for an int in less than a year.  Even 100ths of seconds do.  Cutting to 10ths of a second seems like we're losing too much resolution.\", '{quote}\\nWhat did you have in mind?\\n{quote}\\nI was talking about the lower part of the long where we have 2 billion milliseconds (2147483.648 seconds) to count which will give us enough to timeout... varint cannot save those 4 bytes because there is value in it. Makes sense? I should have been clear earlier.', 'v2 only sends low-order timestamp int.', \"v3 fixes ITC byte arithmetic per Vijay's suggestion.\\n\\n({{ | (input.readInt() << 4 >> 4)}} also works, but Vijay's version is simpler. \", '+1, \\nSorry for the delay spent a lot of time wondering why the test cases where failing :) (was looking at the wrong places)\\nlooks like thats because of the following setting in SP\\n\\n{code}\\nprivate static final boolean OPTIMIZE_LOCAL_REQUESTS = false;\\n{code}\\n\\nOnce we set it to true we should be good to commit i guess.\\nThanks!', \"Oops, didn't mean to include that in the patch.  (CASSANDRA-4617 is open to fix that.)\", 'committed', \"I think this is not working as expected.\\nTruncate hangs on two node cluster using 1.2.0-beta1 binary because truncate response gets dropped every time(and this is why CliTest on trunk is failing today).\\n\\nI think casting long System.currentTimeMillis to int is fragile, and that's causing this line\\n\\nhttps://github.com/apache/cassandra/blob/cassandra-1.2.0-beta1/src/java/org/apache/cassandra/net/MessageDeliveryTask.java#L43\\n\\nalways evaluates to true.\\nWhen I tried, System.currentTimeMillis was like 1348691631776, but currentTime there was like 71900832.\", 'The rebuilding logic in v3 turns out to rely on the sign bit of the int being 1, so that when it gets and-ed with the long, it gets sign-extended:\\n\\n{code}\\n.       Long foo = 0xFFFFFFFFFFFFFFFFL;\\n        int bar = 0xF0000000;\\n        System.out.println(Long.toHexString(foo & bar));\\n        System.out.println(Long.toHexString(foo & 0));\\n{code}\\n\\nThis o \n",
            "my_comment: Maybe just sending the Integer for timeout instead of long will save us some bandwidth :) \"bq. Maybe just sending the Integer for timeout instead of long will save us some bandwidth\n",
            "\n",
            "What did you have in mind?  milliseconds gets too large for an int in less than a year.  Even 100ths of seconds do.  Cutting to 10ths of a second seems like were losing too much resolution.\" {quote}\n",
            "What did you have in mind?\n",
            "{quote}\n",
            "I was talking about the lower part of the long where we have 2 billion milliseconds (2147483.648 seconds) to count which will give us enough to timeout... varint cannot save those 4 bytes because there is value in it. Makes sense? I should have been clear earlier. v2 only sends low-order timestamp int. \"v3 fixes ITC byte arithmetic per Vijays suggestion.\n",
            "\n",
            "({{ | (input.readInt() << 4 >> 4)}} also works but Vijays version is simpler. \" +1 \n",
            "Sorry for the delay spent a lot of time wondering why the test cases where failing :) (was looking at the wrong places)\n",
            "looks like thats because of the following setting in SP\n",
            "\n",
            "{code}\n",
            "private static final boolean OPTIMIZE_LOCAL_REQUESTS = false;\n",
            "{code}\n",
            "\n",
            "Once we set it to true we should be good to commit i guess.\n",
            "Thanks! \"Oops didnt mean to include that in the patch.  (CASSANDRA-4617 is open to fix that.)\" committed \"I think this is not working as expected.\n",
            "Truncate hangs on two node cluster using 1.2.0-beta1 binary because truncate response gets dropped every time(and this is why CliTest on trunk is failing today).\n",
            "\n",
            "I think casting long System.currentTimeMillis to int is fragile and thats causing this line\n",
            "\n",
            "https://github.com/apache/cassandra/blob/cassandra-1.2.0-beta1/src/java/org/apache/cassandra/net/MessageDeliveryTask.java#L43\n",
            "\n",
            "always evaluates to true.\n",
            "When I tried System.currentTimeMillis was like 1348691631776 but currentTime there was like 71900832.\" The rebuilding logic in v3 turns out to rely on the sign bit of the int being 1 so that when it gets and-ed with the long it gets sign-extended:\n",
            "\n",
            "{code}\n",
            ".       Long foo = 0xFFFFFFFFFFFFFFFFL;\n",
            "        int bar = 0xF0000000;\n",
            "        System.out.println(Long.toHexString(foo & bar));\n",
            "        System.out.println(Long.toHexString(foo & 0));\n",
            "{code}\n",
            "\n",
            "This o \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4511\n",
            "issue_type:  Improvement \n",
            "summary:  Secondary index support for CQL3 collections \n",
            "description:  We should allow to 2ndary index on collections. A typical use case would be to add a 'tag set<String>' to say a user profile and to query users based on what tag they have. \n",
            "comments:  [\"I'll note that support for lists should be fairly simple following CASSANDRA-3680. I haven't tried yet but it might well even be that it works out of the box. It would be the same for indexing based on map values.\\n\\nHowever, indexing based on set values (or on map keys, but I don't know if we want that) is more complicated, because we need to index one of the component of the composite name, so it probably require a new Searcher.\", \"Kind of think we should push this to 2.1.  There's a lot of other things I'd rather have in 2.0...\", \"Attaching patch that allows to create index on collections (with the usual syntax) and allow to query them using {{CONTAINS}}, so\\n{noformat}\\nSELECT * FROM myTable WHERE tags CONTAINS 'awesome';\\n{noformat}\\n\\nWhat is indexed is the collection values. For lists and sets there is nothing else to index, but for maps, this means we index the map values and it could make sense to index the map keys. Technically, this isn't adding much difficulty since indexing map keys is similar to indexed set values. On the query side, we could just support\\n{noformat}\\nSELECT * FROM myTable WHERE myMap CONTAINS KEY 'foo';\\n{noformat}\\nand in fact the attached patch contains 80% of what's needed to support this.  But it's not yet supported because of 2 reasons:\\n# I realized mid-implementation that if we wanted to support indexing map keys, this means we might need to support indexing both keys and values of the same map, which internally require to support multiple SecondaryIndex object on the same ColumnDefinition and we'd need a bit of refactor of the 2ndary index API to make that work properly.\\n# it's more of a detail but I was not entirely sure what the best syntax to create indexes could be. I supposed we could have something like\\n{noformat}\\nCREATE INDEX ON KEY test(myMap)\\n{noformat}\\nbut I'm not entirely fan for some reason.\\n\\nAnyway, I've left most parts of the support for this {{CONTAINS KEY}} in the patch (but the syntax is not allowed). If we decide indexing map keys is something we just don't want, then I'm fine pulling them out of the patch.  Otherwise, I'd rather left them (it doesn't add much to the review of the current patch honestly) but finish full support in a followup ticket.\\n\\nI've pushed a dtests for this.\\n\", \"I'm struggling to think of a use case for indexing map keys. /cc [~pmcfadin] [~tupshin]\\n\\nAlso, it occurs to me that we don't need to add new syntax if we did this instead:\\n\\n{code}\\nSELECT * FROM myTable WHERE 'awesome' IN tags;\\n{code}\", 'bq. I\\'m struggling to think of a use case for indexing map keys\\n\\nOne thing that comes into mind is tags, but where you want to attach some data to it. Say, when was the tag added, or maybe whom added it. In that case, you could imagine wanting to index both the keys (the tag itself, to know what object has tag X) and the values (for instance to know \\'which object did user Y tagged\\').\\n\\nAnd btw, technically there is not a whole lot of difficulty adding this, we just need to go a bit over the 2ndary index API to make sure we can add more than one index on a given name. But that API probably need some cleanup anyway.\\n\\nbq. Also, it occurs to me that we don\\'t need to add new syntax\\n\\nTrue. But I\\'ll note that 1) we don\\'t, technically speaking, support this syntax currently, we only support \\'IN ?\\' and \\'IN (...)\\', so it save adding one token to the lexer but doesn\\'t entirely save from updating the grammar and 2) internally, I still think we\\'d want to keep it a separate case from other IN because it has different rules anyway. Overall, I don\\'t mind using IN over CONTAINS if we think that\\'s a better syntax but I don\\'t think one of the arguments should be \"because it makes things easier internally\" (didn\\'t meant to imply this was your argument btw, just making sure we agree on why we would make the choice) because I don\\'t think that\\'s true.\\n\\nIn any case, as far as I\\'m concerned, I don\\'t care a whole lot between CONTAINS an \n",
            "my_comment:  [\"Ill note that support for lists should be fairly simple following CASSANDRA-3680. I havent tried yet but it might well even be that it works out of the box. It would be the same for indexing based on map values.\n",
            "\n",
            "However indexing based on set values (or on map keys but I dont know if we want that) is more complicated because we need to index one of the component of the composite name so it probably require a new Searcher.\" \"Kind of think we should push this to 2.1.  Theres a lot of other things Id rather have in 2.0...\" \"Attaching patch that allows to create index on collections (with the usual syntax) and allow to query them using {{CONTAINS}} so\n",
            "{noformat}\n",
            "SELECT * FROM myTable WHERE tags CONTAINS awesome;\n",
            "{noformat}\n",
            "\n",
            "What is indexed is the collection values. For lists and sets there is nothing else to index but for maps this means we index the map values and it could make sense to index the map keys. Technically this isnt adding much difficulty since indexing map keys is similar to indexed set values. On the query side we could just support\n",
            "{noformat}\n",
            "SELECT * FROM myTable WHERE myMap CONTAINS KEY foo;\n",
            "{noformat}\n",
            "and in fact the attached patch contains 80% of whats needed to support this.  But its not yet supported because of 2 reasons:\n",
            "# I realized mid-implementation that if we wanted to support indexing map keys this means we might need to support indexing both keys and values of the same map which internally require to support multiple SecondaryIndex object on the same ColumnDefinition and wed need a bit of refactor of the 2ndary index API to make that work properly.\n",
            "# its more of a detail but I was not entirely sure what the best syntax to create indexes could be. I supposed we could have something like\n",
            "{noformat}\n",
            "CREATE INDEX ON KEY test(myMap)\n",
            "{noformat}\n",
            "but Im not entirely fan for some reason.\n",
            "\n",
            "Anyway Ive left most parts of the support for this {{CONTAINS KEY}} in the patch (but the syntax is not allowed). If we decide indexing map keys is something we just dont want then Im fine pulling them out of the patch.  Otherwise Id rather left them (it doesnt add much to the review of the current patch honestly) but finish full support in a followup ticket.\n",
            "\n",
            "Ive pushed a dtests for this.\n",
            "\" \"Im struggling to think of a use case for indexing map keys. /cc [~pmcfadin] [~tupshin]\n",
            "\n",
            "Also it occurs to me that we dont need to add new syntax if we did this instead:\n",
            "\n",
            "{code}\n",
            "SELECT * FROM myTable WHERE awesome IN tags;\n",
            "{code}\" bq. I\\m struggling to think of a use case for indexing map keys\n",
            "\n",
            "One thing that comes into mind is tags but where you want to attach some data to it. Say when was the tag added or maybe whom added it. In that case you could imagine wanting to index both the keys (the tag itself to know what object has tag X) and the values (for instance to know \\which object did user Y tagged\\).\n",
            "\n",
            "And btw technically there is not a whole lot of difficulty adding this we just need to go a bit over the 2ndary index API to make sure we can add more than one index on a given name. But that API probably need some cleanup anyway.\n",
            "\n",
            "bq. Also it occurs to me that we don\\t need to add new syntax\n",
            "\n",
            "True. But I\\ll note that 1) we don\\t technically speaking support this syntax currently we only support \\IN ?\\ and \\IN (...)\\ so it save adding one token to the lexer but doesn\\t entirely save from updating the grammar and 2) internally I still think we\\d want to keep it a separate case from other IN because it has different rules anyway. Overall I don\\t mind using IN over CONTAINS if we think that\\s a better syntax but I don\\t think one of the arguments should be \"because it makes things easier internally\" (didn\\t meant to imply this was your argument btw just making sure we agree on why we would make the choice) because I don\\t think that\\s true.\n",
            "\n",
            "In any case as far as I\\m concerned I don\\t care a whole lot between CONTAINS an \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-15591\n",
            "issue_type:  Improvement \n",
            "summary:  When we throw Invalid partitioner class add the cause to the exception and allow to log \n",
            "description:  This is spun off from CASSANDRA-13158\n",
            "comments:  ['[Circle CI|https://circleci.com/gh/dcapwell/cassandra/tree/feature%2FimproveErrorMessageWhenPartitionIsRejected]', '[~brandon.williams] could you review?  Since you are involved in the other JIRA would be good to get you to look at this.', \"Not a full review, but quickly skimming the PR. Please add a unit test if it is reasonably possible to do so. We have some tests for {{DatabaseDescriptor}} and it would be good to add new ones as we touch existing code that doesn't have a unit test. It just needs to cover your change.\", '[~djoshi] pushed 2 tests of the failure case, one for yaml and one for properties.', 'Committed, thanks!'] \n",
            "my_comment:  [Circle CI|https://circleci.com/gh/dcapwell/cassandra/tree/feature%2FimproveErrorMessageWhenPartitionIsRejected] [~brandon.williams] could you review?  Since you are involved in the other JIRA would be good to get you to look at this. \"Not a full review but quickly skimming the PR. Please add a unit test if it is reasonably possible to do so. We have some tests for {{DatabaseDescriptor}} and it would be good to add new ones as we touch existing code that doesnt have a unit test. It just needs to cover your change.\" [~djoshi] pushed 2 tests of the failure case one for yaml and one for properties. Committed thanks! \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-13725\n",
            "issue_type:  Bug \n",
            "summary:  Secondary indexes are always rebuilt at startup \n",
            "description:  Following CASSANDRA-10130, a bug has been introduced that causes a 2i to be rebuilt at startup, even if such index is already built. \n",
            "comments:  ['GitHub user sbtourist opened a pull request:\\n\\n    https://github.com/apache/cassandra/pull/135\\n\\n    CASSANDRA-13725\\n\\n    \\n\\nYou can merge this pull request into a Git repository by running:\\n\\n    $ git pull https://github.com/sbtourist/cassandra CASSANDRA-13725\\n\\nAlternatively you can review and apply these changes as the patch at:\\n\\n    https://github.com/apache/cassandra/pull/135.patch\\n\\nTo close this pull request, make a commit to your master/trunk branch\\nwith (at least) the following in the commit message:\\n\\n    This closes #135\\n    \\n----\\ncommit 915382930a45244d439fd8407407322f6b5fa330\\nAuthor: Sergio Bossa <sergio.bossa@gmail.com>\\nDate:   2017-07-24T13:09:15Z\\n\\n    Indexes created during column family initialization should not be marked as \"not built\", to avoid rebuilding them needlessly.\\n\\n----\\n', 'This is caused by calling {{SIM#markIndexesBuilding}} when creating the index during column family initialization, which marks the index as \"not built\" and causes the index initialization task to rebuild it.\\n\\nGiven there\\'s no need to mark the index when a new column family is created (as the index will be \"not built\" by definition and there can\\'t be any concurrent indexing), we can just pass a boolean up to {{createIndex()}} to distinguish between index creation at different times, i.e. when a column family is [created|https://github.com/sbtourist/cassandra/blob/CASSANDRA-13725/src/java/org/apache/cassandra/db/Keyspace.java#L394] or [reloaded|https://github.com/sbtourist/cassandra/blob/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L129].\\n\\nSuch solution is implemented in the following patch, with a new dtest verifying it:\\n|[trunk|https://github.com/apache/cassandra/pull/135]|[dtest|https://github.com/apache/cassandra-dtest/pull/2]|\\n\\nTest runs are in progress on our internal CI and I will report results as soon as they\\'re ready.\\n', 'Both the patch and the dtest look good to me, and the CI results seem ok, +1.', 'Committed as [6e19e81db8e4c43bf5ef33308de1ae79916bb61c|https://github.com/apache/cassandra/commit/6e19e81db8e4c43bf5ef33308de1ae79916bb61c].', 'Dtests committed as [b724df80d3bbb55b6b41845633e3a9034116f3be|https://github.com/apache/cassandra-dtest/commit/b724df80d3bbb55b6b41845633e3a9034116f3be].'] \n",
            "my_comment:   2017-07-24T13:09:15Z\n",
            "\n",
            "    Indexes created during column family initialization should not be marked as \"not built\" to avoid rebuilding them needlessly.\n",
            "\n",
            "----\n",
            " This is caused by calling {{SIM#markIndexesBuilding}} when creating the index during column family initialization which marks the index as \"not built\" and causes the index initialization task to rebuild it.\n",
            "\n",
            "Given there\\s no need to mark the index when a new column family is created (as the index will be \"not built\" by definition and there can\\t be any concurrent indexing) we can just pass a boolean up to {{createIndex()}} to distinguish between index creation at different times i.e. when a column family is [created|https://github.com/sbtourist/cassandra/blob/CASSANDRA-13725/src/java/org/apache/cassandra/db/Keyspace.java#L394] or [reloaded|https://github.com/sbtourist/cassandra/blob/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L129].\n",
            "\n",
            "Such solution is implemented in the following patch with a new dtest verifying it:\n",
            "|[trunk|https://github.com/apache/cassandra/pull/135]|[dtest|https://github.com/apache/cassandra-dtest/pull/2]|\n",
            "\n",
            "e ready.\n",
            " Both the patch and the dtest look good to me and the CI results seem ok +1. Committed as [6e19e81db8e4c43bf5ef33308de1ae79916bb61c|https://github.com/apache/cassandra/commit/6e19e81db8e4c43bf5ef33308de1ae79916bb61c]. Dtests committed as [b724df80d3bbb55b6b41845633e3a9034116f3be|https://github.com/apache/cassandra-dtest/commit/b724df80d3bbb55b6b41845633e3a9034116f3be]. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1534\n",
            "issue_type:  Bug \n",
            "summary:  errors reading while bootstrapping \n",
            "description:  I loaded a 4 node cluster with 1M rows from stress.py, decommissioned a node, and then began bootstrapping it while performing constant reads against the others with stress.py.  After sleeping for 90s, the bootstrapping node started throwing many errors like this:\n",
            "comments:  [\"has bootstrap-after-decom ever worked?  i think we leave data in the system table that's going to confuse things\", \"I rm'd everything after decom.\", 'does bs against a vanilla no-decom cluster work?', 'No, same problem.', 'patch attached that makes new node properly announce its bootstrap status.', '+1', 'committed', 'Integrated in Cassandra #545 (See [https://hudson.apache.org/hudson/job/Cassandra/545/])\\n    fix setting bootstrap status on startup.\\npatch by jbellis; reviewed by brandonwilliams for CASSANDRA-1534\\n'] \n",
            "my_comment:  [\"has bootstrap-after-decom ever worked?  i think we leave data in the system table thats going to confuse things\" \"I rmd everything after decom.\" does bs against a vanilla no-decom cluster work? No same problem. patch attached that makes new node properly announce its bootstrap status. +1 committed Integrated in Cassandra #545 (See [https://hudson.apache.org/hudson/job/Cassandra/545/])\n",
            "    fix setting bootstrap status on startup.\n",
            "patch by jbellis; reviewed by brandonwilliams for CASSANDRA-1534\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-13794\n",
            "issue_type:  Bug \n",
            "summary:  Fix short read protection logic for querying more rows \n",
            "description:  Discovered by [~benedict] while reviewing CASSANDRA-13747:\n",
            "comments:  [\"Work in progress branch [here|https://github.com/iamaleksey/cassandra/tree/13794-3.0]. Currently missing (new) tests, but I want to get the underlying logic reviewed and approved, first. Would add coverage before committing it.\\n\\nA short summary of the issue: the code right now has two variables swapped, which ultimately results in us always fetching 1 extra row per short read protection requests, in a blocking manner, making it very inefficient. But upon closer look, there are some other inefficiencies here that can and should be addressed:\\n\\n1. One of our stop conditions is {{lastCount == counter.counted()}}. It's supposed to abort a short read if our previous attempt to fetch more rows yielded 0 extra rows. It's not incorrect, but is only a special case of the more general scenario: our previous attempt to fetch more extra rows yielded fewer results than we requested for. That would mean there is no more rows to fetch at that replica, and allows us to abort earlier and more frequently.\\n\\n2. Another of our stop conditions is {{!counter.isDoneForPartition()}}. Once again, it isn't incorrect, but it can be extended further. Due to the way {{isDoneForPartition()}} is defined ({{isDone() || rowInCurrentPartition >= perPartitionLimit}}) and because of that counter being counting-only, it is possible for us to have fetched enough rows total for other partitions short read retries previously to hit the global limit of rows in the counter. That would make {{isDone()}} return {{true}} always, and have {{isDoneForPartition()}} return false positives even if the partition currently processed only has a partition level deletion and/or tombstones. That can affect queries that set per partition limit explicitly or when running {{SELECT DISTINCT}} queries. Spotted that during CASSANDRA-13747 fixing.\\n\\n3. Once we've swapped {{x}} and {{n}} in {{moreContents()}} to fix the logic error, we'd still have some issues. In degenerate cases where we have some nodes missing a fresh partition deletion, for example, the formula would fetch *a lot* of rows {{n * (n - 1)}}, with {{n}} growing exponentially with every attempt.\\n\\nUpon closer inspection, the formula doesn't make 100% sense. It claims that we miss {{n - x}} rows - where {{n = counter.countedInCurrentPartition()}} and {{x = postReconciliationCounter.countedInCurrentPartition()}}, but the number we really miss is {{limit - postReconciliationCounter.counted()}} or {{perPartitionLimit - postReconciliationCounter.countedInCurrentPartition()}}. They might be the same on our first short read protection iteration, but will be diverging further and further with each request. In addition to that, it seems to assume a uniform distribution of tombstones (in the end result) rows in the source partition, which can't be true for most workloads.\\n\\nI couldn't come up with some ideal heuristic that covers all workloads, so I stuck to something safe that respects client paging limits but still attempts to minimise the # of requests we make by fetching (in most cases) more rows than is minimally necessary. I'm not completely sure about it, but I welcome any ideas on how to make it better. Either way, anything we do should be significantly more efficient than what we have now.\\n\\nI've also made some renames, refactorings, and moved a few things around to better understand the code myself, and make it clearer for future contributors - including future me. The most significant noticeable change is application of the per-response counter shift to {{mergeWithShortReadProtection()}} method, instead of overloading {{ShortReadRowProtection}} with responsibilities - I also like it to be next to the global counter creation, so you can see the contrast in arguments.\", 'Marking the ticket as {{Patch Available}}, despite its lack of (new) tests, so that it can be reviewed first. Tests will be committed with the rest of the code.', \"This patch is great (excepting a couple of extraneous edits).  Love the comments.\\n\\ \n",
            "my_comment: our previous attempt to fetch more extra rows yielded fewer results than we requested for. That would mean there is no more rows to fetch at that replica and allows us to abort earlier and more frequently.\n",
            "\n",
            "2. Another of our stop conditions is {{!counter.isDoneForPartition()}}. Once again it isnt incorrect but it can be extended further. Due to the way {{isDoneForPartition()}} is defined ({{isDone() || rowInCurrentPartition >= perPartitionLimit}}) and because of that counter being counting-only it is possible for us to have fetched enough rows total for other partitions short read retries previously to hit the global limit of rows in the counter. That would make {{isDone()}} return {{true}} always and have {{isDoneForPartition()}} return false positives even if the partition currently processed only has a partition level deletion and/or tombstones. That can affect queries that set per partition limit explicitly or when running {{SELECT DISTINCT}} queries. Spotted that during CASSANDRA-13747 fixing.\n",
            "\n",
            "3. Once weve swapped {{x}} and {{n}} in {{moreContents()}} to fix the logic error wed still have some issues. In degenerate cases where we have some nodes missing a fresh partition deletion for example the formula would fetch *a lot* of rows {{n * (n - 1)}} with {{n}} growing exponentially with every attempt.\n",
            "\n",
            "Upon closer inspection the formula doesnt make 100% sense. It claims that we miss {{n - x}} rows - where {{n = counter.countedInCurrentPartition()}} and {{x = postReconciliationCounter.countedInCurrentPartition()}} but the number we really miss is {{limit - postReconciliationCounter.counted()}} or {{perPartitionLimit - postReconciliationCounter.countedInCurrentPartition()}}. They might be the same on our first short read protection iteration but will be diverging further and further with each request. In addition to that it seems to assume a uniform distribution of tombstones (in the end result) rows in the source partition which cant be true for most workloads.\n",
            "\n",
            "I couldnt come up with some ideal heuristic that covers all workloads so I stuck to something safe that respects client paging limits but still attempts to minimise the # of requests we make by fetching (in most cases) more rows than is minimally necessary. Im not completely sure about it but I welcome any ideas on how to make it better. Either way anything we do should be significantly more efficient than what we have now.\n",
            "\n",
            "Ive also made some renames refactorings and moved a few things around to better understand the code myself and make it clearer for future contributors - including future me. The most significant noticeable change is application of the per-response counter shift to {{mergeWithShortReadProtection()}} method instead of overloading {{ShortReadRowProtection}} with responsibilities - I also like it to be next to the global counter creation so you can see the contrast in arguments.\" Marking the ticket as {{Patch Available}} despite its lack of (new) tests so that it can be reviewed first. Tests will be committed with the rest of the code. \"This patch is great (excepting a couple of extraneous edits).  Love the comments.\n",
            "\\ \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-16082\n",
            "issue_type:  Improvement \n",
            "summary:  Add a new jmxtool which can dump what JMX objects exist and diff \n",
            "description:  In order to help validate metric upgrade we first need to know what is new, what was removed, and what was changed.  To help with this, we should add a new jmxtool which can dump the objects from JMX and diff them.\n",
            "comments:  ['I took [~spmallette] groovy code and turned it into a tool we can run in CI.', \"nice! i'm glad that ended up being of some use.\", \"Yeah, worked very well for reviewing CASSANDRA-15909, so looking to produce a gold set of metrics and add tests to guard against removing of metrics or changing types (adding is ok).\\r\\n\\r\\nOne thing I did notice, so would need to think about in terms of CI, is that Stages didn't match in the test clusters, seems one had migration and the other had read... this is a problem for CI as it would cause flaky tests, so would need to figure out solution (maybe do a few read/write with schema changes to stabilize).\", 'interesting. I think that ultimately some sort of workload needs to be executed prior to doing the jmx tests to make sure that all the metrics are actually initialized. iirc there were definitely a few that were documented that i never managed to see anywhere. ', \"[~Bereng] was hoping you could take a look at the changes to ToolRunner in the PR?  ToolRunner tried to read stdout and stderr in the same thread which caused a dead lock.  stdout and stderr are normally bounded queues so when full the publisher is blocked waiting on the consumer, the tests were reading stderr (which never closed) before stdout which caused stdout to fill up and block the forked process.  To work around this, stdout and stderr are handled by their own thread.  Also, the buffer wasn't shared in a safe way so changed how tests access the stdout and stderr buffers\", '[~spmallette] if you could would love your feedback as well, I mostly took your work and ran with it.', \"[~dcapwell] nice to see {{ToolRunner}} is being used. I dropped some comments in the PR. Also CI looks like it's reporting some error you'd like to look into.\", \"Thanks for the review [~Bereng].  \\r\\n\\r\\nbq. I dropped some comments in the PR\\r\\n\\r\\nreplied to or fixed all comments.\\r\\n\\r\\nbq. Also CI looks like it's reporting some error you'd like to look into.\\r\\n\\r\\nI never run eclipse warnings locally so didn't see this, sorry for asking for a review of a build that fails!  Fixed the issue (and will try to add from now on)\", \"> would love your feedback as well,\\r\\n\\r\\nI have no additional thoughts on this one. i'll just say again that i'm stoked that you were able to convert my little script into something useful!\", 'bq. sorry for asking for a review of a build that fails\\r\\n\\r\\nNo need to apologize! np. Thanks for your comments. I am going crazy with a flaky atm but I will try to finish the review of this one later as I had focused on {{ToolRunner}} only so far.', '[~dcapwell] apologies for the late reply as I was OOO on Friday. PR lgtm assuming CI is good. I would move the ToolRunner POC out of this ticket into CASSANDRA-15591 +1', 'bq.  I would move the ToolRunner POC out of this ticket \\r\\n\\r\\nSounds good, was too lazy to create another branch =)x', '[~dcapwell] I left a couple extra comments. 1 is cosmetic-ish. The one on {{onComplete}} is the only loose end to me. Assuming that is resolved I am +1 on the PR.', '+1d this on the GitHub PR a while ago, but forgot to record it here.', 'going to start the commit, but think I need to rebase as [~maedhroz] changed 2 4.0 metrics names.', 'CI results https://app.circleci.com/pipelines/github/dcapwell/cassandra/521/workflows/bf5e7cfe-b721-4776-bbf0-4e9ffa1ab37b'] \n",
            "my_comment:  I took [~spmallette] groovy code and turned it into a tool we can run in CI. \"nice! im glad that ended up being of some use.\" \"Yeah worked very well for reviewing CASSANDRA-15909 so looking to produce a gold set of metrics and add tests to guard against removing of metrics or changing types (adding is ok).\n",
            "\n",
            "One thing I did notice so would need to think about in terms of CI is that Stages didnt match in the test clusters seems one had migration and the other had read... this is a problem for CI as it would cause flaky tests so would need to figure out solution (maybe do a few read/write with schema changes to stabilize).\" interesting. I think that ultimately some sort of workload needs to be executed prior to doing the jmx tests to make sure that all the metrics are actually initialized. iirc there were definitely a few that were documented that i never managed to see anywhere.  \"[~Bereng] was hoping you could take a look at the changes to ToolRunner in the PR?  ToolRunner tried to read stdout and stderr in the same thread which caused a dead lock.  stdout and stderr are normally bounded queues so when full the publisher is blocked waiting on the consumer the tests were reading stderr (which never closed) before stdout which caused stdout to fill up and block the forked process.  To work around this stdout and stderr are handled by their own thread.  Also the buffer wasnt shared in a safe way so changed how tests access the stdout and stderr buffers\" [~spmallette] if you could would love your feedback as well I mostly took your work and ran with it. \"[~dcapwell] nice to see {{ToolRunner}} is being used. I dropped some comments in the PR. Also CI looks like its reporting some error youd like to look into.\" \"Thanks for the review [~Bereng].  \n",
            "\n",
            "bq. I dropped some comments in the PR\n",
            "\n",
            "replied to or fixed all comments.\n",
            "\n",
            "bq. Also CI looks like its reporting some error youd like to look into.\n",
            "\n",
            "I never run eclipse warnings locally so didnt see this sorry for asking for a review of a build that fails!  Fixed the issue (and will try to add from now on)\" \"> would love your feedback as well\n",
            "\n",
            "I have no additional thoughts on this one. ill just say again that im stoked that you were able to convert my little script into something useful!\" bq. sorry for asking for a review of a build that fails\n",
            "\n",
            "No need to apologize! np. Thanks for your comments. I am going crazy with a flaky atm but I will try to finish the review of this one later as I had focused on {{ToolRunner}} only so far. [~dcapwell] apologies for the late reply as I was OOO on Friday. PR lgtm assuming CI is good. I would move the ToolRunner POC out of this ticket into CASSANDRA-15591 +1 bq.  I would move the ToolRunner POC out of this ticket \n",
            "\n",
            "Sounds good was too lazy to create another branch =)x [~dcapwell] I left a couple extra comments. 1 is cosmetic-ish. The one on {{onComplete}} is the only loose end to me. Assuming that is resolved I am +1 on the PR. +1d this on the GitHub PR a while ago but forgot to record it here. going to start the commit but think I need to rebase as [~maedhroz] changed 2 4.0 metrics names. CI results https://app.circleci.com/pipelines/github/dcapwell/cassandra/521/workflows/bf5e7cfe-b721-4776-bbf0-4e9ffa1ab37b \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-18617\n",
            "issue_type:  Improvement \n",
            "summary:  Disable the deprecated keyspace/table thresholds and convert them to Guardrails \n",
            "description:  The non-guardrail thresholds 'keyspace_count_warn_threshold' and 'table_count_warn_threshold' configuration settings were first added with CASSANDRA-16309 in 4.0-beta4 and have subsequently been deprecated since 4.1-alpha in CASSANDRA-17195 when they were replaced/migrated to guardrails as part of CEP-3 (Guardrails).\n",
            "comments:  [\"Part of this change is to add converters that will take the old threshold value, including the number of system keyspaces/tables, and convert that to a guardrail value that excludes system keyspaces/tables.\\r\\n\\r\\nTo determine the number of system keyspaces, there is an existing {{SchemaConstants.getSystemKeyspaces}} method that seems reasonable to use.\\r\\n\\r\\nHowever, I'm finding it harder to find an existing way to get the number of system tables.\\r\\n\\r\\nSome test code uses {{metadata}} method in system keyspace classes to get metadata including the tables, e.g. {{{}SystemKeyspace.metadata{}}}. However, aside from maybe doing more work than is needed, this doesn't work when called from converter code because just accessing {{SystemKeyspace}} triggers static initialization, and at some point that leads to needing some value from {{{}DatabaseDescriptor{}}}. But the initialization of {{DatabaseDescriptor}} is what triggered the loading of the config in the first place (so it ends up with an NPE trying to access an uninitialized field).\\r\\n\\r\\nSo I think something is needed similar to {{getSystemKeyspaces}} also located in {{SchemaConstants}} where static initialization problems are less likely. A downside to this approach is that it depends on being updated whenever a new system table is added, but I supposed it's no worse than the existing situation with {{{}getSystemKeyspaces{}}}.\", 'Placing all the system table names in a static constant would work, and we could have a utest verifying that that constants matches the actual schema. Although that would be quite a big constant, since there are 45 system tables to be manually put in it.\\r\\n\\r\\nMaybe another way we could do this is keeping {{table_count_warn_threshold}} in {{Config.java}} but not in {{{}cassandra.yaml{}}}, and then do the calculations in {{{}GuardrailsOptions#getTablesWarnThreshold{}}}, instead of using a converter? That method is called every time the guardrail is called, when schema initialization has already happened. So something like:\\r\\n{code:java}\\r\\n@Override\\r\\npublic int getTablesWarnThreshold()\\r\\n{\\r\\n    if (config.tables_warn_threshold < 0 && config.table_count_warn_threshold > 0)\\r\\n    {\\r\\n        int totalSystemTables = SchemaConstants.getSystemKeyspaces()\\r\\n                                               .stream()\\r\\n                                               .filter(keyspace -> !SchemaConstants.isVirtualSystemKeyspace(keyspace))\\r\\n                                               .map(Keyspace::open)\\r\\n                                               .mapToInt(keyspace -> keyspace.getColumnFamilyStores().size())\\r\\n                                               .sum();\\r\\n        return Math.min(-1, config.table_count_warn_threshold - totalSystemTables);\\r\\n    }\\r\\n    return config.tables_warn_threshold;\\r\\n}\\r\\n{code}\\r\\nwdyt?', \"I think that's a good idea to check for the deprecated threshold value in {{{}GuardrailsOptions{}}}.\\r\\n\\r\\nMy only worry would be if others might consider a bit confusing to still have {{table_count_warn_threshold}} in {{Config.java}}?\\r\\n\\r\\nI have a WIP branch here:\\r\\nhttps://github.com/djatnieks/cassandra/tree/CASSANDRA-18617\\r\\n\\xa0\", \"{quote}My only worry would be if others might consider a bit confusing to still have table_count_warn_threshold in {{{}Config.java{}}}?\\r\\n{quote}\\r\\nYep, I'm on the fence on this one. Maintaining a static list with the 45 system table names or not using the standard {{@Replaces}} tag mechanism are not ideal solutions. A middle point could be keeping a static int on {{SchemaConstants}} storing the number of system tables. We would still have to update it every time we add a table, but it would be less awkward than a long list of strings. With an explanatory comment on the property and a utest verifying that that constant matches the actual number of tables, it should be easy to detect on CI that that number has to be increased. wdyt?\", '{quote}keeping a static int on SchemaC \n",
            "my_comment:  [\"Part of this change is to add converters that will take the old threshold value including the number of system keyspaces/tables and convert that to a guardrail value that excludes system keyspaces/tables.\n",
            "\n",
            "To determine the number of system keyspaces there is an existing {{SchemaConstants.getSystemKeyspaces}} method that seems reasonable to use.\n",
            "\n",
            "However Im finding it harder to find an existing way to get the number of system tables.\n",
            "\n",
            "Some test code uses {{metadata}} method in system keyspace classes to get metadata including the tables e.g. {{{}SystemKeyspace.metadata{}}}. However aside from maybe doing more work than is needed this doesnt work when called from converter code because just accessing {{SystemKeyspace}} triggers static initialization and at some point that leads to needing some value from {{{}DatabaseDescriptor{}}}. But the initialization of {{DatabaseDescriptor}} is what triggered the loading of the config in the first place (so it ends up with an NPE trying to access an uninitialized field).\n",
            "\n",
            "So I think something is needed similar to {{getSystemKeyspaces}} also located in {{SchemaConstants}} where static initialization problems are less likely. A downside to this approach is that it depends on being updated whenever a new system table is added but I supposed its no worse than the existing situation with {{{}getSystemKeyspaces{}}}.\" Placing all the system table names in a static constant would work and we could have a utest verifying that that constants matches the actual schema. Although that would be quite a big constant since there are 45 system tables to be manually put in it.\n",
            "\n",
            "Maybe another way we could do this is keeping {{table_count_warn_threshold}} in {{Config.java}} but not in {{{}cassandra.yaml{}}} and then do the calculations in {{{}GuardrailsOptions#getTablesWarnThreshold{}}} instead of using a converter? That method is called every time the guardrail is called when schema initialization has already happened. So something like:\n",
            "{code:java}\n",
            "@Override\n",
            "public int getTablesWarnThreshold()\n",
            "{\n",
            "    if (config.tables_warn_threshold < 0 && config.table_count_warn_threshold > 0)\n",
            "    {\n",
            "        int totalSystemTables = SchemaConstants.getSystemKeyspaces()\n",
            "                                               .stream()\n",
            "                                               .filter(keyspace -> !SchemaConstants.isVirtualSystemKeyspace(keyspace))\n",
            "                                               .map(Keyspace::open)\n",
            "                                               .mapToInt(keyspace -> keyspace.getColumnFamilyStores().size())\n",
            "                                               .sum();\n",
            "        return Math.min(-1 config.table_count_warn_threshold - totalSystemTables);\n",
            "    }\n",
            "    return config.tables_warn_threshold;\n",
            "}\n",
            "{code}\n",
            "wdyt? \"I think thats a good idea to check for the deprecated threshold value in {{{}GuardrailsOptions{}}}.\n",
            "\n",
            "My only worry would be if others might consider a bit confusing to still have {{table_count_warn_threshold}} in {{Config.java}}?\n",
            "\n",
            "I have a WIP branch here:\n",
            "https://github.com/djatnieks/cassandra/tree/CASSANDRA-18617\n",
            "\\xa0\" \"{quote}My only worry would be if others might consider a bit confusing to still have table_count_warn_threshold in {{{}Config.java{}}}?\n",
            "{quote}\n",
            "Yep Im on the fence on this one. Maintaining a static list with the 45 system table names or not using the standard {{@Replaces}} tag mechanism are not ideal solutions. A middle point could be keeping a static int on {{SchemaConstants}} storing the number of system tables. We would still have to update it every time we add a table but it would be less awkward than a long list of strings. With an explanatory comment on the property and a utest verifying that that constant matches the actual number of tables it should be easy to detect on CI that that number has to be increased. wdyt?\" {quote}keeping a static int on SchemaC \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6217\n",
            "issue_type:  Bug \n",
            "summary:  replace doesn't clean up system.peers if you have a new IP \n",
            "description:  When you use replace_token (or replace_node or replace_address) if the new node has a different IP, the old node will still be in system.peers \n",
            "comments:  ['Existing nodes will already have the correct peers state due to the token conflict(s) from the replacer, but the replacer will still have the dead node in its own peers table.  The simplest thing to do is finish replacing by removing the replace_address from the table, since either it will be our own (which should not appear there) or it will be the old node.  Trivial patch to do so.', '+1 LGTM', 'Committed.'] \n",
            "my_comment:  Existing nodes will already have the correct peers state due to the token conflict(s) from the replacer but the replacer will still have the dead node in its own peers table.  The simplest thing to do is finish replacing by removing the replace_address from the table since either it will be our own (which should not appear there) or it will be the old node.  Trivial patch to do so. +1 LGTM Committed. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-18029\n",
            "issue_type:  Bug \n",
            "summary:  fix starting Paxos auto repair \n",
            "description:  This test was not run in CI because of its name (not ending on Test) so it went undetected.\n",
            "comments:  ['I fixed here what I could: [https://github.com/apache/cassandra/pull/1994]\\r\\n\\r\\nOne test is still failing, not sure what to do with it.\\r\\n\\r\\ncc: [~benedict]\\xa0', 'I fixed all tests.\\r\\n\\r\\nThe most surprising fix consists of adding \"PaxosState.startAutoRepairs()\" to CassandraDaemon (and Instance), that oneliner just fixes \"paxosAutoRepair\\' test.\\r\\n\\r\\nPaxosState.startAutoRepairs() is not called anywhere in the codebase. How are we actually starting autorepairs then?\\r\\n\\r\\nI run them 500x\\xa0 and one test method is flaky:\\r\\n\\r\\nhttps://app.circleci.com/pipelines/github/instaclustr/cassandra/1561/workflows/62aecce3-7f29-4550-ab77-2c79922d80f4/jobs/7161/tests#failed-test-0', 'https://github.com/apache/cassandra/pull/1994', 'I added fixed version as 4.1-rc. It seems to me we are not starting auto repairs anywhere if I am not missing something here.', 'Why are you signalling this for review with a broken test?', '[~benedict]\\xa0, this was the one we discussed in the Slack channel last week. Do you mind to review it, please?\\xa0\\r\\n\\r\\nFor reference - https://the-asf.slack.com/archives/CK23JSY2K/p1667504971163019?thread_ts=1667504727.263969&cid=CK23JSY2K', \"{quote}Why are you signalling this for review with a broken test?\\r\\n{quote}\\r\\nDidn't he actually fix it in the last commit? I might have misunderstood\", 'You know what, lets just move it to open :) I dont feel like I should fix a flaky test of a feature I am not familiar with at all.', \"The changes look fine, and I don't see why it shouldn't be merged since it fixes things, even if it exposes a failing test?\", 'Thanks [~benedict] , I suggest this ticket should be renamed to \"fix starting Paxos auto repair\" where we ship renamed tests supporting that feature, even one of them happens to be flaky. Not sure I have spare cycles to get into internals of this to come up with a fix.', \"Good suggestion, the title was indeed misleading, I've updated it.\", 'fyi this is j8 pre-commit [https://app.circleci.com/pipelines/github/instaclustr/cassandra/1561/workflows/c8fae944-d876-4c2a-a8fd-05bb0b8a693b]\\r\\n\\r\\nso there are two flakies\\r\\n\\r\\n1)\\xa0[paxosRepairHistoryIsntUpdatedInForcedRepair|https://app.circleci.com/pipelines/github/instaclustr/cassandra/1561/workflows/c8fae944-d876-4c2a-a8fd-05bb0b8a693b/jobs/7173/tests#failed-test-0]\\r\\n2) [legacyPurgeRepairLoop|https://app.circleci.com/pipelines/github/instaclustr/cassandra/1561/workflows/c8fae944-d876-4c2a-a8fd-05bb0b8a693b/jobs/7173/tests#failed-test-1]\\r\\n\\xa0\\r\\n\\r\\nPlease let me know if you want me to proceed to merge this and I ll be back on track. We will create tickets for these flakes. I am not familiar with Paxos and I am not able to evaluate if we can just accept that or it has some more serious consequences. Especially 2) is quite interesting.', '[~benedict] could you please explicitly confirm here that adding starting of Paxos repairs in CassandraDemon here (1) is really the change which should be included in 4.1 and trunk? Just to be super clear here. I am wondering how was that test passing before when repairs were not starting. \\r\\n\\r\\n(1) https://github.com/apache/cassandra/blob/c323949c55d02d952121d5611975e075a049ec32/src/java/org/apache/cassandra/service/CassandraDaemon.java#L494', 'Yes, that should be started, however it is not necessary for correctness. It simply keeps the backlog of uncommitted command state to a minimum.\\r\\n\\r\\nUsers switching to {{paxos_stage_purging: repaired}} rely on running regular/incremental/paxos repair for clearing {{system.paxos}} which is the more important job, this auto repair just means that the coordinated paxos repairs should finish a bit quicker.\\r\\n', \"Given the explanation above I do not think this ticket should be an rc blocker, given we have no progress on how to fix the two flaky tests. (We should still try and get it in, there's only one or two other tickets we are currently waiting on…)\", \"This ticket isn't about fixing the tests, just starting paxos auto repair, wh \n",
            "my_comment: repaired}} rely on running regular/incremental/paxos repair for clearing {{system.paxos}} which is the more important job this auto repair just means that the coordinated paxos repairs should finish a bit quicker.\n",
            " \"Given the explanation above I do not think this ticket should be an rc blocker given we have no progress on how to fix the two flaky tests. (We should still try and get it in theres only one or two other tickets we are currently waiting on…)\" \"This ticket isnt about fixing the tests just starting paxos auto repair wh \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8004\n",
            "issue_type:  Bug \n",
            "summary:  Run LCS for both repaired and unrepaired data \n",
            "description:  If a user has leveled compaction configured, we should run that for both the unrepaired and the repaired data. I think this would make things a lot easier for end users\n",
            "comments:  ['+1 this idea', 'Good idea. So if Level 2 stable is repaired, you will find all overlapping stables in L2 repaired and compact them to satisfy the non overlapping constrain? ', '[~kohlisankalp] not initially, what I want to avoid here is the really painful migration to using incremental backups. For the first incremental run, the repaired leveling will be totally empty and we can move sstables over directly. The future incremental repairs will do a \"can I move this sstable from unrepaired level X to repaired level X?\", and the answer will most often be \"nope\", then we drop it to level 0.\\n\\nIt might be a good idea to consider doing in the future (3.0) though. I\\'m targeting 2.1 here and want to keep it as simple as possible.', 'pushed a branch here for this: https://github.com/krummas/cassandra/commit/476b27dc503c3541ee31dacdd70191fee8a819a5\\n\\n* Introduces a \"WrappingCompactionStrategy\" that contains the logic for handling repaired/unrepaired sstables.\\n** Could be a bit confusing and should probably be refactored for 3.0 - it would be nicer with a \"CompactionStrategyManager\" or similar that does not extend AbstractCompactionStrategy, but we currently call cfs.getCompactionStrategy() in many places so having the WCS makes it transparent to any users.\\n* As mentioned in the description this makes it possible, for the first run, to move sstables from the leveling in unrepaired straight over to the repaired-leveling. After the first run, we try to move sstables over, if it fails, they are sent to L0.\\n* keeps 2 instances of the same compaction strategy, changing the compaction strategy is now handled by WrappingCompactionStrategy.\\n* The compaction strategies now track which sstables they can run compaction on (LCS always did this, now STCS does it as well). So the compaction strategy will only ever see either repaired or unrepaired sstables.\\n* As mentioned in CASSANDRA-5351 (and the original reason we did STCS on the unrepaired data) the write amplification gets a lot higher when having 2 parallel levelings, so maybe we should have an option to configure the different compaction strategies separately - you could configure STCS for the unrepaired and LCS for the repaired if the write amplification gets too high for the use case.\\n* An added benefit of running LCS for the unrepaired data is that it makes each sstable contain a smaller range - making it more likely that the sstable is fully contained within the repaired range and the anticompaction step can simply update the repairedAt timestamp and not have to rewrite the entire sstable to split out the repaired ranges.\\n* Also handles the case where someone runs incremental repair once, and then forgets about it, then all the data would be size tiered in the current implementation, with this there will be a small/old repaired leveling and a big unrepaired leveling.\\n\\nThoughts, comments?', \"Haven't looked at the patch yet, but the idea sound very good/reasonable to me.\", 'WrappingCompactionStrategy.startup() again adds stables when they are already done in maybeReloadCompactionStrategy. Also we call startup on both strategies twice.  ', 'pushed updated patch here: https://github.com/krummas/cassandra/commits/marcuse/8004\\n\\npretty big/scary change to go into a minor release, but it would make migrating to incremental repairs so much easier and safer so imo it is worth it.', '\"it would make migrating to incremental repairs so much easier\"\\n+1. Incremental repair is what I like the most in 2.1 and this is very important for it. \\nLet me review the new patch. ', 'LGTM +1', '(I believe [~iamaleksey] wanted to have a look as well before commit.)', \"Still looking at it. Other than a failing CrcCheckChanceTest, things mostly look good - or rather as good as we can make them in 2.1, without major internal changes (hopefully we'll be able to redesign the APIs surrounding compaction in 3.x).\\n\\nNeed a bit more time to make sure it doesn't break anything else though (incl. 3rd party comp \n",
            "my_comment: https://github.com/krummas/cassandra/commits/marcuse/8004\n",
            "\n",
            "pretty big/scary change to go into a minor release but it would make migrating to incremental repairs so much easier and safer so imo it is worth it. \"it would make migrating to incremental repairs so much easier\"\n",
            "+1. Incremental repair is what I like the most in 2.1 and this is very important for it. \n",
            "Let me review the new patch.  LGTM +1 (I believe [~iamaleksey] wanted to have a look as well before commit.) \"Still looking at it. Other than a failing CrcCheckChanceTest things mostly look good - or rather as good as we can make them in 2.1 without major internal changes (hopefully well be able to redesign the APIs surrounding compaction in 3.x).\n",
            "\n",
            "Need a bit more time to make sure it doesnt break anything else though (incl. 3rd party comp \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2941\n",
            "issue_type:  Improvement \n",
            "summary:  Expose number of rpc timeouts for individual hosts metric via jmx \n",
            "description:  We have a total number timeouts for each node. It's better for monitoring to break down this total number into number of timeouts per host that this node tried to connect to. \n",
            "comments:  ['expose the number of timeouts per host.\\nexpose the delta of this metric.\\n', \"- does not apply to 0.8 for me\\n- i don't see anything to prevent dropping timeouts b/c of race in timeoutreporter.apply.  (using NBHM + replace would fix this)\\n- if you did the recentTimeouts create first, then the timeouts put, you wouldn't have to special case recent == null later\", \"patch are old, need to rebase. i'll do it.\\n\\ntimeoutreporter.apply is only called in one thread, right? In expireMap, a timerTask will be created to monitor the cache, yes/no ?\\n\\nIf the previous is true, the reason for me to do it this way is that I only do 'write' operation to the hashmap in one thread so that we will not corrupt the data structure. Although get**** is called from multi threaded, only 'read' operations of hashmap is performed so we don't need 'lock' here. I think this is the reason I try not to create an atomicLong and insert into hashmap.\", 'rebased my patch.', \"bq. timeoutreporter.apply is only called in one thread, right?\\n\\nyou're right, that should be fine.\\n\\nbq. rebased my patch.\\n\\nWhat did you rebase against?  There have been no commits in the meantime but it does not apply to 0.8 head.\", 'Sorry for the confusion. This patch worked against the current trunk.', \"bq. Although get**** is called from multi threaded, only 'read' operations of hashmap is performed so we don't need 'lock' here.\\n\\nActually, we still need to establish a happens-before for the read, or we have no guarantees that the JMX thread will ever see the updates made by the timeout reporter.  So we could either use a Map of AtomicLong or a ConcurrentMap of Long.\", 'I am not clear why we have no guarantees that the JMX thread will ever see the updates made by the timeout reporter. The current structure is that, if there is a timeout happened, apply() will be called for it and if this is the first time for a certain IP address, an atomicLong will be created for it. Since this is the first time for this IP address to time out, it is natural not to see its updates before. From then on, it will get updated whenever JMX threads call getRecent***(). Maybe I miss something here?', \"I'm talking about timeoutsPerHost, not recentTimeoutsPerHost.  since tPH is a plain HashMap there is no happens-before relationship between the updates and the reads.\", 'change timeoutsPerHost to use AtomicLong.\\nre-structure the code of getRecentTimeoutPerHost()', 'committed, thanks!', 'Integrated in Cassandra-0.8 #289 (See [https://builds.apache.org/job/Cassandra-0.8/289/])\\n    expose rpc timeouts per host in MessagingServiceMBean\\npatch by Melvin Wang; reviewed by jbellis for CASSANDRA-2941\\n\\njbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1160449\\nFiles : \\n* /cassandra/branches/cassandra-0.8/CHANGES.txt\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingServiceMBean.java\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java\\n', \"for the records, this patch had the following lines:\\n{noformat}\\n        AtomicLong c = timeoutsPerHost.get(ip);\\n        if (c == null)\\n            c = timeoutsPerHost.put(ip, new AtomicLong());\\n        c.incrementAndGet();\\n{noformat}\\nwhich are a guaranteed NPE.\\nI've fixed that directly though (in r1164068).\", \"Ah, my bad. Thinking of python's dictionary while I did this :) Sorry.\", 'Integrated in Cassandra-0.8 #310 (See [https://builds.apache.org/job/Cassandra-0.8/310/])\\n    Fix typo introduced by CASSANDRA-2941\\n\\nslebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1164068\\nFiles : \\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java\\n'] \n",
            "my_comment: \n",
            "* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2963\n",
            "issue_type:  New Feature \n",
            "summary:  Add a convenient way to reset a node's schema \n",
            "description:  People often encounter a schema disagreement where just one node is out of sync.  To get it back in sync, they shutdown the node, move the Schema* and Migration* files out of the system ks, and then start it back up.  Rather than go through this process, it would be nice if you could just tell the node to reset its schema. \n",
            "comments:  ['One possibility: truncate the schema and migration CFs.  But just a local truncate (ColumnFamilyStore.truncate) rather than pushing it out to the cluster.\\n\\nSchema management is done in SystemTable.java.', \"First try at an apache cassandra patch, the possible problems are the way the futures are handled in SystemTable.java.\\nI also don't know how to write proper unit test for that type of functionnality.\", \"Thanks for the patch!  I don't think this should be a thrift-level call though, since most clients don't care which machine they're connected to, and many client libs abstract this away.  A JMX call with a nodetool command would be more appropriate.\", \"You mean I installed thrift for nothing! :-)\\nAlright, i'll keep the SystemTable code there and push the call through JMX.\", 'Pierre-Yves, are you still planning a v2 here?', \"Patch (against trunk) is attached to add {{nodetool resetlocalschema}}. Truncate part is basically the same as Pierre-Yves's patch.\", 'What the patch does only addresses the part of the problem because even after schema and migrations gets truncated we still have old data and version in Schema. I suggest to reset schema version to INITIAL_VESION and announce that to the ring right after truncate is done which will request migration push to the node (the behavior of announce will be changed after CASSANDRA-1391). Also note that while node is in the process of such migration no reads or writes could be served.', 'Now when CASSANDRA-1391 is committed, to reset the schema you will just need to truncate schema_{keyspaces, columnfamilies, columns} and re-set Schema.instance to initial (blank) state.', \"In this patch, I add the way to reset Schema after truncating schema_* CF, and then send migration request to other live node.\\nI'm not sure if I do the right way for last part (migration request), so please let me know if I did wrong.\\n\", \"Almost there, last things are:\\n\\n - we don't want to remove data like Schema.init() does just a metadata about keyspaces/ColumnFamilies;\\n - the better name for Schema.init() would be Schema.clear();\\n - MigrationManager.resetLocalSchema should propagate IOException\", \"v2 attached.\\n\\nbq. we don't want to remove data like Schema.init() does just a metadata about keyspaces/ColumnFamilies;\\n\\nIn v2, Schema.clear only removes metadata. That causes me to modify Table.initCf to reload metadata. I think the way I did is fine, but please check.\\n\\nbq. the better name for Schema.init() would be Schema.clear();\\n\\nDone.\\n\\nbq. MigrationManager.resetLocalSchema should propagate IOException\\n\\nDone.\", 'I have made some styling improvements and changed MigrationManager.resetLocalSchema() to reset local schema even if there no nodes around (because when the new nodes will come up it will request schema from them anyway) + I have made sure that if there any nodes schema is requested from the first node with version >= 1.1.', 'lgtm.', 'Committed', \"Isn't ninja pushing this to 1.1.0 being a bit loose with the definition of a code freeze? I'm not asking to revert but at least some form of justification for why something tagged 'New Feature' ends up in 1.1.0 way after the freeze would have been nice.\", \"I'm okay with it since Pierre and Yuki started work well before the freeze, it addresses a *very* common pain point for users, and there's minimal interplay with existing code so risk of causing regressions elsewhere is low.\", \"Yeah but my point is that it clearly don't respect the freeze (it's a 'New Feature' ticket), so at least a comment with that kind of justification *before* committing would be nice, otherwise the freeze will become the far west. Communication and transparency are important.\", \"I'm a little confused at the new logic in initCf (see CASSANDRA-4402).  Is the idea that we clear out the schema so it can be added back in by gossip, but we don't want to clear out Table.columnFamilyStores?  Why not?\", \"Looking back I think that this was does because due to the nature of clea \n",
            "my_comment: truncate the schema and migration CFs.  But just a local truncate (ColumnFamilyStore.truncate) rather than pushing it out to the cluster.\n",
            "\n",
            "Schema management is done in SystemTable.java. \"First try at an apache cassandra patch the possible problems are the way the futures are handled in SystemTable.java.\n",
            "I also dont know how to write proper unit test for that type of functionnality.\" \"Thanks for the patch!  I dont think this should be a thrift-level call though since most clients dont care which machine theyre connected to and many client libs abstract this away.  A JMX call with a nodetool command would be more appropriate.\" \"You mean I installed thrift for nothing! :-)\n",
            "Alright ill keep the SystemTable code there and push the call through JMX.\" Pierre-Yves are you still planning a v2 here? \"Patch (against trunk) is attached to add {{nodetool resetlocalschema}}. Truncate part is basically the same as Pierre-Yvess patch.\" What the patch does only addresses the part of the problem because even after schema and migrations gets truncated we still have old data and version in Schema. I suggest to reset schema version to INITIAL_VESION and announce that to the ring right after truncate is done which will request migration push to the node (the behavior of announce will be changed after CASSANDRA-1391). Also note that while node is in the process of such migration no reads or writes could be served. Now when CASSANDRA-1391 is committed to reset the schema you will just need to truncate schema_{keyspaces columnfamilies columns} and re-set Schema.instance to initial (blank) state. \"In this patch I add the way to reset Schema after truncating schema_* CF and then send migration request to other live node.\n",
            "Im not sure if I do the right way for last part (migration request) so please let me know if I did wrong.\n",
            "\" \"Almost there last things are:\n",
            "\n",
            " - we dont want to remove data like Schema.init() does just a metadata about keyspaces/ColumnFamilies;\n",
            " - the better name for Schema.init() would be Schema.clear();\n",
            " - MigrationManager.resetLocalSchema should propagate IOException\" \"v2 attached.\n",
            "\n",
            "bq. we dont want to remove data like Schema.init() does just a metadata about keyspaces/ColumnFamilies;\n",
            "\n",
            "In v2 Schema.clear only removes metadata. That causes me to modify Table.initCf to reload metadata. I think the way I did is fine but please check.\n",
            "\n",
            "bq. the better name for Schema.init() would be Schema.clear();\n",
            "\n",
            "Done.\n",
            "\n",
            "bq. MigrationManager.resetLocalSchema should propagate IOException\n",
            "\n",
            "Done.\" I have made some styling improvements and changed MigrationManager.resetLocalSchema() to reset local schema even if there no nodes around (because when the new nodes will come up it will request schema from them anyway) + I have made sure that if there any nodes schema is requested from the first node with version >= 1.1. lgtm. Committed \"Isnt ninja pushing this to 1.1.0 being a bit loose with the definition of a code freeze? Im not asking to revert but at least some form of justification for why something tagged New Feature ends up in 1.1.0 way after the freeze would have been nice.\" \"Im okay with it since Pierre and Yuki started work well before the freeze it addresses a *very* common pain point for users and theres minimal interplay with existing code so risk of causing regressions elsewhere is low.\" \"Yeah but my point is that it clearly dont respect the freeze (its a New Feature ticket) so at least a comment with that kind of justification *before* committing would be nice otherwise the freeze will become the far west. Communication and transparency are important.\" \"Im a little confused at the new logic in initCf (see CASSANDRA-4402).  Is the idea that we clear out the schema so it can be added back in by gossip but we dont want to clear out Table.columnFamilyStores?  Why not?\" \"Looking back I think that this was does because due to the nature of clea \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4311\n",
            "issue_type:  Bug \n",
            "summary:  clean up messagingservice protocol limitations \n",
            "description:  Weaknesses of the existing protocol:\n",
            "comments:  ['I think we can address all of these by making these changes to VERSION_12 protocol:\\n\\n- send version and connection metadata just once per connection.  This will give the other side enough information to know whether to expect a compressed or varint stream\\n- *both* sides exchange CURRENT_VERSION when connection is established; each side will then have version information immediately upon contacting another node\\n\\nAs a consequence,\\n- When version changes, we need to drop existing connections and reconnect (already implied but not yet implemented by CASSANDRA-3127)', \"I think the changes to ITC demonstrate what I have in mind pretty clearly (attached).  I'll flesh the rest of this out shortly.\", 'This looks like a good solution so far.\\n\\nbq. When version changes, we need to drop existing connections and reconnect\\n\\nLet me take this opportunity to suggest that set/getVersion be moved out of Gossiper (which does nothing but expose a Map) and into MS.', 'Pushed to https://github.com/jbellis/cassandra/branches/4311-2', \"looks good to me, just a bit confusing calling it a 'header' when it is essentially a handshake packet\\n\\ncould we negotiate SSL the way we do compression? nothing 'secret' is shared during the handshake phase anyway, after that we could upgrade sockets\", 'MS.setVersion needs a minor fix to prevent NPE:\\n\\n{noformat}\\n     public Integer setVersion(InetAddress address, int version)\\n     {\\n         logger.debug(\"Setting version {} for {}\", version, address);\\n         Integer v = versions.put(address, version);\\n         return v == null ? version : v;\\n     }\\n{noformat}\\n\\n+1 otherwise.', 'bq. MS.setVersion needs a minor fix to prevent NPE\\n\\nDone and committed.\\n\\nbq. could we negotiate SSL the way we do compression\\n\\nSure, can you open a separate ticket for that?'] \n",
            "my_comment: v;\n",
            "     }\n",
            "{noformat}\n",
            "\n",
            "+1 otherwise. bq. MS.setVersion needs a minor fix to prevent NPE\n",
            "\n",
            "Done and committed.\n",
            "\n",
            "bq. could we negotiate SSL the way we do compression\n",
            "\n",
            "Sure can you open a separate ticket for that? \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-17197\n",
            "issue_type:  New Feature \n",
            "summary:  Diagnostic events for guardrails \n",
            "description:  Add diagnostic events for guardrails, so we can monitor when each type of guardrail is triggered. \n",
            "comments:  ['Here is the patch adding diagnostic events for guardrails:\\r\\n||PR||CI||\\r\\n|[trunk|https://github.com/apache/cassandra/pull/1485]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1340/workflows/58cc1a02-7a5a-4d60-869f-c698b56d66df] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1340/workflows/2ea9c9a1-288f-4c64-9839-611e640da42b]|\\r\\n\\r\\nIt creates a new type of diagnostic event called [{{GuardrailEvent}}|https://github.com/adelapena/cassandra/blob/17197-trunk/src/java/org/apache/cassandra/db/guardrails/GuardrailEvent.java]. There are two types on guardrail event, which are {{WARNED}} and {{{}FAILED{}}}, matching the soft and hard activation of the guardrail. These events carry two properties, which are the name of the specific guardrail and the specific error message.\\r\\n\\r\\nAs for testing, there is a consumer/listener for the new type of diagnostic events attached to every {{{}GuardrailTester{}}}, so every time that we check the activation of a guardrail we also check that the proper events have also been emitted.', \"An important detail is that the messages emitted when a guardrail is triggered might contain user data. None of the current guardrails does this, but the proposed guardrails for collection items and size (CASSANDRA-17153) will include the primary key of the offending row, and it's likely that other incoming guardrails will do the same.\\r\\n\\r\\nThis user data shouldn't be included into diagnostic events, so it isn't sent to external systems monitoring diagnostic events. I have added [a commit|https://github.com/apache/cassandra/pull/1485/commits/97329b94fb3de770aaae64880cef2b9cff857a00] to the PR that allows to redact sensitive data in the messages that are included in guardrail diagnostic events. The approach for redacting the messages is based on a previous patch by [~Gerrrr].\\r\\n\\r\\nI'd be fine doing this in a separate ticket, given that we don't yet have any guardrail publishing user data. However, since this is a security thing, I think I'd prefer to include this with diagnostic events, so we don't miss it when adding other guardrails.\\r\\n\\r\\nHere is CI for the updated patch:\\r\\n||PR||CI||\\r\\n|[trunk|https://github.com/apache/cassandra/pull/1485]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1345/workflows/1638df69-2729-4222-872f-4f3e081bff1b] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1345/workflows/ef04d0d2-04a4-4675-83db-7fb0ffe21ea4]|\", 'CC [~smiklosovic] [~dcapwell]\\xa0', 'CASSANDRA-17430 has just added identifying names to guardrails, identically to what was proposed in this patch. That simplifies the patch a bit after rebasing on top of those changes:\\xa0\\r\\n||PR||CI||\\r\\n|[trunk|https://github.com/apache/cassandra/pull/1485]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1360/workflows/7aeb9214-e7fd-40d0-b551-5740c577004e] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1360/workflows/657078e3-a6c2-4305-95c7-66c472fe3bc1]|', 'I am on this 100% now.', '[~adelapena] could you please rebase this on top of the current trunk? I am getting conflicts when I am trying to rebase it on my own.', \"Great news! I think I rebased very recently and the PR doesn't show conflicts, but I have rebased it again just in case.\\r\\n||PR||CI||\\r\\n|[trunk|https://github.com/apache/cassandra/pull/1485]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1368/workflows/88ab802f-ccea-4176-942b-914551df13bd] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1368/workflows/039fc777-3663-488a-b635-750ecccc0552]|\", \"Ok so I dropped just a suggestion to add some more test lines but otherwise LGTM. I did check CI and also LGTM but the repeated runs have 2 failing workers. Test results were not uploaded and looking at some of the failed logs the tests I looked at passed. So it's probably some env issue causing the red. This would need to be confirmed or a new repeatable run be triggered \n",
            "my_comment:  Here is the patch adding diagnostic events for guardrails:\n",
            "||PR||CI||\n",
            "|[trunk|https://github.com/apache/cassandra/pull/1485]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1340/workflows/58cc1a02-7a5a-4d60-869f-c698b56d66df] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1340/workflows/2ea9c9a1-288f-4c64-9839-611e640da42b]|\n",
            "\n",
            "It creates a new type of diagnostic event called [{{GuardrailEvent}}|https://github.com/adelapena/cassandra/blob/17197-trunk/src/java/org/apache/cassandra/db/guardrails/GuardrailEvent.java]. There are two types on guardrail event which are {{WARNED}} and {{{}FAILED{}}} matching the soft and hard activation of the guardrail. These events carry two properties which are the name of the specific guardrail and the specific error message.\n",
            "\n",
            "As for testing there is a consumer/listener for the new type of diagnostic events attached to every {{{}GuardrailTester{}}} so every time that we check the activation of a guardrail we also check that the proper events have also been emitted. \"An important detail is that the messages emitted when a guardrail is triggered might contain user data. None of the current guardrails does this but the proposed guardrails for collection items and size (CASSANDRA-17153) will include the primary key of the offending row and its likely that other incoming guardrails will do the same.\n",
            "\n",
            "This user data shouldnt be included into diagnostic events so it isnt sent to external systems monitoring diagnostic events. I have added [a commit|https://github.com/apache/cassandra/pull/1485/commits/97329b94fb3de770aaae64880cef2b9cff857a00] to the PR that allows to redact sensitive data in the messages that are included in guardrail diagnostic events. The approach for redacting the messages is based on a previous patch by [~Gerrrr].\n",
            "\n",
            "Id be fine doing this in a separate ticket given that we dont yet have any guardrail publishing user data. However since this is a security thing I think Id prefer to include this with diagnostic events so we dont miss it when adding other guardrails.\n",
            "\n",
            "Here is CI for the updated patch:\n",
            "||PR||CI||\n",
            "|[trunk|https://github.com/apache/cassandra/pull/1485]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1345/workflows/1638df69-2729-4222-872f-4f3e081bff1b] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1345/workflows/ef04d0d2-04a4-4675-83db-7fb0ffe21ea4]|\" CC [~smiklosovic] [~dcapwell]\\xa0 CASSANDRA-17430 has just added identifying names to guardrails identically to what was proposed in this patch. That simplifies the patch a bit after rebasing on top of those changes:\\xa0\n",
            "||PR||CI||\n",
            "|[trunk|https://github.com/apache/cassandra/pull/1485]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1360/workflows/7aeb9214-e7fd-40d0-b551-5740c577004e] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1360/workflows/657078e3-a6c2-4305-95c7-66c472fe3bc1]| I am on this 100% now. [~adelapena] could you please rebase this on top of the current trunk? I am getting conflicts when I am trying to rebase it on my own. \"Great news! I think I rebased very recently and the PR doesnt show conflicts but I have rebased it again just in case.\n",
            "||PR||CI||\n",
            "|[trunk|https://github.com/apache/cassandra/pull/1485]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1368/workflows/88ab802f-ccea-4176-942b-914551df13bd] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1368/workflows/039fc777-3663-488a-b635-750ecccc0552]|\" \"Ok so I dropped just a suggestion to add some more test lines but otherwise LGTM. I did check CI and also LGTM but the repeated runs have 2 failing workers. Test results were not uploaded and looking at some of the failed logs the tests I looked at passed. So its probably some env issue causing the red. This would need to be confirmed or a new repeatable run be triggered \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-10209\n",
            "issue_type:  Bug \n",
            "summary:  Missing role manager in cassandra.yaml causes unexpected behaviour \n",
            "description:  On upgrading to 2.2+, if the new {{role_manager}} option is not added to {{cassandra.yaml}}, an instance of the default {{CassandraRoleManager}} is created during initialization of {{DatabaseDescriptor}}. This is a problem as the set of role options supported by {{CRM}} depends on the configured {{IAuthenticator}}, which at that point in time is always {{AllowAllAuthenticator}}.\n",
            "comments:  ['Patches:\\n* [2.2 branch|https://github.com/beobal/cassandra/tree/10209-2.2]\\n* [3.0 branch|https://github.com/beobal/cassandra/tree/10209-3.0]\\n* [trunk branch|https://github.com/beobal/cassandra/tree/10209-trunk]\\n\\nCI Tests:\\n* [2.2 testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10209-2.2-testall/]\\n* [2.2 dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10209-2.2-dtest/]\\n* [3.0 testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10209-3.0-testall/]\\n* [3.0 dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10209-3.0-dtest/]\\n* [trunk testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10209-trunk-testall/]\\n* [trunk dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10209-trunk-dtest/]', \"+1\\n\\nnit: I'd reverse the order of the comparison to avoid a double negative (!= .. else)\", 'committed with that tweak (since Sam is out this week)', 'This patch causes an NPE in CQLSSTableWriterClientTest. CI showed this commit as the regression, and I can repro - previous commit passes this test OK.\\n\\n{noformat}\\n    [junit] Testsuite: org.apache.cassandra.io.sstable.CQLSSTableWriterClientTest\\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.63 sec\\n    [junit] \\n    [junit] Testcase: testWriterInClientMode(org.apache.cassandra.io.sstable.CQLSSTableWriterClientTest):       Caused an ERROR\\n    [junit] null\\n    [junit] java.lang.ExceptionInInitializerError\\n    [junit]     at org.apache.cassandra.io.sstable.CQLSSTableWriter$Builder.getStatement(CQLSSTableWriter.java:496)\\n    [junit]     at org.apache.cassandra.io.sstable.CQLSSTableWriter$Builder.forTable(CQLSSTableWriter.java:351)\\n    [junit]     at org.apache.cassandra.io.sstable.CQLSSTableWriterClientTest.testWriterInClientMode(CQLSSTableWriterClientTest.java:74)\\n    [junit] Caused by: java.lang.NullPointerException\\n    [junit]     at org.apache.cassandra.service.ClientState.<clinit>(ClientState.java:69)\\n    [junit] \\n    [junit] \\n    [junit] Test org.apache.cassandra.io.sstable.CQLSSTableWriterClientTest FAILED\\n{noformat}', \"Reverted.\\n\\nCarl proposes this as a fix: https://github.com/carlyeks/cassandra/commit/204f7bdd8ea0a18d5c642cb7d42104749b82a62b\\n\\nWhich looks reasonable but I'm not sure what other resources should be excluded from client mode, and I'd rather not do it halfway.\", \"bq. I'm not sure what other resources should be excluded from client mode, and I'd rather not do it halfway.\\n\\nNot sure I completely follow; it isn't that those resources are excluded from client mode, rather that when *not* in client mode (and so cassandra.yaml isn't read), don't attempt to figure out which of the {{system_auth}} tables are not modifiable. Which seems reasonable, as if an IAuthenticator/IAuthorizer/IRoleManager is set in the yaml, those won't be correct anyway.\", \"Do you want to recommit with Carl's patch then?\", \"Sure, just wanted to check I wasn't misunderstanding what you meant\", \"Committed with [~carlyeks]'s additions as {{0c0f1ff1b1051627f38a8bf6cb0776241586dfce}}.\\nI notice that since the revert, {{UFTest.testTypesWithAndWithoutNulls}} has been failing with a timeout. This is doubly weird given it was just a revert, plus the same test is fine on 3.0 & trunk. I've also run being running {{UFTest}} in a loop locally and seen no errors in 65 runs. \\n\\n[~mshuler], any ideas about the above? Also, another oddity is that {{CQLSSTableWriterClientTest}} (where the offending test from 2.2 was moved to) never failed on 3.0 or trunk, even before the revert. I would have expected 3.0 to have hit the same NPE from [this build|http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_testall/90/], and indeed running the test locally with that revision fails in exactly the expected way. \\n\\n\"] \n",
            "my_comment: https://github.com/carlyeks/cassandra/commit/204f7bdd8ea0a18d5c642cb7d42104749b82a62b\n",
            "\n",
            "Which looks reasonable but Im not sure what other resources should be excluded from client mode and Id rather not do it halfway.\" \"bq. Im not sure what other resources should be excluded from client mode and Id rather not do it halfway.\n",
            "\n",
            "Not sure I completely follow; it isnt that those resources are excluded from client mode rather that when *not* in client mode (and so cassandra.yaml isnt read) dont attempt to figure out which of the {{system_auth}} tables are not modifiable. Which seems reasonable as if an IAuthenticator/IAuthorizer/IRoleManager is set in the yaml those wont be correct anyway.\" \"Do you want to recommit with Carls patch then?\" \"Sure just wanted to check I wasnt misunderstanding what you meant\" \"Committed with [~carlyeks]s additions as {{0c0f1ff1b1051627f38a8bf6cb0776241586dfce}}.\n",
            "I notice that since the revert {{UFTest.testTypesWithAndWithoutNulls}} has been failing with a timeout. This is doubly weird given it was just a revert plus the same test is fine on 3.0 & trunk. Ive also run being running {{UFTest}} in a loop locally and seen no errors in 65 runs. \n",
            "\n",
            "[~mshuler] any ideas about the above? Also another oddity is that {{CQLSSTableWriterClientTest}} (where the offending test from 2.2 was moved to) never failed on 3.0 or trunk even before the revert. I would have expected 3.0 to have hit the same NPE from [this build|http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_testall/90/] and indeed running the test locally with that revision fails in exactly the expected way. \n",
            "\n",
            "\"] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-609\n",
            "issue_type:  Bug \n",
            "summary:  Easy to OOM on log replay since memtable limits are ignored \n",
            "description: None\n",
            "comments: None\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-18430\n",
            "issue_type:  Improvement \n",
            "summary:  When decommissioning should set Severity to limit traffic \n",
            "description:  When we are decommissioning we first set LEAVING, then LEFT, then disable networking; timeouts start to follow at this last stage. LEFT nodes should not be seen as part of the ring, but that may not be seen before the network is disabled.  To better mitigate timeouts we should set severity as part of decom during the LEAVING phase; by setting severity reads should deprioritize traffic to this node.\n",
            "comments:  ['Can you rebase this to more relevant commits (without reverts at least)?', 'rebased with cleaner history', 'I ran this by circle with the new tests repeated, and they seem have problems sometimes with vnodes under both [j8|https://app.circleci.com/pipelines/github/driftx/cassandra/970/workflows/5169200c-8c69-4b43-ad4b-fd4c7c49172c/jobs/20538/] and [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/970/workflows/c89aead7-4f97-48b3-bf76-623f8e50c950/jobs/20524/].', '[~brandon.williams] thanks for running, I only ran with PAID, so guess there is some issue with FREE?\\r\\n\\r\\n{code}\\r\\n[junit-timeout] WARN  16:13:34 Dropping message (from:/127.0.0.8:7012, type:MUTATION verb:MUTATI\\r\\n\\r\\n********************************************************************************************************\\r\\nThis step produced more than the 100 MB limit of output, additional output will not be recorded.\\r\\nIf you need this amount of output, we suggest writing it to a file and using store_artifacts to save it.\\r\\n********************************************************************************************************\\r\\n\\r\\nExited with code exit status 1\\r\\n{code}\\r\\n\\r\\nHeh, not a lot of details to go off of...', 'ok looks like timeouts if I download the xml... Ill look into this... I will see what I can do.. I was doing 10 requests for each CL for each instance in dc1... this is likely what is causing the test timeout', 'Heh, that was with PAID.  Indeed, they all look like timeouts to me.', 'Ah, thought PAID used higher the median, but see my build https://app.circleci.com/pipelines/github/dcapwell/cassandra/2029/workflows/994de63b-e82c-44e0-a2a2-6fa37dd3029c/jobs/23534/tests is the same.\\r\\n\\r\\nLooking into the timeouts now', \"new patch looks to have no timeouts, but j11 had the following errors\\r\\n\\r\\n{code}\\r\\norg.apache.cassandra.distributed.shared.ShutdownException: Uncaught exceptions were thrown during test\\r\\n\\tat org.apache.cassandra.distributed.impl.AbstractCluster.checkAndResetUncaughtExceptions(AbstractCluster.java:1076)\\r\\n\\tat org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1062)\\r\\n\\tat org.apache.cassandra.distributed.test.topology.DecommissionAvoidTimeouts.test(DecommissionAvoidTimeouts.java:147)\\r\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\r\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\r\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\r\\n\\tSuppressed: java.lang.AssertionError: Unknown keyspace system_distributed\\r\\n\\t\\tat org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:325)\\r\\n\\t\\tat org.apache.cassandra.db.Keyspace.lambda$open$0(Keyspace.java:163)\\r\\n\\t\\tat org.apache.cassandra.utils.concurrent.LoadingMap.blockingLoadIfAbsent(LoadingMap.java:105)\\r\\n\\t\\tat org.apache.cassandra.schema.Schema.maybeAddKeyspaceInstance(Schema.java:251)\\r\\n\\t\\tat org.apache.cassandra.db.Keyspace.open(Keyspace.java:163)\\r\\n\\t\\tat org.apache.cassandra.db.Keyspace.open(Keyspace.java:152)\\r\\n\\t\\tat org.apache.cassandra.service.PendingRangeCalculatorService.lambda$new$1(PendingRangeCalculatorService.java:58)\\r\\n{code}\\r\\n\\r\\n{code}\\r\\njava.lang.AssertionError: Expected endpoint /127.0.0.6:7012 to be the last replica, but found /127.0.0.4:7012; [Full(/127.0.0.5:7012,(288230376151711613,432345564227567483]), Full(/127.0.0.6:7012,(288230376151711613,432345564227567483]), Full(/127.0.0.7:7012,(288230376151711613,432345564227567483]), Full(/127.0.0.3:7012,(288230376151711613,432345564227567483]), Full(/127.0.0.1:7012,(288230376151711613,432345564227567483]), Full(/127.0.0.4:7012,(288230376151711613,432345564227567483])]\\r\\n\\t\\tat org.apache.cassandra.distributed.test.topology.DecommissionAvoidTimeouts$BB.sortedByProximity(DecommissionAvoidTimeouts.java:192)\\r\\n{code}\\r\\n\\r\\n{code}\\r\\njava.lang.AssertionError: Expected endpoint /127.0.0.6:7012 to  \n",
            "my_comment: Expected endpoint /127.0.0.6:7012 to  \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4351\n",
            "issue_type:  Improvement \n",
            "summary:  Consider storing more informations on peers in system tables \n",
            "description:  Currently, the only thing we keep in system tables about other peers is their token and IP addresses. We should probably also record the new ring_id, but since CASSANDRA-4018 makes system table easily queriable, may it could be worth adding some more information (basically most of what we gossip could be a candidate (schema UUID, status, C* version, ...)) as a simple way to expose the ring state to users (even if it's just a \"view\" of the ring state from one specific node I believe it's still nice).\n",
            "comments:  ['Sounds reasonable.', \"The idea for this ticket is that in an ideal world, I could imagine to have a schema like:\\n{noformat}\\nCREATE TABLE System.peers (\\n    ring_id uuid PRIMARY KEY,\\n    tokens set<blob>,\\n    peer inet,\\n    schema_version uuid,   // so that client can check if the cluster is in agreement\\n    release_version text,  // quick check to see where we're at of a rolling upgrade\\n    rpc_address,\\n    dc text,\\n    rack text              // a smart client that want to discover node could use that to maintain it's metadata\\n)\\n{noformat}\\n\\nNow the main thing I'm not sure is that currently we have a bunch of cases where we call SystemTable.removeTokens() that I'm not sure how to adapt to such schema. That is, I'm not fully sure I understand why removeTokens is called in some of those cases.\\n\", \"Attaching a patch for this. It pretty much change the peers table to look like what's described above. It also change the local table to use an actual set instead of some hand-made serialization of the tokens.\\n\\nI'm also attaching a second patch to write tokens as string instead of bytes. The pros being that if we do that, the output of reading the local and peers table is more readable (and I don't think the slightly higher space on the disk matter). That being said, if people really prefer keeping them as blob, I won't insist too much.\", '+1 on 0001.\\n\\nFor 0002 ... what if instead we change LongToken (the default on 1.2 and what people \"should\" be using) to/fromString to hex-encode with a constant width, the way CASSANDRA-4550 wanted?  Of course then we\\'d need to switch it to unsigned comparison to be consistent with what the hex implies.  Dunno, maybe it\\'s not worth the trouble.', 'Committed 0001 but holding this open a little long to see if we decide something on the 2nd part.\\n\\nbq. what if instead we change LongToken to/fromString to hex-encode with a constant width, the way CASSANDRA-4550 wanted? Of course then we\\'d need to switch it to unsigned comparison\\n\\nThat\\'s not a bad idea (since we have no backward compatibility problem) so why not (and switching to unsigned comparison is probably not a big deal (though we do have to be careful about the fact that the minimum token shouldn\\'t be a valid token, so tokens value will have to be in [1, 2^64-1])). That being said, I\\'m not sure about the \"instead\" in the sentence above. Was that to be understood as \"in addition\" to 0002?', \"No, I did mean instead, since then the human-readable token as seen in nodetool is the same as what you'd get from cqlsh.\", \"Oh I see. Was mislead by the fact that cqlsh don't print blob in hex currently. But if we fix that then yes, that'd be the same, at least for Murmur3Partitioner.\\n\\nThough I do note that Murmur3Partitioner is only an option for brand new clusters (i.e. almost no-one will have Murmur3Partitioner at first). But again, I don't care too much. I suppose that in the long run, with vnodes, the pretty printing of tokens won't be very useful anymore.\", \"You're right, 0002 is useful even if we change m3p, +1\", \"Alright, 0002 committed. I don't know if we want to bother with the change to M3P then, but we can always do that in another ticket in any case, so closing that one now.\"] \n",
            "my_comment:  Sounds reasonable. \"The idea for this ticket is that in an ideal world I could imagine to have a schema like:\n",
            "{noformat}\n",
            "CREATE TABLE System.peers (\n",
            "    ring_id uuid PRIMARY KEY\n",
            "    tokens set<blob>\n",
            "    peer inet\n",
            "    schema_version uuid   // so that client can check if the cluster is in agreement\n",
            "    release_version text  // quick check to see where were at of a rolling upgrade\n",
            "    rpc_address\n",
            "    dc text\n",
            "    rack text              // a smart client that want to discover node could use that to maintain its metadata\n",
            ")\n",
            "{noformat}\n",
            "\n",
            "Now the main thing Im not sure is that currently we have a bunch of cases where we call SystemTable.removeTokens() that Im not sure how to adapt to such schema. That is Im not fully sure I understand why removeTokens is called in some of those cases.\n",
            "\" \"Attaching a patch for this. It pretty much change the peers table to look like whats described above. It also change the local table to use an actual set instead of some hand-made serialization of the tokens.\n",
            "\n",
            "Im also attaching a second patch to write tokens as string instead of bytes. The pros being that if we do that the output of reading the local and peers table is more readable (and I dont think the slightly higher space on the disk matter). That being said if people really prefer keeping them as blob I wont insist too much.\" +1 on 0001.\n",
            "\n",
            "For 0002 ... what if instead we change LongToken (the default on 1.2 and what people \"should\" be using) to/fromString to hex-encode with a constant width the way CASSANDRA-4550 wanted?  Of course then we\\d need to switch it to unsigned comparison to be consistent with what the hex implies.  Dunno maybe it\\s not worth the trouble. Committed 0001 but holding this open a little long to see if we decide something on the 2nd part.\n",
            "\n",
            "bq. what if instead we change LongToken to/fromString to hex-encode with a constant width the way CASSANDRA-4550 wanted? Of course then we\\d need to switch it to unsigned comparison\n",
            "\n",
            "That\\s not a bad idea (since we have no backward compatibility problem) so why not (and switching to unsigned comparison is probably not a big deal (though we do have to be careful about the fact that the minimum token shouldn\\t be a valid token so tokens value will have to be in [1 2^64-1])). That being said I\\m not sure about the \"instead\" in the sentence above. Was that to be understood as \"in addition\" to 0002? \"No I did mean instead since then the human-readable token as seen in nodetool is the same as what youd get from cqlsh.\" \"Oh I see. Was mislead by the fact that cqlsh dont print blob in hex currently. But if we fix that then yes thatd be the same at least for Murmur3Partitioner.\n",
            "\n",
            "Though I do note that Murmur3Partitioner is only an option for brand new clusters (i.e. almost no-one will have Murmur3Partitioner at first). But again I dont care too much. I suppose that in the long run with vnodes the pretty printing of tokens wont be very useful anymore.\" \"Youre right 0002 is useful even if we change m3p +1\" \"Alright 0002 committed. I dont know if we want to bother with the change to M3P then but we can always do that in another ticket in any case so closing that one now.\"] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5244\n",
            "issue_type:  Bug \n",
            "summary:  Compactions don't work while node is bootstrapping \n",
            "description:  It seems that there is a race condition in StorageService that prevents compactions from completing while node is in a bootstrap state.\n",
            "comments:  [\"Thanks for the detective work, Jouni.  I'll let Brandon comment on solutions; in the meantime, marking Minor since while inconvenient this does not compromise correctness.\", 'This is more severe than we originally thought, and causes CASSANDRA-5129 when there is a secondary index:\\n\\n{noformat}\\n\"CompactionExecutor:1\" daemon prio=10 tid=0x00007effbc03c800 nid=0x7abf waiting for monitor entry [0x00007effc843a000]\\n   java.lang.Thread.State: BLOCKED (on object monitor)\\n    at org.apache.cassandra.service.StorageService.reportSeverity(StorageService.java:905)\\n    - waiting to lock <0x00000000ca576ac8> (a org.apache.cassandra.service.StorageService)\\n    at org.apache.cassandra.db.compaction.CompactionInfo$Holder.started(CompactionInfo.java:141)\\n    at org.apache.cassandra.metrics.CompactionMetrics.beginCompaction(CompactionMetrics.java:90)\\n    at org.apache.cassandra.db.compaction.CompactionManager$9.run(CompactionManager.java:813)\\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\\n    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\\n    at java.util.concurrent.FutureTask.run(FutureTask.java:138)\\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\\n    at java.lang.Thread.run(Thread.java:662)\\n{noformat}', \"It seems to me the only reason we're synchronizing here is for the increment, and we don't need to get our own severity out of gossip, so we can just track a local AtomicDouble instead.\", '+1', 'Committed.', 'this looks good.'] \n",
            "my_comment: BLOCKED (on object monitor)\n",
            "    at org.apache.cassandra.service.StorageService.reportSeverity(StorageService.java:905)\n",
            "    - waiting to lock <0x00000000ca576ac8> (a org.apache.cassandra.service.StorageService)\n",
            "    at org.apache.cassandra.db.compaction.CompactionInfo$Holder.started(CompactionInfo.java:141)\n",
            "    at org.apache.cassandra.metrics.CompactionMetrics.beginCompaction(CompactionMetrics.java:90)\n",
            "    at org.apache.cassandra.db.compaction.CompactionManager$9.run(CompactionManager.java:813)\n",
            "    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n",
            "    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n",
            "    at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n",
            "    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n",
            "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n",
            "    at java.lang.Thread.run(Thread.java:662)\n",
            "{noformat} \"It seems to me the only reason were synchronizing here is for the increment and we dont need to get our own severity out of gossip so we can just track a local AtomicDouble instead.\" +1 Committed. this looks good. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-16308\n",
            "issue_type:  Improvement \n",
            "summary:  Add droppable tombstone metrics to nodetool tablestats \n",
            "description:  This is a useful metric to troubleshoot tombstone cleanup problems and is not currently exposed on table stats.\n",
            "comments:  ['I will pick this up.', 'Hi thanks for working on this ticket [~tejavadali]. Instructions on how to prepare a patch (ie. how to format commit messages) can be found [on this doc|https://cassandra.apache.org/_/development/patches.html].\\r\\n\\r\\nWhen you have a patch ready please submit a PR to the [github mirror|https://github.com/apache/cassandra] with this ticket number + short description (CASSANDRA-16308) and it will automagically link to this ticket and set the JIRA status to Patch Available. I can take a look and submit CI if it looks good to test.', 'LGTM, submitted CI:\\r\\n\\r\\n\\r\\n|[4.0|https://github.com/apache/cassandra/compare/cassandra-4.0...pauloricardomg:tejavadali/CASSANDRA-16308-4.0]|[tests|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1340/]|\\r\\n|[trunk|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:tejavadali/CASSANDRA-16308-trunk]|[tests|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1341/]|', 'LGTM too, test failures are unrelated, +1.'] \n",
            "my_comment:  I will pick this up. Hi thanks for working on this ticket [~tejavadali]. Instructions on how to prepare a patch (ie. how to format commit messages) can be found [on this doc|https://cassandra.apache.org/_/development/patches.html].\n",
            "\n",
            "When you have a patch ready please submit a PR to the [github mirror|https://github.com/apache/cassandra] with this ticket number + short description (CASSANDRA-16308) and it will automagically link to this ticket and set the JIRA status to Patch Available. I can take a look and submit CI if it looks good to test. LGTM submitted CI:\n",
            "\n",
            "\n",
            "|[4.0|https://github.com/apache/cassandra/compare/cassandra-4.0...pauloricardomg:tejavadali/CASSANDRA-16308-4.0]|[tests|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1340/]|\n",
            "|[trunk|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:tejavadali/CASSANDRA-16308-trunk]|[tests|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1341/]| LGTM too test failures are unrelated +1. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-17571\n",
            "issue_type:  Bug \n",
            "summary:  Config upper bound should be handled earlier \n",
            "description:  Config upper bound should be handled on startup/config setup and not during conversion \n",
            "comments:  ['Marking as 4.1 block as there was a discussion to add extended classes for Int to handle old int parameters upper bound and changing those in Config will be considered breaking change after a release. CC [~dcapwell]\\xa0 and [~maedhroz]\\xa0and [~mck]\\xa0\\r\\n\\r\\nI will push the suggested classes in the next few hours for approval before moving any config to them.\\xa0', \"Prototype in this [commit |https://github.com/ekaterinadimitrova2/cassandra/commit/1ab9f32ef34402a0f74036d768a22449170052b6] - only a few parameters were migrated for test purposes and to see how it will look like.\\r\\nAlso, I will split in separate commits the parameters in groups on migration with attached tests to them and CI to be sure gradually nothing Is missed but I want to confirm that the approach is still what we want. CC [~adelapena]\\xa0in case he has time to provide input.\\r\\n\\r\\nCurrently if people provide the new config with the new format we handle the former int parameters by returning cast value from their getters, \\xa0but on startup the user might set a bigger long value and think wrongly that one will be used when in practice the Integer.MAX_VALUE will be used. We need just to fail the user they can't set that big value, mimic the behavior of when they provide old value bigger than int. We also limit with these classes that people cannot set anything that will overflow during conversion to the smallest allowed unit instead of setting MAX_VALUE silently.\\xa0\", \"[~dcapwell]\\xa0, [~maedhroz], [~mck], [~adelapena]\\xa0is anyone of you available to confirm \\xa0the drafted prototype and do review this week or should I ask anyone else from the community if you won't have time?\\xa0\", \"[~e.dimitrova] I'll take a look at the prototype today or tomorrow.\", 'Thanks, looking forward for your feedback. I am on standby ready to incorporate any feedback and move the former int config to the extended classes before the 1st is here. Unfortunately, it will become noisy but it should be quick type change, using the old methods.\\xa0', \"I spent my evening again thinking about this as really those extensions become convoluted but I don't see a better way at this point.\\xa0\\r\\n\\r\\nLike I can get back validation utility methods in the DD and leave considering better general handling with next version but this is error-prone.\\xa0\\r\\n\\r\\nCreating a new range max annotation to all new properties in Config is not really easy option as we would need conversions... The best is to do it in the constructors at this point.\\xa0\\r\\n\\r\\nI will sleep on it and finish tomorrow morning. In case someone has something better in the meantime - I am open to hear it.\\xa0\", \"On the bright side, this is not a regression as we handle the old config input types in Converters(former ints cannot be set long value) so in my humble opinion even if we fix the new config upper bound after the freeze on the 1st, this is not a regression we have here and no API change will be involved. \\xa0I don't plan too prolong it but if we want more time to shape the solution a bit or for final review - I think we have it. CC [~mck]\\xa0in case he disagrees with this statement\", 'Working to migrate the parameters to the extended classes so we can do upper bound (former int type Config parameters and overflow of big value bigger unit to smaller supported internally) checks in the constructors. Might be not most beautiful but at least we will be more consistent and less error-prone then utility methods in the DD.\\xa0', \"WIP:\\r\\n\\r\\n[https://github.com/ekaterinadimitrova2/cassandra/pull/new/17571-trunk-rebased]\\r\\n\\r\\nIntermediate CI runs:\\r\\n\\r\\n[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1569/workflows/f67be620-844b-4151-9cd9-e7b16b9e0578]\\xa0- the DD test was fixed after that but didn't rerun the whole suite only for it...\\r\\n\\r\\n[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1570/workflows/a8f6c673-a5e6-4986-8077-e2955d6ab949] –> I probably need to loop the t \n",
            "my_comment:  Marking as 4.1 block as there was a discussion to add extended classes for Int to handle old int parameters upper bound and changing those in Config will be considered breaking change after a release. CC [~dcapwell]\\xa0 and [~maedhroz]\\xa0and [~mck]\\xa0\n",
            "\n",
            "I will push the suggested classes in the next few hours for approval before moving any config to them.\\xa0 \"Prototype in this [commit |https://github.com/ekaterinadimitrova2/cassandra/commit/1ab9f32ef34402a0f74036d768a22449170052b6] - only a few parameters were migrated for test purposes and to see how it will look like.\n",
            "Also I will split in separate commits the parameters in groups on migration with attached tests to them and CI to be sure gradually nothing Is missed but I want to confirm that the approach is still what we want. CC [~adelapena]\\xa0in case he has time to provide input.\n",
            "\n",
            "Currently if people provide the new config with the new format we handle the former int parameters by returning cast value from their getters \\xa0but on startup the user might set a bigger long value and think wrongly that one will be used when in practice the Integer.MAX_VALUE will be used. We need just to fail the user they cant set that big value mimic the behavior of when they provide old value bigger than int. We also limit with these classes that people cannot set anything that will overflow during conversion to the smallest allowed unit instead of setting MAX_VALUE silently.\\xa0\" \"[~dcapwell]\\xa0 [~maedhroz] [~mck] [~adelapena]\\xa0is anyone of you available to confirm \\xa0the drafted prototype and do review this week or should I ask anyone else from the community if you wont have time?\\xa0\" \"[~e.dimitrova] Ill take a look at the prototype today or tomorrow.\" Thanks looking forward for your feedback. I am on standby ready to incorporate any feedback and move the former int config to the extended classes before the 1st is here. Unfortunately it will become noisy but it should be quick type change using the old methods.\\xa0 \"I spent my evening again thinking about this as really those extensions become convoluted but I dont see a better way at this point.\\xa0\n",
            "\n",
            "Like I can get back validation utility methods in the DD and leave considering better general handling with next version but this is error-prone.\\xa0\n",
            "\n",
            "Creating a new range max annotation to all new properties in Config is not really easy option as we would need conversions... The best is to do it in the constructors at this point.\\xa0\n",
            "\n",
            "I will sleep on it and finish tomorrow morning. In case someone has something better in the meantime - I am open to hear it.\\xa0\" \"On the bright side this is not a regression as we handle the old config input types in Converters(former ints cannot be set long value) so in my humble opinion even if we fix the new config upper bound after the freeze on the 1st this is not a regression we have here and no API change will be involved. \\xa0I dont plan too prolong it but if we want more time to shape the solution a bit or for final review - I think we have it. CC [~mck]\\xa0in case he disagrees with this statement\" Working to migrate the parameters to the extended classes so we can do upper bound (former int type Config parameters and overflow of big value bigger unit to smaller supported internally) checks in the constructors. Might be not most beautiful but at least we will be more consistent and less error-prone then utility methods in the DD.\\xa0 \"WIP:\n",
            "\n",
            "[https://github.com/ekaterinadimitrova2/cassandra/pull/new/17571-trunk-rebased]\n",
            "\n",
            "Intermediate CI runs:\n",
            "\n",
            "[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1569/workflows/f67be620-844b-4151-9cd9-e7b16b9e0578]\\xa0- the DD test was fixed after that but didnt rerun the whole suite only for it...\n",
            "\n",
            "[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1570/workflows/a8f6c673-a5e6-4986-8077-e2955d6ab949] –> I probably need to loop the t \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-11356\n",
            "issue_type:  Bug \n",
            "summary:  EC2MRS ignores broadcast_rpc_address setting in cassandra.yaml \n",
            "description:  EC2MRS ignores broadcast_rpc_address setting in cassandra.yaml.  This is problematic for those users who were using EC2MRS with an internal rpc_address before the change introduced in [CASSANDRA-5899|https://issues.apache.org/jira/browse/CASSANDRA-5899], because the change results in EC2MRS always using the public ip regardless of what the user has set for broadcast_rpc_address. \n",
            "comments:  ['We should be able to introduce an option to make this configurable, since this affects vpc deployments.', \"I'm not sure we should even be setting {{broadcast_rpc_address}} on EC2MRS. While setting {{broadcast_address}} is useful to the snitch's objective of making nodes from different dcs connect over the public address for internal communication, {{broadcast_rpc_address}} is a deployment option independent of EC2MRS choice. I'd be in favor of not setting this at all on EC2MRS and adding an upgrade notice for users who've been relying on this behavior.\", 'On EC2 users need to choose which {{rpc_address}} to broadcast to other nodes: if the private IP or the public IP (since both are routable to the private IF).  Before CASSANDRA-5899 it broadcasted {{rpc_address}} which defaulted to {{listen_address}}, which was typically set to private IP on EC2 deployments. CASSANDRA-5899 added ability to choose which IP to broadcast via the {{broadcast_rpc_address}}, but it also changed {{Ec2MultiRegionSnitch}} to *always* broadcast the public IP, regardless of {{broadcast_rpc_address}}, what makes impossible for nodes to advertise their private IP for client connections if they want to.\\n\\nThis patch updates {{Ec2MultiRegionSnitch}} to only set {{broadcast_rpc_address}} to the public IP if this property is unset, allowing operators to overide this to the private IP if they want to. \\n\\nBefore {{DatabaseDescriptor}} was setting {{broadcastRpcAddress = rpcAddress}}, so it was impossible to know if {{broadcastRpcAddress == null}} in order to decide whether or not to override the property on {{Ec2MultiRegionSnitch}}, so I modified all uses of {{DatabaseDescriptor.getBroadcastRpcAddress()}} to use {{FBUtilities.getBroadcastRpcAddress()}} instead which will fallback to {{DatabaseDescriptor.getRpcAddress()}} if {{DatabaseDescriptor.getBroadcastRpcAddress() == null}}.\\n\\nPatch and tests available below:\\n\\n||2.2||3.0||3.9||trunk||\\n|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-11356]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-11356]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.9...pauloricardomg:3.9-11356]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-11356]|\\n|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-11356-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-11356-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.9-11356-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-11356-testall/lastCompletedBuild/testReport/]|\\n|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-11356-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-11356-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.9-11356-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-11356-dtest/lastCompletedBuild/testReport/]|\\n\\nCould you have a look [~thobbs]? Thanks!', 'Overall, the patch looks good to me.  I think we need a couple of doc updates, though.  A description of the change in behavior in NEWS.txt would be good.  In 3.9 and trunk, {{doc/source/operating/snitch.rst}} could be updated to explain this behavior as well.\\n\\n[~jasobrown] as a former user of the ec2 snitch, do you have time to make a quick review of this as well?', \"Updated {{NEWS.txt}} and added the following note to {{doc/source/operating/snitch.rst}} on 3.9 and trunk:\\n\\nbq. By default, Ec2MultiRegionSnitch advertises the public instance IP as ``rpc_address``, allowing cross-DC discover \n",
            "my_comment: if the private IP or the public IP (since both are routable to the private IF).  Before CASSANDRA-5899 it broadcasted {{rpc_address}} which defaulted to {{listen_address}} which was typically set to private IP on EC2 deployments. CASSANDRA-5899 added ability to choose which IP to broadcast via the {{broadcast_rpc_address}} but it also changed {{Ec2MultiRegionSnitch}} to *always* broadcast the public IP regardless of {{broadcast_rpc_address}} what makes impossible for nodes to advertise their private IP for client connections if they want to.\n",
            "\n",
            "This patch updates {{Ec2MultiRegionSnitch}} to only set {{broadcast_rpc_address}} to the public IP if this property is unset allowing operators to overide this to the private IP if they want to. \n",
            "\n",
            "Before {{DatabaseDescriptor}} was setting {{broadcastRpcAddress = rpcAddress}} so it was impossible to know if {{broadcastRpcAddress == null}} in order to decide whether or not to override the property on {{Ec2MultiRegionSnitch}} so I modified all uses of {{DatabaseDescriptor.getBroadcastRpcAddress()}} to use {{FBUtilities.getBroadcastRpcAddress()}} instead which will fallback to {{DatabaseDescriptor.getRpcAddress()}} if {{DatabaseDescriptor.getBroadcastRpcAddress() == null}}.\n",
            "\n",
            "Patch and tests available below:\n",
            "\n",
            "||2.2||3.0||3.9||trunk||\n",
            "|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-11356]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-11356]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.9...pauloricardomg:3.9-11356]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-11356]|\n",
            "|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-11356-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-11356-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.9-11356-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-11356-testall/lastCompletedBuild/testReport/]|\n",
            "|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-11356-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-11356-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.9-11356-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-11356-dtest/lastCompletedBuild/testReport/]|\n",
            "\n",
            "Could you have a look [~thobbs]? Thanks! Overall the patch looks good to me.  I think we need a couple of doc updates though.  A description of the change in behavior in NEWS.txt would be good.  In 3.9 and trunk {{doc/source/operating/snitch.rst}} could be updated to explain this behavior as well.\n",
            "\n",
            "[~jasobrown] as a former user of the ec2 snitch do you have time to make a quick review of this as well? \"Updated {{NEWS.txt}} and added the following note to {{doc/source/operating/snitch.rst}} on 3.9 and trunk:\n",
            "\n",
            "bq. By default Ec2MultiRegionSnitch advertises the public instance IP as ``rpc_address`` allowing cross-DC discover \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-15265\n",
            "issue_type:  Bug \n",
            "summary:  Index summary redistribution can start even when compactions are paused \n",
            "description:  When we pause autocompaction for upgradesstables/scrub/cleanup etc we pause all compaction strategies to make sure we can grab all sstables, index summary redistribution does not pause and this can cause us to fail the operation. \n",
            "comments:  ['Patch adds a flag in `CompactionManager` which states if non-strategy compactions should be paused, only index summary redistributions uses it for now:\\r\\n\\r\\n[3.0|https://github.com/krummas/cassandra/commits/marcuse/15265-3.0], [cci|https://circleci.com/workflow-run/3caa20be-4cca-4075-bf91-b5209e4d5abf]\\r\\n[3.11|https://github.com/krummas/cassandra/commits/marcuse/15265-3.11], [cci|https://circleci.com/workflow-run/3737dab7-d8bb-4d74-9b49-c73f062e59ad]\\r\\n[trunk|https://github.com/krummas/cassandra/commits/marcuse/15265-trunk], [cci|https://circleci.com/workflow-run/2a3a6917-4fde-43d1-b896-42b0e7186115]', \"LGTM.  \\r\\n\\r\\nOne tiny stylistic suggestion: I think it can be clearer to a reader when predicates are phrased so that they read like a predicate.  In this case we can't easily use our normal {{is}} prefix, but I guess we could use {{areGlobalCompactionsPaused}} or alternatively {{isGlobalCompactionStopRequested}} for consistency with the {{isStopRequested}}\\r\\n\\r\\nAbsolutely not blocking; happy for you to name and commit however you like.\", 'changed the method to {{isGlobalCompactionPaused()}}, added a comment around {{ensureCapacity()}} and committed, thanks\\r\\n\\r\\ntests: [3.0|https://circleci.com/workflow-run/8882a8a6-8593-4d3e-8ec1-05bcab855a44] [3.11|https://circleci.com/workflow-run/6b057c7e-1b4a-4f11-9af8-eb3ec2dd8cc9] [trunk|https://circleci.com/workflow-run/457f8304-c477-45e7-b195-06cf67c22450]'] \n",
            "my_comment: [3.0|https://circleci.com/workflow-run/8882a8a6-8593-4d3e-8ec1-05bcab855a44] [3.11|https://circleci.com/workflow-run/6b057c7e-1b4a-4f11-9af8-eb3ec2dd8cc9] [trunk|https://circleci.com/workflow-run/457f8304-c477-45e7-b195-06cf67c22450] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6349\n",
            "issue_type:  Bug \n",
            "summary:  IOException in MessagingService.run() causes orphaned storage server socket \n",
            "description:  The refactoring of reading the message header in MessagingService.run() vs IncomingTcpConnection seems to mishandle IOException as the loop is broken and MessagingService.SocketThread never seems to get reinitialized.\n",
            "comments:  ['One of the options is to close the socket on IOException and continue', 'LGTM, committed', \"I suspect these changes introduced an infinite loop if the ServerSocket gets closed (not sure how that is happening though). We've been seeing some major problems with Cassandra 2.0.3 when a new cluster is coming up for the first time, and it seems to be a result of this. With logging set to debug, system.log is getting pummelled with these exception messages:\\n\\n{noformat}\\nDEBUG [ACCEPT-localhost-grid/10.96.99.178] 2013-12-06 22:55:39,759 MessagingService.java (line 905) Error reading the socket null\\njava.net.SocketException: Socket closed\\n        at java.net.PlainSocketImpl.socketAccept(Native Method)\\n        at java.net.AbstractPlainSocketImpl.accept(Unknown Source)\\n        at java.net.ServerSocket.implAccept(Unknown Source)\\n        at sun.security.ssl.SSLServerSocketImpl.accept(Unknown Source)\\n        at org.apache.cassandra.net.MessagingService$SocketThread.run(MessagingService.java:865)\\n{noformat}\\n\\nIt looks like once in this state, nothing will break it out; prior to this change the IOException catch block was throwing another exception, now it just keeps looping, using the (seemingly closed) ServerSocket. Restarting Cassandra seems to be the only way to resolve this. I'll probably be recommending we drop back to 2.0.2 until this problem is fixed (or we can understand why the ServerSocket is closed...)\\n\", \"The current SocketThread's code detects whether the ServerSocket is closing by catching AsynchronousCloseException/ClosedChannelException and breaking the endless loop.\\nAnd it looks like SSLServerSocketImpl throws a different exception (SocketException) which the thread doesn't handle\\n\\nDo we really need {{while(true)}} there? Why can't we use {{while (!server.isClosed())}} instead?\\n\", 'CASSANDRA-6468', 'Please also note that handling the protocol magic and version handshake in the while loop allows an attacker to open a connection and not send any data, preventing any further connections. Prior revisions handled all the handshaking in the resulting thread where it might be more appropriate.'] \n",
            "my_comment: Socket closed\n",
            "        at java.net.PlainSocketImpl.socketAccept(Native Method)\n",
            "        at java.net.AbstractPlainSocketImpl.accept(Unknown Source)\n",
            "        at java.net.ServerSocket.implAccept(Unknown Source)\n",
            "        at sun.security.ssl.SSLServerSocketImpl.accept(Unknown Source)\n",
            "        at org.apache.cassandra.net.MessagingService$SocketThread.run(MessagingService.java:865)\n",
            "{noformat}\n",
            "\n",
            "It looks like once in this state nothing will break it out; prior to this change the IOException catch block was throwing another exception now it just keeps looping using the (seemingly closed) ServerSocket. Restarting Cassandra seems to be the only way to resolve this. Ill probably be recommending we drop back to 2.0.2 until this problem is fixed (or we can understand why the ServerSocket is closed...)\n",
            "\" \"The current SocketThreads code detects whether the ServerSocket is closing by catching AsynchronousCloseException/ClosedChannelException and breaking the endless loop.\n",
            "And it looks like SSLServerSocketImpl throws a different exception (SocketException) which the thread doesnt handle\n",
            "\n",
            "Do we really need {{while(true)}} there? Why cant we use {{while (!server.isClosed())}} instead?\n",
            "\" CASSANDRA-6468 Please also note that handling the protocol magic and version handshake in the while loop allows an attacker to open a connection and not send any data preventing any further connections. Prior revisions handled all the handshaking in the resulting thread where it might be more appropriate. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-16932\n",
            "issue_type:  Bug \n",
            "summary:  Gossip Fixes \n",
            "description:  Testing with CEP-10 discovered faults with gossip where status updates may be processed in an order that invalidates their application. These fixes are necessary for simulation to run correctly, but also potentially affect gossip time to settle. \n",
            "comments:  ['Patch [here|https://github.com/belliottsmith/cassandra/tree/16932-trunk]'] \n",
            "my_comment:  Patch [here|https://github.com/belliottsmith/cassandra/tree/16932-trunk] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7279\n",
            "issue_type:  Bug \n",
            "summary:  MultiSliceTest.test_with_overlap* unit tests failing in trunk \n",
            "description:  Example:\n",
            "comments:  ['MultiSliceTest was added in commit 60fb923 where it passes.\\n{{git bisect start trunk 60fb923018a6fd2dabf04a1d4500f7b29a23a6f1}} gets me f31f689 as the first bad commit, which is the merge commit for CASSANDRA-6689', 'So this is only failing in trunk not 2.1 branch?\\n', 'the test only exists in trunk', \"This is probably my mess in that case, so I'll take a look\", \"I'm already debugging... pretty sure this is CASSANDRA-5086\", 'OK, knock yourself out :)', 'So the issue is the ABSC is blindly adding the overlapping slices so it ends up with dups.  \\n\\nWe can  fix this by:\\n  - cropping the slices so none of them intersect.\\n  - changing ABSC.maybeAppendColumn to use addColumn vs addInternal (so it checks the previous value)\\n\\nI prefer the first but there would be a number of places to change if we did it at the iterator level.  Since this is only exposed in thrift the simplest place would be there.', 'Actually it can be done in SliceQueryFilter constructor', \"Do you mean ABTC? Pretty sure ABSC treats it correctly; probably when slicing we should use the max of (previous slice end) and (new slice start) for the bound of the new iterator. I'd kind of like to fix this in a similar way to how we deal with it in ABSC though, although it would be more work (so probably best left until later); in trunk we could use the new SearchIterator (with some small tweaks) to solve the problem by always using the current position as a lower bound for the search for the next lower bound.\", \"Digging into the history of this more.  Origionally Multiple slice ranges didn't allow overlapping ranges CASSANDRA-3855 but that restriction was removed in CASSANDRA-5573\\n\\n\\n\", \"No It's in ABSC, I think it was broken in CASSANDRA-7107 since that stopped using addCell and instead just appends the cell to the array.  I think fixing it in slices so they don't overlap is the right approach.\", \"Well, there's lots of places you could consider the bug to be; I meant that ABTC listens to overlapping slices and returns the overlap, whereas ABSC doesn't, and iirc the on-disk indexed slice iterator also doesn't, so if ABTC didn't also we'd be safe again. But fixing it in the Slice def is as good a solution and more permanent in the face of more slice iterator sources. We could even simplify ABSC's slice iterators in that case as well.\", 'Attached patch to sort and crop overlapping slices.  I also had to change AtomicBtreeColumns to deal with excluding a point when the start of the finish of the previous slice == the start of the next slice ', \"Applied the patch to trunk and tested out the modified tests (haven't run a full 'ant test') - MultiSliceTest, QueryPagerTest, RangeTombstoneTest pass, but ColumnFamilyStoreTest failed with:\\n{noformat}\\ntest:\\n     [echo] running unit tests\\n    [junit] WARNING: multiple versions of ant detected in path for junit \\n    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class\\n    [junit]      and jar:file:/home/mshuler/git/cassandra/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class\\n    [junit] Testsuite: org.apache.cassandra.db.ColumnFamilyStoreTest\\n    [junit] Tests run: 35, Failures: 8, Errors: 0, Skipped: 0, Time elapsed: 17.085 sec\\n    [junit] \\n    [junit] ------------- Standard Output ---------------\\n    [junit] ERROR 20:07:21 Unable to delete build/test/cassandra/data/Keyspace1/Indexed2-a7e63420e1ec11e3a5c09b2001e5c823/Keyspace1-Indexed2.birthdate_index-ka-1-Data.db (it will be removed on server restart; we'll also retry after GC)\\n    [junit] ERROR 20:07:21 Unable to delete build/test/cassandra/data/Keyspace1/Indexed2-a7e63420e1ec11e3a5c09b2001e5c823/Keyspace1-Indexed2.birthdate_index-ka-1-Data.db (it will be removed on server restart; we'll also retry after GC)\\n    [junit] ERROR 20:07:23 Missing component: build/test/cassandra/data/Keyspace1/Standard3-a7e60d12e1ec11e3a5c09b2001e5c823/Keyspace1-Standard3-ka-1-Summary.db\\n    [junit] ERROR 20:07:23 Missing component: b \n",
            "my_comment: b \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2975\n",
            "issue_type:  Improvement \n",
            "summary:  Upgrade MurmurHash to version 3 \n",
            "description:  MurmurHash version 3 was finalized on June 3. It provides an enormous speedup and increased robustness over version 2, which is implemented in Cassandra. Information here:\n",
            "comments:  ['So v3 is faster to compute a 128bit hash, than v2 is to compute a 64bit one?', 'Surprising, but yes. It\\'s dramatically faster. The MurmurHash author reports a 50% speedup over v2 at http://code.google.com/p/smhasher/wiki/MurmurHash3. I ran my own simple benchmark on the Java version comparing the existing MurmurHash.hash64() function to the MurmurHash.hash3_x64_128() I added and found an even larger advantage. The improvement is so huge that I wonder a little bit if there isn\\'t a flaw in my test, but here it is:\\n\\n{code:java}\\nstart = System.currentTimeMillis();\\nlong[] reta = {0, 0};\\nByteBuffer buf = strToByteBuffer(key);\\nfor (int i=0; i<cnt; i++)\\n{\\n  buf.clear();\\n  reta = MurmurHash.hash3_x64_128(buf, 0, key.length(), (int) reta[0]);\\n}\\nend = System.currentTimeMillis();\\nSystem.err.println(\"Ran v3 \" + cnt + \" times in \" + (end - start) + \" ms.\");\\n{code}\\n\\nSimilarly for v2.\\n\\nOutput:\\n{code}\\nRan v2 100000000 times in 19993 ms.\\nRan v3 100000000 times in 3104 ms.\\n{code}\\n\\nFWIW, I also ran some tests where I generated random strings and seeds and submitted them to both the reference implementation and the Java port and found no differences.', 'Interesting.  Sounds like a free lunch. :)\\n\\nBesides speed, we\\'d need to make sure Murmur3 gives us as good a hash distribution as Murmur2, so our bloom filter false positive rate doesn\\'t go up -- see BloomFilterTest, which runs with \"ant long-test\". ', \"Also, it's not quite as simple as ripping out M2 and replacing with M3 -- we need to continue to support M2 for compatibility with old data files.  Look at uses of Descriptor.hasStringsInBloomFilter for a similar change.\", \"The test didn't seem to produce any errors:\\n\\n{code}\\n[junit] Testsuite: org.apache.cassandra.utils.LongBloomFilterTest\\n[junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 75.354 sec\\n[junit] \\n[junit] Testsuite: org.apache.cassandra.utils.LongLegacyBloomFilterTest\\n[junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 76.545 sec\\n{code}\\n\\nI'll have to familiarize myself more with Cassandra before dealing with the other issue you pointed out. I've just been using the BloomFilter class and only have a very general understanding of the overall system.\", 'Can you dig a little deeper and compare actual FP rates?\\n\\nlogging/printing fpratio before\\n\\n{code}\\n        assert fp_ratio < 1.03 : fp_ratio;\\n{code}\\n\\nshould be fine.', 'Summary:\\n\\n{code}\\nMean FP rates for version 2:\\nLongBloomFilterTest: 0.997967059178744\\nLongLegacyBloomFilterTest: 0.997908061594203\\n\\nMean FP rates for version 3:\\nLongBloomFilterTest: 0.998045621980676\\nLongLegacyBloomFilterTest: 0.998863888888889\\n{code}\\n\\n\\nDetails:\\n\\n{code}\\nVersion 2:\\n     [echo] running long tests\\n    [junit] WARNING: multiple versions of ant detected in path for junit \\n    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class\\n    [junit]      and jar:file:/Users/jbl/git/cassandra/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class\\n    [junit] Testsuite: org.apache.cassandra.utils.LongBloomFilterTest\\n    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 106.213 sec\\n    [junit] \\n    [junit] ------------- Standard Error -----------------\\n    [junit] fp_ratio = 0.9973043478260869\\n    [junit] fp_ratio = 0.9965793478260869\\n    [junit] fp_ratio = 0.9996123188405797\\n    [junit] fp_ratio = 1.0004746376811595\\n    [junit] fp_ratio = 0.998409420289855\\n    [junit] fp_ratio = 0.9920978260869565\\n    [junit] fp_ratio = 0.9979420289855072\\n    [junit] fp_ratio = 0.9940797101449276\\n    [junit] fp_ratio = 0.9983913043478261\\n    [junit] fp_ratio = 1.0006159420289855\\n    [junit] fp_ratio = 1.0000362318840579\\n    [junit] fp_ratio = 1.0000615942028985\\n    [junit] ------------- ---------------- ---------------\\nmean = 0.997967059178744\\n\\n    [junit] Testsuite: org.apache.cassandra.utils.LongLegacyBloomFilterTest\\n    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 61 \n",
            "my_comment: 61 \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4004\n",
            "issue_type:  Sub-task \n",
            "summary:  Add support for ReversedType \n",
            "description:  It would be nice to add a native syntax for the use of ReversedType. I'm sure there is anything in SQL that we inspired ourselves from, so I would propose something like:\n",
            "comments:  ['Attached simple patch implementing the syntax above (a simple test has been push in the dtests).', 'I think I\\'d prefer to move this into our \"implementation extensions\" section as {{WITH CLUSTERING ORDERED BY (time DESC)}} or something similar.\\n\\nI also don\\'t see any code to account for, if we\\'ve already ordered it as DESC in the clustering declaration, that\\'s going to affect what we need to do for ORDER BY DESC (or ASC).', 'The idea of my patch is that adding DESC to a field in a table declaration changes the order of records *logically* (it also does it physically, but that is an implementation detail). The typical example (which I think is very common) is for time series, where you\\'re often interested by having record \"by default\" in reverse time order (from the more recent to the older). In that case a SELECT without any ORDER BY would return records directly in reverse time order. So at least given that definition, no, I don\\'t think there is anything to do for the code of ORDER BY in SELECT.\\n\\nI reckon there is another way to add ReversedType, which I\\'m assuming is what you are referring to, which consists in *not* changing the logical order, but to change the physical order. In which case, yes, we would have to take that into account for ORDER BY during SELECT.\\n\\nI\\'ll admit I\\'m less of a fan of that second option though. It seems that changing the logical order is 1) more natural (to me at least obviously) and 2) avoids having to use ORDER BY DESC in all your query.\\n\\nbq. I think I\\'d prefer to move this into our \"implementation extensions\" section as WITH CLUSTERING ORDERED BY (time DESC) or something similar\\n\\nWhy not, but I do prefer \"my\" notation in that it\\'s more concise and, at least if it changes the logical order, I guess it doesn\\'t feel like being just an \\'implementation extension\\'.', 'bq. The idea of my patch is that adding DESC to a field in a table declaration changes the order of records logically \\n\\nThat makes sense for the old world of ReversedType and {{reversed}} slice flag, but I don\\'t think it makes sense for CQL. When I ask for \"ORDER BY X DESC\" I expect the largest X first, period. So a table declaration like this makes sense as an optimization if DESC is your most frequent query type, but it shouldn\\'t change the semantics of the query itself.', 'bq. but I don\\'t think it makes sense for CQL.\\n\\nWhy wouldn\\'t it?  The notion of what is largest or smallest only make sense once you\\'ve defined what ordering you\\'re talking about (what I\\'m calling the logical ordering). We do still allow in CQL custom orderings (which is useful), so why giving a simple syntax to define the reverse ordering or an existing one wouldn\\'t make sense? With my first patch, \"ORDER BY X DESC\" does *always* return the largest X first, given the ordering.\\n\\nbq. but it shouldn\\'t change the semantics of the query itself\\n\\nTo be precise, it doesn\\'t change the semantic of the query, it changes the logical ordering (which happens to be the same than the physical one but that last part is an implementation detail) of records in the table.\\n\\n\\nNow, looking more closely at the alternative of keeping the logical ordering unchanged but changing the physical ordering (in order to get faster reversed queries), I think this just doesn\\'t work. And by \"doesn\\'t work\", I mean that as soon as we have composites, it would be costly to implement (making it useless). Typically, suppose you follow that idea and declare:\\n{noformat}\\nCREATE TABLE timeseries (\\n  key text,\\n  kind int,\\n  time timestamp,\\n  value text,\\n  PRIMARY KEY (key, kind, time)\\n) WITH CLUSTERING ORDER BY (kind ASC, time DESC)\\n{noformat}\\n\\nNow, if the query is:\\n{noformat}\\nSELECT kind, time FROM timeseries WHERE key = <somevalue> LIMIT 200;\\n{noformat}\\nthen, if the DESC above is \"just an optimisation for reversed queries, then the expected result is (say):\\n{noformat}\\nkind | time\\n-----------\\n   0 |    0\\n   0 |    1\\n   ...\\n   0 |   99\\n   0 \n",
            "my_comment:  Attached simple patch implementing the syntax above (a simple test has been push in the dtests). I think I\\d prefer to move this into our \"implementation extensions\" section as {{WITH CLUSTERING ORDERED BY (time DESC)}} or something similar.\n",
            "\n",
            "e often interested by having record \"by default\" in reverse time order (from the more recent to the older). In that case a SELECT without any ORDER BY would return records directly in reverse time order. So at least given that definition no I don\\t think there is anything to do for the code of ORDER BY in SELECT.\n",
            "\n",
            "I reckon there is another way to add ReversedType which I\\m assuming is what you are referring to which consists in *not* changing the logical order but to change the physical order. In which case yes we would have to take that into account for ORDER BY during SELECT.\n",
            "\n",
            "I\\ll admit I\\m less of a fan of that second option though. It seems that changing the logical order is 1) more natural (to me at least obviously) and 2) avoids having to use ORDER BY DESC in all your query.\n",
            "\n",
            "bq. I think I\\d prefer to move this into our \"implementation extensions\" section as WITH CLUSTERING ORDERED BY (time DESC) or something similar\n",
            "\n",
            "Why not but I do prefer \"my\" notation in that it\\s more concise and at least if it changes the logical order I guess it doesn\\t feel like being just an \\implementation extension\\. bq. The idea of my patch is that adding DESC to a field in a table declaration changes the order of records logically \n",
            "\n",
            "That makes sense for the old world of ReversedType and {{reversed}} slice flag but I don\\t think it makes sense for CQL. When I ask for \"ORDER BY X DESC\" I expect the largest X first period. So a table declaration like this makes sense as an optimization if DESC is your most frequent query type but it shouldn\\t change the semantics of the query itself. bq. but I don\\t think it makes sense for CQL.\n",
            "\n",
            "e talking about (what I\\m calling the logical ordering). We do still allow in CQL custom orderings (which is useful) so why giving a simple syntax to define the reverse ordering or an existing one wouldn\\t make sense? With my first patch \"ORDER BY X DESC\" does *always* return the largest X first given the ordering.\n",
            "\n",
            "bq. but it shouldn\\t change the semantics of the query itself\n",
            "\n",
            "To be precise it doesn\\t change the semantic of the query it changes the logical ordering (which happens to be the same than the physical one but that last part is an implementation detail) of records in the table.\n",
            "\n",
            "\n",
            "Now looking more closely at the alternative of keeping the logical ordering unchanged but changing the physical ordering (in order to get faster reversed queries) I think this just doesn\\t work. And by \"doesn\\t work\" I mean that as soon as we have composites it would be costly to implement (making it useless). Typically suppose you follow that idea and declare:\n",
            "{noformat}\n",
            "CREATE TABLE timeseries (\n",
            "  key text\n",
            "  kind int\n",
            "  time timestamp\n",
            "  value text\n",
            "  PRIMARY KEY (key kind time)\n",
            ") WITH CLUSTERING ORDER BY (kind ASC time DESC)\n",
            "{noformat}\n",
            "\n",
            "Now if the query is:\n",
            "{noformat}\n",
            "SELECT kind time FROM timeseries WHERE key = <somevalue> LIMIT 200;\n",
            "{noformat}\n",
            "then if the DESC above is \"just an optimisation for reversed queries then the expected result is (say):\n",
            "{noformat}\n",
            "kind | time\n",
            "-----------\n",
            "   0 |    0\n",
            "   0 |    1\n",
            "   ...\n",
            "   0 |   99\n",
            "   0 \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9031\n",
            "issue_type:  Bug \n",
            "summary:  nodetool info -T throws ArrayOutOfBounds when the node has not joined the cluster \n",
            "description:  To reproduce, bring up a node that does not join the cluster, either using -Dcassandra.write_survey=true or -Dcassandra.join_ring=false, then run 'nodetool info -T'. You'll get the following stack trace:\n",
            "comments:  [\"Thanks for the patch.\\nAlthough your patch makes it clear tokens are not available yet, I think it'd be better to skip showing DC/Rack info rather than throwing exception.\\nIn addition to your patch, adding token check before displaying DC/Rack in {{NodeCmd#printInfo}} would be nice.\\n\\nWDYT?\", 'Sounds good to me. Do you want me to submit another patch?', 'Sorry for the delay.\\n{{TokenMetadata#getToken}} throws AssertionException when the node is not yet a member of the cluster.\\nChanging that methods behavior is a bit scary since it is used in other parts.\\n\\nInstead, we can use host ID to get endpoint from nodetool.\\nThat way, we can display DC/Rack as well.\\n\\npatch pushed to here: https://github.com/yukim/cassandra/tree/9031', '[~Stefania] to review', \"Maybe there is something I am missing so please double check but isn't this code\\n\\n{code}\\n    public String getEndpoint()\\n    {\\n        Map<String, String> hostIdToEndpoint = ssProxy.getHostIdMap();\\n        return hostIdToEndpoint.get(ssProxy.getLocalHostId());\\n    }\\n{code}\\n\\nalways going to return {{FBUtilities.getBroadcastAddress()}}?\\n\\nAside from this, code is +1.\\n\\nTechnically we should have a dtest, but since the nodetool tests are yet to be implemented, perhaps we should link this ticket to CASSANDRA-9349?\\n\", 'Thanks for the review.\\n\\nI guess the reason we are doing weired implementation for nodetool now is lack of the way to get endpoint, dc and rack info of connected node straight from JMX.\\nWe can add JMX interface for those in 3.0. (will create that later.)\\n\\nFor fix in 2.1 and 2.2, I think we need to stick with what we have for now.', 'Oh sorry about it, I forgot we are still client side.\\n\\n+1 to commit.\\n\\n', 'Committed, thanks!'] \n",
            "my_comment: https://github.com/yukim/cassandra/tree/9031 [~Stefania] to review \"Maybe there is something I am missing so please double check but isnt this code\n",
            "\n",
            "{code}\n",
            "    public String getEndpoint()\n",
            "    {\n",
            "        Map<String String> hostIdToEndpoint = ssProxy.getHostIdMap();\n",
            "        return hostIdToEndpoint.get(ssProxy.getLocalHostId());\n",
            "    }\n",
            "{code}\n",
            "\n",
            "always going to return {{FBUtilities.getBroadcastAddress()}}?\n",
            "\n",
            "Aside from this code is +1.\n",
            "\n",
            "Technically we should have a dtest but since the nodetool tests are yet to be implemented perhaps we should link this ticket to CASSANDRA-9349?\n",
            "\" Thanks for the review.\n",
            "\n",
            "I guess the reason we are doing weired implementation for nodetool now is lack of the way to get endpoint dc and rack info of connected node straight from JMX.\n",
            "We can add JMX interface for those in 3.0. (will create that later.)\n",
            "\n",
            "For fix in 2.1 and 2.2 I think we need to stick with what we have for now. Oh sorry about it I forgot we are still client side.\n",
            "\n",
            "+1 to commit.\n",
            "\n",
            " Committed thanks! \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6380\n",
            "issue_type:  Bug \n",
            "summary:  SSTableReader.loadSummary may leave an open file \n",
            "description:  When {{SSTableReader.loadSummary}} catches _IOException_ it tries to delete {{summariesFile}}, but the {{iStream}} is still open and the file is locked, so {{FileUtils.deleteWithConfirm}} fails, at least on Windows \n",
            "comments:  ['Attaching the patch to close the stream to unlock the file', \"'finally' block does close iStream, so I think there isn't a problem in current code.\", \"LGTM; committed\\n\\n(Didn't see Yuki's comment.  The problem is that the catch block runs before the finally, so it tries to delete before the close, which works on linux but not windows.)\"] \n",
            "my_comment:  Attaching the patch to close the stream to unlock the file \"finally block does close iStream so I think there isnt a problem in current code.\" \"LGTM; committed\n",
            "\n",
            "(Didnt see Yukis comment.  The problem is that the catch block runs before the finally so it tries to delete before the close which works on linux but not windows.)\"] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-212\n",
            "issue_type:  Bug \n",
            "summary:  Range queries do not yet span multiple nodes \n",
            "description:  Need ability to continue a query on the next node in the ring, if necessary \n",
            "comments:  [\"All this needs is a bool added to RangeReply that is true if it stopped b/c the next local key was not part of the range, or the asked-for limit was reached.\\n\\nIf a reply comes back with that bool false, then the coordinator node, the one talking to the client, sends the query to the next node in the ring down from the one it just got a reply from and appends those results to the list it's going to return to the client.\\n\\nRepeat until the i'm-done-bool comes back true.\\n\", 'Sounds like a pretty serious bug in the general case, but might not happen very often in practice.', '(the relevant starting point is StorageProxy.getKeyRange)', 'This can be a bit tricky because the key ranges btw 2 consecutive nodes overlap (because of replication). When moving to the next node, you want to be careful not to pick up duplicated keys.\\n\\nAlso, the approach that Jonathan described forces nodes to be scanned sequentially. Sometimes, it is more efficient to scan multiple nodes in parallel, especially if maxResult is unspecified.', \"> When moving to the next node, you want to be careful not to pick up duplicated keys. \\n\\nthat is why you give the last value received from node #1 as the start_with parameter to node #2.  so at most there will be one duplicate.\\n\\n> Sometimes, it is more efficient to scan multiple nodes in parallel\\n\\nUnless you have trivially small amounts of data on each node, in which case it doesn't matter, any number that's going to fit in memory is going to be better served by sequential scanning since the odds are excellent that you won't have to cross to another node.\", 'first cut at http://github.com/phatduckk/Cassandra/commit/b535b00f2917995f93f5838a98e08931e2b52680\\nI did a bunch of refactoring to get a lot of the get*Endpoint*() type methods to take an offset.\\n\\nstill need to work on the replicas.\\n\\nanyways... wanted feedback on the approach. anyone wanna take a look and lemme know what you think?', 'looks good, modulo the headers being in all the diffs (whitespace?)\\n\\n> still need to work on the replicas\\n\\nall you need to do is change\\n\\ncommand = new RangeCommand(command.table, command.columnFamily, command.startWith, command.stopAt, command.maxResults - rangeKeys.size());\\n\\nto pass in the last key from the previous node as the start.  right?  or are you talking about something else?', 'attempt at #212\\n\\nunit tests and nosetests pass', 'i\\'m a little confused by the git commit message.\\n\\nwhich of those are \"mission accomplished\" and which are \"to-dos\" if any? :)', \"we still need this part\\n\\n> this needs a bool added to RangeReply that is true if it stopped b/c the next local key was not part of the range, or the asked-for limit was reached\\n\\notherwise we have to do extra queries for every range command that doesn't hit the max results or the end key.\\n\\nalso,\\n\\n            while (endPoint != null)\\n\\n                if (endPoint.toString().equals(firstEndpoint)) break\\n\\nthese seem redundant, shouldn't it be while (true) if (endpoint == null) { throw } ?\\n\", \"created an object called RangeResult that encapsulates the keys and a boolean flag called isInRange.\\n\\ni plumbed this thru RangeReply and to StorageProxy.getKeyRange\\n\\nthere's a squashed commit with all changes at:\\nhttp://github.com/phatduckk/Cassandra/commit/a5b2d264a1567a97b04e6d7874e0114a3ebd32d8\\n\\nis the RangeResult stuff ok? It seemed to be the best way to flag whether a node had gone thru its entire range or not\\n\\n(ignore ws diff - i'll prune those when i make a patch)\", 'Looks good to me overall.\\n\\nRangeResult looks an awful lot like RangeReply w/ less methods.  Could we just use RangeReply and cut out the middleman?  I dunno.  Java feels so clunky here.\\n\\nOne way to return multiple values would be like this\\n\\n        return new HashMap<String, Object>() {{\\n            put(\"keys\", listOfStrings);\\n            put(\"finished\", true);\\n        }};\\n\\nWho says constructor blocks aren\\'t useful? :)\\n\\nCan we the bool in RangeReply it som \n",
            "my_comment:  [\"All this needs is a bool added to RangeReply that is true if it stopped b/c the next local key was not part of the range or the asked-for limit was reached.\n",
            "\n",
            "If a reply comes back with that bool false then the coordinator node the one talking to the client sends the query to the next node in the ring down from the one it just got a reply from and appends those results to the list its going to return to the client.\n",
            "\n",
            "Repeat until the im-done-bool comes back true.\n",
            "\" Sounds like a pretty serious bug in the general case but might not happen very often in practice. (the relevant starting point is StorageProxy.getKeyRange) This can be a bit tricky because the key ranges btw 2 consecutive nodes overlap (because of replication). When moving to the next node you want to be careful not to pick up duplicated keys.\n",
            "\n",
            "Also the approach that Jonathan described forces nodes to be scanned sequentially. Sometimes it is more efficient to scan multiple nodes in parallel especially if maxResult is unspecified. \"> When moving to the next node you want to be careful not to pick up duplicated keys. \n",
            "\n",
            "that is why you give the last value received from node #1 as the start_with parameter to node #2.  so at most there will be one duplicate.\n",
            "\n",
            "> Sometimes it is more efficient to scan multiple nodes in parallel\n",
            "\n",
            "Unless you have trivially small amounts of data on each node in which case it doesnt matter any number thats going to fit in memory is going to be better served by sequential scanning since the odds are excellent that you wont have to cross to another node.\" first cut at http://github.com/phatduckk/Cassandra/commit/b535b00f2917995f93f5838a98e08931e2b52680\n",
            "I did a bunch of refactoring to get a lot of the get*Endpoint*() type methods to take an offset.\n",
            "\n",
            "still need to work on the replicas.\n",
            "\n",
            "anyways... wanted feedback on the approach. anyone wanna take a look and lemme know what you think? looks good modulo the headers being in all the diffs (whitespace?)\n",
            "\n",
            "> still need to work on the replicas\n",
            "\n",
            "all you need to do is change\n",
            "\n",
            "command = new RangeCommand(command.table command.columnFamily command.startWith command.stopAt command.maxResults - rangeKeys.size());\n",
            "\n",
            "to pass in the last key from the previous node as the start.  right?  or are you talking about something else? attempt at #212\n",
            "\n",
            "unit tests and nosetests pass i\\m a little confused by the git commit message.\n",
            "\n",
            "which of those are \"mission accomplished\" and which are \"to-dos\" if any? :) \"we still need this part\n",
            "\n",
            "> this needs a bool added to RangeReply that is true if it stopped b/c the next local key was not part of the range or the asked-for limit was reached\n",
            "\n",
            "otherwise we have to do extra queries for every range command that doesnt hit the max results or the end key.\n",
            "\n",
            "also\n",
            "\n",
            "            while (endPoint != null)\n",
            "\n",
            "                if (endPoint.toString().equals(firstEndpoint)) break\n",
            "\n",
            "these seem redundant shouldnt it be while (true) if (endpoint == null) { throw } ?\n",
            "\" \"created an object called RangeResult that encapsulates the keys and a boolean flag called isInRange.\n",
            "\n",
            "i plumbed this thru RangeReply and to StorageProxy.getKeyRange\n",
            "\n",
            "theres a squashed commit with all changes at:\n",
            "http://github.com/phatduckk/Cassandra/commit/a5b2d264a1567a97b04e6d7874e0114a3ebd32d8\n",
            "\n",
            "is the RangeResult stuff ok? It seemed to be the best way to flag whether a node had gone thru its entire range or not\n",
            "\n",
            "(ignore ws diff - ill prune those when i make a patch)\" Looks good to me overall.\n",
            "\n",
            "RangeResult looks an awful lot like RangeReply w/ less methods.  Could we just use RangeReply and cut out the middleman?  I dunno.  Java feels so clunky here.\n",
            "\n",
            "One way to return multiple values would be like this\n",
            "\n",
            "        return new HashMap<String Object>() {{\n",
            "            put(\"keys\" listOfStrings);\n",
            "            put(\"finished\" true);\n",
            "        }};\n",
            "\n",
            "Who says constructor blocks aren\\t useful? :)\n",
            "\n",
            "Can we the bool in RangeReply it som \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-235\n",
            "issue_type:  Sub-task \n",
            "summary:  Move system CFs into own Table \n",
            "description: None\n",
            "comments: None\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8984\n",
            "issue_type:  Improvement \n",
            "summary:  Introduce Transactional API for behaviours that can corrupt system state \n",
            "description:  As a penultimate (and probably final for 2.1, if we agree to introduce it there) round of changes to the internals managing sstable writing, I've introduced a new API called \"Transactional\" that I hope will make it much easier to write correct behaviour. As things stand we conflate a lot of behaviours into methods like \"close\" - the recent changes unpicked some of these, but didn't go far enough. My proposal here introduces an interface designed to support four actions (on top of their normal function):\n",
            "comments:  [\"Patch available [here|https://github.com/belliottsmith/cassandra/commits/transactional-writers]\\n\\n[~krummas] [~JoshuaMcKenzie] WDYT? You've seen the ugliness of these codepaths more than anyone else: does this go some way towards sanitising them? The final cleanup will have to wait until CASSANDRA-8568 and CASSANDRA-7066, but I think this gets us close to the finish line.\\n\\nThe main annoying thing about this patch is that we've always used close() to complete a change, and now this will rollback a change without a preceding commit, which means it's possible I've missed the introduction of a commit() somewhere if it isn't covered by a unit test (I've tried to search exhaustively for occurrences, but this is hard to be certain about). Hopefully that would become apparent very quickly, though, and I don't see a good alternative for safe rollback. \", 'All unit tests are timing out on Windows - attaching a sample of one to this ticket. I\\'ve reviewed most of the patch and will leave feedback for what I have thus far - I still need to digest the changes to SSTableWriter as they\\'re fairly extensive.\\n\\nComments:\\n* I\\'m not too keen on prepareToCommit being part of the usage pattern without actually being present in the interface but I can see why you went that route. I don\\'t have any alternative suggestions unfortunately.\\n* The passing around of Throwables to merge deep in the stack (Transactional, SafeMemory*, Ref* etc) is a little cludgy and has some pretty deeply nested tag-along variables. Not sure why we can\\'t just return a Throwable up that stack and merge at the top level when we know we might need to merge rather than passing the Throwable all the way down from the top to merge at the bottom...?\\n* The StateManager abstraction and the Transactional Interface seem to be a poor fit to several of the implementers. Having 2/3 of the methods resolve to the noOpTransition seems like we\\'re conflating the idea of \"classes that have resources that need to be cleaned up\" with \"classes that have a set of state transitions they go through and a logical abort process\".\\n* With regard to the StateManager requiring a beginTransition / rejectedTransition combo and specific completeTransition - we\\'re trading one set of manually managed states for another. Still error-prone and has quite a bit of duplication where it\\'s implemented (rather than noOpTransition)\\n* autoclose seems to have some redundant assignment - we switch on state and if it\\'s COMMITTED, we set state = COMMITTED, ABORTED we set it to ABORTED.\\n* Consider renaming StateManager.autoclose(). Something like \\'finalize()\\' might be more accurate, as \\'state.autoclose()\\' describes the context in which it\\'s called rather than what it\\'s doing.\\n\\nnits:\\n* Inconsistent prepareForCommit vs. prepareToCommit in comment in Transactional\\n* Unused Logger added to IndexSummary -> was this intentional?\\n* You left a comment in SSTRW.prepareToCommitAndMaybeThrow that should be removed:\\n{noformat}\\n// No early open to finalize and replace\\n{noformat}\\n\\nIn general this patch and the recent trend in our code-base on the 2.1+ branches makes me uneasy. Moving the state tracking logic from within the SSTRW and SSTW into their own abstraction helps separate our concerns and increase modularity at the cost of increased complexity w/regards to the depth of the type system and object interaction, similarly to the introduction of the formalized ref-counting infrastructure. Each additional step we\\'ve take to shore up our stability w/regards to SSTable lifecycles is increasing our net complexity and the contrast between where we started and where we are now is pretty striking. Now, that\\'s not to say that I prefer the alternative of being back where we started with regards to having an error-prone brittle interface for ref-counting for instance, but in general I\\'m left feeling wary when I see more wide-spread changes in the same vein particularly as we\\'re approaching a .4 release on 2. \n",
            "my_comment: does this go some way towards sanitising them? The final cleanup will have to wait until CASSANDRA-8568 and CASSANDRA-7066 but I think this gets us close to the finish line.\n",
            "\n",
            "e fairly extensive.\n",
            "\n",
            "Comments:\n",
            "* I\\m not too keen on prepareToCommit being part of the usage pattern without actually being present in the interface but I can see why you went that route. I don\\t have any alternative suggestions unfortunately.\n",
            "* The passing around of Throwables to merge deep in the stack (Transactional SafeMemory* Ref* etc) is a little cludgy and has some pretty deeply nested tag-along variables. Not sure why we can\\t just return a Throwable up that stack and merge at the top level when we know we might need to merge rather than passing the Throwable all the way down from the top to merge at the bottom...?\n",
            "e conflating the idea of \"classes that have resources that need to be cleaned up\" with \"classes that have a set of state transitions they go through and a logical abort process\".\n",
            "e trading one set of manually managed states for another. Still error-prone and has quite a bit of duplication where it\\s implemented (rather than noOpTransition)\n",
            "* autoclose seems to have some redundant assignment - we switch on state and if it\\s COMMITTED we set state = COMMITTED ABORTED we set it to ABORTED.\n",
            "* Consider renaming StateManager.autoclose(). Something like \\finalize()\\ might be more accurate as \\state.autoclose()\\ describes the context in which it\\s called rather than what it\\s doing.\n",
            "\n",
            "nits:\n",
            "* Inconsistent prepareForCommit vs. prepareToCommit in comment in Transactional\n",
            "* Unused Logger added to IndexSummary -> was this intentional?\n",
            "* You left a comment in SSTRW.prepareToCommitAndMaybeThrow that should be removed:\n",
            "{noformat}\n",
            "// No early open to finalize and replace\n",
            "{noformat}\n",
            "\n",
            "e approaching a .4 release on 2. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2227\n",
            "issue_type:  Improvement \n",
            "summary:  add cache loading to row/key cache tests \n",
            "description: None\n",
            "comments: None\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4750\n",
            "issue_type:  New Feature \n",
            "summary:  Add jmx/nodetool methods to enable/disable hinted handoff \n",
            "description:  Title says it all. \n",
            "comments:  ['Methods to enable/disable using JMX have already been created.', \"Hmm, this looks good, but I'm not sure the behavior for setHintedHandoffEnabled is entirely correct.  It prevents future hints, but delivery can still occur for existing hints.  So if you encounter a situation where you have a ton of stored hints and just want to stop them, you're kind of stuck.\", 'I think that would be good to save current methods and add methods for pause/resume hints delivery processes. So we will have separate methods to disable/enable the future hints storing and for pause/resume hints delivery processes. It will be implemented in HHOM as boolean flag that could be changed through JMX and nodetool. Also that flag should be save own state in system.local CF (patch will be creaed only for 1.2 version).\\n\\nWhat do you think about it? ', \"Brandon, let's try to figure out what we want to do in this ticket. \\na) we want to get 4 methods: for enable/disable and for pause/resume hints delivery process. \\nWe will have ability for full control of hints delivery process.\\nb) we want to get 2 methods: (enable future hints storing + resume hints delivery process) and (disable future hints storing + pause hints delivery process) methods.\\nWe will have ability to stop/start the current hints delivery and future hints storing processes together.\\nc) we want to get 2 methods: for resume/pause hints delivery process.\\nWe will have ability to pause/resume the current hints delivery process only (without of any ability to control future hints storing).\\n\\nSo what do you think about it?\", \"I'm fine with a), but I don't see any reason to persist any flags in the system CF, that's what the yaml is for.\", 'Please review the patch.', \"v3 fixes the yaml indentation so it parse correctly, adds a check/break out of the inner loop over the page size, and finally logs that hints are paused at the end so it's clear that they may not all be delivered.\", 'Alexey, can you review v3?', 'This looks good to me.', 'Committed.'] \n",
            "my_comment: for resume/pause hints delivery process.\n",
            "We will have ability to pause/resume the current hints delivery process only (without of any ability to control future hints storing).\n",
            "\n",
            "So what do you think about it?\" \"Im fine with a) but I dont see any reason to persist any flags in the system CF thats what the yaml is for.\" Please review the patch. \"v3 fixes the yaml indentation so it parse correctly adds a check/break out of the inner loop over the page size and finally logs that hints are paused at the end so its clear that they may not all be delivered.\" Alexey can you review v3? This looks good to me. Committed. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-15375\n",
            "issue_type:  Bug \n",
            "summary:  Remove BackPressureStrategy \n",
            "description:  This is odd:\n",
            "comments:  [\"Which version of Cassandra are we discussing here?\\r\\n\\r\\nMy preference is to remove the back pressure feature entirely, since it's never really been tested at scale that I know of.  In 4.0 we have the capability of pushing the actual internode backpressure back to clients, in a similar manner to [~sumanth.pasupuleti] achieved recently in CASSANDRA-15013\\r\\n\\r\\nBut if that's too contentious, sure, let's clean up our logging :)\", \"I found this in trunk, but assume it's elsewhere.\\r\\n\\r\\nI don't have any test data showing it's affect on performance either way, I've never enabled it.\", '[~benedict] I think we might be a bit late to remove the back pressure mechanism from 4.0.  I intend to fix the logging, and maybe we evaluate it for removal in the next release?', \"It's never too late: it's about 5m of work to remove, it has precisely two insertion points in the code, and it's never actually used.  If we are interested at all in doing it, I can do it right now.\", 'bq. My preference is to remove the back pressure feature entirely, since it\\'s never really been tested at scale that I know of.\\r\\n\\r\\nA large chunk of the work for 4.0 is testing, so removing something because its not tested isn\\'t really fair since this wouldn\\'t be unique. Also, based off this JIRA, you can disable it; so don\\'t see a reason to remove.\\r\\n\\r\\nbq. In 4.0 we have the capability of pushing the actual internode backpressure back to clients\\r\\n\\r\\nIs there a jira for actually testing the different back pressures we have before 4.0 releases?  If not can we?\\r\\n\\r\\nI would personally love to see \"this is how 4.0 doesn\\'t crash under load\" and tests to back it up; if there is a test for this already do let me know =)', \"bq. since it's never really been tested at scale that I know of\\r\\n\\r\\n^ This was a euphemism for “this feature has never been used, and is probably bad”.\\xa0 It was implemented some time ago by DataStax, never advertised in any way by OSS, and has never ben updated (making it either the first perfect feature, or broken).\\xa0 It has perhaps been used by DataStax in their own offerings, but never by OSS. \\xa0It is unlikely (m?)any even know it exists.\\r\\n\\r\\nGiven the 4.0 networking changes, this feature no longer provides any utility for stability.  We now limit the amount of data inbound from any specific (and all) coordinators so that we cannot be overwhelmed, and vice-versa, and this happens instantly i.e. responsively*.\\r\\n\\r\\nThis feature, however, makes some basic implementation errors, and appears to have several problematic semantics, particularly with vnodes, responsiveness and choppiness (imposing three arbitrary rates of LOW, HIGH, INFINITE for all unique combination of message recipient (probably really problematic with vnodes, and high RF), updated once every WriteRpcTimeout - assuming the system clock doesn’t get updated by e.g. NTP).\\r\\n\\r\\nThe only behaviour missing from internode is the ability to notify clients of back pressure, either by propagating to the client connection or by throwing overloaded exceptions.  However this is also implemented poorly here, “applying backpressure” by consuming a {{RequestPoolExecutor}} thread until permitted to proceed.  Thanks to CASSANDRA-15013 this will only be suboptimal, but prior to 4.0 this would have lead to really problematic cluster behaviours.\\r\\n\\r\\nIt’s worth noting that the above was all perhaps a reasonable set of trade-offs when first implemented, though the original ticket lead to a great deal of debate about the reasonableness of the approach (CASSANDRA-9318).  However it also suggests to me we are better removing this unused, unmaintained feature that is no longer particularly needed, and if we have time implementing the version that makes sense in the current context.\\r\\n\\r\\n(*That all said, 4.0 stability at scale is part of the 4.0 testing plan, and determining reasonable numbers for the limits is a remaining exercise - they are almost certainly too high today to gua \n",
            "my_comment: its about 5m of work to remove it has precisely two insertion points in the code and its never actually used.  If we are interested at all in doing it I can do it right now.\" bq. My preference is to remove the back pressure feature entirely since it\\s never really been tested at scale that I know of.\n",
            "\n",
            "A large chunk of the work for 4.0 is testing so removing something because its not tested isn\\t really fair since this wouldn\\t be unique. Also based off this JIRA you can disable it; so don\\t see a reason to remove.\n",
            "\n",
            "bq. In 4.0 we have the capability of pushing the actual internode backpressure back to clients\n",
            "\n",
            "Is there a jira for actually testing the different back pressures we have before 4.0 releases?  If not can we?\n",
            "\n",
            "I would personally love to see \"this is how 4.0 doesn\\t crash under load\" and tests to back it up; if there is a test for this already do let me know =) \"bq. since its never really been tested at scale that I know of\n",
            "\n",
            "^ This was a euphemism for “this feature has never been used and is probably bad”.\\xa0 It was implemented some time ago by DataStax never advertised in any way by OSS and has never ben updated (making it either the first perfect feature or broken).\\xa0 It has perhaps been used by DataStax in their own offerings but never by OSS. \\xa0It is unlikely (m?)any even know it exists.\n",
            "\n",
            "Given the 4.0 networking changes this feature no longer provides any utility for stability.  We now limit the amount of data inbound from any specific (and all) coordinators so that we cannot be overwhelmed and vice-versa and this happens instantly i.e. responsively*.\n",
            "\n",
            "This feature however makes some basic implementation errors and appears to have several problematic semantics particularly with vnodes responsiveness and choppiness (imposing three arbitrary rates of LOW HIGH INFINITE for all unique combination of message recipient (probably really problematic with vnodes and high RF) updated once every WriteRpcTimeout - assuming the system clock doesn’t get updated by e.g. NTP).\n",
            "\n",
            "The only behaviour missing from internode is the ability to notify clients of back pressure either by propagating to the client connection or by throwing overloaded exceptions.  However this is also implemented poorly here “applying backpressure” by consuming a {{RequestPoolExecutor}} thread until permitted to proceed.  Thanks to CASSANDRA-15013 this will only be suboptimal but prior to 4.0 this would have lead to really problematic cluster behaviours.\n",
            "\n",
            "It’s worth noting that the above was all perhaps a reasonable set of trade-offs when first implemented though the original ticket lead to a great deal of debate about the reasonableness of the approach (CASSANDRA-9318).  However it also suggests to me we are better removing this unused unmaintained feature that is no longer particularly needed and if we have time implementing the version that makes sense in the current context.\n",
            "\n",
            "(*That all said 4.0 stability at scale is part of the 4.0 testing plan and determining reasonable numbers for the limits is a remaining exercise - they are almost certainly too high today to gua \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-18681\n",
            "issue_type:  Bug \n",
            "summary:  Internode legacy SSL storage port certificate is not hot reloaded on update \n",
            "description:  In CASSANDRA-16666 the SSLContext cache was changed to clear individual {{EncryptionOptions}} from the SslContext cache if they needed reloading to reduce resource consumption. Before the change if ANY cert needed hot reloading, the SSLContext cache would be cleared for ALL certs.\n",
            "comments:  ['4.1 [Branch|https://github.com/jonmeredith/cassandra/tree/C18681-4.1] [PR|https://github.com/apache/cassandra/pull/2693]\\r\\n5.0 [Branch|https://github.com/jonmeredith/cassandra/tree/C18681-5.0] [PR|https://github.com/apache/cassandra/pull/2694]\\r\\nTrunk [Branch|https://github.com/jonmeredith/cassandra/tree/C18681-trunk] [PR|https://github.com/apache/cassandra/pull/2695]\\r\\n\\r\\nCI Results (pending):\\r\\n||Branch||Source||Circle CI||Jenkins||\\r\\n|cassandra-4.1|[branch|https://github.com/jonmeredith/cassandra/tree/commit_remote_branch/CASSANDRA-18681-cassandra-4.1-27E812B5-58D5-44D7-8C5E-3B0D3AA5F767]|[build|https://app.circleci.com/pipelines/github/jonmeredith/cassandra?branch=commit_remote_branch%2FCASSANDRA-18681-cassandra-4.1-27E812B5-58D5-44D7-8C5E-3B0D3AA5F767]|[build|https://ci-cassandra.apache.org/job/Cassandra-devbranch/2595/]|\\r\\n|cassandra-5.0|[branch|https://github.com/jonmeredith/cassandra/tree/commit_remote_branch/CASSANDRA-18681-cassandra-5.0-27E812B5-58D5-44D7-8C5E-3B0D3AA5F767]|[build|https://app.circleci.com/pipelines/github/jonmeredith/cassandra?branch=commit_remote_branch%2FCASSANDRA-18681-cassandra-5.0-27E812B5-58D5-44D7-8C5E-3B0D3AA5F767]|[build|https://ci-cassandra.apache.org/job/Cassandra-devbranch/2596/]|\\r\\n|trunk|[branch|https://github.com/jonmeredith/cassandra/tree/commit_remote_branch/CASSANDRA-18681-trunk-27E812B5-58D5-44D7-8C5E-3B0D3AA5F767]|[build|https://app.circleci.com/pipelines/github/jonmeredith/cassandra?branch=commit_remote_branch%2FCASSANDRA-18681-trunk-27E812B5-58D5-44D7-8C5E-3B0D3AA5F767]|[build|unknown]|\\r\\n', '+1, thanks for the patch!', \"Going to rework a little, I don't like the different check between shouldReload and clearSslContext. It's ok for the default implementation, but may not be good for a custom SSLContextFactoryInstance.\", \"I've remembered why I did it this way. The legacy ssl storage port encryption options are not registered for hot reloading, so you have to match invalidate if the original encryption options shouldReload returned true.\\r\\n\", '+1, looks good to me', 'Refactored to just explicitly add initialize the legacy ssl encryption options.\\r\\n\\r\\nCI Results (pending):\\r\\n||Branch||Source||Circle CI||Jenkins||\\r\\n|cassandra-4.1|[branch|https://github.com/jonmeredith/cassandra/tree/commit_remote_branch/CASSANDRA-18681-cassandra-4.1-B319E212-DEE9-4BD5-8FA1-CEB9D630C414]|[build|https://app.circleci.com/pipelines/github/jonmeredith/cassandra?branch=commit_remote_branch%2FCASSANDRA-18681-cassandra-4.1-B319E212-DEE9-4BD5-8FA1-CEB9D630C414]|[build|https://ci-cassandra.apache.org/job/Cassandra-devbranch/2598/]|\\r\\n|cassandra-5.0|[branch|https://github.com/jonmeredith/cassandra/tree/commit_remote_branch/CASSANDRA-18681-cassandra-5.0-B319E212-DEE9-4BD5-8FA1-CEB9D630C414]|[build|https://app.circleci.com/pipelines/github/jonmeredith/cassandra?branch=commit_remote_branch%2FCASSANDRA-18681-cassandra-5.0-B319E212-DEE9-4BD5-8FA1-CEB9D630C414]|[build|https://ci-cassandra.apache.org/job/Cassandra-devbranch/2599/]|\\r\\n|trunk|[branch|https://github.com/jonmeredith/cassandra/tree/commit_remote_branch/CASSANDRA-18681-trunk-B319E212-DEE9-4BD5-8FA1-CEB9D630C414]|[build|https://app.circleci.com/pipelines/github/jonmeredith/cassandra?branch=commit_remote_branch%2FCASSANDRA-18681-trunk-B319E212-DEE9-4BD5-8FA1-CEB9D630C414]|[build|unknown]|', 'Also included a minor fix to examples/sslfactory/build.xml to resolve an error about duplicate logback libraries on the class path.', \"None of the above CircleCI runs contains the repeated runs of the modified files ({{DefaultSslContextFactoryTest}}, {{PEMBasedSslContextFactoryTest}} and {{SSLFactoryTest}}). I think this is due to a bug in the non-public script used to generate the CircleCI config file.\\r\\n\\r\\nThose repeated runs can be generated with the project's [{{.circleci/generate.sh}}|https://github.com/apache/cassandra/blob/trunk/.circleci/generate.sh] script.\\r\\n\\r\\nThe absence of repeated runs can be easily detected by looking at the CI resul \n",
            "my_comment:  4.1 [Branch|https://github.com/jonmeredith/cassandra/tree/C18681-4.1] [PR|https://github.com/apache/cassandra/pull/2693]\n",
            "5.0 [Branch|https://github.com/jonmeredith/cassandra/tree/C18681-5.0] [PR|https://github.com/apache/cassandra/pull/2694]\n",
            "Trunk [Branch|https://github.com/jonmeredith/cassandra/tree/C18681-trunk] [PR|https://github.com/apache/cassandra/pull/2695]\n",
            "\n",
            "CI Results (pending):\n",
            "||Branch||Source||Circle CI||Jenkins||\n",
            "|cassandra-4.1|[branch|https://github.com/jonmeredith/cassandra/tree/commit_remote_branch/CASSANDRA-18681-cassandra-4.1-27E812B5-58D5-44D7-8C5E-3B0D3AA5F767]|[build|https://app.circleci.com/pipelines/github/jonmeredith/cassandra?branch=commit_remote_branch%2FCASSANDRA-18681-cassandra-4.1-27E812B5-58D5-44D7-8C5E-3B0D3AA5F767]|[build|https://ci-cassandra.apache.org/job/Cassandra-devbranch/2595/]|\n",
            "|cassandra-5.0|[branch|https://github.com/jonmeredith/cassandra/tree/commit_remote_branch/CASSANDRA-18681-cassandra-5.0-27E812B5-58D5-44D7-8C5E-3B0D3AA5F767]|[build|https://app.circleci.com/pipelines/github/jonmeredith/cassandra?branch=commit_remote_branch%2FCASSANDRA-18681-cassandra-5.0-27E812B5-58D5-44D7-8C5E-3B0D3AA5F767]|[build|https://ci-cassandra.apache.org/job/Cassandra-devbranch/2596/]|\n",
            "|trunk|[branch|https://github.com/jonmeredith/cassandra/tree/commit_remote_branch/CASSANDRA-18681-trunk-27E812B5-58D5-44D7-8C5E-3B0D3AA5F767]|[build|https://app.circleci.com/pipelines/github/jonmeredith/cassandra?branch=commit_remote_branch%2FCASSANDRA-18681-trunk-27E812B5-58D5-44D7-8C5E-3B0D3AA5F767]|[build|unknown]|\n",
            " +1 thanks for the patch! \"Going to rework a little I dont like the different check between shouldReload and clearSslContext. Its ok for the default implementation but may not be good for a custom SSLContextFactoryInstance.\" \"Ive remembered why I did it this way. The legacy ssl storage port encryption options are not registered for hot reloading so you have to match invalidate if the original encryption options shouldReload returned true.\n",
            "\" +1 looks good to me Refactored to just explicitly add initialize the legacy ssl encryption options.\n",
            "\n",
            "CI Results (pending):\n",
            "||Branch||Source||Circle CI||Jenkins||\n",
            "|cassandra-4.1|[branch|https://github.com/jonmeredith/cassandra/tree/commit_remote_branch/CASSANDRA-18681-cassandra-4.1-B319E212-DEE9-4BD5-8FA1-CEB9D630C414]|[build|https://app.circleci.com/pipelines/github/jonmeredith/cassandra?branch=commit_remote_branch%2FCASSANDRA-18681-cassandra-4.1-B319E212-DEE9-4BD5-8FA1-CEB9D630C414]|[build|https://ci-cassandra.apache.org/job/Cassandra-devbranch/2598/]|\n",
            "|cassandra-5.0|[branch|https://github.com/jonmeredith/cassandra/tree/commit_remote_branch/CASSANDRA-18681-cassandra-5.0-B319E212-DEE9-4BD5-8FA1-CEB9D630C414]|[build|https://app.circleci.com/pipelines/github/jonmeredith/cassandra?branch=commit_remote_branch%2FCASSANDRA-18681-cassandra-5.0-B319E212-DEE9-4BD5-8FA1-CEB9D630C414]|[build|https://ci-cassandra.apache.org/job/Cassandra-devbranch/2599/]|\n",
            "|trunk|[branch|https://github.com/jonmeredith/cassandra/tree/commit_remote_branch/CASSANDRA-18681-trunk-B319E212-DEE9-4BD5-8FA1-CEB9D630C414]|[build|https://app.circleci.com/pipelines/github/jonmeredith/cassandra?branch=commit_remote_branch%2FCASSANDRA-18681-trunk-B319E212-DEE9-4BD5-8FA1-CEB9D630C414]|[build|unknown]| Also included a minor fix to examples/sslfactory/build.xml to resolve an error about duplicate logback libraries on the class path. \"None of the above CircleCI runs contains the repeated runs of the modified files ({{DefaultSslContextFactoryTest}} {{PEMBasedSslContextFactoryTest}} and {{SSLFactoryTest}}). I think this is due to a bug in the non-public script used to generate the CircleCI config file.\n",
            "\n",
            "Those repeated runs can be generated with the projects [{{.circleci/generate.sh}}|https://github.com/apache/cassandra/blob/trunk/.circleci/generate.sh] script.\n",
            "\n",
            "The absence of repeated runs can be easily detected by looking at the CI resul \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7496\n",
            "issue_type:  Bug \n",
            "summary:  ClassCastException in MessagingService \n",
            "description:  Got the following exception when running repair on a 3 node ccm cluster\n",
            "comments:  [\"Looks like the cast to WCI should be moved back after the shouldHint() check as CallbackInfo.shouldHint() is hardcoded to return false, thus it could never downcast anyway.\\n\\nSo probably this will fix it (move the cast after the shouldHint() check):\\n{code}\\n        if (expiredCallbackInfo.shouldHint())\\n        {\\n            try\\n            {\\n                Mutation mutation = (Mutation) ((WriteCallbackInfo) expiredCallbackInfo).sentMessage.payload;\\n                return StorageProxy.submitHint(mutation, expiredCallbackInfo.target, null);\\n            }\\n            finally\\n            {\\n                //We serialized a hint so we don't need this mutation anymore\\n                mutation.release();\\n            }\\n        }\\n{code} \\n\", \"Patch with your fix attached.  Can you describe your ccm scenario that caused this?  I'd like to add a dtest for this.\", \"Committed so we can roll rc3.\\n\\nIf you want to follow up with a test, it looks to me like this will happen 100% of the time when we have a callback on a non-write, expired message.  (Not actually sure what this includes -- we have lots of non-write messages with callbacks, but most aren't allowed to expire.  Clearly there must be some.)\", \"[~tjake] I produced this by building a 3 node cluster in ccm. Then I ran 'stress write' to get some data into the cluster (with RF=2), calling flush every tens seconds so I could monitor the amount of data per node (didn't need much data, just something to play with). Then I ran \\n{code}ccm node1 nodetool repair Keyspace1 Standard1{code}\\n, node1 would get the exception in under 30 seconds (usually occurring a few times during the span over the repair). \\n\\nAs I was testing out some changes I'm working on, I don't believe the cause of the messages timing out during repair is a problem in 2.1 (just my own bugs to work out on my branch).\"] \n",
            "my_comment:  [\"Looks like the cast to WCI should be moved back after the shouldHint() check as CallbackInfo.shouldHint() is hardcoded to return false thus it could never downcast anyway.\n",
            "\n",
            "So probably this will fix it (move the cast after the shouldHint() check):\n",
            "{code}\n",
            "        if (expiredCallbackInfo.shouldHint())\n",
            "        {\n",
            "            try\n",
            "            {\n",
            "                Mutation mutation = (Mutation) ((WriteCallbackInfo) expiredCallbackInfo).sentMessage.payload;\n",
            "                return StorageProxy.submitHint(mutation expiredCallbackInfo.target null);\n",
            "            }\n",
            "            finally\n",
            "            {\n",
            "                //We serialized a hint so we dont need this mutation anymore\n",
            "                mutation.release();\n",
            "            }\n",
            "        }\n",
            "{code} \n",
            "\" \"Patch with your fix attached.  Can you describe your ccm scenario that caused this?  Id like to add a dtest for this.\" \"Committed so we can roll rc3.\n",
            "\n",
            "If you want to follow up with a test it looks to me like this will happen 100% of the time when we have a callback on a non-write expired message.  (Not actually sure what this includes -- we have lots of non-write messages with callbacks but most arent allowed to expire.  Clearly there must be some.)\" \"[~tjake] I produced this by building a 3 node cluster in ccm. Then I ran stress write to get some data into the cluster (with RF=2) calling flush every tens seconds so I could monitor the amount of data per node (didnt need much data just something to play with). Then I ran \n",
            "{code}ccm node1 nodetool repair Keyspace1 Standard1{code}\n",
            " node1 would get the exception in under 30 seconds (usually occurring a few times during the span over the repair). \n",
            "\n",
            "As I was testing out some changes Im working on I dont believe the cause of the messages timing out during repair is a problem in 2.1 (just my own bugs to work out on my branch).\"] \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2138\n",
            "issue_type:  Improvement \n",
            "summary:  add option to enable efficient cross-dc replication \n",
            "description:  In CASSANDRA-1530 we made cross-dc replication more efficient, but 0.7.0 will ignore any forwarding messages it receives, so let's make this optional to allow smoother upgrades. \n",
            "comments:  ['attached patch gossips ReleaseVersionString and enables 1530 (StorageService.useEfficientCrossDCWrites) when the entire cluster is on 0.7.1 or higher', '{code}\\nGossiper.instance.addLocalApplicationState(ApplicationState.RELEASE_VERSION, valueFactory.releaseVersion());\\n{code}\\nneeds to be moved to joinTokenRing() after the gossiper has been started or it will NPE.\\n\\n+1 with that change.', 'committed', 'Integrated in Cassandra-0.7 #263 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/263/])\\n    enable #1530 only after cluster is all on 0.7.1\\n> patch by jbellis; reviewed by brandonwilliams for CASSANDRA-2138\\n'] \n",
            "my_comment:  attached patch gossips ReleaseVersionString and enables 1530 (StorageService.useEfficientCrossDCWrites) when the entire cluster is on 0.7.1 or higher {code}\n",
            "Gossiper.instance.addLocalApplicationState(ApplicationState.RELEASE_VERSION valueFactory.releaseVersion());\n",
            "{code}\n",
            "needs to be moved to joinTokenRing() after the gossiper has been started or it will NPE.\n",
            "\n",
            "+1 with that change. committed Integrated in Cassandra-0.7 #263 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/263/])\n",
            "    enable #1530 only after cluster is all on 0.7.1\n",
            "> patch by jbellis; reviewed by brandonwilliams for CASSANDRA-2138\n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1035\n",
            "issue_type:  Improvement \n",
            "summary:  Implement User/Keyspace throughput Scheduler \n",
            "description:  To support multiple applications on top of a single Cassandra cluster (and to protect against badly behaving clients) having a very simple scheduler for client operations would be very beneficial.\n",
            "comments:  [\"I don't think scheduling of MUTATION-STAGE can solve this; if a user or app is pouring enough ops in to affect QOS of others, then it's going to cause backpressure which is of necessity per-node not per-user-or-app.\\n\\nI think there would need to be some kind of rate-limiting at the coordinator node instead, since that's the only place you can backpressure user-or-app instead of entire peer nodes.\", 'Agreed that this should be done at the coordinator node.', 'Throttling on the coordinator makes a lot more sense, good point.\\n\\nI began exploring plugging in alternative ThreadPools in Thrift and Avro, but making the implementation consistent and trying to avoid patches to those projects is enough reason to drop that idea.\\n\\nInstead, how about converting StorageProxy into a non-static class, and pooling StorageProxies? To get a StorageProxy, you would check one out from an \"IRequestScheduler\" (name tbd). Since StorageProxy is stateless, the default impl of IRS could always return the same StorageProxy, without any queueing. An alternative IRS could implement the weighted scheduling described above.\\n\\nThoughts?', \"I've always thought  it was a little weird (or that I misunderstood something) that there wasn't a separate stage there. I would think it'd make sense to have a stage just for protocol handling and a separate one for the storage proxy stuff.\", \"The reason there isn't a stage is that Thrift and Avro independently manage their own client threads/selectors inside of a black box. They will attempt to get copies of StorageProxy from within their thread pool, so we can block their threads.\", 'so you would have some fairly large number of SP instances, say 1000, and app X would get 800, and app Y and Z would get 100 each, to tune QOS?  That seems reasonable.', 'of course that assumes each app is doing the same amount of work per SP call, which is dubious in the presence of mutliget + batch_mutate.', \"For multi-tenant clusters, this will be very important so we're hoping that we can help get this done prior to the 0.7 release.\", \"Having researched all the options, the only viable ones seem to be able to schedule based on keyspace. User scheduling can happen, but requires some change with the Auth API/classes. Just providers to get the username per se. \\n\\nThere would be a configuration option to schedule based on user/keyspace/none (this would be the identifier) \\n\\nOpen Question: \\n1 - Scheduler returns TimedOutException if no available tokens for user/keyspace (considering a token based approach) \\n2 - Scheduler blocks thread/request until a token is available and services the request or times out whichever happens first. \\n\\nSince we don't control the threads/thread-scheduling, the scheduler will have to maintain some sort of bucketing system and perform wait/notify, for a round-robin approach. Ideas welcome. \\n\\nSince this is node based, there's nothing stopping a client from hitting up another controller node and the request being re-routed back to the initial node if that has the data. CASSANDRA-685 would solve that, but I'm not sure of it's status.\", \"the auth api is super alpha.  if keyspace-based scheduling can be adequate for you then I strongly suggest going with that for 0.7.\\n\\ni don't see any benefit to throwing TOE.  if client retries immediately it makes thing worse.  simply blocking seems more natural imo.\", '0001 contains the scheduler related new classes.\\n0002 has the configuration related changes\\n0003 has the CassandraServer related changes', \"Awesome work: this is a great first cut!\\n\\n* Should FairShareScheduler be renamed to RoundRobinScheduler, since it doesn't deal with time slices? (sorry, nitpick)\\n* FairShareScheduler.getQueue is racy between contains/put/get: the 'queues' object should probably be replaced with NonBlockingHashMap, and putIfAbsent should be used instead of contains/put\\n* Comments in conf/cassandra.yaml should indicate that this is specifically for _client_ requests\\n\\nAlso, sim \n",
            "my_comment: the queues object should probably be replaced with NonBlockingHashMap and putIfAbsent should be used instead of contains/put\n",
            "* Comments in conf/cassandra.yaml should indicate that this is specifically for _client_ requests\n",
            "\n",
            "Also sim \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8627\n",
            "issue_type:  Improvement \n",
            "summary:  Support Total/Recent latency histogram metrics for range slices \n",
            "description:  The Metrics histogram is pretty bad at non-normal data like latencies and (empirically tested and theoretically) is untrustworthy at 99th percentile. For applications that care about the percentiles having the more statistically accurate version is beneficial.  Adding the deprecated methods like other latency histograms for CASSANDRA-7338 temporarily would help.\n",
            "comments:  ['Committed, thanks.'] \n",
            "my_comment:  Committed thanks. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-18803\n",
            "issue_type:  Bug \n",
            "summary:  Refactor validation logic in StorageService.rebuild \n",
            "description:  This is a follow-up ticket of CASSANDRA-14319 \n",
            "comments:  ['[~aweisberg] would you mind to take a look? Super easy. On your +1 I ll do all the builds for 6 branches.', '+1 TY', '[3.0|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3035/workflows/7f55fcda-1cf0-43db-8471-ebd54be87c9e]\\r\\n\\r\\n[3.11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3034/workflows/e10a709e-97e1-4795-b4ca-7e3aac3253cd]\\r\\n\\r\\n[4.0 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/f2e2c30f-6d37-48cc-874a-104f34f67b50]\\r\\n[4.0 j8|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/af8cc33a-69ca-4164-800f-3b049a8ac6da]\\r\\n\\r\\n[4.1 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5297fd1a-f33b-4a17-b56d-d0522b65c95b]\\r\\n[4.1 j8|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5626cfef-a6a7-453c-9abf-43c30678ec41]\\r\\n\\r\\n[5.0 j17|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/70cf2550-e278-426a-a63b-533c25c4eccf]\\r\\n[5.0 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/2a5dcdc5-17f9-4311-9aeb-60fcea5787d9]\\r\\n\\r\\nThese builds are technically not same as what is in the current branches. I just moved one check outside of try-catch to add it logically where it belongs (where other checks are) and I moved logging into try catch (was outside of it).'] \n",
            "my_comment:  [~aweisberg] would you mind to take a look? Super easy. On your +1 I ll do all the builds for 6 branches. +1 TY [3.0|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3035/workflows/7f55fcda-1cf0-43db-8471-ebd54be87c9e]\n",
            "\n",
            "[3.11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3034/workflows/e10a709e-97e1-4795-b4ca-7e3aac3253cd]\n",
            "\n",
            "[4.0 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/f2e2c30f-6d37-48cc-874a-104f34f67b50]\n",
            "[4.0 j8|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/af8cc33a-69ca-4164-800f-3b049a8ac6da]\n",
            "\n",
            "[4.1 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5297fd1a-f33b-4a17-b56d-d0522b65c95b]\n",
            "[4.1 j8|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5626cfef-a6a7-453c-9abf-43c30678ec41]\n",
            "\n",
            "[5.0 j17|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/70cf2550-e278-426a-a63b-533c25c4eccf]\n",
            "[5.0 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/2a5dcdc5-17f9-4311-9aeb-60fcea5787d9]\n",
            "\n",
            "These builds are technically not same as what is in the current branches. I just moved one check outside of try-catch to add it logically where it belongs (where other checks are) and I moved logging into try catch (was outside of it). \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9134\n",
            "issue_type:  Bug \n",
            "summary:  Fix leak detected errors in unit tests \n",
            "description:  There are several of these errors when running unit tests on trunk:\n",
            "comments:  ['Most of the leaks were fixed by CASSANDRA-9117, the patch attached covers 2 remaining unit tests (other than the RefTests).', 'Hi [~thobbs]], would you mind reviewing?', '+1, committed as {{1f65a12}}.  Thanks!'] \n",
            "my_comment:  Most of the leaks were fixed by CASSANDRA-9117 the patch attached covers 2 remaining unit tests (other than the RefTests). Hi [~thobbs]] would you mind reviewing? +1 committed as {{1f65a12}}.  Thanks! \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14411\n",
            "issue_type:  Bug \n",
            "summary:  Use Bounds instead of Range to represent sstable first/last token when checking how to anticompact sstables \n",
            "description:  There is currently a chance of missing marking a token as repaired due to the fact that we use Range which are (a, b] to represent first/last token in sstables instead of Bounds which are [a, b]. \n",
            "comments:  ['https://github.com/krummas/cassandra/commits/marcuse/14411\\r\\n\\r\\ntests:\\r\\nhttps://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14411', 'minimal patches for 2.2 -> 3.11: \\r\\nhttps://github.com/krummas/cassandra/commits/marcuse/14411-2.2\\r\\nhttps://github.com/krummas/cassandra/commits/marcuse/14411-3.0\\r\\nhttps://github.com/krummas/cassandra/commits/marcuse/14411-3.11\\r\\n\\r\\ntests for 3.11: https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14411-3%2E11', 'circle seems to be down, but +1 assuming tests look good', 'committed as {{334dca9aa825e6d353aa04fd97016ac1077ff132}}, thanks!'] \n",
            "my_comment: https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14411-3%2E11 circle seems to be down but +1 assuming tests look good committed as {{334dca9aa825e6d353aa04fd97016ac1077ff132}} thanks! \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3690\n",
            "issue_type:  Bug \n",
            "summary:  Streaming CommitLog backup \n",
            "description:  Problems with the current SST backups\n",
            "comments:  [\"The socket business sounds complicated.  CASSANDRA-1602 is a lot more straightforward, I'd recommend starting with that.\", 'Hi Jonathan, But there is additional IO which the server has to do to copy the archive logs to a different location (not locally)... \\nWhile streaming the Commit log back to the server we have to copy it first and then read it back which is also a over head in recovery. \\n\\nSomething like copying the data to S3 in amazon and copying right back for the node for recovery. (this backup will also be used for test cluster refresh for prod data and BI which is completely a different system)\\nRecovery in most case are loose of instance or the whole cluster (Virtual machines).', \"0001 => Adds a configuration so we can avoid recycling in case some one wants to copy the files across to another location like a archive logs\\n0002 => Adds CommitLogListener, implementation can recive the updates to the commitlogs.\\n0003 => helper JMX in case the user wants to query the active CL's\\n0004 => this can go to the tools folder/we dont need to commit it to the core.\", 'bq. But there is additional IO which the server has to do to copy the archive logs to a different location (not locally)... While streaming the Commit log back to the server we have to copy it first and then read it back which is also a over head in recovery.\\n\\nWhat if you mounted the archive logs via s3fs?', \"We spent some time looking into s3fs, and ran into problems between fuse (which s3fs depends on) and mmap. I put together a small java app to simulate writing to local disc vs. s3fs using mmapp'ed and non-mmap'ed files. (Note most of my java sample code is based on CommitLogSegment's implementation.) We found the non-mmap'ed files wrote to s3fs without a hitch, but writing to the mmap'ed S3 mount failed. The failure was different between OpenJDK 6 vs. Sun JDK, but they were both SIGBUS errors. Also, I ran the tests on my local ubuntu box running Linux 3.0.0-12, fuse version (fusermount --version) at 2.8.4. \\n\\nAt this point, it seems like s3fs isn't as viable as one would hope.\\n\\n\\n\\n\", 'That does rule out using s3fs in read/write mode, but I imagine that would be a pretty bad idea from a latency standpoint anyway.  But in the context of just mounting log files for replay/recover, CommitLog uses RandomAccessReader which is ordinary buffered i/o.', 'Starting to think... What we really want is a Async Triggers CASSANDRA-1311 which listens for all the updates + a way to restore the data with mutation before starting the node. In someways thats what the original patch was trying to do will it make sense to merge these two efforts?', \"I'm skeptical of trying to do this on top of triggers.  First, CASSANDRA-1311 seems to lean towards coordinator-level triggers rather than replica-level.  Second, I don't think it makes sense for a Trigger-level API to deal with raw bytes, which would mean losing efficiency from having to re-serialize RowMutations.\\n\\nI like the postgresql approach: http://www.postgresql.org/docs/9.1/static/continuous-archiving.html -- briefly, you configure an {{archive_command}} that tells it how you want it to copy full log segments off-server when full, and set up a recovery.conf file when you want to recover, which includes a {{restore_command}} that is the inverse of the archive command.\\n\\nThe main difference is that postgresql's default wal segment size is 16MB, which gives them a finer resolution than our 128MB.  I can't think of a reason we can't lower ours, though.\", 'Hi Jonathan,\\n\\nAttached patch does exactly what we discussed here. Its almost the same as PostgreSQL :) \\n\\nIn addition we can start the node with -Dcassandra.join_ring=false and then use JMX to restore files one by one via JMX.\\n\\nPlz let me know.', \"I don't see any uses of CommitLogRecover outside of CommitLog, which makes me think this is an unrelated refactoring.  Is that correct?  If so, let's split that out of this ticket.\", 'Hi Jonathan, The refactor is mainly \n",
            "my_comment: http://www.postgresql.org/docs/9.1/static/continuous-archiving.html -- briefly you configure an {{archive_command}} that tells it how you want it to copy full log segments off-server when full and set up a recovery.conf file when you want to recover which includes a {{restore_command}} that is the inverse of the archive command.\n",
            "\n",
            "The main difference is that postgresqls default wal segment size is 16MB which gives them a finer resolution than our 128MB.  I cant think of a reason we cant lower ours though.\" Hi Jonathan\n",
            "\n",
            "Attached patch does exactly what we discussed here. Its almost the same as PostgreSQL :) \n",
            "\n",
            "In addition we can start the node with -Dcassandra.join_ring=false and then use JMX to restore files one by one via JMX.\n",
            "\n",
            "Plz let me know. \"I dont see any uses of CommitLogRecover outside of CommitLog which makes me think this is an unrelated refactoring.  Is that correct?  If so lets split that out of this ticket.\" Hi Jonathan The refactor is mainly \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-17904\n",
            "issue_type:  Improvement \n",
            "summary:  Consider to not warn about deprecated properties in logs when the value is not deprecated \n",
            "description:  When there is an initialisation of database descriptor for tools, for example via \"Util.initDatabaseDescriptor()\", it will eventually buble up to \"YamlConfigurationLoader.check\" where this is logged:\n",
            "comments:  ['Kindly pinging [~e.dimitrova] to raise awareness.', \"Thanks [~smiklosovic], those three are special case where only the value format was changed and their names were kept (the only 3 which had proper names and no unit as part of the name... ), that's why your valid observation - they emit deprecation warning even with the value being in the new format. You made me wonder whether it was worth it to special case them at all, I will take a look tomorrow.\\xa0\", \"Slept on it and had some discussion with [~marcuse]\\xa0and [~dcapwell]\\xa0about those three parameters as I was wondering whether we missed to take some action around them when we added the new flags - _allow_duplicate_config_keys_ and _allow_new_old_config_keys_ in\\xa0CASSANDRA-17379\\r\\n\\r\\nThe key names haven't been changed so we technically didn't deprecate the three properties but we added an option to be able to add value both in numeric and numeric+unit format.\\xa0\\r\\n\\r\\nThe fix should be to make those not deprecated in the Replaces annotation. I will add it soon. Also, I plan to add quick additional note in the config docs to remind people that _allow_duplicate_config_keys_\\xa0is the only way to not be able to add that property more than once with both formats; Those three are a special case that is already mentioned in the docs but I think it will be nice to stress on them when talking about the flags.\\r\\n\\r\\n\\xa0I will push a patch soon, thanks\", 'The same patch was added for both [4.1|https://github.com/ekaterinadimitrova2/cassandra/tree/17904-4.1]\\xa0and [trunk|https://github.com/ekaterinadimitrova2/cassandra/tree/17904-trunk].\\r\\n\\r\\nI tested manually to see that the warnings are not presented anymore on startup. I can push sanity check CI pre-commit when\\xa0[~smiklosovic]\\xa0or any other committer reviewer approves the change.\\xa0', 'tested and works fine, +1 on good build.', 'Thank you for the quick review.\\r\\n\\r\\nCI currently running, I will check the results later:\\r\\n * 4.1 - [j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1928/workflows/3f9da019-1301-41b7-85c4-0767ff2dbd8f], [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1928/workflows/bff4ff51-cece-47b5-9d3c-ffe06363fabc]\\r\\n * trunk – [j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1929/workflows/f70dd874-0b32-4028-9e9e-40e4f739b436], [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1929/workflows/8c22347c-4b63-488d-98d6-5c0a81bc47b7]', 'Both failures known and already reported:\\r\\n\\r\\nCASSANDRA-16861\\xa0- test_compression_cql_options\\r\\n\\r\\nCASSANDRA-17005\\xa0- test_multiple_repair\\r\\n\\r\\nStarting commit soon', 'Committed, thanks:\\r\\n\\r\\nd8bbeb9e39..4c85c6a403\\xa0 cassandra-4.1 -> cassandra-4.1\\r\\n\\r\\n\\xa0\\xa0 e89b214d06..d80d934ed2\\xa0 trunk -> trunk'] \n",
            "my_comment:  Kindly pinging [~e.dimitrova] to raise awareness. \"Thanks [~smiklosovic] those three are special case where only the value format was changed and their names were kept (the only 3 which had proper names and no unit as part of the name... ) thats why your valid observation - they emit deprecation warning even with the value being in the new format. You made me wonder whether it was worth it to special case them at all I will take a look tomorrow.\\xa0\" \"Slept on it and had some discussion with [~marcuse]\\xa0and [~dcapwell]\\xa0about those three parameters as I was wondering whether we missed to take some action around them when we added the new flags - _allow_duplicate_config_keys_ and _allow_new_old_config_keys_ in\\xa0CASSANDRA-17379\n",
            "\n",
            "The key names havent been changed so we technically didnt deprecate the three properties but we added an option to be able to add value both in numeric and numeric+unit format.\\xa0\n",
            "\n",
            "The fix should be to make those not deprecated in the Replaces annotation. I will add it soon. Also I plan to add quick additional note in the config docs to remind people that _allow_duplicate_config_keys_\\xa0is the only way to not be able to add that property more than once with both formats; Those three are a special case that is already mentioned in the docs but I think it will be nice to stress on them when talking about the flags.\n",
            "\n",
            "\\xa0I will push a patch soon thanks\" The same patch was added for both [4.1|https://github.com/ekaterinadimitrova2/cassandra/tree/17904-4.1]\\xa0and [trunk|https://github.com/ekaterinadimitrova2/cassandra/tree/17904-trunk].\n",
            "\n",
            "I tested manually to see that the warnings are not presented anymore on startup. I can push sanity check CI pre-commit when\\xa0[~smiklosovic]\\xa0or any other committer reviewer approves the change.\\xa0 tested and works fine +1 on good build. Thank you for the quick review.\n",
            "\n",
            "CI currently running I will check the results later:\n",
            " * 4.1 - [j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1928/workflows/3f9da019-1301-41b7-85c4-0767ff2dbd8f] [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1928/workflows/bff4ff51-cece-47b5-9d3c-ffe06363fabc]\n",
            " * trunk – [j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1929/workflows/f70dd874-0b32-4028-9e9e-40e4f739b436] [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1929/workflows/8c22347c-4b63-488d-98d6-5c0a81bc47b7] Both failures known and already reported:\n",
            "\n",
            "CASSANDRA-16861\\xa0- test_compression_cql_options\n",
            "\n",
            "CASSANDRA-17005\\xa0- test_multiple_repair\n",
            "\n",
            "Starting commit soon Committed thanks:\n",
            "\n",
            "d8bbeb9e39..4c85c6a403\\xa0 cassandra-4.1 -> cassandra-4.1\n",
            "\n",
            "\\xa0\\xa0 e89b214d06..d80d934ed2\\xa0 trunk -> trunk \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-12580\n",
            "issue_type:  Bug \n",
            "summary:  Fix merkle tree size calculation \n",
            "description:  On CASSANDRA-5263 it was introduced dynamic merkle tree sizing based on estimated number of partitions as {{estimatedDepth = lg(numPartitions)}}, but on [CompactionManager.doValidationCompaction|https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1052] this is being calculated as:\n",
            "comments:  ['Attaching patch to fix the calculation formula to:\\n\\n{{int depth = numPartitions > 0 ? (int) Math.min(Math.ceil(Math.log(numPartitions) / Math.log(2)), 20) : 0;}}\\n\\nBesides fixing from {{ln}} to {{lg}}, this also changes the rounding formula from {{floor}} to  {{ceil}}, so we overestimate the depth rather than underestimate.\\n\\nI added a new test on {{ValidationTest}} that runs a validation compaction with N=128 and N=1500 keys and expect the merkle tree depth to be {{ceil(lg(N))}}. I also modified the other tests on this class to use a {{ListenableFuture}} ({{CompletableFuture}} on 3.0+) instead of {{SimpleCondition}}, since the JUnit assertions are not enforced in other threads.\\n\\n\\nPatch and tests available below:\\n||2.1||2.2||3.0||trunk||\\n|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-12580]|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-12580]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-12580]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-12580]|\\n|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.1-12580-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-12580-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-12580-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12580-testall/lastCompletedBuild/testReport/]|\\n|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.1-12580-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-12580-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-12580-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12580-dtest/lastCompletedBuild/testReport/]|\\n', 'Nice catch. Patch looks good to me.\\n', 'Committed to 2.2+ as {{c70ce6307da824529762ff40673642b6f86972aa}}.\\n(Skipped 2.1 because it is not critical at this point.)'] \n",
            "my_comment: 0;}}\n",
            "\n",
            "Besides fixing from {{ln}} to {{lg}} this also changes the rounding formula from {{floor}} to  {{ceil}} so we overestimate the depth rather than underestimate.\n",
            "\n",
            "I added a new test on {{ValidationTest}} that runs a validation compaction with N=128 and N=1500 keys and expect the merkle tree depth to be {{ceil(lg(N))}}. I also modified the other tests on this class to use a {{ListenableFuture}} ({{CompletableFuture}} on 3.0+) instead of {{SimpleCondition}} since the JUnit assertions are not enforced in other threads.\n",
            "\n",
            "\n",
            "Patch and tests available below:\n",
            "||2.1||2.2||3.0||trunk||\n",
            "|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-12580]|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-12580]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-12580]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-12580]|\n",
            "|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.1-12580-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-12580-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-12580-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12580-testall/lastCompletedBuild/testReport/]|\n",
            "|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.1-12580-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-12580-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-12580-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12580-dtest/lastCompletedBuild/testReport/]|\n",
            " Nice catch. Patch looks good to me.\n",
            " Committed to 2.2+ as {{c70ce6307da824529762ff40673642b6f86972aa}}.\n",
            "(Skipped 2.1 because it is not critical at this point.) \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4136\n",
            "issue_type:  Bug \n",
            "summary:  get_paged_slices doesn't reset startColumn after first row \n",
            "description:  As an example, consider the WordCount example (see CASSANDRA-3883).  WordCountSetup inserts 1000 rows, each with three columns: text3, text4, int1.  (Some other miscellaneous columns are inserted in a few rows, but we can ignore them here.)\n",
            "comments:  ['Attached patch adds the ability to do paging through multiple rows. The support added by the patch is limited to what get_paged_slices requires (in particular using a SliceQueryFilter where finish != \"\" is not supported with that new option). The patch contains a unit test.', 'With this and the 3883 patches I get\\n\\n{noformat}\\n$ cat /tmp/word_count5/part-r-00000\\n0       250\\n1       250\\n2       250\\n3       250\\nword1   2002\\nword2   1\\n{noformat}\\n\\nwhich is the expected result.\\n\\n+1', 'Committed, thanks'] \n",
            "my_comment:  Attached patch adds the ability to do paging through multiple rows. The support added by the patch is limited to what get_paged_slices requires (in particular using a SliceQueryFilter where finish != \"\" is not supported with that new option). The patch contains a unit test. With this and the 3883 patches I get\n",
            "\n",
            "{noformat}\n",
            "$ cat /tmp/word_count5/part-r-00000\n",
            "0       250\n",
            "1       250\n",
            "2       250\n",
            "3       250\n",
            "word1   2002\n",
            "word2   1\n",
            "{noformat}\n",
            "\n",
            "which is the expected result.\n",
            "\n",
            "+1 Committed thanks \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9271\n",
            "issue_type:  Improvement \n",
            "summary:  IndexSummaryManagerTest.testCompactionRace times out periodically \n",
            "description:  The issue is that the amount of time the test takes is highly variable to it being biased towards creating a condition where the test has to retry the compaction it is attempting.\n",
            "comments:  ['Change implemented here https://github.com/apache/cassandra/commit/40687bd44bf21b1e5caf4c42b85035e66e51e6bd', 'http://cassci.datastax.com/job/trunk_testall/63/testReport/org.apache.cassandra.io.sstable/IndexSummaryManagerTest/testCompactionRace/', 'This test is consistently timing out on the Windows CI env:\\nhttp://cassci.datastax.com/job/cassandra-2.2_utest_win32/5/testReport/junit/org.apache.cassandra.io.sstable/IndexSummaryManagerTest/testCompactionRace/', \"A couple of observations: 1) there's a non-deterministic time required for this test to pass, 2) major compactions on CI take an order of magnitude longer to complete than they do on my SSD-enabled laptop, and 3) upping the timeout on all unit tests to 120 makes this test pass on the CI box *sometimes*.\\n\\nI think we should move testCompactionRace out to a long test. [~benedict]: you're the author of this test and it's been flaky w/regards to passing off and on since introduction. Thoughts on long-testing it?\", \"Actually [~tjake] is the author; I just committed it along with the fix(es). From my POV we should *both* 1) move it to a long test; 2) make it more robust\\n\\nI'm pretty sure performing an actual major compaction is unnecessary, and this is largely where the problem comes from (with the indeterminate sleeping as we wait for cessation of other active tasks). What we need is for the instances of reader to change as we try marking the files for redistribution, and we should probably just do this in a direct tight loop so that both code sections are spinning in direct contention. This both increases the likelihood of the failure scenario, but also ensures that if we don't encounter it we can stop promptly (by just spinning for a fixed interval, and calling it a day if we haven't failed).\", \"bq. Actually T Jake Luciani is the author; I just committed it along with the fix(es)\\nNow that you mention that, I'm pretty sure we've already gone over this in JIRA comments on another ticket.\", 're-committed [~aweisberg] fix in 4362e71', 'This is still happening and pretty much hard failing.', \"I ran six builds of this in cassci and it it didn't fail in any of them.\\nhttp://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-C-9271-2-testall/\\n\\nhttps://github.com/apache/cassandra/compare/trunk...aweisberg:C-9271-2\\nhttps://github.com/apache/cassandra/compare/trunk...aweisberg:C-9271-2.diff\", 'I stopped at 106 loop runs over this test - lgtm :)', \"It may fix this behaviour, but unfortunately I am -1 on using Thread.yield(). It depends on the priority of the thread, and that is generally not good practice. It could lead to spinning burning a full CPU flat out until compactions acquiesce. I don't like making application behaviour even slightly less good in order to fix a problematic test.\\n\\nAs I stated above, it should be possible to modify this test to simply mark and unmark compacting directly, rather than actually perform a major compaction.\", \"OK I'll back out Thread.yield().\", 'I propose we remove the test and replace it with this single line. The goal of the test was to ensure we could not mark compacting the incorrect instance of an sstable. This test checks that.', 'OK. I am +1 on replacing it with the simpler assertion on the invariant maintained by Tracker.'] \n",
            "my_comment: youre the author of this test and its been flaky w/regards to passing off and on since introduction. Thoughts on long-testing it?\" \"Actually [~tjake] is the author; I just committed it along with the fix(es). From my POV we should *both* 1) move it to a long test; 2) make it more robust\n",
            "\n",
            "Im pretty sure performing an actual major compaction is unnecessary and this is largely where the problem comes from (with the indeterminate sleeping as we wait for cessation of other active tasks). What we need is for the instances of reader to change as we try marking the files for redistribution and we should probably just do this in a direct tight loop so that both code sections are spinning in direct contention. This both increases the likelihood of the failure scenario but also ensures that if we dont encounter it we can stop promptly (by just spinning for a fixed interval and calling it a day if we havent failed).\" \"bq. Actually T Jake Luciani is the author; I just committed it along with the fix(es)\n",
            "Now that you mention that Im pretty sure weve already gone over this in JIRA comments on another ticket.\" re-committed [~aweisberg] fix in 4362e71 This is still happening and pretty much hard failing. \"I ran six builds of this in cassci and it it didnt fail in any of them.\n",
            "http://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-C-9271-2-testall/\n",
            "\n",
            "https://github.com/apache/cassandra/compare/trunk...aweisberg:C-9271-2\n",
            "https://github.com/apache/cassandra/compare/trunk...aweisberg:C-9271-2.diff\" I stopped at 106 loop runs over this test - lgtm :) \"It may fix this behaviour but unfortunately I am -1 on using Thread.yield(). It depends on the priority of the thread and that is generally not good practice. It could lead to spinning burning a full CPU flat out until compactions acquiesce. I dont like making application behaviour even slightly less good in order to fix a problematic test.\n",
            "\n",
            "As I stated above it should be possible to modify this test to simply mark and unmark compacting directly rather than actually perform a major compaction.\" \"OK Ill back out Thread.yield().\" I propose we remove the test and replace it with this single line. The goal of the test was to ensure we could not mark compacting the incorrect instance of an sstable. This test checks that. OK. I am +1 on replacing it with the simpler assertion on the invariant maintained by Tracker. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-17034\n",
            "issue_type:  Improvement \n",
            "summary:  CEP-11: Memtable API implementation \n",
            "description:  Pluggable memtable API as described in [CEP-11|https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-11%3A+Pluggable+memtable+implementations].\n",
            "comments:  ['Patch uploaded here: [branch|https://github.com/blambov/cassandra/tree/CASSANDRA-17034]\\xa0[pull request|https://github.com/apache/cassandra/pull/1295]\\r\\n\\r\\nFor reviewers it will be better to look at the individual commits. The first one is sizeable but trivial, adding the flush reason to all places that request a flush. The second is the bulk of the work, which splits {{Memtable}} into an interface and hierarchy of classes that add pieces of memtable functionality, ending in {{SkipListMemtable}} which is the current implementation; flushing functionality is moved outside of {{ColumnFamilyStore}}, and new functionality to write temporary sstables for streaming from long-lived memtables is added. The rest of the commits add a mechanism for providing shard boundaries that split the owned space equally, and a basic common interface for reading memtables and sstables.\\r\\n\\r\\nUsages of the API, i.e. alternative memtable implementations, will be provided soon.', 'The [CEP-19 discussion|https://lists.apache.org/thread/fdvf1wmxwnv5jod59jznbnql23nqosty] raised some good points on the memtable configuration, paraphrased below:\\r\\n - Per-node configuration: this is already supported in the form of memtable \"templates\", which are specified in the yaml and can be referenced by the schema.\\r\\n - Priority of schema over yaml: it is a potential operational problem that the schema can specify a configuration that is not controllable per node.\\r\\n - Default templates: it may be a good idea to have multiple default templates that can be overridden if necessary.\\r\\n - Template fallback: with multiple templates, let the yaml have the option of remapping one template to another.\\r\\n\\r\\nI personally prefer to not complicate the solution too much, and am leaning towards implementing the following:\\r\\n - Renaming \"memtable_template\" to \"memtable_configuration\".\\r\\n - A \"default\" configuration is always defined and is used if the schema does not specify one.\\r\\n - The default configuration can be overridden in the yaml.\\r\\n - The yaml can specify any number of memtable configurations, and they can inherit properties (e.g. {{{}extends: default, shards: 32{}}}).\\r\\n - The table schema can only specify the memtable configuration to use and is not allowed to modify its properties.\\r\\n\\r\\nLet\\'s continue the discussion here.', \"SGTM. I agree there's no point overcomplicating this, and this achieves every goal that has been raised.\", \"[~blambov] Not quite done with my review pass, but just wanted to clarify something about the pieces of the Memtable API that are in this Jira but only used by CASSANDRA-17240. (ex. {{ShardBoundaries}} and friends) Assuming we can resolve this issue before the 4.1 freeze, is the reason we want these items included *now* that we want to be able to plug in CASSANDRA-17240 to 4.1 builds even though it doesn't land in-tree until 5.0?\\r\\n\\r\\nAt the end of the day, CASSANDRA-17240 is going to happen, and as long as the bits we're committing up-front here aren't destabilizing, it doesn't really matter. Just doing my due diligence to make sure the patch isn't any larger than it has to be. Thoughts?\", \"Done with my first pass at review. I've left a ton of nits and questions inline in the PR. Aside from what I mentioned above in my previous comment, the only thing I'm really worried about in terms of the impact on 4.1 is [this|https://github.com/apache/cassandra/pull/1295/files#r848687665]. Overall, things look pretty good, and the copious inline documentation made most things easy to reason about.\\r\\n\\r\\nI've gone back and forth mentally about the depth of the hierarchy under {{Memtable}}, specifically whether composition would have made sense (i.e. having strategies for allocation, commitlog stuff, etc.). Perhaps the fact that either we'll control these implementations pretty tightly or there won't be too many external ones makes this unimportant. (I'll also ignore for the moment the divergence this might create for public/non-publ \n",
            "my_comment: 32{}}}).\n",
            " - The table schema can only specify the memtable configuration to use and is not allowed to modify its properties.\n",
            "\n",
            "Let\\s continue the discussion here. \"SGTM. I agree theres no point overcomplicating this and this achieves every goal that has been raised.\" \"[~blambov] Not quite done with my review pass but just wanted to clarify something about the pieces of the Memtable API that are in this Jira but only used by CASSANDRA-17240. (ex. {{ShardBoundaries}} and friends) Assuming we can resolve this issue before the 4.1 freeze is the reason we want these items included *now* that we want to be able to plug in CASSANDRA-17240 to 4.1 builds even though it doesnt land in-tree until 5.0?\n",
            "\n",
            "At the end of the day CASSANDRA-17240 is going to happen and as long as the bits were committing up-front here arent destabilizing it doesnt really matter. Just doing my due diligence to make sure the patch isnt any larger than it has to be. Thoughts?\" \"Done with my first pass at review. Ive left a ton of nits and questions inline in the PR. Aside from what I mentioned above in my previous comment the only thing Im really worried about in terms of the impact on 4.1 is [this|https://github.com/apache/cassandra/pull/1295/files#r848687665]. Overall things look pretty good and the copious inline documentation made most things easy to reason about.\n",
            "\n",
            "Ive gone back and forth mentally about the depth of the hierarchy under {{Memtable}} specifically whether composition would have made sense (i.e. having strategies for allocation commitlog stuff etc.). Perhaps the fact that either well control these implementations pretty tightly or there wont be too many external ones makes this unimportant. (Ill also ignore for the moment the divergence this might create for public/non-publ \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8537\n",
            "issue_type:  Bug \n",
            "summary:  ConcurrentModificationException while executing 'nodetool cleanup' \n",
            "description:  After adding a new node to an existing cluster (7 already started nodes), and waiting a few minutes to be sure that data migration to the new node is completed, I began to use the command nodetool cleanup sequentially on each old node. When I issued this command on the third node, after a few minutes I got a ConcurrentModificationException.\n",
            "comments:  ['Seeing this same stacktrace in a 2.1.2 cluster:\\n\\n@cassandra11:~$ nodetool cleanup\\nerror: null\\n-- StackTrace --\\njava.util.ConcurrentModificationException\\n        at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)\\n        at java.util.ArrayList$Itr.next(ArrayList.java:831)\\n        at org.apache.cassandra.db.index.SecondaryIndexManager.deleteFromIndexes(SecondaryIndexManager.java:476)\\n        at org.apache.cassandra.db.compaction.CompactionManager$CleanupStrategy$Full.cleanup(CompactionManager.java:833)\\n        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupOne(CompactionManager.java:704)\\n        at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:97)\\n        at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:370)\\n        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:267)\\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\\n        at java.lang.Thread.run(Thread.java:745)', \"Update, this is happening across multiple nodes and I'm guessing from the stacktrace that it's related to secondary indexes.\", 'Additional details, tried the following procedures with the same result:\\n\\nrestart Cassandra then cleanup.\\nrepair -pr then cleanup (No errors with repair)\\nrepair then cleanup (No errors with repair)\\nnodetool scrub then cleanup (No errors with scrub)\\nnodetool rebuild_index (the only index on the table) then cleanup (No errors on the rebuild_index)', 'seems we reuse the same CleanupStrategy over all threads in the multithreaded cleanup, patch fixes that', '+1', 'committed, thanks', '@marcus just to be clear. This affects concurrent compactions not multithreaded compactions which we anyway removed in 2.1. \\n\\nAs a temporary workaround until this is released, just set concurrent compactors to 1 in the yaml. '] \n",
            "my_comment: null\n",
            "-- StackTrace --\n",
            "java.util.ConcurrentModificationException\n",
            "        at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)\n",
            "        at java.util.ArrayList$Itr.next(ArrayList.java:831)\n",
            "        at org.apache.cassandra.db.index.SecondaryIndexManager.deleteFromIndexes(SecondaryIndexManager.java:476)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager$CleanupStrategy$Full.cleanup(CompactionManager.java:833)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupOne(CompactionManager.java:704)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:97)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:370)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:267)\n",
            "        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n",
            "        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
            "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
            "        at java.lang.Thread.run(Thread.java:745) \"Update this is happening across multiple nodes and Im guessing from the stacktrace that its related to secondary indexes.\" Additional details tried the following procedures with the same result:\n",
            "\n",
            "restart Cassandra then cleanup.\n",
            "repair -pr then cleanup (No errors with repair)\n",
            "repair then cleanup (No errors with repair)\n",
            "nodetool scrub then cleanup (No errors with scrub)\n",
            "nodetool rebuild_index (the only index on the table) then cleanup (No errors on the rebuild_index) seems we reuse the same CleanupStrategy over all threads in the multithreaded cleanup patch fixes that +1 committed thanks @marcus just to be clear. This affects concurrent compactions not multithreaded compactions which we anyway removed in 2.1. \n",
            "\n",
            "As a temporary workaround until this is released just set concurrent compactors to 1 in the yaml.  \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Technical-Debt-Large-Scale/my_validation/raw/main/cassandra_issues_inspected_merged.xlsx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOABKjdEd6HF",
        "outputId": "9bd924f7-aecd-478d-d8e5-527168f69d1b"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-01 20:07:47--  https://github.com/Technical-Debt-Large-Scale/my_validation/raw/main/cassandra_issues_inspected_merged.xlsx\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Technical-Debt-Large-Scale/my_validation/main/cassandra_issues_inspected_merged.xlsx [following]\n",
            "--2024-07-01 20:07:47--  https://raw.githubusercontent.com/Technical-Debt-Large-Scale/my_validation/main/cassandra_issues_inspected_merged.xlsx\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 573631 (560K) [application/octet-stream]\n",
            "Saving to: ‘cassandra_issues_inspected_merged.xlsx’\n",
            "\n",
            "cassandra_issues_in 100%[===================>] 560.19K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-07-01 20:07:47 (10.6 MB/s) - ‘cassandra_issues_inspected_merged.xlsx’ saved [573631/573631]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas"
      ],
      "metadata": {
        "id": "OEWp80L76Rs0"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_issues_gt = pandas.read_excel(\"cassandra_issues_inspected_merged.xlsx\")\n",
        "df_issues_gt.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "Q2ff8O696lK-",
        "outputId": "6c620d70-0885-43a7-91b2-51cd5a33de46"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         issue_key                                            summary  \\\n",
              "0  CASSANDRA-11272  NullPointerException (NPE) during bootstrap st...   \n",
              "1   CASSANDRA-1321  loadSchemaFromYaml should push migrations to c...   \n",
              "2   CASSANDRA-1641           auto-guessed memtable sizes are too high   \n",
              "\n",
              "    issue_type issue_status issue_priority  \\\n",
              "0          Bug     Resolved         Normal   \n",
              "1  Improvement     Resolved         Normal   \n",
              "2          Bug     Resolved         Normal   \n",
              "\n",
              "                                         description  \\\n",
              "0  After bootstrapping fails due to stream closed...   \n",
              "1                                                NaN   \n",
              "2  I've seen two cases now of the memtable sizes ...   \n",
              "\n",
              "                                            comments  \\\n",
              "0  ['Which Cassandra version are you using? ', 'T...   \n",
              "1  ['announces the last migration, which ends up ...   \n",
              "2  [\"I'd like to introduce a dependency on number...   \n",
              "\n",
              "  architectural_impact_manual architectural_impact  TP  TN  FP  FN  \n",
              "0                         YES                  YES   1   0   0   0  \n",
              "1                         YES                  YES   1   0   0   0  \n",
              "2                         YES                  YES   1   0   0   0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b5f94f1b-97ec-4d24-b697-8384d4711aa6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_key</th>\n",
              "      <th>summary</th>\n",
              "      <th>issue_type</th>\n",
              "      <th>issue_status</th>\n",
              "      <th>issue_priority</th>\n",
              "      <th>description</th>\n",
              "      <th>comments</th>\n",
              "      <th>architectural_impact_manual</th>\n",
              "      <th>architectural_impact</th>\n",
              "      <th>TP</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CASSANDRA-11272</td>\n",
              "      <td>NullPointerException (NPE) during bootstrap st...</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>After bootstrapping fails due to stream closed...</td>\n",
              "      <td>['Which Cassandra version are you using? ', 'T...</td>\n",
              "      <td>YES</td>\n",
              "      <td>YES</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CASSANDRA-1321</td>\n",
              "      <td>loadSchemaFromYaml should push migrations to c...</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['announces the last migration, which ends up ...</td>\n",
              "      <td>YES</td>\n",
              "      <td>YES</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CASSANDRA-1641</td>\n",
              "      <td>auto-guessed memtable sizes are too high</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Normal</td>\n",
              "      <td>I've seen two cases now of the memtable sizes ...</td>\n",
              "      <td>[\"I'd like to introduce a dependency on number...</td>\n",
              "      <td>YES</td>\n",
              "      <td>YES</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5f94f1b-97ec-4d24-b697-8384d4711aa6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b5f94f1b-97ec-4d24-b697-8384d4711aa6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b5f94f1b-97ec-4d24-b697-8384d4711aa6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d07808e2-fa77-4165-bb7c-df8dd603232a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d07808e2-fa77-4165-bb7c-df8dd603232a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d07808e2-fa77-4165-bb7c-df8dd603232a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_issues_gt",
              "summary": "{\n  \"name\": \"df_issues_gt\",\n  \"rows\": 226,\n  \"fields\": [\n    {\n      \"column\": \"issue_key\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 226,\n        \"samples\": [\n          \"CASSANDRA-12186\",\n          \"CASSANDRA-5407\",\n          \"CASSANDRA-12580\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 226,\n        \"samples\": [\n          \"anticompaction log message doesn't include the parent repair session id\",\n          \"Repair exception when getPositionsForRanges returns empty iterator\",\n          \"Fix merkle tree size calculation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Improvement\",\n          \"Task\",\n          \"New Feature\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Resolved\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_priority\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Normal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 212,\n        \"samples\": [\n          \"Add diagnostic events for guardrails, so we can monitor when each type of guardrail is triggered.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 226,\n        \"samples\": [\n          \"['I created a small patch for this issue, it just adds the parent repair session UUID to the \\\"Started\\\" and \\\"Completed\\\" log entry. The example above with my patch would be:\\\\n\\\\n{noformat}\\\\nDEBUG [AntiEntropyStage:1] 2016-07-13 01:57:30,956  RepairMessageVerbHandler.java:149 - Got anticompaction request AnticompactionRequest{parentRepairSession=27103de0-489d-11e6-a6d6-cd06faa0aaa2} org.apache.cassandra.repair.messages.AnticompactionRequest@34449ff4\\\\n<...>\\\\n<snip>\\\\n<...>\\\\nINFO  [CompactionExecutor:5] 2016-07-13 02:07:47,512  CompactionManager.java:511 - Starting anticompaction for trivial_ks.weitest on 1/[BigTableReader(path=\\\\'/var/lib/cassandra/data/trivial_ks/weitest-538b07d1489b11e6a9ef61c6ff848952/mb-1-big-Data.db\\\\')] sstables, parentRepairSession=27103de0-489d-11e6-a6d6-cd06faa0aaa2\\\\nINFO  [CompactionExecutor:5] 2016-07-13 02:07:47,513  CompactionManager.java:540 - SSTable BigTableReader(path=\\\\'/var/lib/cassandra/data/trivial_ks/weitest-538b07d1489b11e6a9ef61c6ff848952/mb-1-big-Data.db\\\\') fully contained in range (-9223372036854775808,-9223372036854775808], mutating repairedAt instead of anticompacting\\\\nINFO  [CompactionExecutor:5] 2016-07-13 02:07:47,570  CompactionManager.java:578 - Completed anticompaction successfully, parentRepairSession=27103de0-489d-11e6-a6d6-cd06faa0aaa2\\\\n{noformat}\\\\n\\\\n', \\\"This looks good, but how about using the same {{\\\\\\\\[repair #b14328a9-dcbc-4bc3-b1d2-47ea48256757\\\\\\\\]}} prefix so it's consistent with the rest of repair logging? If you agree, could you provide another patch with this change? You can probably include this prefix in other anti-compaction messages as well.\\\\n\\\\nAlso, I think we can/should include this on 3.0 as well, since this is not invasive and will help troubleshooting. Can you provide a 3.0, trunk, and 4.0 patches prepared for commit (add CHANGES.TXT entry and commit message according to [these guidelines|http://cassandra.apache.org/doc/latest/development/patches.html]) ?\\\", 'I have changed so I use the {{\\\\\\\\[repair #b14328a9-dcbc-4bc3-b1d2-47ea48256757\\\\\\\\]}} prefix and added that to the other anti-compaction logging. I have put the patches on github: [3.0|https://github.com/tommystendahl/cassandra/tree/cassandra-12186-30], [3.X|https://github.com/tommystendahl/cassandra/tree/cassandra-12186-3x] and [trunk|https://github.com/tommystendahl/cassandra/tree/cassandra-12186-trunk].', 'LGTM, thanks! Submitted CI to make sure this will not break any tests and will commit if everything looks all right:\\\\n\\\\n||trunk||\\\\n|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-12186]|\\\\n|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12186-testall/lastCompletedBuild/testReport/]|\\\\n|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12186-dtest/lastCompletedBuild/testReport/]|', 'Committed as 2256778726319fb76b6d85c4a47a957116c78147 on 3.0 and merged up. Thanks!']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"architectural_impact_manual\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"NO\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"architectural_impact\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"NO\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colunas_uteis = ['issue_key', 'issue_type','summary', 'description', 'comments', 'architectural_impact_manual']\n",
        "df_issues_gt = df_issues_gt[colunas_uteis]\n",
        "df_issues_gt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "mWivQQE46qSl",
        "outputId": "6ab78ed0-2964-4bb0-821e-3f592c09af83"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           issue_key   issue_type  \\\n",
              "0    CASSANDRA-11272          Bug   \n",
              "1     CASSANDRA-1321  Improvement   \n",
              "2     CASSANDRA-1641          Bug   \n",
              "3     CASSANDRA-2296          Bug   \n",
              "4     CASSANDRA-3117          Bug   \n",
              "..               ...          ...   \n",
              "221   CASSANDRA-9631          Bug   \n",
              "222   CASSANDRA-9636          Bug   \n",
              "223   CASSANDRA-9858          Bug   \n",
              "224   CASSANDRA-9880          Bug   \n",
              "225  CASSANDRA-12251          Bug   \n",
              "\n",
              "                                               summary  \\\n",
              "0    NullPointerException (NPE) during bootstrap st...   \n",
              "1    loadSchemaFromYaml should push migrations to c...   \n",
              "2             auto-guessed memtable sizes are too high   \n",
              "3    Scrub resulting in \"bloom filter claims to be ...   \n",
              "4    StorageServiceMBean is missing a getCompaction...   \n",
              "..                                                 ...   \n",
              "221  Unnecessary required filtering for query on in...   \n",
              "222  Duplicate columns in selection causes Assertio...   \n",
              "223  SelectStatement.Parameters fields should be in...   \n",
              "224  ScrubTest.testScrubOutOfOrder should generate ...   \n",
              "225  Move migration tasks to non-periodic queue, as...   \n",
              "\n",
              "                                           description  \\\n",
              "0    After bootstrapping fails due to stream closed...   \n",
              "1                                                  NaN   \n",
              "2    I've seen two cases now of the memtable sizes ...   \n",
              "3    Doing a scrub on a node which I upgraded from ...   \n",
              "4    Without a getter, you can assign a new value b...   \n",
              "..                                                 ...   \n",
              "221  Let's create and populate a simple table compo...   \n",
              "222  Prior to CASSANDRA-9532, unaliased duplicate f...   \n",
              "223  SelectStatement.Parameters fields should be in...   \n",
              "224  ScrubTest#testScrubOutOfOrder is failing on tr...   \n",
              "225  example failure:\\n\\nhttp://cassci.datastax.com...   \n",
              "\n",
              "                                              comments  \\\n",
              "0    ['Which Cassandra version are you using? ', 'T...   \n",
              "1    ['announces the last migration, which ends up ...   \n",
              "2    [\"I'd like to introduce a dependency on number...   \n",
              "3    [\"With debug logging turned on it looks like t...   \n",
              "4    ['+1', 'committed.', 'Integrated in Cassandra-...   \n",
              "..                                                 ...   \n",
              "221  ['Is there a chance that CASSANDRA-8418 introd...   \n",
              "222  [\"I'm seeing the same error als on a query not...   \n",
              "223  ['https://github.com/JeremiahDJordan/cassandra...   \n",
              "224  [\"Patch attached as link.\\nLet's see what cass...   \n",
              "225  [\"This looks to me like in drain/StorageServic...   \n",
              "\n",
              "    architectural_impact_manual  \n",
              "0                           YES  \n",
              "1                           YES  \n",
              "2                           YES  \n",
              "3                           YES  \n",
              "4                           YES  \n",
              "..                          ...  \n",
              "221                          NO  \n",
              "222                          NO  \n",
              "223                          NO  \n",
              "224                          NO  \n",
              "225                          NO  \n",
              "\n",
              "[226 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e29eb2e2-1999-4e72-8838-3ae0db9253f4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_key</th>\n",
              "      <th>issue_type</th>\n",
              "      <th>summary</th>\n",
              "      <th>description</th>\n",
              "      <th>comments</th>\n",
              "      <th>architectural_impact_manual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CASSANDRA-11272</td>\n",
              "      <td>Bug</td>\n",
              "      <td>NullPointerException (NPE) during bootstrap st...</td>\n",
              "      <td>After bootstrapping fails due to stream closed...</td>\n",
              "      <td>['Which Cassandra version are you using? ', 'T...</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CASSANDRA-1321</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>loadSchemaFromYaml should push migrations to c...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['announces the last migration, which ends up ...</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CASSANDRA-1641</td>\n",
              "      <td>Bug</td>\n",
              "      <td>auto-guessed memtable sizes are too high</td>\n",
              "      <td>I've seen two cases now of the memtable sizes ...</td>\n",
              "      <td>[\"I'd like to introduce a dependency on number...</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CASSANDRA-2296</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Scrub resulting in \"bloom filter claims to be ...</td>\n",
              "      <td>Doing a scrub on a node which I upgraded from ...</td>\n",
              "      <td>[\"With debug logging turned on it looks like t...</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CASSANDRA-3117</td>\n",
              "      <td>Bug</td>\n",
              "      <td>StorageServiceMBean is missing a getCompaction...</td>\n",
              "      <td>Without a getter, you can assign a new value b...</td>\n",
              "      <td>['+1', 'committed.', 'Integrated in Cassandra-...</td>\n",
              "      <td>YES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>CASSANDRA-9631</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Unnecessary required filtering for query on in...</td>\n",
              "      <td>Let's create and populate a simple table compo...</td>\n",
              "      <td>['Is there a chance that CASSANDRA-8418 introd...</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>CASSANDRA-9636</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Duplicate columns in selection causes Assertio...</td>\n",
              "      <td>Prior to CASSANDRA-9532, unaliased duplicate f...</td>\n",
              "      <td>[\"I'm seeing the same error als on a query not...</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>CASSANDRA-9858</td>\n",
              "      <td>Bug</td>\n",
              "      <td>SelectStatement.Parameters fields should be in...</td>\n",
              "      <td>SelectStatement.Parameters fields should be in...</td>\n",
              "      <td>['https://github.com/JeremiahDJordan/cassandra...</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>CASSANDRA-9880</td>\n",
              "      <td>Bug</td>\n",
              "      <td>ScrubTest.testScrubOutOfOrder should generate ...</td>\n",
              "      <td>ScrubTest#testScrubOutOfOrder is failing on tr...</td>\n",
              "      <td>[\"Patch attached as link.\\nLet's see what cass...</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>CASSANDRA-12251</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Move migration tasks to non-periodic queue, as...</td>\n",
              "      <td>example failure:\\n\\nhttp://cassci.datastax.com...</td>\n",
              "      <td>[\"This looks to me like in drain/StorageServic...</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>226 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e29eb2e2-1999-4e72-8838-3ae0db9253f4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e29eb2e2-1999-4e72-8838-3ae0db9253f4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e29eb2e2-1999-4e72-8838-3ae0db9253f4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7cf094d6-9f00-4c3e-a931-b528f676cd08\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7cf094d6-9f00-4c3e-a931-b528f676cd08')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7cf094d6-9f00-4c3e-a931-b528f676cd08 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d3036c81-7ffc-4ffc-a100-8f48d820c200\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_issues_gt')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d3036c81-7ffc-4ffc-a100-8f48d820c200 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_issues_gt');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_issues_gt",
              "summary": "{\n  \"name\": \"df_issues_gt\",\n  \"rows\": 226,\n  \"fields\": [\n    {\n      \"column\": \"issue_key\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 226,\n        \"samples\": [\n          \"CASSANDRA-12186\",\n          \"CASSANDRA-5407\",\n          \"CASSANDRA-12580\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Improvement\",\n          \"Task\",\n          \"New Feature\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 226,\n        \"samples\": [\n          \"anticompaction log message doesn't include the parent repair session id\",\n          \"Repair exception when getPositionsForRanges returns empty iterator\",\n          \"Fix merkle tree size calculation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 212,\n        \"samples\": [\n          \"Add diagnostic events for guardrails, so we can monitor when each type of guardrail is triggered.\",\n          \"{noformat}\\nERROR [ReadStage:3] 2013-08-07 06:58:21,485 CassandraDaemon.java (line 192) Exception in thread Thread[ReadStage:3,5,main]\\njava.lang.AssertionError: Added column does not sort as the last column\\n    at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:131)\\n    at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:119)\\n    at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:114)\\n    at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:171)\\n    at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:136)\\n    at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:84)\\n    at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:291)\\n    at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)\\n    at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1390)\\n    at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1213)\\n    at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1125)\\n    at org.apache.cassandra.db.Table.getRow(Table.java:347)\\n    at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:70)\\n    at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1047)\\n    at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1593)\\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\\n    at java.lang.Thread.run(Thread.java:722)\\n{noformat}\\n\\ntest_column_index_stress in wide_rows_test will reproduce this within ~20 runs and bisect strongly points to a regression in CASSANDRA-5762\",\n          \"As the title says, otherwise you get an NPE when the options are missing from the yaml.  It's included in my second patch on CASSANDRA-3045 and is a one line fix.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 226,\n        \"samples\": [\n          \"['I created a small patch for this issue, it just adds the parent repair session UUID to the \\\"Started\\\" and \\\"Completed\\\" log entry. The example above with my patch would be:\\\\n\\\\n{noformat}\\\\nDEBUG [AntiEntropyStage:1] 2016-07-13 01:57:30,956  RepairMessageVerbHandler.java:149 - Got anticompaction request AnticompactionRequest{parentRepairSession=27103de0-489d-11e6-a6d6-cd06faa0aaa2} org.apache.cassandra.repair.messages.AnticompactionRequest@34449ff4\\\\n<...>\\\\n<snip>\\\\n<...>\\\\nINFO  [CompactionExecutor:5] 2016-07-13 02:07:47,512  CompactionManager.java:511 - Starting anticompaction for trivial_ks.weitest on 1/[BigTableReader(path=\\\\'/var/lib/cassandra/data/trivial_ks/weitest-538b07d1489b11e6a9ef61c6ff848952/mb-1-big-Data.db\\\\')] sstables, parentRepairSession=27103de0-489d-11e6-a6d6-cd06faa0aaa2\\\\nINFO  [CompactionExecutor:5] 2016-07-13 02:07:47,513  CompactionManager.java:540 - SSTable BigTableReader(path=\\\\'/var/lib/cassandra/data/trivial_ks/weitest-538b07d1489b11e6a9ef61c6ff848952/mb-1-big-Data.db\\\\') fully contained in range (-9223372036854775808,-9223372036854775808], mutating repairedAt instead of anticompacting\\\\nINFO  [CompactionExecutor:5] 2016-07-13 02:07:47,570  CompactionManager.java:578 - Completed anticompaction successfully, parentRepairSession=27103de0-489d-11e6-a6d6-cd06faa0aaa2\\\\n{noformat}\\\\n\\\\n', \\\"This looks good, but how about using the same {{\\\\\\\\[repair #b14328a9-dcbc-4bc3-b1d2-47ea48256757\\\\\\\\]}} prefix so it's consistent with the rest of repair logging? If you agree, could you provide another patch with this change? You can probably include this prefix in other anti-compaction messages as well.\\\\n\\\\nAlso, I think we can/should include this on 3.0 as well, since this is not invasive and will help troubleshooting. Can you provide a 3.0, trunk, and 4.0 patches prepared for commit (add CHANGES.TXT entry and commit message according to [these guidelines|http://cassandra.apache.org/doc/latest/development/patches.html]) ?\\\", 'I have changed so I use the {{\\\\\\\\[repair #b14328a9-dcbc-4bc3-b1d2-47ea48256757\\\\\\\\]}} prefix and added that to the other anti-compaction logging. I have put the patches on github: [3.0|https://github.com/tommystendahl/cassandra/tree/cassandra-12186-30], [3.X|https://github.com/tommystendahl/cassandra/tree/cassandra-12186-3x] and [trunk|https://github.com/tommystendahl/cassandra/tree/cassandra-12186-trunk].', 'LGTM, thanks! Submitted CI to make sure this will not break any tests and will commit if everything looks all right:\\\\n\\\\n||trunk||\\\\n|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-12186]|\\\\n|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12186-testall/lastCompletedBuild/testReport/]|\\\\n|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12186-dtest/lastCompletedBuild/testReport/]|', 'Committed as 2256778726319fb76b6d85c4a47a957116c78147 on 3.0 and merged up. Thanks!']\",\n          \"['What is causing the breakage?  Is it possible to add a test that exposes the problem?', \\\"my fix in CASSANDRA-5250 just fixed it for LCS (tests if intersecting sstables is empty in LeveledScanner), and this re-broke it for STCS\\\\n\\\\ni'll try to write a unit test for this\\\", 'adds a unit test that would have found the bug', 'LGTM, committed']\",\n          \"['Attaching patch to fix the calculation formula to:\\\\n\\\\n{{int depth = numPartitions > 0 ? (int) Math.min(Math.ceil(Math.log(numPartitions) / Math.log(2)), 20) : 0;}}\\\\n\\\\nBesides fixing from {{ln}} to {{lg}}, this also changes the rounding formula from {{floor}} to  {{ceil}}, so we overestimate the depth rather than underestimate.\\\\n\\\\nI added a new test on {{ValidationTest}} that runs a validation compaction with N=128 and N=1500 keys and expect the merkle tree depth to be {{ceil(lg(N))}}. I also modified the other tests on this class to use a {{ListenableFuture}} ({{CompletableFuture}} on 3.0+) instead of {{SimpleCondition}}, since the JUnit assertions are not enforced in other threads.\\\\n\\\\n\\\\nPatch and tests available below:\\\\n||2.1||2.2||3.0||trunk||\\\\n|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-12580]|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-12580]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-12580]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-12580]|\\\\n|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.1-12580-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-12580-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-12580-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12580-testall/lastCompletedBuild/testReport/]|\\\\n|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.1-12580-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-12580-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-12580-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12580-dtest/lastCompletedBuild/testReport/]|\\\\n', 'Nice catch. Patch looks good to me.\\\\n', 'Committed to 2.2+ as {{c70ce6307da824529762ff40673642b6f86972aa}}.\\\\n(Skipped 2.1 because it is not critical at this point.)']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"architectural_impact_manual\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"NO\",\n          \"YES\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: convert df_issues_gt to json file\n",
        "\n",
        "import json\n",
        "\n",
        "df_issues_gt_json = df_issues_gt.to_json(orient=\"records\")\n",
        "\n",
        "with open(\"df_issues_gt.json\", \"w\") as json_file:\n",
        "    json_file.write(df_issues_gt_json)\n"
      ],
      "metadata": {
        "id": "Y96u3YKl7XCK"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"df_issues_gt.json\", \"r\") as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "\n",
        "for issue in json_data:\n",
        "  print(f\"issue_id: {issue['issue_key']}\")\n",
        "  print(f\"issue_type: {issue['issue_type']}\")\n",
        "  print(f\"summary: {issue['summary']}\")\n",
        "  print(f\"description: {issue['description']}\")\n",
        "  print(f\"architectural impact: {issue['architectural_impact_manual']}\")\n",
        "  print(f\"comments: {issue['comments']}\")\n",
        "  if issue[\"comments\"]:\n",
        "    my_comment = convert_comment_to_text(issue['comments'])\n",
        "    print(f\"my_comment: {my_comment}\")\n",
        "  print(\"---\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZqSl4tO8dNU",
        "outputId": "56cf0c32-4833-42c0-82ec-25f368bf1074"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "DEBUG [pool-2-thread-359] 2011-10-18 10:34:45,376 StorageProxy.java (line 821) %s disagrees (%s)\n",
            "\n",
            "\n",
            "simple fix: replace with %s with {} ... may want to consider logging better comment?\n",
            "architectural impact: NO\n",
            "comments: ['Fixed the erroneous debug logging statement by replacing %s with {}, as supported by SLF4J. Also made use of the {}-notation on some of the other debug logging statements in the class.', 'committed.  Thanks, Jackson and Tommy!']\n",
            "my_comment: Fixed the erroneous debug logging statement by replacing %s with {} as supported by SLF4J. Also made use of the {}-notation on some of the other debug logging statements in the class. committed.  Thanks Jackson and Tommy!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5571\n",
            "issue_type: Improvement\n",
            "summary: Reject bootstrapping endpoints that are already in the ring with different gossip data\n",
            "description: The ring can be silently broken by improperly bootstrapping an endpoint that has an existing entry in the gossip table. In the case where a node attempts to bootstrap with the same IP address as an existing ring member, the old token metadata is dropped without warning, resulting in range shifts for the cluster.\n",
            "\n",
            "This isn't so bad for non-vnode cases where, in general, tokens are explicitly assigned, and a bootstrap on the same token would result in no range shifts. For vnode cases, the convention is to just let nodes come up by selecting their own tokens, and a bootstrap will override the existing tokens for that endpoint.\n",
            "\n",
            "While there are some other issues open for adding an explicit rebootstrap feature for vnode cases, given the changes in operator habits for vnode rings, it seems a bit too easy to make this happen. Even more undesirable is the fact that it's basically silent.\n",
            "\n",
            "This is a proposal for checking for this exact case: bootstraps on endpoints with existing ring entries that have different hostIDs and/or tokens should be rejected with an error message describing what happened and how to override the safety check. It looks like the override can be supported using the existing \"nodetool removenode -force\".\n",
            "\n",
            "I can work up a patch for this.\n",
            "architectural impact: NO\n",
            "comments: ['Rick, are you still planning to take a stab at this?', 'Hi, Jingsi Zhu is no longer at Facebook so this email address is no longer being monitored. If you need assistance, please contact another person who is currently at the company.\\n', \"And that's why you shouldn't use work email accounts for OSS participation.\", \"I will note that in working on CASSANDRA-5916, I realize this won't be as straightforward as I originally thought, either, and will likely need to take the same approach we end up taking there.\", 'With CASSANDRA-5916 done, we probably need to rearrange it a bit so we always do a shadow gossip round, then punt if the entry exists in gossip, otherwise hand off the info when doing a replace.', '5571-2.0-v1.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-5571]) uses a shadow gossip round to check for endpoint collisions.', \"The problem with this patch is that it checks gossip regardless of whether it's going to bootstrap or not; this means you can't just simply restart an existing node.\", 'Bah, not sure how I missed that.  5517-2.0-v2.patch (the branch is also updated) only checks for collisions when bootstrapping.', 'We probably ought to make Gossiper.isDeadState public, and then check that not only is the state not null, but also not a dead state.  This way the IP can be reused without waiting 3 days for the dead state to be evicted/', '5571-2.0-v3.patch tolerates dead nodes when checking for endpoint collisions on bootstrap.', \"Committed, with a minor change of consolidating the 'should we bootstrap' into a single function, since maintaining it in one place is scary enough.\", 'To anyone using vnodes and looking to reduce their exposure to this before 2.0.2, you have two obvious workarounds :\\n\\n1) manually pick tokens, and bootstrap your new vnode node with all 256 of them in in intiial_token comma delimited list\\n\\nOR\\n\\n1) continue to let num_tokens pick tokens for you and bootstrap\\n2) collect all this node\\'s tokens into a comma delimited list with a command like\\n{noformat}\\nIP=__NODE_IP__ ; nodetool ring | fgrep -w \"$IP\" | awk \\'{print $NF}\\' |xargs -d,\\n\\nor\\n\\nIP =__NODE_IP__ ; nodetool -h $IP info -T |grep Token | awk \\'{print $NF}\\' | tr \\'\\\\n\\' \\',\\'\\n{noformat}\\n3) put the comma delimited list from 2 into the initial_token line in cassandra.yaml before the next time the node restarts', 'I noticed that checkForEndpointCollision() is called before starting Gossiper (that is started immediately after this check) . \\nThis causes a problem the first time I bootstrap a cluster since all nodes call checkForEndpointCollision() but no one have yet started Gossiper so no one answer the gossip messages leading all nodes to timeout and die with an \"Unable to gossip with any seeds\" RTE.\\n\\nThis is an issue especially using Cassandra with Priam (https://github.com/Netflix/Priam) were all nodes starts automatically at the very same time with the same configuration. \\nUnfortunately working around the problem in Priam is fairly complicated since it would require synchronizing the whole cluster to bootstrap in a specific order with different configuration. \\n\\nThe question then is: may we move the checkForEndpointCollision() call after Gossiper is started (about ten lines later in StorageService)?\\n\\nOn the contrary if this check need to happen before Gossiper is started, another option could be to allow GossipDigestSynVerbHandler.doVerb() to respond even if Gossiper is not yet enabled (right now it checks for Gossiper.instance.isEnabled() or it silently discard the request).\\n\\n', \"bq. This causes a problem the first time I bootstrap a cluster since all nodes call checkForEndpointCollision() but no one have yet started Gossiper\\n\\nOnly bootstrapping nodes call it, and seeds never bootstrap, which is what the other nodes are trying to communicate with.  You can probably either a) make sure your seeds have started first, or b) just not bootstrap at all, since it's a fresh cluster where it doesn't make any sense to do so.  In any case, I recommend taking this to the Cassandra and/or Priam MLs, since this ticket isn't the real source of your problem.\", \"Thank you Brandon, you are right, Priam has been probably abusing this non-feature for a while essentially starting the cluster in a non-conventional (less-coordinated) way.\\n\\nMy understanding is that this issue's patch breaks that, although not so much for the new check that has been introduced but rather for the timing and the collaterals of this check (i.e. other nodes ignoring the gossip message), and that's why I raised the problem here, wondering if there is any way we can mitigate the collateral effects of this patch while Priam improves the way it bootstrap a cluster.\\n\\nAnyhow, I'm already trying to put this in the Priam dev agenda, and, as you suggested, I'll try to discuss this also on the Cassandra ml.\\n\\nUpdate: For reference here you can find one of the issues reporting the problem on the Priam side: https://github.com/Netflix/Priam/issues/313\\n\", 'I would assume that the 1.2.x branch is also affected by this issue. Are there any plans to backport this enhancement to the 1.2 branch?', \"Your assumption is correct, but 1.2 is not receiving updates any longer.  It's probably trivial to apply yourself if you choose that route.\", 'Just caught that, misread the date I had seen elsewhere. Thank you for replying.']\n",
            "my_comment: https://github.com/Netflix/Priam/issues/313\n",
            "\" I would assume that the 1.2.x branch is also affected by this issue. Are there any plans to backport this enhancement to the 1.2 branch? \"Your assumption is correct but 1.2 is not receiving updates any longer.  Its probably trivial to apply yourself if you choose that route.\" Just caught that misread the date I had seen elsewhere. Thank you for replying.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6013\n",
            "issue_type: Bug\n",
            "summary: CAS may return false but still commit the insert\n",
            "description: If a Paxos proposer proposes some value/update and that propose fail, there is no guarantee on whether this value will be accepted or not ultimately. Paxos guarantees that we'll agree on \"a\" value (for a given round in our case), but does not guarantee that the proposer of the agreed upon value will know it.  In particular, if for a given proposal at least one accepter has accepted it but not a quorum does, then that value might (but that's not guaranteed either) be replayed (and committed) by another proposer.\n",
            "\n",
            "Currently, if a proposer A proposes some update U but it is rejected, A will sleep a bit and retry U. But if U was accepted by at least one acceptor, some other proposer B might replay U, succeed and commit it. If A does its retry after that happens, he will prepare, check the condition, and probably find that the conditions don't apply anymore since U has been committed already. It will thus return false, even though U has been in fact committed.\n",
            "\n",
            "Unfortunately I'm not sure there is an easy way for a proposer whose propose fails to know if the update will prevail or not eventually. Which mean the only acceptable solution I can see would be to return to the user \"I don't know\" (through some exception for instance). Which is annoying because having a proposal rejected won't be an extremely rare occurrence, even with relatively light contention, and returning \"I don't know\" often is a bit unfriendly.\n",
            "architectural impact: NO\n",
            "comments: ['bq. if for a given proposal at least one accepter has accepted it but not a quorum does, then that value might (but that\\'s not guaranteed either) be replayed (and committed) by another proposer\\n\\nWhy not have the new leader require a quorum of replicas to say \"I have this unfinished business\" before replaying it?\\n\\n(I\\'m pretty sure I had this logic in originally but you talked me out of it in the name of code simplification.)', 'Okay, so the problem is not the retry per se, but when we have a \"split decision\" on the nodes that reply.  We can reduce the likelihood of that happening by waiting for all known live endpoints if that\\'s required to hear from a majority.\\n\\nIf we still don\\'t hear from a majority, we can return a timeout; it\\'s valid for a transaction to be committed after a timeout.\\n\\nPatch for the above attached.', \"Unfortunately, I think this is a little grimmer than that. The problem is that a proposer shouldn't move on unless the propose was successful (in which case it returns to the client) or it is sure that the propose will *not* be replayed (if it is sure of that, then retrying the proposed value with a newer ballot is safe; the current problem is that we retry with a newer ballot when we're not sure of that). In other words, we should timeout unless we are either successful or all nodes have answered and none have accepted. I'm attaching a v2 doing that (but still tries to timeout as little as possible without compromising correctness).\\n\\nUnfortunately, this mean we'll timeout as soon as a proposer gets a propose reject but at least one acceptor had accepted it, which is not an extremely rare condition even with moderate contention. That being said, the current behavior is plain wrong, so unless someone has a much better idea that is easy to implement, we should probably go ahead with this for now.\\n\", 'It looks to me like both uses of requiredTargets should actually be totalTargets.  v3 attached.', \"I don't follow. getSuccessful/getAcceptCount is supposed to returned how many successful accepts we got. So that's how much time we decremented remainingRequired, i.e. its initial value (requiredTargets) minus it's current value. Similarly, in isFullyRefused, we want to validate that remainingRequired was never decremented (no-one accepted), so we want to compare it's current value with its initial value, requiredTargets (comparing to totalTargets will in fact always fail).\\n\\nI guess the code is more straightforward if we keep the number of accepts instead of the number of remaining accept: attaching v4 with that version (which is equivalent to v2, but with the updated comment of v3).\\n\", '+1', 'Committed, thanks']\n",
            "my_comment: attaching v4 with that version (which is equivalent to v2 but with the updated comment of v3).\n",
            "\" +1 Committed thanks\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-609\n",
            "issue_type: Bug\n",
            "summary: Easy to OOM on log replay since memtable limits are ignored\n",
            "description: None\n",
            "architectural impact: NO\n",
            "comments: [\"respect memtable thresholds when replaying commit log; use normal write path, but add a writeCommitLog flag so we don't re-write out another CL entry for the replay\", 'To clarify: under low load, you won\\'t much exceed a single memtable\\'s worth of inserts since when a memtable flushes it marks the commitlog header as \"start replay from here.\"  But under high load, you can have a significant amount of inserts done while flushes are queued up in sort + write executors, and this is where you run into trouble on replay.', \"During recovery, we used to rely on CFS.switchMemtable to call onMemtableFlush(ctx) to discard commit log files. With this patch, onMemtableFlush(ctx) won't be called during recovery. When will the commit log files be deleted?\\n\", 'I was thinking that the final table.flush is a \"normal\" flush so that should take care of it.  But thinking about it more we do have a corner case of \"the last write in the log happened to exactly hit the threshold and triggered a no-discard flush, so the final flush saw a clean memtable and did nothing.\"\\n\\nPatch 2 (applies on top of first) addresses this.  (Most of the patch is converting CL to a true singleton so replay has access to the normal context operations w/o ugly hacks.)', \"My bad. I got confused about how log files are deleted during recovery. It turns out that all log files are deleted explicitly in RecoveryManager.doRecovery(). The deletion doesn't rely on the flushing logic. So even v1 of the patch is fine.\", \"I didn't know that, either.  I think that's a bug -- if something goes wrong during recovery we definitely shouldn't be deleting data that hasn't been replayed.  I'd feel safer taking that out, what do you think?\", \"Currently, CL.discardCompletedSegments() doesn't really discard log files properly during recovery. The problem is that clHeaders_ is never populated during recovery.\\n\\nDeleting log files explicitly during recovery may not be that bad. If anything goes wrong during recovery, we will get either an IOException or a RuntimeException. In either case, log files won't be deleted.\", \"You're right, because it assumes that when CL is instantiated it's starting fresh.  I'll commit patch 1.\", 'Integrated in Cassandra #282 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/282/])\\n    respect memtable thresholds when replaying commit log\\npatch by jbellis; reviewed by Jun Rao for \\n']\n",
            "my_comment: under low load you won\\t much exceed a single memtable\\s worth of inserts since when a memtable flushes it marks the commitlog header as \"start replay from here.\"  But under high load you can have a significant amount of inserts done while flushes are queued up in sort + write executors and this is where you run into trouble on replay. \"During recovery we used to rely on CFS.switchMemtable to call onMemtableFlush(ctx) to discard commit log files. With this patch onMemtableFlush(ctx) wont be called during recovery. When will the commit log files be deleted?\n",
            "\" I was thinking that the final table.flush is a \"normal\" flush so that should take care of it.  But thinking about it more we do have a corner case of \"the last write in the log happened to exactly hit the threshold and triggered a no-discard flush so the final flush saw a clean memtable and did nothing.\"\n",
            "\n",
            "Patch 2 (applies on top of first) addresses this.  (Most of the patch is converting CL to a true singleton so replay has access to the normal context operations w/o ugly hacks.) \"My bad. I got confused about how log files are deleted during recovery. It turns out that all log files are deleted explicitly in RecoveryManager.doRecovery(). The deletion doesnt rely on the flushing logic. So even v1 of the patch is fine.\" \"I didnt know that either.  I think thats a bug -- if something goes wrong during recovery we definitely shouldnt be deleting data that hasnt been replayed.  Id feel safer taking that out what do you think?\" \"Currently CL.discardCompletedSegments() doesnt really discard log files properly during recovery. The problem is that clHeaders_ is never populated during recovery.\n",
            "\n",
            "Deleting log files explicitly during recovery may not be that bad. If anything goes wrong during recovery we will get either an IOException or a RuntimeException. In either case log files wont be deleted.\" \"Youre right because it assumes that when CL is instantiated its starting fresh.  Ill commit patch 1.\" Integrated in Cassandra #282 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/282/])\n",
            "    respect memtable thresholds when replaying commit log\n",
            "patch by jbellis; reviewed by Jun Rao for \n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7479\n",
            "issue_type: Bug\n",
            "summary: Consistency level ANY does not send Commit to all endpoints for LOCAL_SERIAL\n",
            "description: If the consistency level is ANY and using LOCAL_SERIAL, the Commit is only send to all local endpoints. \n",
            "Commit needs to be sent to all endpoints in all DCs. \n",
            "architectural impact: NO\n",
            "comments: ['[~slebresne] to review', 'Committed (with minor code style update), thanks']\n",
            "my_comment: [~slebresne] to review Committed (with minor code style update) thanks\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8116\n",
            "issue_type: Bug\n",
            "summary: HSHA fails with default rpc_max_threads setting\n",
            "description: The HSHA server fails with 'Out of heap space' error if the rpc_max_threads is left at its default setting (unlimited) in cassandra.yaml.\n",
            "\n",
            "I'm not proposing any code change for this but have submitted a patch for a comment change in cassandra.yaml to indicate that rpc_max_threads needs to be changed if you use HSHA.\n",
            "architectural impact: NO\n",
            "comments: ['Committed.', \"Is it guaranteed to OOM?  Can't we just check for that combination and provide a sensible error instead of OOMing and letting the user figure it out?\", \"It's pretty much guaranteed to OOM because the number of handlers per SelectorThread is based on the the max pool size which for the default is Integer.MAX_VALUE. This change happened as part of CASSANDRA-7594.\\n\\nI suppose you could check and throw if the value is Integer.MAX_VALUE but you aren't going to be able to check every value. It also happens when the node is started so is pretty immediate which is why I suggested a doc change rather than a code change.\", 'Just as an FYI - I bisected dtest thrift_hsha_test.ThriftHSHATest.test_6285 failures to the CASSANDRA-7594 commit, and had to increase CCM_MAX_HEAP_SIZE to 4G before the ccm nodes would start successfully without a heap OOM.', 'Setting rpc_max_threads=20 in the thrift_hsha_test test_6285 does appear to keep dtest running with the default ccm heap.', 'bq. I suppose you could check and throw if the value is Integer.MAX_VALUE but you aren\\'t going to be able to check every value.\\n\\n\"unlimited\" is the default value, so I think there\\'s still quite a bit of value in checking for just that.  I\\'ll put together a patch.', '8116-throw-exc-2.0.txt throws a ConfigurationException when hsha is used with unlimited rpc_max_threads.', 'is there any reason to have a default of unlimited?  it just seems like it would be a simple change to have a reasonable default.', '+1 on the latest patch because that will be far clearer to the user than the OOM they get at the moment. ', 'Thanks, committed 8116-throw-exc-2.0.txt as 1b332bc1c02786623e2baf773e9f46af9c04f21f.', \"bq. is there any reason to have a default of unlimited? it just seems like it would be a simple change to have a reasonable default.\\n\\nI'm not sure about the rationale for that default.  Would you mind opening a new ticket to discuss a better default?  I'm not sure if it's something we would want to change in 2.0 or 2.1.\", 'The latest 2.0.x release of Cassandra using hsha with default settings either stalls after a few minutes of operation or crashes.\\n\\nThis does not seem like it should have a priority of \"Minor\". This is a major problem. The longer that 2.0.11 is the \"latest\" version the bigger the problem becomes for new users and existing users that have automation and high levels of trust in minor version upgrades.\\n\\n']\n",
            "my_comment: Committed. \"Is it guaranteed to OOM?  Cant we just check for that combination and provide a sensible error instead of OOMing and letting the user figure it out?\" \"Its pretty much guaranteed to OOM because the number of handlers per SelectorThread is based on the the max pool size which for the default is Integer.MAX_VALUE. This change happened as part of CASSANDRA-7594.\n",
            "\n",
            "I suppose you could check and throw if the value is Integer.MAX_VALUE but you arent going to be able to check every value. It also happens when the node is started so is pretty immediate which is why I suggested a doc change rather than a code change.\" Just as an FYI - I bisected dtest thrift_hsha_test.ThriftHSHATest.test_6285 failures to the CASSANDRA-7594 commit and had to increase CCM_MAX_HEAP_SIZE to 4G before the ccm nodes would start successfully without a heap OOM. Setting rpc_max_threads=20 in the thrift_hsha_test test_6285 does appear to keep dtest running with the default ccm heap. bq. I suppose you could check and throw if the value is Integer.MAX_VALUE but you aren\\t going to be able to check every value.\n",
            "\n",
            "\"unlimited\" is the default value so I think there\\s still quite a bit of value in checking for just that.  I\\ll put together a patch. 8116-throw-exc-2.0.txt throws a ConfigurationException when hsha is used with unlimited rpc_max_threads. is there any reason to have a default of unlimited?  it just seems like it would be a simple change to have a reasonable default. +1 on the latest patch because that will be far clearer to the user than the OOM they get at the moment.  Thanks committed 8116-throw-exc-2.0.txt as 1b332bc1c02786623e2baf773e9f46af9c04f21f. \"bq. is there any reason to have a default of unlimited? it just seems like it would be a simple change to have a reasonable default.\n",
            "\n",
            "Im not sure about the rationale for that default.  Would you mind opening a new ticket to discuss a better default?  Im not sure if its something we would want to change in 2.0 or 2.1.\" The latest 2.0.x release of Cassandra using hsha with default settings either stalls after a few minutes of operation or crashes.\n",
            "\n",
            "This does not seem like it should have a priority of \"Minor\". This is a major problem. The longer that 2.0.11 is the \"latest\" version the bigger the problem becomes for new users and existing users that have automation and high levels of trust in minor version upgrades.\n",
            "\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8537\n",
            "issue_type: Bug\n",
            "summary: ConcurrentModificationException while executing 'nodetool cleanup'\n",
            "description: After adding a new node to an existing cluster (7 already started nodes), and waiting a few minutes to be sure that data migration to the new node is completed, I began to use the command nodetool cleanup sequentially on each old node. When I issued this command on the third node, after a few minutes I got a ConcurrentModificationException.\n",
            "\n",
            "~$ nodetool cleanup\n",
            "error: null\n",
            "-- StackTrace --\n",
            "java.util.ConcurrentModificationException\n",
            "        at java.util.ArrayList$Itr.checkForComodification(Unknown Source)\n",
            "        at java.util.ArrayList$Itr.next(Unknown Source)\n",
            "        at org.apache.cassandra.db.index.SecondaryIndexManager.deleteFromIndexes(SecondaryIndexManager.java:476)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager$CleanupStrategy$Full.cleanup(CompactionManager.java:833)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupOne(CompactionManager.java:704)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:97)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:370)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:267)\n",
            "        at java.util.concurrent.FutureTask.run(Unknown Source)\n",
            "        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
            "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
            "        at java.lang.Thread.run(Unknown Source)\n",
            "\n",
            "architectural impact: NO\n",
            "comments: ['Seeing this same stacktrace in a 2.1.2 cluster:\\n\\n@cassandra11:~$ nodetool cleanup\\nerror: null\\n-- StackTrace --\\njava.util.ConcurrentModificationException\\n        at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)\\n        at java.util.ArrayList$Itr.next(ArrayList.java:831)\\n        at org.apache.cassandra.db.index.SecondaryIndexManager.deleteFromIndexes(SecondaryIndexManager.java:476)\\n        at org.apache.cassandra.db.compaction.CompactionManager$CleanupStrategy$Full.cleanup(CompactionManager.java:833)\\n        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupOne(CompactionManager.java:704)\\n        at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:97)\\n        at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:370)\\n        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:267)\\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\\n        at java.lang.Thread.run(Thread.java:745)', \"Update, this is happening across multiple nodes and I'm guessing from the stacktrace that it's related to secondary indexes.\", 'Additional details, tried the following procedures with the same result:\\n\\nrestart Cassandra then cleanup.\\nrepair -pr then cleanup (No errors with repair)\\nrepair then cleanup (No errors with repair)\\nnodetool scrub then cleanup (No errors with scrub)\\nnodetool rebuild_index (the only index on the table) then cleanup (No errors on the rebuild_index)', 'seems we reuse the same CleanupStrategy over all threads in the multithreaded cleanup, patch fixes that', '+1', 'committed, thanks', '@marcus just to be clear. This affects concurrent compactions not multithreaded compactions which we anyway removed in 2.1. \\n\\nAs a temporary workaround until this is released, just set concurrent compactors to 1 in the yaml. ']\n",
            "my_comment: null\n",
            "-- StackTrace --\n",
            "java.util.ConcurrentModificationException\n",
            "        at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)\n",
            "        at java.util.ArrayList$Itr.next(ArrayList.java:831)\n",
            "        at org.apache.cassandra.db.index.SecondaryIndexManager.deleteFromIndexes(SecondaryIndexManager.java:476)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager$CleanupStrategy$Full.cleanup(CompactionManager.java:833)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupOne(CompactionManager.java:704)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:97)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:370)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:267)\n",
            "        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n",
            "        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
            "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
            "        at java.lang.Thread.run(Thread.java:745) \"Update this is happening across multiple nodes and Im guessing from the stacktrace that its related to secondary indexes.\" Additional details tried the following procedures with the same result:\n",
            "\n",
            "restart Cassandra then cleanup.\n",
            "repair -pr then cleanup (No errors with repair)\n",
            "repair then cleanup (No errors with repair)\n",
            "nodetool scrub then cleanup (No errors with scrub)\n",
            "nodetool rebuild_index (the only index on the table) then cleanup (No errors on the rebuild_index) seems we reuse the same CleanupStrategy over all threads in the multithreaded cleanup patch fixes that +1 committed thanks @marcus just to be clear. This affects concurrent compactions not multithreaded compactions which we anyway removed in 2.1. \n",
            "\n",
            "As a temporary workaround until this is released just set concurrent compactors to 1 in the yaml. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9031\n",
            "issue_type: Bug\n",
            "summary: nodetool info -T throws ArrayOutOfBounds when the node has not joined the cluster\n",
            "description: To reproduce, bring up a node that does not join the cluster, either using -Dcassandra.write_survey=true or -Dcassandra.join_ring=false, then run 'nodetool info -T'. You'll get the following stack trace:\n",
            "\n",
            "{code}ID                     : e384209f-f7a9-4cff-8fd5-03adfaa0d846\n",
            "Gossip active          : true\n",
            "Thrift active          : true\n",
            "Native Transport active: true\n",
            "Load                   : 76.69 KB\n",
            "Generation No          : 1427229938\n",
            "Uptime (seconds)       : 728\n",
            "Heap Memory (MB)       : 109.93 / 826.00\n",
            "Off Heap Memory (MB)   : 0.01\n",
            "Exception in thread \"main\" java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n",
            "\tat java.util.ArrayList.rangeCheck(ArrayList.java:635)\n",
            "\tat java.util.ArrayList.get(ArrayList.java:411)\n",
            "\tat org.apache.cassandra.tools.NodeProbe.getEndpoint(NodeProbe.java:676)\n",
            "\tat org.apache.cassandra.tools.NodeProbe.getDataCenter(NodeProbe.java:694)\n",
            "\tat org.apache.cassandra.tools.NodeCmd.printInfo(NodeCmd.java:666)\n",
            "\tat org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:1277){code}\n",
            "\n",
            "After applying the attached patch, the new error is:\n",
            "{code}ID                     : a7d76a2a-82d2-4faa-94e1-a30df6663ebb\n",
            "Gossip active          : true\n",
            "Thrift active          : false\n",
            "Native Transport active: false\n",
            "Load                   : 89.36 KB\n",
            "Generation No          : 1427231804\n",
            "Uptime (seconds)       : 12\n",
            "Heap Memory (MB)       : 135.49 / 826.00\n",
            "Off Heap Memory (MB)   : 0.01\n",
            "Exception in thread \"main\" java.lang.RuntimeException: This node does not have any tokens. Perhaps it is not part of the ring?\n",
            "\tat org.apache.cassandra.tools.NodeProbe.getEndpoint(NodeProbe.java:678)\n",
            "\tat org.apache.cassandra.tools.NodeProbe.getDataCenter(NodeProbe.java:698)\n",
            "\tat org.apache.cassandra.tools.NodeCmd.printInfo(NodeCmd.java:676)\n",
            "\tat org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:1313){code}\n",
            "architectural impact: NO\n",
            "comments: [\"Thanks for the patch.\\nAlthough your patch makes it clear tokens are not available yet, I think it'd be better to skip showing DC/Rack info rather than throwing exception.\\nIn addition to your patch, adding token check before displaying DC/Rack in {{NodeCmd#printInfo}} would be nice.\\n\\nWDYT?\", 'Sounds good to me. Do you want me to submit another patch?', 'Sorry for the delay.\\n{{TokenMetadata#getToken}} throws AssertionException when the node is not yet a member of the cluster.\\nChanging that methods behavior is a bit scary since it is used in other parts.\\n\\nInstead, we can use host ID to get endpoint from nodetool.\\nThat way, we can display DC/Rack as well.\\n\\npatch pushed to here: https://github.com/yukim/cassandra/tree/9031', '[~Stefania] to review', \"Maybe there is something I am missing so please double check but isn't this code\\n\\n{code}\\n    public String getEndpoint()\\n    {\\n        Map<String, String> hostIdToEndpoint = ssProxy.getHostIdMap();\\n        return hostIdToEndpoint.get(ssProxy.getLocalHostId());\\n    }\\n{code}\\n\\nalways going to return {{FBUtilities.getBroadcastAddress()}}?\\n\\nAside from this, code is +1.\\n\\nTechnically we should have a dtest, but since the nodetool tests are yet to be implemented, perhaps we should link this ticket to CASSANDRA-9349?\\n\", 'Thanks for the review.\\n\\nI guess the reason we are doing weired implementation for nodetool now is lack of the way to get endpoint, dc and rack info of connected node straight from JMX.\\nWe can add JMX interface for those in 3.0. (will create that later.)\\n\\nFor fix in 2.1 and 2.2, I think we need to stick with what we have for now.', 'Oh sorry about it, I forgot we are still client side.\\n\\n+1 to commit.\\n\\n', 'Committed, thanks!']\n",
            "my_comment: https://github.com/yukim/cassandra/tree/9031 [~Stefania] to review \"Maybe there is something I am missing so please double check but isnt this code\n",
            "\n",
            "{code}\n",
            "    public String getEndpoint()\n",
            "    {\n",
            "        Map<String String> hostIdToEndpoint = ssProxy.getHostIdMap();\n",
            "        return hostIdToEndpoint.get(ssProxy.getLocalHostId());\n",
            "    }\n",
            "{code}\n",
            "\n",
            "always going to return {{FBUtilities.getBroadcastAddress()}}?\n",
            "\n",
            "Aside from this code is +1.\n",
            "\n",
            "Technically we should have a dtest but since the nodetool tests are yet to be implemented perhaps we should link this ticket to CASSANDRA-9349?\n",
            "\" Thanks for the review.\n",
            "\n",
            "I guess the reason we are doing weired implementation for nodetool now is lack of the way to get endpoint dc and rack info of connected node straight from JMX.\n",
            "We can add JMX interface for those in 3.0. (will create that later.)\n",
            "\n",
            "For fix in 2.1 and 2.2 I think we need to stick with what we have for now. Oh sorry about it I forgot we are still client side.\n",
            "\n",
            "+1 to commit.\n",
            "\n",
            " Committed thanks!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1007\n",
            "issue_type: Improvement\n",
            "summary: Make memtable flush thresholds per-CF instead of global\n",
            "description: This is particularly useful in the scenario where you have a few CFs with a high volume of overwrite operations; increasing the memtable size/op count means that you can do the overwrite in memory before it ever hits disk.  Once on disk compaction is much more work for the system.\n",
            "\n",
            "But, you don't want to give _all_ your CFs that high of a threshold because the memory is better used elsewhere, and because it makes commitlog replay unnecessarily painful.\n",
            "architectural impact: NO\n",
            "comments: ['Working on this.', 'Still working on this, Todd?', \"Yeah, got distracted with some things. I'll try and have it finished this weekend. Just have to rebase what I have with trunk\", 'does it make sense to still have a global default which is used when the flush thresholds are not specified in the CF (i.e. *optionally* set them per CF)?', 'Yes, but it should be a sane default (see CASSANDRA-1469) not an extra layer of configurability.', 'CHANGES:\\n\\nM       conf/cassandra.yaml\\n- Moves memtable settings into cf by demonstration.\\n\\nM       src/java/org/apache/cassandra/db/Table.java\\nM       src/java/org/apache/cassandra/db/ColumnFamilyStore.java\\nM       src/java/org/apache/cassandra/db/Memtable.java\\n- All the places where we previously asked for the global flushtime/size/ops now ask for it on a per-CF basis.\\n\\nM       src/java/org/apache/cassandra/config/CFMetaData.java\\n- The bulk of the changes. This adds the new field, handles all its constructors, into and out of avro/thrift, validation, and defaulting.\\n\\nM       src/java/org/apache/cassandra/config/DatabaseDescriptor.java\\nM       src/java/org/apache/cassandra/config/Config.java\\nM       src/java/org/apache/cassandra/config/RawColumnFamily.java\\nM       src/java/org/apache/cassandra/config/Converter.java\\n- Changed the way we read in the YAML, and updated the converter to handle 0.6 -> 0.7 transitions.\\n\\nM       src/java/org/apache/cassandra/thrift/CassandraServer.java\\nM       src/java/org/apache/cassandra/avro/CassandraServer.java\\n- Updated add/update CF to add new fields and validate them appropriately.\\n\\nM       interface/cassandra.genavro\\nM       interface/thrift/gen-java/org/apache/cassandra/thrift/Constants.java\\nM       interface/thrift/gen-java/org/apache/cassandra/thrift/CfDef.java\\nM       interface/cassandra.thrift\\n- Adds three new fields to the CfDef, and adds the generated bindings.', \"Nope, tests are now crashing and burning.\\n\\nEDIT1:\\ntest/conf/cassandra.yaml was still using global memtable settings, so that was the first error.\\nNow it's complaining that KSMetaData.deflate() isn't defined, which is broken.\", 'Tests fail to compile:\\n\\n    [javac] Compiling 80 source files to /srv/cassandra/build/test/classes\\n    [javac] /srv/cassandra/test/unit/org/apache/cassandra/db/DefsTest.java:686: cannot find symbol\\n    [javac] symbol  : constructor CFMetaData(java.lang.String,java.lang.String,org.apache.cassandra.db.ColumnFamilyType,org.apache.cassandra.db.marshal.UTF8Type,<nulltype>,java.lang.String,int,boolean,double,int,int,org.apache.cassandra.db.marshal.BytesType,int,int,java.util.Map<byte[],org.apache.cassandra.config.ColumnDefinition>)\\n    [javac] location: class org.apache.cassandra.config.CFMetaData\\n    [javac]         return new CFMetaData(ks,\\n    [javac]                ^\\n\\n', 'Updated but still broken.', \"Apparantly it's only broken for me.\", 'Updated the pretty print to not crash on usage. Now uses commons-lang ToStringBuilder instead.', 'Committed.', 'Integrated in Cassandra #567 (See [https://hudson.apache.org/hudson/job/Cassandra/567/])\\n    Make memtable flush thresholds per-CF instead of global.  Patch by Jon Hermes, reviewed by brandonwilliams for CASSANDRA-1007\\n']\n",
            "my_comment: class org.apache.cassandra.config.CFMetaData\n",
            "    [javac]         return new CFMetaData(ks\n",
            "    [javac]                ^\n",
            "\n",
            " Updated but still broken. \"Apparantly its only broken for me.\" Updated the pretty print to not crash on usage. Now uses commons-lang ToStringBuilder instead. Committed. Integrated in Cassandra #567 (See [https://hudson.apache.org/hudson/job/Cassandra/567/])\n",
            "    Make memtable flush thresholds per-CF instead of global.  Patch by Jon Hermes reviewed by brandonwilliams for CASSANDRA-1007\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-10164\n",
            "issue_type: Bug\n",
            "summary: Re-apply MV updates on commitlog replay\n",
            "description: If a node crashes between the Commit log update and local memtable update of the materialized view the node replica could lose MV data.  This is really only an issue for RF=1 since the other replicas will likely apply successfully.\n",
            "\n",
            "In any case we should fix this so MV updates are always applied even during commit log replay (with care to not re-add the mutations to the commit log).   \n",
            "architectural impact: NO\n",
            "comments: [\"[patch|https://github.com/tjake/cassandra/tree/10164]\\n[tests|http://cassci.datastax.com/job/tjake-10164-testall/1/]\\n[dtests|http://cassci.datastax.com/job/tjake-10164-dtest/1/]\\n\\n*  I cleaned up the areas that were trying to catch write timeouts from submitMV since that isn't possible as it's all done locally now and actual view updates are async.\\n\\n* Added logic to make MV updates from CL replay not write to the commit log (since we always flush after CL replay anyway)\\n\\n*  Added logic to avoid CL when updating mutations from a streamed sstable (it will flush before transaction is complete)\\n\\n* Found/Fixed a little bug in the builder that would not build > 128 rows in a partition.  I'll add a test for this...\", 'Added wide partition builder test', '+1', 'committed']\n",
            "my_comment: [\"[patch|https://github.com/tjake/cassandra/tree/10164]\n",
            "[tests|http://cassci.datastax.com/job/tjake-10164-testall/1/]\n",
            "[dtests|http://cassci.datastax.com/job/tjake-10164-dtest/1/]\n",
            "\n",
            "*  I cleaned up the areas that were trying to catch write timeouts from submitMV since that isnt possible as its all done locally now and actual view updates are async.\n",
            "\n",
            "* Added logic to make MV updates from CL replay not write to the commit log (since we always flush after CL replay anyway)\n",
            "\n",
            "*  Added logic to avoid CL when updating mutations from a streamed sstable (it will flush before transaction is complete)\n",
            "\n",
            "* Found/Fixed a little bug in the builder that would not build > 128 rows in a partition.  Ill add a test for this...\" Added wide partition builder test +1 committed\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-10363\n",
            "issue_type: Bug\n",
            "summary: NullPointerException returned with select ttl(value), IN, ORDER BY and paging off\n",
            "description: Running this query with paging off returns a NullPointerException:\n",
            "\n",
            "cqlsh:test> SELECT value, ttl(value), last_modified FROM test where useruid='userid1' AND direction IN ('out','in') ORDER BY last_modified; \n",
            "ServerError: <ErrorMessage code=0000 [Server error] message=\"java.lang.NullPointerException\">\n",
            "\n",
            "Here's the stack trace from the system.log:\n",
            "\n",
            "ERROR [SharedPool-Worker-1] 2015-09-17 13:11:03,937  ErrorMessage.java:251 - Unexpected exception during request\n",
            "java.lang.NullPointerException: null\n",
            "        at org.apache.cassandra.db.marshal.LongType.compareLongs(LongType.java:41) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at org.apache.cassandra.db.marshal.TimestampType.compare(TimestampType.java:48) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at org.apache.cassandra.db.marshal.TimestampType.compare(TimestampType.java:38) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:2419) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:2406) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at java.util.TimSort.countRunAndMakeAscending(TimSort.java:351) ~[na:1.8.0_40]\n",
            "        at java.util.TimSort.sort(TimSort.java:216) ~[na:1.8.0_40]\n",
            "        at java.util.Arrays.sort(Arrays.java:1512) ~[na:1.8.0_40]\n",
            "        at java.util.ArrayList.sort(ArrayList.java:1454) ~[na:1.8.0_40]\n",
            "        at java.util.Collections.sort(Collections.java:175) ~[na:1.8.0_40]\n",
            "        at org.apache.cassandra.cql3.statements.SelectStatement.orderResults(SelectStatement.java:1400) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:1255) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:299) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:276) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:224) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:67) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:238) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at com.datastax.bdp.cassandra.cql3.DseQueryHandler$StatementExecution.execute(DseQueryHandler.java:291) ~[dse.jar:4.7.3]\n",
            "        at com.datastax.bdp.cassandra.cql3.DseQueryHandler$Operation.executeWithTiming(DseQueryHandler.java:223) ~[dse.jar:4.7.3]\n",
            "        at com.datastax.bdp.cassandra.cql3.DseQueryHandler$Operation.executeWithAuditLogging(DseQueryHandler.java:259) ~[dse.jar:4.7.3]\n",
            "        at com.datastax.bdp.cassandra.cql3.DseQueryHandler.process(DseQueryHandler.java:94) ~[dse.jar:4.7.3]\n",
            "        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:122) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:439) [cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:335) [cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]\n",
            "        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]\n",
            "        at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]\n",
            "        at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]\n",
            "        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_40]\n",
            "        at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [cassandra-all-2.1.8.689.jar:2.1.8.689]\n",
            "        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_40]\n",
            "\n",
            "Here's the full reproduction:\n",
            "\n",
            "CREATE KEYSPACE TEST \n",
            "WITH replication = {'class': 'SimpleStrategy', 'replication_factor':3} \n",
            "AND durable_writes = true;\n",
            "\n",
            "USE test;\n",
            "\n",
            "CREATE TABLE test ( \n",
            "useruid varchar, \n",
            "direction varchar, \n",
            "last_modified timestamp, \n",
            "value varchar, \n",
            "PRIMARY KEY ((useruid, direction), last_modified) \n",
            ");\n",
            "\n",
            "//insert 4 entries in the table \n",
            "INSERT INTO test (useruid,direction,last_modified,value) VALUES ('userid1', 'out', '2013-05-13 15:18:51', 'a value1'); \n",
            "INSERT INTO test (useruid,direction,last_modified,value) VALUES ('userid1', 'out', '2013-05-13 15:12:51', 'a value2'); \n",
            "INSERT INTO test (useruid,direction,last_modified,value) VALUES ('userid1', 'none', '2013-05-13 15:20:51', 'a value3'); \n",
            "INSERT INTO test (useruid,direction,last_modified,value) VALUES ('userid1', 'in', '2013-05-13 15:34:51', 'a value4');\n",
            "\n",
            "First query to check the value in the table, and its results : \n",
            "\n",
            "SELECT value, ttl(value), last_modified FROM test; \n",
            "value | ttl(value) | last_modified\n",
            "----------+------------+--------------------------\n",
            "a value4 | null | 2013-05-13 15:34:51+0000\n",
            "a value2 | null | 2013-05-13 15:12:51+0000\n",
            "a value1 | null | 2013-05-13 15:18:51+0000\n",
            "a value3 | null | 2013-05-13 15:20:51+0000\n",
            "\n",
            "(4 rows)\n",
            "\n",
            "Run this query using the IN clause and the ORDER BY clause, but it fails with an error:\n",
            "\n",
            "SELECT value, ttl(value), last_modified FROM test where useruid='userid1' AND direction IN ('out','in') ORDER BY last_modified; \n",
            "InvalidRequest: code=2200 [Invalid query] message=\"Cannot page queries with both ORDER BY and a IN restriction on the partition key; you must either remove the ORDER BY or the IN and sort client side, or disable paging for this query\"\n",
            "\n",
            "If you run the same query without the ttl(value) in the SELECT part, it also shows the same error:\n",
            " \n",
            "SELECT value, last_modified FROM test where useruid='userid1' AND direction IN ('out','in') ORDER BY last_modified; \n",
            "InvalidRequest: code=2200 [Invalid query] message=\"Cannot page queries with both ORDER BY and a IN restriction on the partition key; you must either remove the ORDER BY or the IN and sort client side, or disable paging for this query\"\n",
            "\n",
            "This message suggests these JIRAs are the reason for this message:\n",
            "\n",
            "https://issues.apache.org/jira/browse/CASSANDRA-7853 select . . . in . . . order by regression\n",
            "Resolution: Duplicate of CASSANDRA-7514\n",
            "Fix Version/s: None\n",
            "\n",
            "https://issues.apache.org/jira/browse/CASSANDRA-7514 Support paging in cqlsh\n",
            "Resolution: Fixed\n",
            "Fix Version/s: 2.1.1\n",
            "\n",
            "https://issues.apache.org/jira/browse/CASSANDRA-6722 cross-partition ordering should have warning or be disallowed when paging\n",
            "Resolution: Fixed\n",
            "Fix Version/s: 2.0.6\n",
            "\n",
            "If I turn off paging:\n",
            "\n",
            "cqlsh:test> paging off;\n",
            "Disabled Query paging.\n",
            "\n",
            "Then re-run the query without the ttl(value) I see the results:\n",
            "\n",
            "cqlsh:test> SELECT value, last_modified FROM test where useruid='userid1' AND direction IN ('out','in') ORDER BY last_modified;\n",
            "\n",
            "value | last_modified\n",
            "----------+--------------------------\n",
            "a value2 | 2013-05-13 15:12:51+0000\n",
            "a value1 | 2013-05-13 15:18:51+0000\n",
            "a value4 | 2013-05-13 15:34:51+0000\n",
            "\n",
            "(3 rows)\n",
            "\n",
            "However, if you now re-run this query with the ttl(value) you get a NullPointerException:\n",
            "\n",
            "cqlsh:test> SELECT value, ttl(value), last_modified FROM test where useruid='userid1' AND direction IN ('out','in') ORDER BY last_modified; \n",
            "ServerError: <ErrorMessage code=0000 [Server error] message=\"java.lang.NullPointerException\">\n",
            "architectural impact: NO\n",
            "comments: ['The patch for 2.1 is [here|https://github.com/apache/cassandra/compare/trunk...blerer:10363-2.1] and the patch for 2.2 is [here|https://github.com/apache/cassandra/compare/trunk...blerer:10363-2.2].\\n\\nThe problem comes from the fact that the column index used by the {{Comparator}} performing the sort is the one of the fetched columns and not the one of the {{ResultSet}} columns. \\nIn the case where no {{Selector}} is used the two indexes are the same. It is not always the case when some {{Selectors}} are used.\\n\\nThe patches make sure than the index used to sort the columns are the proper ones.\\nThey also fix {{UntypedResultSet}} which was providing access to the columns used for ordering. \\n\\n* The unit test results for 2.1 are [here|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-10363-2.1-testall/4/]\\n* The dtest results for 2.1 are [here|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-10363-2.1-dtest/4/]\\n* The unit test results for 2.2 are [here|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-10363-2.2-testall/2/]\\n* The dtest results for 2.2 are [here|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-10363-2.2-dtest/2/]\\n* The unit test results for 3.0 are [here|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-10363-3.0-testall/3/]\\n* The dtest results for 3.0 are [here|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-10363-3.0-dtest/3/]', '+1. My only nit is the naming of {{Selector::isReturningColumn}}, which seems a little clunky. Do you think {{isSimpleSelectorFactory}} would work there, or is it too specific? It would be more consistent with the existing methods, {{isAggregateSelectorFactory/isWriteTimeSelectorFactory/isTTLSelectorFactory}}. ', \"I've attached a patch backporting this to 2.0, not for actually committing but so those unable to upgrade just yet can patch their own systems if necessary. The test changes the expectations for a few scenarios from the 2.1+ version because CASSANDRA-4911 isn't in 2.0 & so {{ORDER BY}} can only contain columns in the selection.\\n\\n[branch|https://github.com/beobal/cassandra/tree/10363-2.0], [testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10363-2.0-testall/], [dtests|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10363-2.0-dtest/] (test runs pending)\\n\\nEdit: there are a few dtest failures in the run above, but checking these against 2.0 there aren't any new failures.\", 'Thanks for the 2.0 patch it looks good to me.', 'Committed in 2.1 at f587397c9c41c1a68b4e46fc16bad8d48c975e4d and merged in 2.2, 3.0 and trunk', 'This should probably get a CHANGES.txt entry.', 'Sorry, I forgot it. \\nI pushed the entry in 2.1 at 86583af4ca0eac34725136adee3143f9b14b75b4 and merged it into 2.2, 3.0 and trunk']\n",
            "my_comment: there are a few dtest failures in the run above but checking these against 2.0 there arent any new failures.\" Thanks for the 2.0 patch it looks good to me. Committed in 2.1 at f587397c9c41c1a68b4e46fc16bad8d48c975e4d and merged in 2.2 3.0 and trunk This should probably get a CHANGES.txt entry. Sorry I forgot it. \n",
            "I pushed the entry in 2.1 at 86583af4ca0eac34725136adee3143f9b14b75b4 and merged it into 2.2 3.0 and trunk\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-10805\n",
            "issue_type: New Feature\n",
            "summary: Additional Compaction Logging\n",
            "description: Currently, viewing the results of past compactions requires parsing the log and looking at the compaction history system table, which doesn't have information about, for example, flushed sstables not previously compacted.\n",
            "\n",
            "This is a proposal to extend the information captured for compaction. Initially, this would be done through a JMX call, but if it proves to be useful and not much overhead, it might be a feature that could be enabled for the compaction strategy all the time.\n",
            "\n",
            "Initial log information would include:\n",
            "- The compaction strategy type controlling each column family\n",
            "- The set of sstables included in each compaction strategy\n",
            "- Information about flushes and compactions, including times and all involved sstables\n",
            "- Information about sstables, including generation, size, and tokens\n",
            "- Any additional metadata the strategy wishes to add to a compaction or an sstable, like the level of an sstable or the type of compaction being performed\n",
            "\n",
            "architectural impact: NO\n",
            "comments: [\"I've pushed a [branch|https://github.com/carlyeks/cassandra/tree/ticket/10805]. It adds a CompactionLogger class which the compaction strategies can use. Currently, most of the work happens in either the Flush runnable or the CompactionTask, but more statements could be added for the strategies to use.\\n\\nThere is an added table parameter {{log_all}} which, if set to true, will start logging right away. Otherwise, there is a JMX operation to start compaction logging for a CF.\", '* Can we use logback to do the logging to file? It should be possible to create a special logger that goes to a separate file. Feels wrong to implement our own log rotation etc for this\\n* To enable/disable, can we just change the log level of that logger?\\n* Would be nice if the logging could be a bit more self-describing and human readable - JSON?', \"* I was initially using logback, but changed because I was getting incomplete files (since I didn't know when a new file was created). Looking at the [logback docs|http://logback.qos.ch/manual/appenders.html#RollingFileAppender] it seems like I probably just need to implement these two classes to make sure the logs are complete\\n* That will work well; I'll make sure the logger has the name of the table it is assigned to in order to capture just the output from one table\\n* Good point; this could also simplify some of the multiple-line events\", \"I had the idea for this JIRA a few months ago, but was too busy/distracted to do anything with it.  Really glad to see that it's not only been added by others, but that it's also actively being addressed.  Fantastic.\\n\\nWe're doing some comparative studies of STCS and DTCS for a huge C* user and these enhancements will really provide actionable metrics for people desiring to tune their compaction procedures.  \", \"I've been looking at logback and how we might be able to use logback directly. The biggest problem is that there isn't a notification for when the log file changes. We need to know that the logfile is changing so that we can log out the sstables that are already on disk so that each logfile is independent (and old ones can be deleted).\\nWe won't be able to use the logback loggers, or logback.xml, because we won't be able to use the loggers as they are currently defined, since additional information needs to be passed on creation of the logger to know what tables' files to log on a new file.\\nWe can still use the infrastructure of logback to be able to execute the mechanics of doing the log rotations so that we aren't responsible for it; it just won't be as seamless as updating the logback.xml files and having it reread it.\", \"I've pushed a new version of this branch which is updated for the CASSANDRA-6696 changes, and outputs JSON objects. I haven't looked at how to use just the parts of logback that we would want to yet, but if the current approach looks OK except for reimplementing the log rolling, I'll take a look at it early next week.\", 'it looks good, a few comments;\\n\\n* timestamps on all log entries\\n* it could perhaps be useful to log if the compaction strategy handles repaired or unrepaired data and which data directory it is handling instead of the strategy id? Or perhaps log the mapping on startup so we can figure it out?\\n\\nand a nit - remove redundant \"this.\" in CompactionLogger.\\n\\nAnd an idea - feel free to ignore, could we make the serialization \\'pluggable\\' in CompactionLogger? Then we could for example have all nodes in a cluster write to a socket somewhere so that we don\\'t have to ship log files to visualize? We could do this in a followup ticket when/if anyone needs it though', \"This looks really promising. I've played around with the branch and did some minor changes in a [PR|https://github.com/carlyeks/cassandra/pull/1/files].\\n\\nHowever, I'm still not sure why you plan to implement your own file rolling logic. Getting files rolled by logback and archive them manually afterwards would work perfectly fine for me.\", \"[~spodxx@gmail.com] It isn't possible to use logback as we do everywhere else here, as we need to know when the log file has rotated and add all of the current sstables. If we don't have that, the log file won't represent the state of the compaction strategy at the time we start. We'll end up replacing the custom logging logic with logback, but right now I'm just using something really simple to focus on the rest of the logging system.\", 'setting as patch available', \"I've pushed a new version which addresses the comments. I started a {{CompactionLogger.Writer}} interface; wanted to know if this made sense, or if we should change to having these be objects that could be serialized as JSON to be serialized differently for different interfaces.\\n\\nI've just kicked off new utests/dtests, so we'll see how it looks.\", 'Wanted to run all tests with the logging default to on, but I get this on startup:\\n{code}\\njava.lang.NullPointerException\\n        at org.apache.cassandra.db.compaction.CompactionStrategyManager.getStrategyFolders(CompactionStrategyManager.java:674)\\n        at org.apache.cassandra.db.compaction.CompactionLogger.startStrategy(CompactionLogger.java:172)\\n        at org.apache.cassandra.db.compaction.CompactionLogger.lambda$compactionStrategyMap$1(CompactionLogger.java:126)\\n        at java.util.ArrayList.forEach(ArrayList.java:1249)\\n        at org.apache.cassandra.db.compaction.CompactionLogger.lambda$forEach$0(CompactionLogger.java:120)\\n        at java.util.Arrays$ArrayList.forEach(Arrays.java:3880)\\n        at org.apache.cassandra.db.compaction.CompactionLogger.forEach(CompactionLogger.java:120)\\n        at org.apache.cassandra.db.compaction.CompactionLogger.compactionStrategyMap(CompactionLogger.java:126)\\n        at org.apache.cassandra.db.compaction.CompactionLogger.startStrategies(CompactionLogger.java:213)\\n        at org.apache.cassandra.db.compaction.CompactionLogger.enable(CompactionLogger.java:221)\\n        at org.apache.cassandra.db.compaction.CompactionStrategyManager.startup(CompactionStrategyManager.java:150)\\n        at org.apache.cassandra.db.compaction.CompactionStrategyManager.reload(CompactionStrategyManager.java:246)\\n        at org.apache.cassandra.db.compaction.CompactionStrategyManager.<init>(CompactionStrategyManager.java:86)\\n        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:408)\\n        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:367)\\n        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:577)\\n        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:554)\\n        at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:383)\\n        at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:320)\\n        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:130)\\n        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:107)\\n        at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:889)\\n        at org.apache.cassandra.service.StartupChecks$8.execute(StartupChecks.java:297)\\n        at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:106)\\n        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:169)\\n        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:551)\\n        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:680)\\n{code}\\n\\nother than that, code LGTM', 'I pushed a new update that includes a fix, and then also pushed a test to make sure that  activate the compaction logger for all column families. [utest|http://cassci.datastax.com/job/carlyeks-ticket-10805-logall-testall/] [dtest|http://cassci.datastax.com/job/carlyeks-ticket-10805-logall-dtest/]\\n\\nI need to dig into the dtest results to figure out whether they are being caused by the new logging.', '[~krummas] I reran the logall branch dtests, and they look much better now.', 'Nice, +1', 'Thanks, [~krummas]. Commited as [e16d8a7|https://git-wip-us.apache.org/repos/asf/cassandra/?p=cassandra.git;a=commit;h=e16d8a7a667d50271a183a95be894126cb2a5414].']\n",
            "my_comment: [\"Ive pushed a [branch|https://github.com/carlyeks/cassandra/tree/ticket/10805]. It adds a CompactionLogger class which the compaction strategies can use. Currently most of the work happens in either the Flush runnable or the CompactionTask but more statements could be added for the strategies to use.\n",
            "\n",
            "There is an added table parameter {{log_all}} which if set to true will start logging right away. Otherwise there is a JMX operation to start compaction logging for a CF.\" * Can we use logback to do the logging to file? It should be possible to create a special logger that goes to a separate file. Feels wrong to implement our own log rotation etc for this\n",
            "* To enable/disable can we just change the log level of that logger?\n",
            "* Would be nice if the logging could be a bit more self-describing and human readable - JSON? \"* I was initially using logback but changed because I was getting incomplete files (since I didnt know when a new file was created). Looking at the [logback docs|http://logback.qos.ch/manual/appenders.html#RollingFileAppender] it seems like I probably just need to implement these two classes to make sure the logs are complete\n",
            "* That will work well; Ill make sure the logger has the name of the table it is assigned to in order to capture just the output from one table\n",
            "* Good point; this could also simplify some of the multiple-line events\" \"I had the idea for this JIRA a few months ago but was too busy/distracted to do anything with it.  Really glad to see that its not only been added by others but that its also actively being addressed.  Fantastic.\n",
            "\n",
            "Were doing some comparative studies of STCS and DTCS for a huge C* user and these enhancements will really provide actionable metrics for people desiring to tune their compaction procedures.  \" \"Ive been looking at logback and how we might be able to use logback directly. The biggest problem is that there isnt a notification for when the log file changes. We need to know that the logfile is changing so that we can log out the sstables that are already on disk so that each logfile is independent (and old ones can be deleted).\n",
            "We wont be able to use the logback loggers or logback.xml because we wont be able to use the loggers as they are currently defined since additional information needs to be passed on creation of the logger to know what tables files to log on a new file.\n",
            "We can still use the infrastructure of logback to be able to execute the mechanics of doing the log rotations so that we arent responsible for it; it just wont be as seamless as updating the logback.xml files and having it reread it.\" \"Ive pushed a new version of this branch which is updated for the CASSANDRA-6696 changes and outputs JSON objects. I havent looked at how to use just the parts of logback that we would want to yet but if the current approach looks OK except for reimplementing the log rolling Ill take a look at it early next week.\" it looks good a few comments;\n",
            "\n",
            "* timestamps on all log entries\n",
            "* it could perhaps be useful to log if the compaction strategy handles repaired or unrepaired data and which data directory it is handling instead of the strategy id? Or perhaps log the mapping on startup so we can figure it out?\n",
            "\n",
            "and a nit - remove redundant \"this.\" in CompactionLogger.\n",
            "\n",
            "And an idea - feel free to ignore could we make the serialization \\pluggable\\ in CompactionLogger? Then we could for example have all nodes in a cluster write to a socket somewhere so that we don\\t have to ship log files to visualize? We could do this in a followup ticket when/if anyone needs it though \"This looks really promising. Ive played around with the branch and did some minor changes in a [PR|https://github.com/carlyeks/cassandra/pull/1/files].\n",
            "\n",
            "However Im still not sure why you plan to implement your own file rolling logic. Getting files rolled by logback and archive them manually afterwards would work perfectly fine for me.\" \"[~spodxx@gmail.com] It isnt possible to use logback as we do everywhere else here as we need to know when the log file has rotated and add all of the current sstables. If we dont have that the log file wont represent the state of the compaction strategy at the time we start. Well end up replacing the custom logging logic with logback but right now Im just using something really simple to focus on the rest of the logging system.\" setting as patch available \"Ive pushed a new version which addresses the comments. I started a {{CompactionLogger.Writer}} interface; wanted to know if this made sense or if we should change to having these be objects that could be serialized as JSON to be serialized differently for different interfaces.\n",
            "\n",
            "Ive just kicked off new utests/dtests so well see how it looks.\" Wanted to run all tests with the logging default to on but I get this on startup:\n",
            "{code}\n",
            "java.lang.NullPointerException\n",
            "        at org.apache.cassandra.db.compaction.CompactionStrategyManager.getStrategyFolders(CompactionStrategyManager.java:674)\n",
            "        at org.apache.cassandra.db.compaction.CompactionLogger.startStrategy(CompactionLogger.java:172)\n",
            "        at org.apache.cassandra.db.compaction.CompactionLogger.lambda$compactionStrategyMap$1(CompactionLogger.java:126)\n",
            "        at java.util.ArrayList.forEach(ArrayList.java:1249)\n",
            "        at org.apache.cassandra.db.compaction.CompactionLogger.lambda$forEach$0(CompactionLogger.java:120)\n",
            "        at java.util.Arrays$ArrayList.forEach(Arrays.java:3880)\n",
            "        at org.apache.cassandra.db.compaction.CompactionLogger.forEach(CompactionLogger.java:120)\n",
            "        at org.apache.cassandra.db.compaction.CompactionLogger.compactionStrategyMap(CompactionLogger.java:126)\n",
            "        at org.apache.cassandra.db.compaction.CompactionLogger.startStrategies(CompactionLogger.java:213)\n",
            "        at org.apache.cassandra.db.compaction.CompactionLogger.enable(CompactionLogger.java:221)\n",
            "        at org.apache.cassandra.db.compaction.CompactionStrategyManager.startup(CompactionStrategyManager.java:150)\n",
            "        at org.apache.cassandra.db.compaction.CompactionStrategyManager.reload(CompactionStrategyManager.java:246)\n",
            "        at org.apache.cassandra.db.compaction.CompactionStrategyManager.<init>(CompactionStrategyManager.java:86)\n",
            "        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:408)\n",
            "        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:367)\n",
            "        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:577)\n",
            "        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:554)\n",
            "        at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:383)\n",
            "        at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:320)\n",
            "        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:130)\n",
            "        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:107)\n",
            "        at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:889)\n",
            "        at org.apache.cassandra.service.StartupChecks$8.execute(StartupChecks.java:297)\n",
            "        at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:106)\n",
            "        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:169)\n",
            "        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:551)\n",
            "        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:680)\n",
            "{code}\n",
            "\n",
            "other than that code LGTM I pushed a new update that includes a fix and then also pushed a test to make sure that  activate the compaction logger for all column families. [utest|http://cassci.datastax.com/job/carlyeks-ticket-10805-logall-testall/] [dtest|http://cassci.datastax.com/job/carlyeks-ticket-10805-logall-dtest/]\n",
            "\n",
            "I need to dig into the dtest results to figure out whether they are being caused by the new logging. [~krummas] I reran the logall branch dtests and they look much better now. Nice +1 Thanks [~krummas]. Commited as [e16d8a7|https://git-wip-us.apache.org/repos/asf/cassandra/?p=cassandra.git;a=commit;h=e16d8a7a667d50271a183a95be894126cb2a5414].\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-11356\n",
            "issue_type: Bug\n",
            "summary: EC2MRS ignores broadcast_rpc_address setting in cassandra.yaml\n",
            "description: EC2MRS ignores broadcast_rpc_address setting in cassandra.yaml.  This is problematic for those users who were using EC2MRS with an internal rpc_address before the change introduced in [CASSANDRA-5899|https://issues.apache.org/jira/browse/CASSANDRA-5899], because the change results in EC2MRS always using the public ip regardless of what the user has set for broadcast_rpc_address.\n",
            "architectural impact: NO\n",
            "comments: ['We should be able to introduce an option to make this configurable, since this affects vpc deployments.', \"I'm not sure we should even be setting {{broadcast_rpc_address}} on EC2MRS. While setting {{broadcast_address}} is useful to the snitch's objective of making nodes from different dcs connect over the public address for internal communication, {{broadcast_rpc_address}} is a deployment option independent of EC2MRS choice. I'd be in favor of not setting this at all on EC2MRS and adding an upgrade notice for users who've been relying on this behavior.\", 'On EC2 users need to choose which {{rpc_address}} to broadcast to other nodes: if the private IP or the public IP (since both are routable to the private IF).  Before CASSANDRA-5899 it broadcasted {{rpc_address}} which defaulted to {{listen_address}}, which was typically set to private IP on EC2 deployments. CASSANDRA-5899 added ability to choose which IP to broadcast via the {{broadcast_rpc_address}}, but it also changed {{Ec2MultiRegionSnitch}} to *always* broadcast the public IP, regardless of {{broadcast_rpc_address}}, what makes impossible for nodes to advertise their private IP for client connections if they want to.\\n\\nThis patch updates {{Ec2MultiRegionSnitch}} to only set {{broadcast_rpc_address}} to the public IP if this property is unset, allowing operators to overide this to the private IP if they want to. \\n\\nBefore {{DatabaseDescriptor}} was setting {{broadcastRpcAddress = rpcAddress}}, so it was impossible to know if {{broadcastRpcAddress == null}} in order to decide whether or not to override the property on {{Ec2MultiRegionSnitch}}, so I modified all uses of {{DatabaseDescriptor.getBroadcastRpcAddress()}} to use {{FBUtilities.getBroadcastRpcAddress()}} instead which will fallback to {{DatabaseDescriptor.getRpcAddress()}} if {{DatabaseDescriptor.getBroadcastRpcAddress() == null}}.\\n\\nPatch and tests available below:\\n\\n||2.2||3.0||3.9||trunk||\\n|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-11356]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-11356]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.9...pauloricardomg:3.9-11356]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-11356]|\\n|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-11356-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-11356-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.9-11356-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-11356-testall/lastCompletedBuild/testReport/]|\\n|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-11356-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-11356-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.9-11356-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-11356-dtest/lastCompletedBuild/testReport/]|\\n\\nCould you have a look [~thobbs]? Thanks!', 'Overall, the patch looks good to me.  I think we need a couple of doc updates, though.  A description of the change in behavior in NEWS.txt would be good.  In 3.9 and trunk, {{doc/source/operating/snitch.rst}} could be updated to explain this behavior as well.\\n\\n[~jasobrown] as a former user of the ec2 snitch, do you have time to make a quick review of this as well?', \"Updated {{NEWS.txt}} and added the following note to {{doc/source/operating/snitch.rst}} on 3.9 and trunk:\\n\\nbq. By default, Ec2MultiRegionSnitch advertises the public instance IP as ``rpc_address``, allowing cross-DC discovery by token-aware clients but this may incur additional charges on EC2 for public IP access within the local DC if a token-aware client is used. In order to override this behavior and restrict token-aware clients to the local DC, set ``broadcast_rpc_address`` to the instance's private IP.\\n\\nAfter writing this, I wondered if we should provide this as an option on {{Ec2MultiRegionSnitch}}, but then I thought that we should ultimately fix this limitation by adding a {{broadcast_rpc_address}} column to the {{system.peers}} table, and drivers would pick either the private {{rpc_address}} or the public {{broadcast_rpc_address}} depending on where the client is located, but we should probably fix this in another ticket. WDYT?\", 'Another ticket would definitely be best for that.', 'Agreed.', \"It looks like some of the test runs were problematic, so I've restarted them to get some clearer results.\\n\\n[~jasobrown] last call on adding a second review on this :)\", \"[~thobbs] Sorry, missed the batcall as I was out for the last several days. Will try to get to it in the next 24-48 hours if that's ok.\", 'Cool, that would be just fine.', \"I'm +1 on the patch except for two minor issues:\\n\\n- is 2.2 in critical-fixes only mode now? If so, let's only commit to 3.0 and higher\\n- Can we add a couple of (simple) tests for {{FBUtliities#getBroadcastRpcAddress}}? I can easily see that the functionality is correct *now*, I'm just worried about the future if/when we move things around and we accidentally break things.\", \"Thanks for the review Jason. Nice call for the unit test, updated patch testing {{FBUtilities#getBroadcastRpcAddress}} and resubmitted all tests.\\n\\nRegarding commit to 2.2, I think we're not yet in critical-fixes-only-mode on 2.2 but please correct me if I'm wrong. And I think this is somewhat critical, in a sense that you're prohibited from setting a private {{rpc_address}} when using {{EC2MRS}} while this is a valid case and was possible before CASSANDRA-5899.\", \"[~pauloricardomg] Thanks for adding in the tests :)\\n\\nI checked on the IRC and we're still good for adding this to 2.2, so I'm +1 once the tests complete/pass.\", \"I'm also +1 on committing to 2.2.  Just let me know when the tests pass, and I'll get this committed.\", 'Test results look good, marking as ready to commit. Thanks all!', 'Great, committed as {{91f7387e1f785b18321777311a5c3416af0663c2}} to 2.2 and merged up.']\n",
            "my_comment: if the private IP or the public IP (since both are routable to the private IF).  Before CASSANDRA-5899 it broadcasted {{rpc_address}} which defaulted to {{listen_address}} which was typically set to private IP on EC2 deployments. CASSANDRA-5899 added ability to choose which IP to broadcast via the {{broadcast_rpc_address}} but it also changed {{Ec2MultiRegionSnitch}} to *always* broadcast the public IP regardless of {{broadcast_rpc_address}} what makes impossible for nodes to advertise their private IP for client connections if they want to.\n",
            "\n",
            "This patch updates {{Ec2MultiRegionSnitch}} to only set {{broadcast_rpc_address}} to the public IP if this property is unset allowing operators to overide this to the private IP if they want to. \n",
            "\n",
            "Before {{DatabaseDescriptor}} was setting {{broadcastRpcAddress = rpcAddress}} so it was impossible to know if {{broadcastRpcAddress == null}} in order to decide whether or not to override the property on {{Ec2MultiRegionSnitch}} so I modified all uses of {{DatabaseDescriptor.getBroadcastRpcAddress()}} to use {{FBUtilities.getBroadcastRpcAddress()}} instead which will fallback to {{DatabaseDescriptor.getRpcAddress()}} if {{DatabaseDescriptor.getBroadcastRpcAddress() == null}}.\n",
            "\n",
            "Patch and tests available below:\n",
            "\n",
            "||2.2||3.0||3.9||trunk||\n",
            "|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-11356]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-11356]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.9...pauloricardomg:3.9-11356]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-11356]|\n",
            "|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-11356-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-11356-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.9-11356-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-11356-testall/lastCompletedBuild/testReport/]|\n",
            "|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-11356-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-11356-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.9-11356-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-11356-dtest/lastCompletedBuild/testReport/]|\n",
            "\n",
            "Could you have a look [~thobbs]? Thanks! Overall the patch looks good to me.  I think we need a couple of doc updates though.  A description of the change in behavior in NEWS.txt would be good.  In 3.9 and trunk {{doc/source/operating/snitch.rst}} could be updated to explain this behavior as well.\n",
            "\n",
            "[~jasobrown] as a former user of the ec2 snitch do you have time to make a quick review of this as well? \"Updated {{NEWS.txt}} and added the following note to {{doc/source/operating/snitch.rst}} on 3.9 and trunk:\n",
            "\n",
            "bq. By default Ec2MultiRegionSnitch advertises the public instance IP as ``rpc_address`` allowing cross-DC discovery by token-aware clients but this may incur additional charges on EC2 for public IP access within the local DC if a token-aware client is used. In order to override this behavior and restrict token-aware clients to the local DC set ``broadcast_rpc_address`` to the instances private IP.\n",
            "\n",
            "After writing this I wondered if we should provide this as an option on {{Ec2MultiRegionSnitch}} but then I thought that we should ultimately fix this limitation by adding a {{broadcast_rpc_address}} column to the {{system.peers}} table and drivers would pick either the private {{rpc_address}} or the public {{broadcast_rpc_address}} depending on where the client is located but we should probably fix this in another ticket. WDYT?\" Another ticket would definitely be best for that. Agreed. \"It looks like some of the test runs were problematic so Ive restarted them to get some clearer results.\n",
            "\n",
            "[~jasobrown] last call on adding a second review on this :)\" \"[~thobbs] Sorry missed the batcall as I was out for the last several days. Will try to get to it in the next 24-48 hours if thats ok.\" Cool that would be just fine. \"Im +1 on the patch except for two minor issues:\n",
            "\n",
            "- is 2.2 in critical-fixes only mode now? If so lets only commit to 3.0 and higher\n",
            "- Can we add a couple of (simple) tests for {{FBUtliities#getBroadcastRpcAddress}}? I can easily see that the functionality is correct *now* Im just worried about the future if/when we move things around and we accidentally break things.\" \"Thanks for the review Jason. Nice call for the unit test updated patch testing {{FBUtilities#getBroadcastRpcAddress}} and resubmitted all tests.\n",
            "\n",
            "Regarding commit to 2.2 I think were not yet in critical-fixes-only-mode on 2.2 but please correct me if Im wrong. And I think this is somewhat critical in a sense that youre prohibited from setting a private {{rpc_address}} when using {{EC2MRS}} while this is a valid case and was possible before CASSANDRA-5899.\" \"[~pauloricardomg] Thanks for adding in the tests :)\n",
            "\n",
            "I checked on the IRC and were still good for adding this to 2.2 so Im +1 once the tests complete/pass.\" \"Im also +1 on committing to 2.2.  Just let me know when the tests pass and Ill get this committed.\" Test results look good marking as ready to commit. Thanks all! Great committed as {{91f7387e1f785b18321777311a5c3416af0663c2}} to 2.2 and merged up.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-11580\n",
            "issue_type: Sub-task\n",
            "summary: remove DatabaseDescriptor dependency from SegmentedFile\n",
            "description: Several configurable parameters are pulled from {{DatabaseDescriptor}} from {{SegmentedFile}} and its subclasses.\n",
            "architectural impact: NO\n",
            "comments: [\"||branch||testall||dtest||\\n|[11580|https://github.com/yukim/cassandra/tree/11580]|[testall|http://cassci.datastax.com/view/Dev/view/yukim/job/yukim-11580-testall/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/yukim/job/yukim-11580-dtest/lastCompletedBuild/testReport/]|\\n\\nThe patch that removes DatabaseDescriptor dependency and {{ChunkCache}} singleton access from {{SegmentedFile}}.\\n- All subclasses of {{SegmentedFile}} are merged into {{SegmentedFile}} since difference among those is in {{Rebufferer}} after CASSANDRA-5863.\\n- Introduced {{DiskOptimizationStrategy}} for {{disk_optimization_strategy}} config.\\n- {{SegmentedFile.Builder}}'s {{serializeBound}}/{{deserializedBound}} seem no longer used, so I removed them.\\n\", 'Really great stuff Yuki, I could only find minor things and I had to look really hard to find them :) The items marked as optional are left to your decision since they are more down to individual stylistic preferences:\\n\\n*SegmentedFile:*\\n\\n* Class comments are no longer up-to-date\\n* protected accessors can be private\\n* Optional: class can be final\\n* Comments of onDiskLength mention SegmentIterator that no longer exists\\n* Trivial: alignment error in the constructor parameters at line 66\\n* Cleanup.Tidy(): the chunk cache is invalidated only when the metadata is not null, this was the existing behavior but is it correct? Index files use a chunk cache even if they are not compressed?\\n* Cleanup.Tidy(): can either metdata.close or the cache invalidation throw? \\n* Optional: should we rename metadata to compressionMetadata in Cleanup?\\n* Class comments of {{Builder}} are also no longer up-to-date\\n* Suppress resource warnings for rebufferer in Builder.complete() since it is owned by the SegmentedFile\\n\\n*Other files:*\\n\\n* SStableReader ln 436: {{// special implementation of load to use non-pooled SegmentedFile builders}} can be removed\\n* The EMPTY BufferHolder in Rebuffered.java at line 62 can still be static final\\n* SegmentedFileTest should be renamed to DiskOptimizationStrategyTest\\n* Optional: There is a bit of code duplications in BigTableWriter openFinal and openForBatch as well as SSTableReader openForBatch and load. I wonder if we could introduce helper methods in SSTable to create the index and data builders and to create the index and data segmented files.', 'Thanks for review.\\nI feel like the name {{SegmentedFile}} is not appropriate anymore, and it just can be integrated with {{RandomAccessReader.Builder}}.\\nLet me work on that along with fixing your review points.\\n', 'bq. I feel like the name {{SegmentedFile}} is not appropriate anymore, and it just can be integrated with {{RandomAccessReader.Builder}}.\\n\\nI totally agree, sounds like a good plan.', \"I ended up renaming {{SegmentedFile}} to {{FileHandle}}. If anyone has better name, I'm open to suggestion.\\nI think I fixed he points in the review, except: The EMPTY BufferHolder in Rebuffered.java at line 62 can still be static final. variable in interface is implicitly declared as public static final, and my IntelliJ gives me warning if I left those.\\n\\n||branch||testall||dtest||\\n|[11580|https://github.com/yukim/cassandra/tree/11580]|[testall|http://cassci.datastax.com/view/Dev/view/yukim/job/yukim-11580-testall/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/yukim/job/yukim-11580-dtest/lastCompletedBuild/testReport/]|\\n\", \"I've pushed some nits [here|https://github.com/stef1927/cassandra/commit/ddee27868592076c7c30c285eb92938450eafe9e]: mostly edits in the comments, unused imports and more restrictive access modifiers in RAR. I've also fixed some resource management problems in unit tests, and in the two new {{open()}} methods in case of exceptions. I've rebased, which resulted in a couple of conflicts, especially in {{CommitLogReader}} where some code has been moved around. \\n\\nIf you're +1 on my changes and the CI results are OK, then I'm also + 1 and we can commit this:\\n\\n|[patch|https://github.com/stef1927/cassandra/commits/11580]|[testall|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-11580-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-11580-dtest/]|\\n\\nIn terms of a better name for {{FileHandle}}, I'm also short of suggestions, it is a factory for RAR basically, but one that owns resources, so the term {{Handle}} is probably as good as {{Factory}} and given that it is created by a builder, I tend to think that {{FileHandle}} is probably better then something with the term factory in it. Feel free to start a discussion on IRC re. a better name if you want, once you are back from holiday, or commit with {{FileHandle}}.\\n\", 'Change and tests looks good to me, thanks!\\nThe patch needs some rebasing, will do before code freeze.', 'Committed as {{b4133f38d5ef5fc50047eb4a31307ac97c5b72ee}}, thanks!', '[~yukim] this has introduced a new assertion in SSTable, which makes it a tiny bit harder to instantiate SSTable for tools, as previously the optimisation strategy was not required (or at least the assertion was not there). Do you think it makes sense to add some sort of default no-op optimiser that would work for tools? Assuming it can always be overridden in {{DatabaseDescriptor}}', \"[~ifesdjeen] We may be able to set default {{DiskOptimizationStrategy}} (which is 'ssd' from cassandra.yaml) in {{DatabaseDescriptor}}, but I think there are other configs to be set to properly open SSTable. So right now we need to do {{DatabaseDescriptor.toolInitialization}} anyway which reads config from {{cassandra.yaml}} file and set up that.\\n\\nLater I want to move all SSTable related config from DD and put them in SSTable config or something so we can have more control on configuring opening SSTable programatically.\\n\", \"Unfortunately, I can not use {{DatabaseDescriptor.toolInitialization}} as that'd require the config file to be present (which I don't since it's a client-only tool). I'll just wait until you have the patch for moving SSTable initialization code to SStable and will improve from there. Thank you!\"]\n",
            "my_comment: mostly edits in the comments unused imports and more restrictive access modifiers in RAR. Ive also fixed some resource management problems in unit tests and in the two new {{open()}} methods in case of exceptions. Ive rebased which resulted in a couple of conflicts especially in {{CommitLogReader}} where some code has been moved around. \n",
            "\n",
            "If youre +1 on my changes and the CI results are OK then Im also + 1 and we can commit this:\n",
            "\n",
            "|[patch|https://github.com/stef1927/cassandra/commits/11580]|[testall|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-11580-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-11580-dtest/]|\n",
            "\n",
            "In terms of a better name for {{FileHandle}} Im also short of suggestions it is a factory for RAR basically but one that owns resources so the term {{Handle}} is probably as good as {{Factory}} and given that it is created by a builder I tend to think that {{FileHandle}} is probably better then something with the term factory in it. Feel free to start a discussion on IRC re. a better name if you want once you are back from holiday or commit with {{FileHandle}}.\n",
            "\" Change and tests looks good to me thanks!\n",
            "The patch needs some rebasing will do before code freeze. Committed as {{b4133f38d5ef5fc50047eb4a31307ac97c5b72ee}} thanks! [~yukim] this has introduced a new assertion in SSTable which makes it a tiny bit harder to instantiate SSTable for tools as previously the optimisation strategy was not required (or at least the assertion was not there). Do you think it makes sense to add some sort of default no-op optimiser that would work for tools? Assuming it can always be overridden in {{DatabaseDescriptor}} \"[~ifesdjeen] We may be able to set default {{DiskOptimizationStrategy}} (which is ssd from cassandra.yaml) in {{DatabaseDescriptor}} but I think there are other configs to be set to properly open SSTable. So right now we need to do {{DatabaseDescriptor.toolInitialization}} anyway which reads config from {{cassandra.yaml}} file and set up that.\n",
            "\n",
            "Later I want to move all SSTable related config from DD and put them in SSTable config or something so we can have more control on configuring opening SSTable programatically.\n",
            "\" \"Unfortunately I can not use {{DatabaseDescriptor.toolInitialization}} as thatd require the config file to be present (which I dont since its a client-only tool). Ill just wait until you have the patch for moving SSTable initialization code to SStable and will improve from there. Thank you!\"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-11971\n",
            "issue_type: Improvement\n",
            "summary: More uses of DataOutputBuffer.RECYCLER\n",
            "description: There are a few more possible use cases for {{DataOutputBuffer.RECYCLER}}, which prevents a couple of (larger) allocations.\n",
            "\n",
            "(Will provide a patch soon)\n",
            "architectural impact: NO\n",
            "comments: ['Patch uses recycled {{DataOutputBuffer}}s instead of allocating new ones.\\nAlso introduces {{DataOutputBuffer.asNewBuffer()}} to replace some {{ByteBuffer.wrap(out.getData(), 0, out.getLength())}}.\\n\\n||trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:11971-more-recycler-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-11971-more-recycler-trunk-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-11971-more-recycler-trunk-dtest/lastSuccessfulBuild/]', '+1', 'Thanks!\\nCommitted as [063e91754b22a28a43efccb0c238c577a6bd0b8a|https://github.com/apache/cassandra/commit/063e91754b22a28a43efccb0c238c577a6bd0b8a] to [trunk|https://github.com/apache/cassandra/tree/trunk]\\n']\n",
            "my_comment: Patch uses recycled {{DataOutputBuffer}}s instead of allocating new ones.\n",
            "Also introduces {{DataOutputBuffer.asNewBuffer()}} to replace some {{ByteBuffer.wrap(out.getData() 0 out.getLength())}}.\n",
            "\n",
            "||trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:11971-more-recycler-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-11971-more-recycler-trunk-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-11971-more-recycler-trunk-dtest/lastSuccessfulBuild/] +1 Thanks!\n",
            "Committed as [063e91754b22a28a43efccb0c238c577a6bd0b8a|https://github.com/apache/cassandra/commit/063e91754b22a28a43efccb0c238c577a6bd0b8a] to [trunk|https://github.com/apache/cassandra/tree/trunk]\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-13010\n",
            "issue_type: New Feature\n",
            "summary: nodetool compactionstats should say which disk a compaction is writing to\n",
            "description: None\n",
            "architectural impact: NO\n",
            "comments: [\"I'm interested in taking on this task. I'm new to the project and would appreciate any guidance.\\n\\nThanks.\", \"That's great to hear, [~alourie]! \\n\\nThere's a quick intro for [how to contribute|http://cassandra.apache.org/doc/latest/development/patches.html] on the site. I'm not sure how much guidance you need, but I'll try to give you a  REALLY quick intro - feel free to ask for more (or less) as appropriate:\\n\\n{{nodetool}} is the general administrative tool that most cassandra operators use. It connects to cassandra primarily using JMX. \\n{{nodetool compactionstats}} uses the {{CompactionManager}} MBean to call [getCompactions|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/tools/nodetool/CompactionStats.java#L70]  , which basically [returns a list of maps|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1857] (where the map comes from a [CompactionInfo$Holder|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/CompactionInfo.java#L135] ) - you'll need to include the directory as an element in that map so that the {{nodetool}} command can print it out nicely. \", \"Thanks [~jjirsa]!\\n\\nI've started by adding a targetDirectory to the *CompactionInfo* class. Then I'm working backwards handling new instantiations with appropriate path parameters.\\n\\nThe problem I got at the moment is finding the correct *path* parameter for every such new call.  For instance, in https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/index/internal/CollatedViewIndexBuilder.java#L54; I can't find a proper argument to add as a target directory.\\n\\nAm I going the right direction? I've tried running the test with building ccm cluster and causing it to run compactions, but the target dir shown for a running compaction is empty, so I probably do something wrong somewhere.\\n\\nThanks.\", \"[~jjirsa] I've created a branch with my work at https://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010, please provide feedback and whether I'm on the right path.\\n\\nThanks a lot!\", \"[~rustyrazorblade] says he's happy to review for you.\\n\\n\", '[~jjirsa] Thanks!\\n\\n[~rustyrazorblade] please be gentle :-)', \"Hi [~alourie]!   Thanks for the patch.  I'm taking a look today.  Schedule is a bit busy since I'm on the road but I'll try to get you some feedback tonight.\\n\\nAnything I should look out for?  Any questions you have?\\n\", \"Hi [~rustyrazorblade]\\n\\nIn essence, I'd like to know if I'm on the right path. There are a lot of things in the codebase that I don't understand, but I hope that I'm at least going in right direction.\\n\\nThanks.\", \"I set up 6 data directories and dropped my memtable_heap_space_in_mb to 32 MB to force a ton of flushes and lots of compactions.  I do see directories in the compactionstats, indicating that it's working, but i haven't verified the right directories are listed.  \\n\\nThe output is a little hard to read, since the directories make the output look like this:\\n\\n{code}\\njhaddad@rustyrazorblade ~/dev/cassandra$ bin/nodetool compactionstats\\npending tasks: 6\\n- keyspace1.standard1: 6\\n\\nid                                   compaction type keyspace  table     target directory                                                                            completed total    unit  progress\\n7f3e7a40-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1 /Users/jhaddad/var/lib/cassandra/data1/keyspace1/standard1-f867e470fedb11e6a2c121962153156a 17696623  38340460 bytes 46.16%\\n83c14340-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1 /Users/jhaddad/var/lib/cassandra/data4/keyspace1/standard1-f867e470fedb11e6a2c121962153156a 3651789   33841385 bytes 10.79%\\n7a1beed0-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1 /Users/jhaddad/var/lib/cassandra/data3/keyspace1/standard1-f867e470fedb11e6a2c121962153156a 34012548  37166390 bytes 91.51%\\n7a2e6560-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1 /Users/jhaddad/var/lib/cassandra/data6/keyspace1/standard1-f867e470fedb11e6a2c121962153156a 33334698  36996530 bytes 90.10%\\n7dabcc50-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1 /Users/jhaddad/var/lib/cassandra/data2/keyspace1/standard1-f867e470fedb11e6a2c121962153156a 22993893  37875910 bytes 60.71%\\nActive compaction remaining time :   0h00m04s\\n{code}\\n\\nFrom a user interface perspective, It would be great if the tasks could be separated by directory, rather than inlining the directory in the table.  So in my example something more like:\\n\\n{code}\\njhaddad@rustyrazorblade ~/dev/cassandra$ bin/nodetool compactionstats  \\npending tasks: 6\\n- keyspace1.standard1: 6\\n\\nid                                   compaction type keyspace  table  completed total    unit  progress\\n\\n/Users/jhaddad/var/lib/cassandra/data1/keyspace1/standard1-f867e470fedb11e6a2c121962153156a\\n7f3e7a40-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1  17696623  38340460 bytes 46.16%\\n\\n/Users/jhaddad/var/lib/cassandra/data4/keyspace1/standard1-f867e470fedb11e6a2c121962153156a\\n83c14340-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1 3651789   33841385 bytes 10.79%\\n\\n/Users/jhaddad/var/lib/cassandra/data3/keyspace1/standard1-f867e470fedb11e6a2c121962153156a\\n7a1beed0-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1  34012548  37166390 bytes 91.51%\\n\\n/Users/jhaddad/var/lib/cassandra/data6/keyspace1/standard1-f867e470fedb11e6a2c121962153156a\\n7a2e6560-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1  33334698  36996530 bytes 90.10%\\n\\n/Users/jhaddad/var/lib/cassandra/data2/keyspace1/standard1-f867e470fedb11e6a2c121962153156a\\n7dabcc50-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1  22993893  37875910 bytes 60.71%\\nActive compaction remaining time :   0h00m04s\\n{code}\\n\\nIf there are multiple compactions in a single directory, you would see this:\\n\\n{code}\\n/Users/jhaddad/var/lib/cassandra/data3/keyspace1/standard1-f867e470fedb11e6a2c121962153156a\\n7a1beed0-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1  34012548  37166390 bytes 91.51%\\n7a2e6560-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1  33334698  36996530 bytes 90.10%\\n7dabcc50-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1  22993893  37875910 bytes 60.71%\\n{code}\\n\\nI'm also not sure if there's value in including the keyspace & table directory, since that information is duplicated.  If we limit the result to the data directory we would end up with at most N sections, 1 per data directory specified in the yaml.  I think that would be the *most* useful form of this output (note the line between the directories):\\n\\n{code}\\n/Users/jhaddad/var/lib/cassandra/data4\\n83c14340-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard1 3651789   33841385 bytes 10.79%\\n03c14340-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard2 3651789   13841385 bytes 10.79%\\n\\n/Users/jhaddad/var/lib/cassandra/data3\\n7a1beed0-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard3  34012548  57166390 bytes 91.51%\\n83c14340-fedc-11e6-a477-b714107dace1 Compaction      keyspace1 standard5 3651789   43841385 bytes 10.79%\\n{code}\\n\\nWhat do you think?\", \"[~rustyrazorblade] I've updated the patch with the suggested UI changes. It will now group compactions by a directory.\\n\\nPlease let me know how it looks, and also I'd love a feedback for the actual implementation of the feature.\\n\\nThanks!\", \"On the nit side, I've already noticed a handful of cases that violate the style guide.  { and } should always be on their own lines, even for single line functions.  \\n\\nhttp://cassandra.apache.org/doc/latest/development/code_style.html#general-code-conventions\\n\\nFor instance:\\nCompactionInfo.java: targetDirectory() \\nCompactionIterator.java: setTargetDirectory\\n\\nThere may be others, but I won't bother listing them (for now).  If you can take a pass & ensure the patch conforms to the style guide, I'll focus on the content of the patch.\", \"[~rustyrazorblade] totally fair. I'll ensure the conformity to the style guide.\\n\\nThanks.\", \"I don't believe this line does what it's supposed to:\\n\\n{code}\\nci.setTargetDirectory(cfs.getDirectories().getDirectoryForNewSSTables().getPath());\\n{code}\\n\\nFirst off, getDirectoryForNewSSTables returns *any* directory that's available to be written to. The call {{getDirectoryForNewSSTables}} has this comment:\\n\\n{code}\\n    /**\\n     * Basically the same as calling {@link #getWriteableLocationAsFile(long)} with an unknown size ({@code -1L}),\\n     * which may return any non-blacklisted directory - even a data directory that has no usable space.\\n     * Do not use this method in production code.\\n     *\\n     * @throws FSWriteError if all directories are blacklisted.\\n     */\\n    public File getDirectoryForNewSSTables()\\n    {\\n        return getWriteableLocationAsFile(-1L);\\n    }\\n{code}\\n\\nI believe a better way to approach this would be to use {{SSTableWriter.descriptor}} to pull out the target directory.\", \"[~rustyrazorblade] I've updated the code with a couple of code style fixes and a couple of code updates as well.\\n\\nAt the moment I'm having a trouble with\\nhttps://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010#diff-2106480b863a1ea6485772847314cb06,\\nhttps://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010#diff-c0089857a31093b8546a1ec95541a529 and\\nhttps://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010#diff-d4e3b82e9bebfd2cb466b4a30af07fa4\\n\\nThese are instances where I can't figure out an SSTableWriter not any other writer or reader object, so I'd need some help on figuring this out.\\n\\nI hope that in other instances I got them right though.\", \"[~rustyrazorblade] I've got back to working on this ticket. I think I've covered all possible operations and the patch is now in a good shape.\\r\\n\\r\\nI've tested it with compactions(including split and user-defined), repair, scrub, upgradesstables and cleanup operations; I also tested with multiple data directories. It looks ok for all of them, here are a couple of screenshots:\\r\\n\\r\\n[^cleanup.png]\\r\\n[^multiple operations.png]\\r\\n\\r\\nI think that the patch is ready for review at github (https://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010) or as a patch [^13010.patch]\\r\\n\\r\\nWould appreciate any feedback.\\r\\nThanks.\", '[~rustyrazorblade] Would you be able to have a look at the patch? Thanks!', \"Hey [~alourie], thanks for the patch.  I've got a lot on my plate atm but I'll try to get to it tomorrow.\", \"Looking at the result here, and doing some work / evaluation on my own, I wonder if this would be better suited for a {{nodetool compactioninfo ID}} command, where we could really show what's happening.  I've needed to know what sstables are being compacted, where it's being written, what levels are involved, etc several times now and it might be better to keep the compaction stats report simple and if you want the extra information the details can be listed out in a more logical fashion. \\r\\n\\r\\nWhat do you think?\", '[~rustyrazorblade] Thinking about your suggestion, and with some discussion with others, I think that adding an additional option flag to\\xa0*compactionstats* would be more beneficial. I really like that I can see _all_ the information about all running compactions at the same time, including the directories. Hence, we could keep the current presentation intact and just add a flag to show any additional info. Also, if we wanted to, we could then expand *compactionstats* to accept an ID and only show info for that specific compaction.\\r\\n\\r\\nWhat do you think?', 'I like the idea of hiding the extra stuff behind a flag, I think that would be great.  ', 'Awesome. I just pushed an update of the patch to the github at [https://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010#files_bucket.]\\r\\n\\r\\nThe option for more info is --\\r\\n{noformat}-v/--with-more-info{noformat}\\r\\n\\r\\nPlease let me know what you think.', \"Hey [~alourie], sorry for the delay.  The patch no longer applies cleanly.  Would you mind taking care of the conflicts?  I'll review it immediately.\", \"[~rustyrazorblade] Fixed. If any more updates needed, just let me know, I'm interested in pulling this in, so I'll be online for quite some time :)\\r\\n\\r\\nThanks for the review!\", \"I'm done with my day over here, can't read any more code.  I'll get it reviewed tomorrow, thanks for the quick turnaround!\", \"No worries [~rustyrazorblade] ,\\xa0there's no rush. Thank you for reviewing!\", \"Hey [~alourie] I just took a look at the patch.  There's still a pretty big (200 LOC) conflict between trunk and what's in your branch.  Would you mind rebasing your branch off trunk so it applies cleanly?  According to the git history you merged in some changes from trunk, which are now making a lot harder to do the review, as I have to look through the history to determine what's actually been deleted and what's a change you made.  For instance it looks like you deleted {{doValidationCompaction}}, which you didn't, Blake Eggleston did in {{ c5a7fcaa8e000}}.\\r\\n\\r\\nA few other notes while I'm in here to avoid lots of iterations:\\r\\n\\r\\n# there's almost no comments added despite it touching almost 20 files.  I realize it's not the best commented codebase, but I'd like to see comments on any new variables like {{targetDirectory}}.  Specifically, consider why something is there {{// needed for nodetool compactioninfo output}} is better than {{// holds directory name}}.  Please add comments conveying intent for each class method and variable added.\\r\\n# {{import org.apache.cassandra.cql3.Operation}} was added as an import to {{src/java/org/apache/cassandra/index/internal/CollatedViewIndexBuilder.java}} but not used\\r\\n\\r\\nOutside that, I think it's looking pretty good, I'll be pretty happy to get this merged in soon!\", '[~rustyrazorblade], I don\\'t have a clue what happened during that previous \"fix\". I\\'ve rebased my local stuff again, so hopefully, that is now truly fixed.\\r\\n\\r\\nAdditionally, I did a bit of cleanup and commenting, so now more stuff\\xa0is commented on and the changeset got smaller by 2 files :)\\r\\n\\r\\nSo I hope it\\'s looking better now and is much easier to read and review.\\r\\n\\r\\nThanks!', \"It doesn't look like you updated your branch.  This page still lists a bunch of merges in there and there's still conflicts when I try to merge it in.\\r\\n\\r\\nIt'll be a lot easier if you rebase off trunk and force push it up to your branch.\", \"[~rustyrazorblade] I have no idea what's going on. I'll check.\", \"[~rustyrazorblade] ok, sorry for the mess. I have still no idea how did that all got broken.\\r\\n\\r\\nBut, I did fix it now, so github should work, but just in case I'm attaching the patch files as well.\\r\\n\\r\\nPlease let me know if there are any more issues.\\r\\n\\r\\n\\xa0\\r\\n\\r\\nThanks.\", \"Hi [~rustyrazorblade]\\r\\n\\r\\nI've just rebased this work on top of the latest trunk. Would you be able to have a look? The code is at https://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010?expand=1 or patch at https://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-13010.patch\\r\\n\\r\\nThanks!\", \"Hey Alex, I'll try to get to this tomorrow.\", \"Hey [~alourie], I took a look at the patch. I'm a little confused by the output though. Is this what you're expecting?:\\r\\n{noformat}\\r\\n$ bin/nodetool compactionstats -v   \\r\\npending tasks: 1\\r\\n- tlp_stress.sensor_data: 1\\r\\n\\r\\nid                                            compaction type keyspace   table       completed total     unit  progress\\r\\n/home/jhaddad/dev/cassandra/bin/../data/data/                                                                          \\r\\n8d542480-8f69-11e8-b6a9-4905c6229fa6          Compaction      tlp_stress sensor_data 397746242 691456147 bytes 57.52%  \\r\\nActive compaction remaining time :   0h00m17s\\r\\n{noformat}\", \"I'm going to remove myself as the reviewer, I'm not sure if I'll have time to get another review in anytime soon.\", \"I believe that the output matches the request, and was reviewed more than once. Nevertheless, I'm ready to change it if required.\", \"[~alourie], this patch hasn't really been reviewed.\\xa0 There were multiple iterations where you submitted patches and branches\\xa0incorrectly, and\\xa0figuring that out has been the bulk of the time spent.\\xa0\\xa0\\r\\n\\r\\nRegarind the output, does {{/home/jhaddad/dev/cassandra/bin/../data/data/}} really look right to you?  As a user I don't expect to see relative paths within absolute paths.\\r\\n\\r\\nWe can't even merge this into trunk at the moment since we're in a feature freeze, we're only merging bug fixes for now.  This will have to wait.\", '[~rustyrazorblade] by \"reviewed\" I meant the output format, not the patch. I apologise, I thought I was clear in this regard. Also, it was not clear that the problem was this mixture of relative/absolute paths and not the format. I\\'ll check the code again to make it include absolute paths only.\\r\\n\\r\\nThanks!', '[~rustyrazorblade] - this seems to be a usability bug. It will not affect the core storage / networking layer in the database. This is probably ok to merge during the freeze. WDYT?', \"I don't have any objections.  Would you mind bringing it up on the dev ML to see if anyone does?\", 'hi [~alourie] , do you plan to work on this? Otherwise I would be interested in taking your work and make it happen.', \"[~smiklosovic] Yea, go ahead, I don't plan to work on it.\", 'Note that we should add a flag for this to maintain output compatibility, but we can add it to vtables freely.', 'Yeah I think Alex already provided a flag for this if I am not mistaken.', 'PR: [https://github.com/apache/cassandra/pull/1791]\\r\\nbuild: [https://app.circleci.com/pipelines/github/instaclustr/cassandra/1207/workflows/19676308-1a92-4b2d-a16a-ea8dad9054d5]\\r\\n\\r\\nBelow is the output from the test case to see this feature in action with some real-world load.\\r\\n\\r\\nI renamed the flag to be \"-v\" or \"--verbose\". I am not completely sure if that is what we want but it seem to me it is better that \"more info\" flag before.\\r\\n\\r\\nI am looking for a reviewer, [~rustyrazorblade] [~brandon.williams] would you please take a look if you dont mind and give me the feedback?\\r\\n\\r\\nEDIT: pinging [~djoshi] and [~jjirsa] who are among the watchers as well, maybe they would be willing to spare 10 minutes on this? :)\\r\\n\\r\\n{code:java}\\r\\npending tasks: 37\\r\\n- keyspace_24.table_25: 1\\r\\n- keyspace_24.table_26: 2\\r\\n- keyspace_24.table_27: 1\\r\\n- keyspace_24.table_28: 2\\r\\n- keyspace_24.table_29: 1\\r\\n- keyspace_00.table_01: 1\\r\\n- keyspace_00.table_02: 2\\r\\n- keyspace_00.table_03: 1\\r\\n- keyspace_00.table_04: 2\\r\\n- keyspace_00.table_05: 1\\r\\n- keyspace_12.table_13: 1\\r\\n- keyspace_12.table_14: 2\\r\\n- keyspace_12.table_15: 1\\r\\n- keyspace_12.table_16: 2\\r\\n- keyspace_12.table_17: 1\\r\\n- keyspace_06.table_10: 1\\r\\n- keyspace_06.table_11: 2\\r\\n- keyspace_06.table_07: 2\\r\\n- keyspace_06.table_08: 1\\r\\n- keyspace_06.table_09: 2\\r\\n- keyspace_18.table_21: 2\\r\\n- keyspace_18.table_22: 1\\r\\n- keyspace_18.table_23: 2\\r\\n- keyspace_18.table_19: 2\\r\\n- keyspace_18.table_20: 1\\r\\n\\r\\nid                                   compaction type                   keyspace    table    completed total  unit  progress\\r\\n/my/data/dir0                                                                                                              \\r\\n7e40e800-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_00 table_01 123       123456 bytes 0.10%   \\r\\n7e70d1a0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_00 table_04 123       123456 bytes 0.10%   \\r\\n7e529b40-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_00 table_02 123       123456 bytes 0.10%   \\r\\n7e638b30-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_00 table_03 123       123456 bytes 0.10%   \\r\\n7e7edb60-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_00 table_05 123       123456 bytes 0.10%   \\r\\n                                                                                                                           \\r\\n/my/data/dir1                                                                                                              \\r\\n7e9e9860-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_06 table_08 123       123456 bytes 0.10%   \\r\\n7ecea910-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_06 table_11 123       123456 bytes 0.10%   \\r\\n7ebf3fc0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_06 table_10 123       123456 bytes 0.10%   \\r\\n7eaddaa0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_06 table_09 123       123456 bytes 0.10%   \\r\\n7e8fcb50-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_06 table_07 123       123456 bytes 0.10%   \\r\\n                                                                                                                           \\r\\n/my/data/dir2                                                                                                              \\r\\n7ee194d0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_12 table_13 123       123456 bytes 0.10%   \\r\\n7f1771e0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_12 table_17 123       123456 bytes 0.10%   \\r\\n7f0b3ce0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_12 table_16 123       123456 bytes 0.10%   \\r\\n7eef9e90-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_12 table_14 123       123456 bytes 0.10%   \\r\\n7eff5600-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_12 table_15 123       123456 bytes 0.10%   \\r\\n                                                                                                                           \\r\\n/my/data/dir3                                                                                                              \\r\\n7f257ba0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_18 table_19 123       123456 bytes 0.10%   \\r\\n7f305110-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_18 table_20 123       123456 bytes 0.10%   \\r\\n7f3b2680-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_18 table_21 123       123456 bytes 0.10%   \\r\\n7f4845e0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_18 table_22 123       123456 bytes 0.10%   \\r\\n7f560180-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_18 table_23 123       123456 bytes 0.10%   \\r\\n                                                                                                                           \\r\\n/my/data/dir4                                                                                                              \\r\\n7f8575f0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_24 table_28 123       123456 bytes 0.10%   \\r\\n7f91f910-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_24 table_29 123       123456 bytes 0.10%   \\r\\n7f645960-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_24 table_25 123       123456 bytes 0.10%   \\r\\n7f70dc80-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_24 table_26 123       123456 bytes 0.10%   \\r\\n7f7b8ae0-1d9c-11ed-8f7d-0df44c673538 Compaction                        keyspace_24 table_27 123       123456 bytes 0.10%   \\r\\n                                                                                                                           \\r\\n-------- Other operations --------                                                                                         \\r\\n7f257ba0-1d9c-11ed-8f7d-0df44c673538 View build                        keyspace_18 table_19 123       123456 bytes 0.10%   \\r\\n7e8fcb50-1d9c-11ed-8f7d-0df44c673538 Scrub                             keyspace_06 table_07 123       123456 bytes 0.10%   \\r\\n7eaddaa0-1d9c-11ed-8f7d-0df44c673538 Secondary index build             keyspace_06 table_09 123       123456 bytes 0.10%   \\r\\n7f560180-1d9c-11ed-8f7d-0df44c673538 Validation                        keyspace_18 table_23 123       123456 bytes 0.10%   \\r\\n7eef9e90-1d9c-11ed-8f7d-0df44c673538 Verify                            keyspace_12 table_14 123       123456 bytes 0.10%   \\r\\n7f70dc80-1d9c-11ed-8f7d-0df44c673538 Row cache save                    keyspace_24 table_26 123       123456 bytes 0.10%   \\r\\n7f3b2680-1d9c-11ed-8f7d-0df44c673538 Relocate sstables to correct disk keyspace_18 table_21 123       123456 bytes 0.10%   \\r\\n7f0b3ce0-1d9c-11ed-8f7d-0df44c673538 Stream                            keyspace_12 table_16 123       123456 bytes 0.10%   \\r\\n7e529b40-1d9c-11ed-8f7d-0df44c673538 Key cache save                    keyspace_00 table_02 123       123456 bytes 0.10%   \\r\\n7ecea910-1d9c-11ed-8f7d-0df44c673538 Unknown compaction type           keyspace_06 table_11 123       123456 bytes 0.10%   \\r\\n7f8575f0-1d9c-11ed-8f7d-0df44c673538 Cleanup                           keyspace_24 table_28 123       123456 bytes 0.10%   \\r\\n7e70d1a0-1d9c-11ed-8f7d-0df44c673538 Counter cache save                keyspace_00 table_04 123       123456 bytes 0.10%   \\r\\nActive compaction remaining time :        n/a\\r\\n{code}\\r\\n\\r\\n', \"I think this looks good but shouldn't this be available via vtable too?\", 'Hmmm interesting, let me think about how that would actually look like ... Do you want to expose additional column per entry which would contain the disk path?', 'Yeah, that makes sense.', 'for vtable in nodetool it looks like this:\\r\\n\\r\\n{code}\\r\\npending tasks: 2\\r\\n- cql_test_keyspace.table_00: 2\\r\\n\\r\\nkeyspace          table    task id                              completion ratio kind       progress sstables total  unit  target directory                                                         \\r\\ncql_test_keyspace table_00 6f155250-1e62-11ed-81ed-f13ee02c0a6e 0.10%            Cleanup    123      1        123456 bytes                                                                          \\r\\ncql_test_keyspace table_00 6d465eb0-1e62-11ed-81ed-f13ee02c0a6e 0.10%            Compaction 123      10       123456 bytes /some/dir/cql_test_keyspace/table_00-6b470c40-1e62-11ed-81ed-f13ee02c0a6e\\r\\nActive compaction remaining time :        n/a\\r\\n{code}\\r\\n\\r\\nI fixed one issue - it is not possible to mix \"verbose\" with \"vtable\" output in nodetool. They are mutually exclusive. One can do either verbose or vtable.\\r\\n\\r\\nCQL vtable output contains that dir as well (new column).\\r\\n\\r\\nIt is same PR / branch. I am running the build as we speak.', 'This looks good. \\r\\n\\r\\nbq.  it is not possible to mix \"verbose\" with \"vtable\" output in nodetoo\\r\\n\\r\\nI think it makes sense to keep building on vtables, and thus the vtable output in compactionstats when someone wants more information than what\\'s in the backward-compatible view.  So I think keeping this behind the vtable flag is fine, and helps cuts down on the number of flags.', 'the build with the latest changes is here: https://app.circleci.com/pipelines/github/instaclustr/cassandra/1213/workflows/b8445da5-3f64-46c5-8331-f72f90523835\\r\\nPR is same: https://github.com/apache/cassandra/pull/1791', 'We also need to run the j11 precommit pipeline.', 'j11 precommit build https://app.circleci.com/pipelines/github/instaclustr/cassandra/1213/workflows/2ec59cb3-59fe-470f-bc33-3e7d09360501', 'bq. it is not possible to mix \"verbose\" with \"vtable\" output in nodetool\\r\\n\\r\\nTo be clear, I think we should drop the \\'verbose\\' flag and leave this accessed via the vtable flag.  There\\'s really no utility in having it available via two flags, and it\\'s good to nudge people toward vtables when we can.', 'I am confused. What I was trying to do all the time was to follow the idea of Jon to write that output in such a way that it would group all compactions under the same disk. Now if we do want to have this only for vtable, do I understand it right that we will not have such output anymore? Each vtable entry for compaction will contain just a respective directory, that is all. Is this ok for people? I think the way of doing things per Jon makes sense to me.', \"The scenario I'd like to avoid is adding a flag each time we want to vary the output, and appending the vtable output is a good way of doing that.  The next time we want to add something here, are you suggesting we would also lump it under the verbose flag?\", 'If there is ever some other field added into CompactionInfo and we want to show it to a user, we would add it everywhere - to whatever output. So to answer your question, yes, we would add it under \"verbose\" as well.\\r\\n\\r\\nHaving said that, I have to admit that \"verbose\" is rather unfortunate name for that flag. What I am trying to capture is to have the output grouped per disk as Jon suggested. Maybe \"--group-by-target-dir\" would be more appropriate?\\r\\n\\r\\nIf we go with vtable only, to achieve this \"grouping by disk\", we could at least order the output somehow but that is tricky, we would probably order the cql output too and that is just ... strange.', 'bq. Having said that, I have to admit that \"verbose\" is rather unfortunate name for that flag.\\r\\n\\r\\nI thought this too, until you confirmed the future use of \"verbose\" and then it made sense .\\r\\n\\r\\nbq.  Maybe \"--group-by-target-dir\" would be more appropriate?\\r\\n\\r\\nThis is exactly what I\\'m trying to avoid, because if we do that, then every time we want to change the output we\\'ll have a new flag to append, and it will become a mess to get actual verbose output from compactionstats.\\r\\n\\r\\nbq. If we go with vtable only, to achieve this \"grouping by disk\"\\r\\n\\r\\nThe nice thing about vtables is if you want different output, switch to cql and select it however you like.', 'But I see the problem with the backward compatibility - like we want to have the non-vtable output same as it was. Look, lets go with your idea. I already rewrote the patch (actually simplified) and I am running the build.', 'One point to add is that I do not think that besides this \"group by dir\" there will ever be any other flag like that. What flag that might be? ', \"Very recently we added CASSANDRA-16844 which started the whole compatibility flag situation we are in now, so I'm sure there will be others.\", '8 precommit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1216/workflows/19d0beb7-310b-4170-8863-608bff7adb40\\r\\n11 precommit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1216/workflows/5ed6064f-f72d-4b44-bd0f-a47885e10de1\\r\\n\\r\\nsimplified solution we just talked about is here: https://github.com/apache/cassandra/pull/1801\\r\\n\\r\\none test is repeatedly failing (not related to this PR)', '+1', \"{quote}one test is repeatedly failing (not related to this PR)\\r\\n{quote}\\r\\nI guess that's {{{}org.apache.cassandra.tools.StandaloneUpgraderOnSStablesTest.testUpgradeSnapshot{}}}. It seems caused by this patch, since [repeated runs on CI|https://app.circleci.com/pipelines/github/adelapena/cassandra/2009/workflows/e32acbf3-05e2-4cdb-a60d-154aa4326f78] for [the immediately previous commit|https://github.com/adelapena/cassandra/commit/c4b1c0614e42b4ea2064822d31c28aa5d4f1450a] don't hit it.\\r\\n\\r\\nI have opened CASSANDRA-17849 for it.\", \"bq. one test is repeatedly failing (not related to this PR)\\r\\n\\r\\nI unfortunately confused it with CASSANDRA-17804.\\r\\n\\r\\nbq. I have opened CASSANDRA-17849 for it.\\r\\n\\r\\nThanks, I'll take a look.\", 'Interesting, I was thinking it is some flake as it was totally unrelated and was failing on timeout. I did a repeated run on this ticket though.\\n\\nI take a look too.\\n\\n\\nSent from ProtonMail mobile\\n\\n\\n\\n\\\\']\n",
            "my_comment: https://github.com/apache/cassandra/pull/1801\n",
            "\n",
            "one test is repeatedly failing (not related to this PR) +1 \"{quote}one test is repeatedly failing (not related to this PR)\n",
            "{quote}\n",
            "I guess thats {{{}org.apache.cassandra.tools.StandaloneUpgraderOnSStablesTest.testUpgradeSnapshot{}}}. It seems caused by this patch since [repeated runs on CI|https://app.circleci.com/pipelines/github/adelapena/cassandra/2009/workflows/e32acbf3-05e2-4cdb-a60d-154aa4326f78] for [the immediately previous commit|https://github.com/adelapena/cassandra/commit/c4b1c0614e42b4ea2064822d31c28aa5d4f1450a] dont hit it.\n",
            "\n",
            "I have opened CASSANDRA-17849 for it.\" \"bq. one test is repeatedly failing (not related to this PR)\n",
            "\n",
            "I unfortunately confused it with CASSANDRA-17804.\n",
            "\n",
            "bq. I have opened CASSANDRA-17849 for it.\n",
            "\n",
            "Thanks Ill take a look.\" Interesting I was thinking it is some flake as it was totally unrelated and was failing on timeout. I did a repeated run on this ticket though.\n",
            "\n",
            "I take a look too.\n",
            "\n",
            "\n",
            "Sent from ProtonMail mobile\n",
            "\n",
            "\n",
            "\n",
            "\\\\\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-13396\n",
            "issue_type: Bug\n",
            "summary: Cassandra 3.10: ClassCastException in ThreadAwareSecurityManager\n",
            "description: https://www.mail-archive.com/user@cassandra.apache.org/msg51603.html\n",
            "architectural impact: NO\n",
            "comments: ['https://github.com/apache/cassandra/compare/trunk...edwardcapriolo:CASSANDRA-13396?expand=1', 'I\\'m strongly -1 on this change.\\n\\nThis change will cause weird and hard to catch follow-up issues (see the discussions and issues around that piece code), which _cannot_ be caught by neither unit nor dtests because it\\'s an unsupported setup. We do not support embedding C* in a container (i.e. a JVM not controlled \"by us\"). IMO, supporting C* in such an environment will cause other issues. Technically, it\\'s not a major bug - changed it to wish.', 'How come everyone in Cassandra\\'s first reaction is to -1 everything? \\n\\nThe entire model of apache is \"I have an itch to scratch\". This person WANTS to run Cassandra in a container it is an \"itch\". The immediate opposition position should not be \"BUT DON\\'T SCRATCH THAT ITCH\", because I say so.\\n\\n', \"@[~snazy]: OK but what if the cassandra daemon is not embedded anywhere but is simply running with a classpath containing several slf4j bindings?\\nIt will still crash, right?\\n\\n@[~appodictic]: please don't over-react (and don't hijack my question), it's an open discussion ;-)\", 'LOL I just posted this tweet yesterday.\\n\\nhttps://twitter.com/edwardcapriolo/status/847484593041100800\\n\\nWhat comedy cassandra is. No one even bothers to say \"how can we work together?\" or \"how can we wrote the code to make all users happy\" They just instantly drop a -1 on things. lol', 'So funny that i litteraly wake up, go out of my way to fix an issue for someone, and even though everyone is Cassandra is too busy to reply to emails and help people they are Johnny on the spot to jump on Jira and -1 code.', '\"Open discussions\" in cassandra always start with the concept of \"its not my idea so -1\" which is the exact opposite of \"scratch an itch\". \\n\\n\"We do not support embedding C* in a container\"\\n\\nReally? who says? where is it said? Who is \"we\"?', \"[~apassiou], right, it's true for any other slf4j binding. \\nReason is stuff like CASSANDRA-12535 and CASSANDRA-13173, which are hard to figure out and even harder to ensure its functionality in unit tests. That's why I'm against such a change. We cannot foresee the consequences, because we have not tested other bindings. Even further, the performance implications of using another logger implementation are not determined. Believe me, it's not blindly shooting something down - I had a hard time to fix this issue and do not like to see it happen again. BTW: It's late in the afternoon over here, so it's not a too quick reaction early in the morning.\", '{quote}\\nReason is stuff like CASSANDRA-12535 and CASSANDRA-13173, which are hard to figure out and even harder to ensure its functionality in unit tests. \\n{quote}\\n\\nSo because someone made bugs in the past, which are \"hard to figure out\" and you can not \"foresee the consequences\" . Is this back to the future part 4?\\n\\nPlease verify your claim of \"not supporting containers\" before finding other reasons to not like the idea of fixing an obvious problem.\\n\\n\\n', 'So strange:\\n\\nNo such statement about supporting containers seems to exist.\\n[edward@jackintosh cassandra]$ find . -type f | xargs grep containers\\n./src/java/org/apache/cassandra/db/ColumnFamilyStore.java:     * thread safety.  All we do is wipe the sstable containers clean, while leaving the actual\\n./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:        private final List<TokenTreeBuilder> containers = new ArrayList<>();\\n./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:                containers.add(keys);\\n./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:            if (containers.size() > 0)\\n./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:                for (TokenTreeBuilder tokens : containers)\\n./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:            containers.clear();\\n./conf/jvm.options:# This helps prevent soft faults in containers and makes\\nBinary file ./build/classes/main/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder$MutableDataBlock.class matches\\n[edward@jackintosh cassandra]$ find . -type f | xargs grep Containers\\n\\nIts almost as if people just make up things, and then when you corner them on their position being false they just pivot and make up a new reason not to like the idea.', 'Edward, I appreciate that you wanted to help but please stop hijacking my question, or at least try to be constructive...\\n\\nI don\\'t know what you or Robert have understood when I said \"container\" but in my case it\\'s just an application (a plain Java main()) that instantiates a CassandraDaemon and sometimes other stuff. But one could also do it in a \"unit\" test (which is not really a unit bt more an automatic integration test).\\n\\n@Robert, I can understand your concerns about not-tested behavior of other bindings, then shouldn\\'t it be stated in the docs that other bindings are not supported, and a more explicit error thrown?\\nBut I don\\'t think the performance impact is a good argument because logback and slf4j are configurable by themselves with configuration files it can have a very strong impact on the performance (log patterns, where you log to) even if one uses logback.', \"[~apassiou], C* is meant to run as a standalone application using the dependencies that are in the {{lib/}} folder. Any change to those dependencies and the way C* is started, is basically up to the person who changes the dependencies. We can of course talk about using a different logger implementation instead of logback and discuss the pros and cons. But that is IMO way beyond an {{instanceof}} check.\\n\\nI'm generally concerned about stability and hidden performance issues and a change to a (logger implementation) library, which is nearly everywhere in the hot code path. Mean, we use logback now for a really long time - but we have no test nor production experience running something else. One example: one thing that may happen is some hidden contention in that logger library causing weird outliers - people would complain that C* is slow but don't realize it's in this case because of that change. That's one reason why we are so careful with library updates especially in minor versions. All I'm saying is, that getting _all_ the consequences of such a change is a lot of work.\", \"bq. just an application (a plain Java main()) that instantiates a CassandraDaemon\\n\\nIf that's just for testing, why not just use logback?\", '{quote}\\npeople would complain that C* is slow but don\\'t realize it\\'s in this case because of that change. \\n{quote}\\n\\nFirst, its an obvious bug. The entire point of plug-gable logging implementations is so that you can replace them. \\n \\nSecond, the only person being actually affected would be Anton, because effective no one else is changing logging implementations so no one else is hitting that block.\\n\\nFor Anton (and anyone else) they would have to manually change the files in the lib folder and the configuration. So nothing is \\'hidden\\' to him. He/They make a change and they can report if there actually is a performance issue.  \\n\\nBecause they can \"scratch their itch\" of running Cassandra in a container they might find new problems or they might make new opportunities. For example, they may find that some other implementation is actually better or faster. \\n\\nIf anyone was actually trying to convince me that this bug is intentional, (which is almost laughable). The proper practice would be:\\n\\n{code}\\nif (!logger instanceof XYZ){\\n  throw new IllegalArgumentException(\"we only support XYZ for reasons ABC\");\\n}\\n{code}\\n\\nBut instead we are attempting to pretend the opposite, that the bug is intentional and the correct thing to do is throw a ClassCastException. Which is a joke.\\n', \"Seems pretty reasonable to me\\n\\nCertainly logback isn't the only performant slf4j logger available.\\n\", \"Changes back to bug, because even if the belief is that other loggers shouldn't be encouraged, we surely can do better than throwing a cast exception\\n\\nGiven that log4j2 is likely faster than logback and has been suggested as far back as 2013 CASSANDRA-5883 it seems like artificially forcing logback is a position that would need to be more rigorously defended - I'm +1 on this change conceptually (but this is not a review).\\n\", \"Rigorous defenses are in no short supply around here.\\n\\nI'm sure someone next will argue that this was intended to \\n{code}\\n    /**\\n     * The purpose of this class is\\n     */\\n{code}\\nbecause the purpose code is soooo self documenting it describes itself. Want to fix it? No -1 the comment is perfect and heavily tested!\", 'The problem that I see with this ticket is the following: \"Once we agree to allow people to use the loggers that they wish we somehow become responsible for the bugs that can show up\".\\nAs [~snazy] point it up, some of those issues might be non trivial to figure out. Simply because when someone will open a bug he might not mention that he changed the logging library and we might end up wasting a lot of time to reproduce the problem.\\nDue to that, I tend to be in favor of Robert suggestion of not supporting it (years spend debugging crappy issues have made me somehow paranoiac).\\n\\nNow, I think it also make sense to allow people to use another logging library as long as they know that we do not fully support it.\\nMy proposal woud be to log a warning at startup saying that the logging library that they use is not supported.\\n', \"I'm leaning to go with the proposed patch with a warning message as suggested by Benjamin. But in any case, this needs to be documented. I'd propose that any patch would also have to add a page [here|http://cassandra.apache.org/doc/latest/configuration/index.html] describing if and how logging can be customized and with a list of all relevant config files.\", 'bq. add a page here describing if and how logging can be customized and with a list of all relevant config files\\nbq. it also make sense to allow people to use another logging library\\n\\nDon\\'t get me wrong, but documenting something that is not supported and has never seen CI does not sound good. It would give people just skimming that page the impression that other logging backends _are_ actually supported. Before we document that, we should have full CI for those backends in place - i.e. all utests and dtests for each backend - or _at least_ document something like \"the combination of C* version X.Y and \\\\[logger backend version Z\\\\] was CI-tested on X/Y/Z using this configuration\". If we allow people to use other logging backends, there must be a way to tell them \"version X of Y should be good, because it has been CI tested\".\\nAnyway, supporting another logging backend still sounds like a new feature to me.', 'All I was asking for is avoid crashing, and I am concious that I am not in a standard use-case.\\nI am perfectly fine with a big warning in the log saying that using something other than logback is at my own risk + the the doc that states that nothing but logback is officially supported which makes you not responsible for any bug. \\n\\nOf course, as Benjamin says, there is still the risk of you investigating an issue without knowing that a different logger is used, but if you are really paranoid you can also imaging people configuring their logback in a very inefficient or wrong way and create crappy issues while being perfectly \"legal\".', 'I did not suggest to that we should support different loggers, just because we document Cassandra logging:\\n\\nbq. add a page here describing **if** and **how** logging can be customized and with a list of all relevant config files\\n\\nWe can as well state something like \"Using any logging library not shipped with Cassandra is NOT supported. Use at your own risk.\". If that is the consensus of this discussion.\\n', 'bq. I did not suggest to that we should support different loggers\\n\\nAh, ok, got it wrong then. Adding a page about how to configure logback (and point to the logback docs) sounds good.\\n', \"bq. This change will cause weird and hard to catch follow-up issues (see the discussions and issues around that piece code)\\n\\nSo why isn't there *any* comment around the code the patch updates to explain why this exists in the first place and why it's so important that it's here?\\n\\n\", \"Ninja'd comments for this code.\", '-1 on ninja fixes. ', '{quote}\\nWe can as well state something like \"Using any logging library not shipped with Cassandra is NOT supported. Use at your own risk.\". If that is the consensus of this discussion.\\n{quote}\\n\\n-1 The point of this Jira is not to create some new policy to avoid committing things. Fake blockers like \\'this might cause a bug in the future\\' are not a valid technical reason to reject something. If that is the case shutdown all development on everything. I appreciate the attempt to compromise, but this is not the right direction. \"Not supported\" puts this in a trigger like situation. It will create a policy that will stop active research. Anyone will be free to reject further development using the \"not supported\" argument regardless of how baseless it is. ', 'We do not run any test with another logging library and I do not think that we plan to do it. Which, for me means that we do not officially support any other library. Warning the user about it seems normal to me. As a user I would prefer to know. ', 'There are really three issues:\\n\\n1) The existing comments were clearly inadequate, and that\\'s been ninja\\'d into place. +1 on that. \\n\\n2) Throwing a ClassCastException is objectively wrong. The patch fixes that, and should be committed. \\n\\n3) As a side effect, the patch allows other loggers, almost all of which are untested. The assertion from [~snazy] is that doing so is dangerous, specifically citing past bugs where other loggers which may do IO and cause sandbox access problems. That\\'s a valid concern, and worth a logged warning in my opinion.\\n\\nLike [~spodxx@gmail.com] (and I think [~blerer]) suggest above, I think Ed\\'s patch+warning makes sense to me.\\n\\nIf someone wants to \"officially\" support another logger in order to remove the warning, then I think the burden is on them to open a proper ticket and demonstrate that it\\'s sufficiently tested.', '1) ninja fix \\nHow does meritocracy work when we spend globs of time striking down patches, while simultaneously \\'ninja fixing\\' stuff? Go make a patch and get it reviewed like everyone else. \\n\\n2) Agreed.\\n\\n3) What a backwards argument. The \"critical past bugs\" sited in CASSANDRA-12535 where caused by the person attempting to drop the -1 on this patch. This directly translates to \"No one can edit the buggy code I introduced because THEY might make bugs.\"  Consider throwing a GetOffMyLawn exception\\nhttps://github.com/apache/cassandra/commit/8f15eb1b717548816a9ee8314269d4d1e2ee7084\\n\\n\\n ', '{quote}\\nHow does meritocracy work when we spend globs of time striking down patches, while simultaneously \\'ninja fixing\\' stuff? Go make a patch and get it reviewed like everyone else.\\n{quote}\\n\\nThe project has always allowed ninja fixing minor (especially non-code) things. Comments here are a net positive. There\\'s no reason to fight about adding comments after the fact. \\n\\n{quote}\\nThis directly translates to \"No one can edit the buggy code I introduced because THEY might make bugs.\"\\n{quote}\\n\\nLogging a warning for users isn\\'t the same as throwing an exception. It\\'s not like we\\'re talking about a system property here that requires explicit operator involvement to even run with another logger, it\\'s logging a single warning message that bugs may happen and we haven\\'t actively tested other configs. I don\\'t think that\\'s unreasonable, and it\\'s not \"get off my lawn\". This isn\\'t an unreasonable compromise - we don\\'t crash, but we give operators a chance to know that they\\'re running an untested config. ', 'A true cassandra special. A patch with a dubious ClassCastException and a half finished comment passed a review, and now the next person who touches the code needs to \"sufficiently test\" to earn the \"officially supported\" designation only granted to committers that make untrue statements like \"We do not support embedding C* in a container \" and ninja fix stuff.', '{quote}\\nLogging a warning for users isn\\'t the same as throwing an exception. It\\'s not like we\\'re talking about a system property here that requires explicit operator involvement to even run with another logger, it\\'s logging a single warning message that bugs may happen and we haven\\'t actively tested other configs. I don\\'t think that\\'s unreasonable, and it\\'s not \"get off my lawn\". This isn\\'t an unreasonable compromise - we don\\'t crash, but we give operators a chance to know that they\\'re running an untested config.\\n{quote}\\n\\nThe \"get off my lawn\" is related to this entire process. It had not even checked who added the code originally. I did not quite understand why it got a -1 so fast. -1s are \"rare\" and kill the proposal dead.\\n\\nhttps://www.apache.org/foundation/voting.html\\nFor code-modification votes, +1 votes are in favour of the proposal, but -1 votes are vetos and kill the proposal dead until all vetoers withdraw their -1 votes.\\n\\nThe original reasons given were \"This change will cause weird and hard to catch follow-up issues (see the discussions and issues around that piece code), which cannot be caught by neither unit nor dtests because it\\'s an unsupported setup. We do not support embedding C* in a container (i.e. a JVM not controlled \"by us\")\"\\n\\nLets break this down:\\n1) \"This change will cause weird and hard to catch follow-up issues\"\\nHard to quantify and the statement itself is a hypothesis. Can \"WILL CAUSE\" be proven? No.\\n\\n2) which cannot be caught by neither unit nor dtests because it\\'s an unsupported setup\\nEven though we are SURE issues that \"WILL HAPPEN\" they \"CANNOT BE CAUGHT\" . Amazing how that logic works.\\n\\n3) We do not support embedding C* in a container\\nUntrue. How does one run the CDC daemon? Not a written rule anyway.\\n\\nIf adding a single if statement to block of code and getting 3 completely ludicrous objections from the person who happened to write said code is not \"get off my lawn\" then I don\\'t know what is.\\n\\n', \"{quote}\\nThe project has always allowed ninja fixing minor (especially non-code) things. Comments here are a net positive. There's no reason to fight about adding comments after the fact.\\n{quote}\\nNo. It is important. The -1 er is using his technical insight as a justification for his -1. The incomplete comment shows how much time he really spent working on the given code. No tests, no argument checking, and a half done comment.\", \"Let's focus on the problems and solutions. \\n\\nThere were missing and incomplete comments around a broken piece of code that fixed a very-hard-to-troubleshoot bug. We've fixed the missing and incomplete comments, we still need to fix the broken code, and we can do so without ignoring the past very-hard-to-troubleshoot-bug. \\n\\nWe have a patch that fixes the ClassCastException, which should be reviewed. We have a (non-binding) -1 on that review. One of the thing that 3 committers (including myself) seem to have suggested is at least adding a warning. [~snazy] is that agreeable to you?\\n\\n\\n\", \"[~jjirsa], can live with that - i.e. logging an explicit warning using a new {{StartupCheck}} that also mentions that UDFs/UDAs might be broken, if a logger that's not logback is used.\", 'Happy to see that everybody seem to converge to a reasonable solution ;-)', \"This is my first ticket. [~jjirsa] helped me get started. Since it hasn't been updated in a while, I've taken the liberty of adapting [~appodictic]'s patch to match the consensus described in this ticket. GitHub links below:\\n\\nhttps://github.com/eugenefedoto/cassandra/tree/13396-3.0\\nhttps://github.com/eugenefedoto/cassandra/tree/13396-3.11\\nhttps://github.com/eugenefedoto/cassandra/tree/13396-trunk\", \"[~eugenefedoto] - two quick notes\\n\\n1) In all three versions of your patch, we can't [throw|https://github.com/eugenefedoto/cassandra/commit/5957dd84aaa239d62f40aa4bf5f3159bf7a300d7#diff-30a3dbf7d783cf329b5fb28a8b14332eR110] in {{ThreadAwareSecurityManager.java}} or we'll end up with the same problem we had before.\\n\\n2) In your [StartupCheck|https://github.com/eugenefedoto/cassandra/commit/5957dd84aaa239d62f40aa4bf5f3159bf7a300d7#diff-a5df240149285ae528cdd3c41aa59360R419] , the second log line (on L419) isn't necessary.\\n\\n[~snazy] - Eugene is a new contributor, I've talked him through how to contribute offline via email, and probably shaped his approach (notably, the {{instanceof}} check from Ed's patch isn't sufficient, because if logback has been removed from the classpath, we'll throw a {{NoClassDefFoundError}} instead). Given that, do you want to review? \", 'I made the [suggested changes by Jeff|https://issues.apache.org/jira/browse/CASSANDRA-13396?focusedCommentId=15996968&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15996968].', 'Looking forward to the resolution of this issue in any of the following ways:\\n\\n1) Don\\'t load this security manager and policies if UDF\\'s are configured to be disabled \\n2) Handle other possible loggers conditionally (log4j2 being my case)\\n3) Provide an option to run with insecure UDF\\'s ( by not installing this security manager). Not everyone is exposing UDF\\'s to folks they don\\'t trust. In some use cases it might be a feature to be able to read system properties etc.\\n\\nGlancing at the discussion it sounds like this is heading towards a \"break UDF\\'s but continue\" strategy, which will also work for me since I don\\'t need UDF\\'s but seems likely to trip folks.\\n\\nMy exact itch is documented here: https://github.com/nsoft/jesterj/issues/89\\n\\nIf option 1 or 3 were available, that would greatly simplify my life, because this security manager installs policies in a class initializer and these policies assume a codePath with a url scheme of \"file\" but in my case the scheme is \"onejar\"... which forced me into lots of gyrations to force an early load and then un-set your policies so that the rest of my code could have permissions.\\n', 'And yes I might be interested in providing a patch for 3 if folks seem in favor... 1 is probably beyond my knowledge of Cassandra, but a version of 3 dependent on a system property seems tractable.', \"After some IRC discussion, I've been encouraged to submit a patch. Here's an implementation of my #3 suggestion above: \\n\\nhttps://github.com/nsoft/cassandra/commit/382a44c238b6d4bd7e3d8cc7bbd6710b0a7c5274\\n\\nThough Github's diff has done a fabulous job of obfuscating it, the patch is very simple all I did was add a constant, and two conditions that read the system property represented by the constant and prevent this security manager and its policies from getting installed via the install() method if the system property has been set. \\n\\nCircle CI here: https://circleci.com/gh/nsoft/cassandra/2 (still running as of this comment, but I expect it to pass) Ran tests locally and Installed a version with this patch in JesterJ and everything seems happy there.\", \"FYI, Circle CI did pass. Any commentary or review would be appreciated. I won't be able to release without knowing what direction this issue is going.\", 'Any one have a chance to look at my patch yet?', 'We faced the same underlying issue after upgrading from Cassandra 3.9 to 3.11.0 when using Cassandra embedded for integration testing using JUnit. \\nAs our application uses a different logging backend and we did not want to switch it and provide appropriate redundant configuration for logback, we excluded logback dependencies and only provided our implementation to also avoid any warnings about duplicate bindings. This setup worked fine with Cassandra 3.9, but fails with Cassandra >= 3.10; the server does not startup, because of the missing classes. So in this case any patch working with instanceof checks still attempting to load those classes without specific try/catch would obviously also fail. \\n\\nIn addition to SMAwareReconfigureOnChangeFilter in org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager.install() using multiple logback internals (added with CASSANDRA-12535) I also found the change with CASSANDRA-12509 adding ch.qos.logback.core.hook.DelayingShutdownHook in StorageService#initServer problematic.\\nWould it be an alternative to handle all access to the underlying logging implementation via reflection? \\nE.g. attempting to load logback classes and only if this does not fail, perform implementation specific actions via reflection (otherwise log a warning about missing logback presence, which can be ignored in integration test setups). We are mostly talking about one-time initialization, so the performance impact should be really negligible.\\nThis solution would require users to properly exclude logback logging libs if they want to use other sf4j implementation bindings. Providing multiple logging implementations with sl4fj bindings anyway triggers a warning which should be handled.', \"I had the very same issue when using the {{EmbeddedCassandra}} from {{spark-cassandra-connector}}. Moved to {{logback}} fixes the issue. This was a bit annoying because I had to exclude all other sf4j implementations (there's no way afaik to force one implementation over another in case multiple ones are loaded). Anyway, anything but a ClassCastException is better option imho.\", \"I have the same problem while attempting to use an embedded Cassandra instance for integration tests in Scala projects. Sadly, I found another strange behavior in SBT that avoids me removing {{log4j-slf4j-impl}} from the classpath (if someone is interested [here is the ticket|https://github.com/sbt/sbt/issues/3645]).\\r\\n\\r\\nI'm stuck in 3.9 version, so I'm looking forward to a solution on this.\", 'It is unclear to me whether or not the proposed patch has been/will be accepted into the code base.\\xa0 What is the current thinking and if it is to be added when might that be?\\r\\n\\r\\nI have hit this issue when trying to run Cassandra in a unit test situation where I am testing an implementation of an SPI and am unable to convert product to logback.', \"OK, looks like this has drifted for long enough. I'm gonna look at the patches and review for commit.\\r\\n\\r\\nUPDATE: I'll wait until CASSANDRA-14183 is committed before attacking this (as it touches logging, as well)\", '[~jasobrown], maybe you can have a glance on\\xa0[my comment|#comment-16152569] as well. I have not started working on a patch, because I did not receive any feedback on my suggestion on a conceptual level. I did not want to waste time working on an implementation without knowing of a chance of inclusion. Because we faced some issues in production which are either fixed in 3.11.1 or will be fixed in 3.11.2 we really would like to update, but this still blocks us.', \"[~ehubert] 3.11.2 is up for vote & release this week, so this patch will not be included in that release. Do you have any test cases you can offer here? Please note that allowing other logging implementation seems to be used seems in scope, but will *not*, in any way, be supported. I'll need to reread through all the commentary again to get the full scope of what people are really asking for, but I'm at least willing to get this to completion.\", '[~jasobrown], fair enough. From what I read/understood the majority of users (if not all, definitely including us) facing this issue, wanted to use Cassandra as an embedded server (mostly for integration testing purposes) utilizing class org.apache.cassandra.service.CassandraDaemon - so no production deployment of a standalone server or cluster.\\r\\n\\r\\nWhile running in the same JVM alongside an application using slf4j with any slf4j supported backend != logback after upgrading to Cassandra 3.10 or later you are in trouble and no longer able to start the Cassandra server, although logging/logging performance are none of your (primary) goals/concerns.\\r\\n\\r\\nI doubt there are many users who want to configure a standalone single or multi-node Cassandra installation using a different logging backend to use this in production and have your support, but many users want to write automated integration/scenario tests of their own application interacting with Cassandra using an embedded Cassandra server in addition to plain unit tests using mocks without being forced to switch the logging backend chosen for their application for similar reasons you have. Therefore I see no conflict at all.\\r\\n\\r\\n\\xa0\\r\\n\\r\\nAn implementation could even somehow enforce the differentiation between embedded and standalone usage similar to \"runManaged\" in CassandraDeamon to only allow/support other logging backends (skip special logging backend specific configuration) when CassandraDeamon is used/configured differently than done from main() used by a standalone server installation, if this is really a concern you want to see addressed.\\r\\n\\r\\nEven in this case one should exchange nasty runtime exceptions or even JVM errors (ClassCastException or NoClassDefFoundError) with a dedicated error message:\\r\\n\"When using a Cassandra standalone installation the only supported logging backend is logback.\"\\r\\nFor ClassCastException add something like \"slf4j is currently bound to a different logging framework. Please ensure your classpath only contains logback implementations!\"\\r\\nFor NoClassDefFoundError add something like \"No logback implementation was found. Please ensure your classpath contains the bundled logback implementation!\"\\r\\nYou can decide to abort the startup or have the same behavior as for the embedded case, but only providing a detailed error logging regarding the unsupported setup.\\r\\n\\r\\nFor embedded use cases one could advice programmers to activate the CassandraDaemon differently (e.g. some parameter) and here I would propose to simply not execute all logback specific configuration logic - e.g. try loading specific logback classes via reflection, so in this mode logback could be easily replaced by any slf4j logging backend which the application currently uses without further adjustments.\\r\\n\\r\\nJUnit test cases might be a bit tricky, because I think they involve different classpath setups of the used test runner to simulate/trigger those type of issues.', '[~ehubert] Thank you. This use case summary was very helpful.', '+1 for [~qb3rt] summary, it touches all the points I was going to make and then some.\\r\\n\\r\\n\\xa0', \"Though my embedded usage is not only for unit test, my choice of Cassandra relates more to the fact that you are Apache licensed, pure java and\\xa0clustered rather than performance concerns. When there's a viable alternative I'll worry about whether or not you're faster... in the mean time, I'm very happy to be responsible for (or take credit for) any performance variation from plugging in my\\xa0preferred logging framework (log4j2). Please don't use performance worries as an excuse to not fix this. Generally +1 on Eric's summary also. I don't mind doing something extra to enable pluggable logging so that you can default to your supported config, so long as it doesn't impact\\xa0the\\xa0command line invocation of my project (i.e. requiring -D or -agentlib, etc).\", 'Hi [~jasobrown]! Today, I took some time to prepare a patch against the Cassandra 3.11 branch which basically:\\r\\n * bundles all logback implementation specific functionality in one class (required a bit of code reorganization)\\r\\n * extracted an interface to be able to a) minimize use of reflection and b) be able to provide alternative implementations (the patch itself only provides a no-op fallback implementation)\\r\\n * load and instantiate logging-implementation specific extension according to used slf4j binding via reflection (Cassandra code only works on new interface which has no java class dependencies to specific implementations)\\r\\n\\r\\nSo far there are no new (integration) tests which likely would also require some classpath /\\xa0 ClassLoader magic.\\r\\n\\r\\nI tested the change using \"a neutral\" application use case by utilizing [Cassandra Unit|https://github.com/jsevellec/cassandra-unit].\\r\\n\\r\\nThe \"test\" involved adjusting log4j config from Cassandra Unit test resources, changing the used cassandra-all version in parent pom, excluding logback deps from the pom and executing any of the tests.\\r\\n\\r\\nWith stock Cassandra 3.11.2 we see:\\r\\n{code:java}\\r\\n2018-03-20 10:51:43,753 [pool-2-thread-1] ERROR cassandra.service.CassandraDaemon - Exception encountered during startup\\r\\njava.lang.NoClassDefFoundError: ch/qos/logback/classic/Logger\\r\\n\\xa0\\xa\n",
            "my_comment: ch/qos/logback/classic/Logger\n",
            "\\xa0\\xa\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1367\n",
            "issue_type: Improvement\n",
            "summary: Upgrade to Thrift 0.5.0\n",
            "description: There's finally a new thrift release out. This gives us a chance to standardize on a release instead of just a revision of thrift trunk.\n",
            "\n",
            "http://www.apache.org/dist/incubator/thrift/0.4.0-incubating/thrift-0.4.0.tar.gz\n",
            "architectural impact: NO\n",
            "comments: ['would we be able to keep using a stable release or would things like THRIFT-831 make us decide to go back to revision specific versions?\\n\\nmaybe it depends on whether thrift releases are frequent enough.', \"If it's a serious issue I assume they'll release a 0.3.1 to fix it, otherwise we'll catch it in the next release.\", \"It's not a fix, it's a huge performance increase.\\n\\nIMO having THRIFT-830 and THRIFT-831 would be much more useful than being on 0.3 exactly (which affects clients not at all).\", 'Thrift 0.4 is out now which includes the performance improvements, so we could definitely use that.', '... thrift 0.5 now.', \"This is my first pass at this issue. \\n\\nThe patch does not include the thrift 0.5 jar, so you need to remove the current jar and replace with 0.5 jar\\n\\nAll tests pass except for TestMutations.test_dynamic_indexes_with_system_update_cf\\n\\nI'd appreciate if someone with fresh eyes could look at this.\\n\\n-Jake\", 'Just as an fyi, nirmal just got THRIFT-106 committed for version 0.6 which enables Java client SSL.  Just for those interested.  There will always be feature X enabled in revision Y.  I guess if thrift is coming out with regular versions now, we can just upgrade when version Z comes out (instead of doing builds of revision Y).', 'Removed the configuration changes that should not have been in first patch.', 'Added missing files', 'Fixed so patch will apply cleanly', 'patch has class named CopyOfByteArrayToken\\n\\nas Stu mentioned, BBUtil equals and hashcode are unnecessary\\n\\nByteArrayToken is broken (discards offset and limit information) and should be unnecessary in the first place, if something in BytesToken is causing problems then we should fix that instead (but this is not the cause of the test failure)', 'MurmurHash is broken, it should be using data.get(index) (it also shouldn\\'t take a separate length value since BB knows its length).\\n\\nthe use of position() scares me, it\\'s a bug waiting to happen, e.g. in FBUtilities.hash\\n\\nblock.position()+block.arrayOffset()\\n\\nall of these these should all just be block.arrayOffset().\\n\\nSimilarly remaining() scares me.  We don\\'t \"use up\" our ByteBuffers on purpose except in very unusual cases (e.g. your BBUtil.getLong), all? of these should be limit - offset instead.\\n\\nI _suspect_ that there is a bug from position/remaining causing the test failure: it\\'s building the index on the test rows and being rejected at the row-level bloom filter saying \"this row doesn\\'t exist\" which is completely bogus.\\n\\nstyle: space after commas and between operators please.\\n\\nLet\\'s fix the above and see where that gets us.', \"Ok, I can fix these.  \\n\\nByteArrayToken is needed for MerkleTree serialization.  ByteBuffer isn't Serializable so I convert ByteToken to ByteArrayToken.\\n\\nWe have to use position() as arrayOffset() is almost always zero.  I'm following the logic here http://blog.rapleaf.com/dev/2010/10/19/striving-for-zero-copies-with-thrift-0.5/\\n\\n\", 'i _think_ limit - offset is what capacity() gives you.', 'if thrift is creating multiple bytebuffers from the same byte[] with the same offset it is broken.  look at HeapByteBuffer.slice, the idea is the offset should be the start of the valid bytes in the buffer.  position is mutable by the relative get methods but offset is not.  or hell, look at BB equals/hashcode.', \"bq. ByteArrayToken is needed for MerkleTree serialization. ByteBuffer isn't Serializable so I convert ByteToken to ByteArrayToken\\n\\nMerkleTree doesn't have any member Token fields that I can see...?\", 'RowHash class', 'bq. i think limit - offset is what capacity() gives you. \\n\\nturns out this is wrong.\\n\\nwhat we really want is capacity - offset.  limit is some weird-ass inverse mark.  Buffer doc explains,\\n\\nbq. 0 <= mark <= position <= limit <= capacity ... A read-only buffer does not allow its content to be changed, but its mark, position, and limit values are mutable.', \"... ByteBuffer.wrap cleared things up for me: you're right, we do need to be using position / limit / remaining.  what a brain-damaged api.  It may be a mistake for Thrift to be using ByteBuffer instead of rolling its own but obviously we're stuck with it here.\", 'bq. RowHash class \\n\\nis that the JDK being retarded?  because there are no instances of RowHash to serialize either.', \"Ah sorry, it's the Inner class. This has a Token member\", 'can we fix by overriding Inner serialization instead, with writeObject etc?', 'bq. as Stu mentioned, BBUtil equals and hashcode are unnecessary\\n\\nwe can also get rid of getLong by using bb absolute methods instead', 're above, IMO we should treat position/limit as immutable, we could even check if assertions are enabled and wrap BB in methods that throw errors on non-absolute methods, although that might be more paranoid than necessary', 'bq. can we fix by overriding Inner serialization instead, with writeObject etc? \\n\\nI suppose we could make Token serializable instead the same way, too.', '(re MurmurHash, the \"right\" api is to take byte[] offset length, so it can be used for both ByteBuffer callers and byte[] ones)', 'This patch removes many ButeBufferUtil calls, removes ByteArrayToken, and fixes murmur hash', 'committed.  An impressive effort!', 'Integrated in Cassandra #573 (See [https://hudson.apache.org/hudson/job/Cassandra/573/])\\n    upgrade to Thrift 0.5\\npatch by Jake Luciani; reviewed by jbellis for CASSANDRA-1367\\n']\n",
            "my_comment: youre right we do need to be using position / limit / remaining.  what a brain-damaged api.  It may be a mistake for Thrift to be using ByteBuffer instead of rolling its own but obviously were stuck with it here.\" bq. RowHash class \n",
            "\n",
            "is that the JDK being retarded?  because there are no instances of RowHash to serialize either. \"Ah sorry its the Inner class. This has a Token member\" can we fix by overriding Inner serialization instead with writeObject etc? bq. as Stu mentioned BBUtil equals and hashcode are unnecessary\n",
            "\n",
            "we can also get rid of getLong by using bb absolute methods instead re above IMO we should treat position/limit as immutable we could even check if assertions are enabled and wrap BB in methods that throw errors on non-absolute methods although that might be more paranoid than necessary bq. can we fix by overriding Inner serialization instead with writeObject etc? \n",
            "\n",
            "I suppose we could make Token serializable instead the same way too. (re MurmurHash the \"right\" api is to take byte[] offset length so it can be used for both ByteBuffer callers and byte[] ones) This patch removes many ButeBufferUtil calls removes ByteArrayToken and fixes murmur hash committed.  An impressive effort! Integrated in Cassandra #573 (See [https://hudson.apache.org/hudson/job/Cassandra/573/])\n",
            "    upgrade to Thrift 0.5\n",
            "patch by Jake Luciani; reviewed by jbellis for CASSANDRA-1367\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1406\n",
            "issue_type: Bug\n",
            "summary: Dropping column families doesn't clean up secondary indexes\n",
            "description: None\n",
            "architectural impact: NO\n",
            "comments: ['patch replaces CF graveyard with CFS.removeAllSSTables (which is recursive to handle 2ary index files)', \"putting the deletion in Table.dropCF() is risky wrt KS and CF renames.  It won't hurt anything right now because the delete tries to remove the old files.  If that side-effect ever went away we'd be deleting valid files.  As it stands with this patch we'd be trying to do cleanup of sstables that have had their files moved out from underneath them, which probably isn't the best thing to try.\\n\\nIs this a good ticket to lump in that RenameKeyspace and RenameColumnFamily don't address secondary indices, or should that be a new one?\", 'otoh, requiring the caller who calls dropCf to also call other methods to finish the job is poor encapsulation, and implementing rename as drop + add is itself an implementation detail.  neither approach is completely satisfactory imo.\\n\\n02 adds support for 2ary indexes in the rename methods.', '(tweaked DefsTest to not require renaming to leave an empty directory behind for the old name)', 'dropCf was intended to unload a CFS from a table instance as indicated in its comment.  0003 cleans the interface up so that renaming acts on files like drop does.', \"+1, but I'd like 0003 to be included.\", 'committed w/ 03']\n",
            "my_comment: patch replaces CF graveyard with CFS.removeAllSSTables (which is recursive to handle 2ary index files) \"putting the deletion in Table.dropCF() is risky wrt KS and CF renames.  It wont hurt anything right now because the delete tries to remove the old files.  If that side-effect ever went away wed be deleting valid files.  As it stands with this patch wed be trying to do cleanup of sstables that have had their files moved out from underneath them which probably isnt the best thing to try.\n",
            "\n",
            "Is this a good ticket to lump in that RenameKeyspace and RenameColumnFamily dont address secondary indices or should that be a new one?\" otoh requiring the caller who calls dropCf to also call other methods to finish the job is poor encapsulation and implementing rename as drop + add is itself an implementation detail.  neither approach is completely satisfactory imo.\n",
            "\n",
            "02 adds support for 2ary indexes in the rename methods. (tweaked DefsTest to not require renaming to leave an empty directory behind for the old name) dropCf was intended to unload a CFS from a table instance as indicated in its comment.  0003 cleans the interface up so that renaming acts on files like drop does. \"+1 but Id like 0003 to be included.\" committed w/ 03\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14145\n",
            "issue_type: Improvement\n",
            "summary:  Detecting data resurrection during read\n",
            "description: We have seen several bugs in which deleted data gets resurrected. We should try to see if we can detect this on the read path and possibly fix it. Here are a few examples which brought back data\n",
            "\n",
            "\n",
            "\n",
            "A replica lost an sstable on startup which caused one replica to lose the tombstone and not the data. This tombstone was past gc grace which means this could resurrect data. We can detect such invalid states by looking at other replicas. \n",
            "\n",
            "\n",
            "\n",
            "If we are running incremental repair, Cassandra will keep repaired and non-repaired data separate. Every-time incremental repair will run, it will move the data from non-repaired to repaired. Repaired data across all replicas should be 100% consistent. \n",
            "\n",
            "\n",
            "\n",
            "Here is an example of how we can detect and mitigate the issue in most cases. \n",
            "\n",
            "Say we have 3 machines, A,B and C. All these machines will have data split b/w repaired and non-repaired. \n",
            "\n",
            "1. Machine A due to some bug bring backs data D. This data D is in repaired dataset. All other replicas will have data D and tombstone T \n",
            "\n",
            "2. Read for data D comes from application which involve replicas A and B. The data being read involves data which is in repaired state.  A will respond back to co-ordinator with data D and B will send nothing as tombstone is past gc grace. This will cause digest mismatch. \n",
            "\n",
            "3. This patch will only kick in when there is a digest mismatch. Co-ordinator will ask both replicas to send back all data like we do today but with this patch, replicas will respond back what data it is returning is coming from repaired vs non-repaired. If data coming from repaired does not match, we know there is a something wrong!! At this time, co-ordinator cannot determine if replica A has resurrected some data or replica B has lost some data. We can still log error in the logs saying we hit an invalid state.\n",
            "\n",
            "4. Besides the log, we can take this further and even correct the response to the query. After logging an invalid state, we can ask replica A and B (and also C if alive) to send back all data for this including gcable tombstones. If any machine returns a tombstone which is after this data, we know we cannot return this data. This way we can avoid returning data which has been deleted. \n",
            "\n",
            "\n",
            "\n",
            "Some Challenges with this \n",
            "\n",
            "1. When data will be moved from non-repaired to repaired, there could be a race here. We can look at which incremental repairs have promoted things on which replica to avoid false positives.  \n",
            "\n",
            "2. If the third replica is down and live replica does not have any tombstone, we wont be able to break the tie in deciding whether data was actually deleted or resurrected. \n",
            "\n",
            "3. If the read is for latest data only, we wont be able to detect it as the read will be served from non-repaired data. \n",
            "\n",
            "4. If the replica where we lose a tombstone is the last replica to compact the tombstone, we wont be able to decide if data is coming back or rest of the replicas has lost that data. But we will still detect something is wrong. \n",
            "\n",
            "5. We wont affect 99.9% of the read queries as we only do extra work during digest mismatch.\n",
            "\n",
            "6. CL.ONE reads will not be able to detect this. \n",
            "architectural impact: NO\n",
            "comments: [\"I agree with the problem description and that we should detect inconsistencies in the repaired set when reading data. But, as described in CASSANDRA-13912, I'm not convinced that correcting data as part of read repairs is the way to go for already repaired data. \\r\\n\\r\\nThere could be many different reasons for inconsistencies. Corrupted or missing sstables are relatively easy to reason about in this context. What I'm more concerned of are internal consistency issues due to rare edge cases, e.g. during upgrade paths or race conditions. Returning inconsistent results in rare cases might be less of an issue, compared to risking greater harm by running repair code which was created with other situations in mind.\\r\\n\\r\\nI'm wondering if we shouldn't just start by logging inconsistencies, allowing operators to further investigate. If there's some serious issue with a node, it needs to be replaced anyways. If the hardware is fine and there are no external causes that lead to inconsistencies, we may have a bigger problem to fix.\\r\\n\", \"I agree with Stefan that logging inconsistencies so that operators can investigate further is the sensible way to approach this initially so I've taken a pass at this in the branch linked below.\\r\\n\\r\\nOn digest mismatch, the coordinator adds a new parameter to the requests for the full data reads. When executing the query, replicas generate a digest of the portion of the data read from their repaired sstables. When the coordinator resolves the data requests, it checks for multiple digests and logs + increments a metric if it finds > 1. To mitigate against false positives caused by sstables moving from pending to repaired at slightly different times on each replica, we also track if the reads touched any tables with pending, but locally uncommitted, repair sessions. If any replica had pending sessions during the read, we increment a different metric (I called these confirmed and unconfirmed inconsistencies).\\r\\n \\r\\nPartition range reads don't make digest requests, so in order to detect inconsistency on that side of the read path the coordinator always adds the parameter to request the info on the repaired status. Although the overhead of tracking the repaired status should be minimal, this means the every range read will perform the additional work. With that in mind, to be conservative I've added separate config option/JMX operations to enable/disable it for single partition and range reads.\\r\\n\\r\\nI haven't added any dtests yet, but there's quite a bit of unit test coverage in the patch\\r\\n\\r\\n||branch||utest||dtest||\\r\\n|[14145-trunk|https://github.com/beobal/cassandra/tree/14145-trunk]|[utest|https://circleci.com/gh/beobal/cassandra/300]|[vnodes|https://circleci.com/gh/beobal/cassandra/301] / [no_vnodes|https://circleci.com/gh/beobal/cassandra/302]|\\r\\n\", \"pushed a branch [here|https://github.com/krummas/cassandra/commits/sam/14145] with initial comments:\\r\\n* Only allocate repairedIterators if needed\\r\\n* Since sstables can jump from pending to repaired at any time, we need to check isRepaired last (and grab the pending id to avoid NPE)\\r\\n* Skip the unrepaired sstables when we are adding all the repaired sstables for digest calculation\\r\\n* in {{considerRepairedForTracking}} we should only return true if we are tracking repaired status\\r\\n* A few inline comments/questions\\r\\n** can we really break if reduceFilter returns null in SinglePartitionReadCommand?\\r\\n** Don't think WithDigestAndLowerBound actually works? iterator will not be instanceof IteratorWithLowerBound after merging right? It should probably have dedicated tests as well once fixed\\r\\n** Could we keep the old behaviour in {{queryMemtableAndSSTablesInTimestampOrder}} (ie, don't collect the iterators, just build the result) if not tracking repaired status, code might get a lot uglier though\\r\\n** in {{DefaultRepairedDataInfo}} is it worth keeping around a set of pending repairs when the only use (outside of tests) is to check if it is empty?\\r\\n\\r\\nOther thoughts;\\r\\n* We should probably add metrics for how many extra sstables are touched due to tracking repaired digests\\r\\n* It would be nice if we had Tracing info for these digest comparisons\\r\\n* Could we enable this globally for dtest runs? If there is a dtest that on purpose messes up the repaired data we can disable this feature for those tests\\r\\n* A couple of end-to-end dtests should probably be added\", \"Thanks, I've pulled those changes into my branch.\\r\\n\\r\\nbq.A few inline comments/questions\\r\\nI think\\xa0most of these can be addressed by instead of tracking just pending repairs, we switch to a boolean conclusive/inconclusive flag in \\\\{{RepairedDataInfo}}. We could then mark any info inconclusive if we encounter any of:\\r\\n* pending & uncommitted repair sessions\\r\\n* any unrepaired data which causes\\xa0{{reduceFilter}} to modify the CINF\\r\\n* any unrepaired partition deletion\\r\\n\\r\\nThis would also get rid of the increase in SSTables touched on full data reads as we can re-instate the short circuiting logic. Not sure if this will let us fully revert to the old behaviour in {{queryMemtableAndSSTablesInTimestampOrder}}\\xa0completely, but\\xa0I think it will help.\\r\\n\\r\\nbq. Don't think WithDigestAndLowerBound actually works?\\r\\nYou're right about this, it did work until the commit which brought in the pre-emptive merging of repaired data in \\\\{{SPRC}}. Now though it's just unnecessary as the 8018 optimisation just kicks in during the pre-emptive merge, before the base iters are wrapped {{withDigestInfo}}\\r\\n\\r\\nbq. We should probably add metrics for how many extra sstables are touched due to tracking repaired digests\\r\\nAs mentioned, perhaps we don't need to touch extra sstables. But, the pre-emptive merging probably breaks sstable counting anyway now, as that isn't applied until later. I'll make sure of that and push a fix if necessary.\\r\\n\\r\\nbq. It would be nice if we had Tracing info for these digest comparisons\\r\\nAgreed, I'll add some\\r\\n\\r\\nOn the dtests, I'm working on adding some end-to-end happy path tests to the failure tests you pushed. I think we can enable this globabally by messing with default cluster config, but I'll confirm that.\", 'Many of my comments were overlapping with [~krummas] or were around {{queryMemtableAndSSTablesInTimestampOrder}} so I will wait for the updated branch to make a second pass. I think the changes are going in the right direction based on the changes we discussed\\xa0off-jira (and are mostly outlined above).\\xa0\\r\\n\\r\\nI do have some concerns about the utility of counting unconfirmed vs. confirmed. The majority of unconfirmed will be false positives due to race conditions and I\\'m not sure if that will be a useful signal to operators. That said, I agree its a potential proxy for other database issues like compaction lag after repair. I think it would be nice to have a flag to enable tracking unconfirmed or not. If false, unconfirmed inconsistencies would be\\xa0\"dropped\" (not counted) instead of incrementing either\\xa0count.\\xa0', \"Pushed some further updates to my branch: [here|https://github.com/beobal/cassandra/tree/14145-trunk]\\r\\n\\r\\nThere's still 2 things I'm working on (metrics for additional sstable reads due to digest tracking, dtests), but I think it's worth taking another look when you get a chance [~krummas] & [~jrwest].\\r\\n* Removed tracking of specific pending sessions and replaced with a more general notion of conclusive vs inconclusive digests\\r\\n* Extracted the guts of {{queryMemtableAndSSTablesInTimestampOrder}} to preserve previous behaviour when required and improve testing\\r\\n* Added option to disable reporting of unconfirmed mismatches (caused by mismatching but inconclusive digests)\\r\\n* Added tracing events around digest generation\\r\\n\", \"[~beobal] the changes are a definite improvement. I'm still reviewing but wanted to get you some feedback because of the tz difference. \\r\\n\\r\\n* I’m not a big fan of having to duplicate {{considerRepairedForTracking}} across {{ReadCommand}} and {{TimestampOrderPartitionReader}} because its race-condition prone code. While its not prone to the same sorts of bugs, duplicating {{shouldInclude}} also seems sub-optimal and there is some level of duplication between {{queryMemtableAndDiskInternal}} and {{TimestamOrderedPartitionReader.WithRepairedDataTracking}}. Since we lose a good chunk of the benefits of the optimization, would it make more sense to disable it entirely (at least for the initial commit) if {{isTrackingRepairedStatus()}}? We can revisit it as a performance optimization later. This would remove a bunch of the duplication and reduce the footprint/risk of the patch. \\r\\n* I would turn inconclusive tracking off by default. Especially because of the the window for potential false positives being widened by SinglePartitionReadCommand#L615-621.\\r\\n* Missing new cassandra.yaml entries\\r\\n\\r\\nMinor nits:\\r\\n* ReadCommand#newDataRepairInfo is only used once, consider just inlining its definition at the call site\\r\\n* I find it a little cleaner to get rid of the the default/empty implementation in the RepairedDataInfo and just make DefaultRepairedDataInfo a top-level class. That would get rid of the not-so-used {{NO_OP_REPAIRED_DATA_INFO}} and {{LocalDataResponse}} would be the only place to handle when getRepairedDataInfo is null. \\r\\n* In {{ReadResponse.Serializer::serializedSize}} the two additions to {{size} to account for the new fields could be collapsed into one line. Also, it could be useful to update the comment to explain what the extra byte is for.\\r\\n* The inner class {{WithDigest} inside {{UnfilteredRowIterators#withRepairedDataTracking}} could be more aptly named since it doesn’t perform the digesting\", 'bq. Since we lose a good chunk of the benefits of the optimization, would it make more sense to disable it entirely \\r\\nI agree. We\\'ve been pushing and pulling this around for a while now, in an attempt to retain some of the optimisations of {{queryMemtableAndSSTablesInTimestampOrder}} when tracking is enabled, but nothing so far has really been satisfactory. +1 to just disabling it when tracking for now & revisiting later.\\r\\n\\r\\nbq. I would turn inconclusive tracking off by default\\r\\nFair enough. As everything is off by default for now it probably makes for a better UI to selectively turn things on rather than flipping some switches up and some down. To that end I\\'ve reversed the semantics of that third option from \"only report confirmed\" to \"also report unconfirmed\".\\r\\n\\r\\nbq. Missing new cassandra.yaml entries\\r\\nSorry, forgot to git add the yaml file before pushing the previous commits. Updated for default changes above & pushed.\\r\\n\\r\\nbq. Minor nits\\r\\nAddressed and bundled in a single commit, with a couple of caveats:\\r\\n I came to the conclusion that {{RepairedDataInfo}} really ought to be private to the {{ReadCommand}} using it, so I removed the iface and just made it a private class. The digest and isConclusive are accessed via {{ReadCommand}} itself which I think does a better job of hiding unnecessary information. I did leave {{NO_OP_REPAIRED_DATA_INFO}} in (slightly renamed) as I\\'d rather a safe nullobject there than worry about null checking every use of it in case some path is overlooked or untested. \\r\\n\\r\\nI also felt that the splitting of iterators according to the repaired status of the sstables (or memtable) that they come from was a bit ugly and invasive, so I\\'ve refactored that into another inner class of {{ReadCommand}}, which tidied up {{SPRC/PRRC}} quite a bit.\\r\\n\\r\\nOne last thing, [~jrwest] mentioned off-JIRA that there\\'s an edge case where compaction is backed up and so an sstable may be marked pending, but the session to which it belongs has been purged. In that case, we\\'d mistakenly consider digests inconclusive, so I\\'ve added a check that the session actually exists, and if not we consider the sstable unrepaired. \\r\\n', \"Thanks [~beobal]. The recent updates are a big improvement. The {{InputCollector}} changes make things much more readable. I feel like we've reduced the footprint and risk considerably which is making me more comfortable about merging this late in the game. The feature is well hidden behind a flag: if tracking is not enabled we add a couple bytes to the internode messaging protocol that are not used but otherwise the changes to the existing path are negligible; {{InputCollector}} probably being the biggest. I do have one comment below I think should be addressed before merge. The other one we can\\xa0open\\xa0an improvement JIRA for. So I will give my +1 assuming the comment below is addressed so that we don't miss the deadline due to tz differences. We'll definitely need to test this for correctness and performance further but that can happen after 9/1.\\r\\n * This change I think should be made before we merge: In {{InputCollector}} is {{repairedSSTables}} necessary? It seems like an extra upfront allocation and iteration that we could skip. It also widens the unconfirmed window slightly since the status could change between the call to the constructor and the call to {{addSSTableIterator}}. What about going back to the on-demand allocation of {{repairedIters}} and checking if its {{null}} in {{finalizeIterators}}?\\r\\n * This can be addressed in a subsequent JIRA if you agree: In {{RepairedDataVerifier}}, we could more quickly report confirmed failures if we separated tracking of confirmed and unconfirmed digests. If the number of confirmed digests is > 1 we still have a confirmed issue regardless of the number of unconfirmed digests. We could separately increment unconfirmed in this case if the unconfirmed digests don't\\xa0match any of the confirmed ones (if it does we would assume its consistent).\\r\\n\\r\\nMinor Nits (up to you if you want to fix before merge):\\r\\n * Re: comments in cassandra.yaml, until we’ve benchmarked the changes maybe we shouldn’t try to characterize (“slight”) the performance impact besides to say it exists. Same goes for the identical comment in {{Config.java}}\", \"I like the new InputCollector approach, makes it really clean\\r\\n\\r\\nI pushed a small (totally untested) update [here|https://github.com/krummas/cassandra/commits/sam/14145] - only allocates the repairedSSTables set if needed and avoids iterating all sstables unless we track repaired status. Also makes InputCollector static.\\r\\n\\r\\nWe should benchmark this as we still do a few more allocations (I think). But I'm fine punting that until later.\\r\\n\\r\\n+1 on the patch, but we need the dtest changes in as well (enabling globally etc)\", \"This has two +1s and CI looks good (the dtests are running a branch which enables this globally), but unfortunately this conflicts quite heavily with CASSANDRA-14408 which landed yesterday. So I'm going to mark it Ready To Commit and take a look at the rebase on Monday. If I get through that in a reasonable time (a day or two) and CI still looks good, I'll commit then if nobody objects too much.\\r\\n\\r\\n||utests||dtests||\\r\\n|[utests|https://circleci.com/gh/beobal/cassandra/390]|[vnodes|https://circleci.com/gh/beobal/cassandra/389] / [no vnodes|https://circleci.com/gh/beobal/cassandra/388]|\\r\\n\", \"The changes required by transient replication were fairly minor and mostly confined to tests. The only real thing that needed updating in the main code was to discount responses from transient replicas in {{DataResolver}}. In light of that, I've also slightly changed the way tracking is requested, so that we only request tracking from full replicas.\\r\\n\\r\\nI've pushed a dtest branch with a few specific tests for this and with tracking enabled for all tests here: [dtests|https://github.com/beobal/cassandra-dtest/tree/14145]\\r\\nAlthough I have spot-checked a few fixtures, I haven't exhaustively verified this for everything yet. I'll update when CI runs are done\\r\\n\", '+1 on the latest changes', 'Current CI\\r\\n||branch||trunk||\\r\\n|[utests|https://circleci.com/gh/beobal/cassandra/422]|[utests|https://circleci.com/gh/beobal/cassandra/429]|\\r\\n|[dtests-with-vnodes|https://circleci.com/gh/beobal/cassandra/423]|[dtests-with-vnodes|https://circleci.com/gh/beobal/cassandra/428]|\\r\\n|[dtests-no-vnodes|https://circleci.com/gh/beobal/cassandra/424]|[dtests-no-vnodes|https://circleci.com/gh/beobal/cassandra/430]|\\r\\n', 'Thanks all, committed to trunk in {{5fbb938adaafd91e7bea1672f09a03c7ac5b9b9d}}', 'GitHub user beobal opened a pull request:\\n\\n    https://github.com/apache/cassandra-dtest/pull/37\\n\\n    Add tests for CASSANDRA-14145\\n\\n    * Enable read time repaired data tracking for all tests by default\\r\\n    * Add some specific tests for mismatching/matching repaired digests\\n\\nYou can merge this pull request into a Git repository by running:\\n\\n    $ git pull https://github.com/beobal/cassandra-dtest 14145\\n\\nAlternatively you can review and apply these changes as the patch at:\\n\\n    https://github.com/apache/cassandra-dtest/pull/37.patch\\n\\nTo close this pull request, make a commit to your master/trunk branch\\nwith (at least) the following in the commit message:\\n\\n    This closes #37\\n    \\n----\\ncommit a6d673df641800e708f8ad593644e63fb175fb39\\nAuthor: Marcus Eriksson <marcuse@...>\\nDate:   2018-08-23T18:40:43Z\\n\\n    Add intial tests for CASSANDRA-14145\\n    \\n    Enable read time repaired data tracking for all tests by default\\n\\n----\\n', \"Github user jrwest commented on a diff in the pull request:\\n\\n    https://github.com/apache/cassandra-dtest/pull/37#discussion_r218263635\\n  \\n    --- Diff: repair_tests/incremental_repair_test.py ---\\n    @@ -207,6 +208,7 @@ def test_manual_session_fail(self):\\n             self.fixture_dtest_setup.setup_overrides.cluster_options = ImmutableMapping({'hinted_handoff_enabled': 'false',\\n                                                                                          'num_tokens': 1,\\n                                                                                          'commitlog_sync_period_in_ms': 500})\\n    +        self.fixture_dtest_setup.init_default_config()\\n    --- End diff --\\n    \\n    Also I'm not sure its clear to me why `init_default_config()` is called before every test. Is it because the config changes in the preceding lines weren't actually being picked up prior? \\n\", \"Github user jrwest commented on a diff in the pull request:\\n\\n    https://github.com/apache/cassandra-dtest/pull/37#discussion_r218262733\\n  \\n    --- Diff: repair_tests/incremental_repair_test.py ---\\n    @@ -207,6 +208,7 @@ def test_manual_session_fail(self):\\n             self.fixture_dtest_setup.setup_overrides.cluster_options = ImmutableMapping({'hinted_handoff_enabled': 'false',\\n                                                                                          'num_tokens': 1,\\n                                                                                          'commitlog_sync_period_in_ms': 500})\\n    +        self.fixture_dtest_setup.init_default_config()\\n    --- End diff --\\n    \\n    I believe `self.init_default_config()` and `self.fixture_dtest_setup.init_default_config()` are synonymous: https://github.com/apache/cassandra-dtest/blob/master/dtest.py#L228-L233\\n\", 'Github user jrwest commented on a diff in the pull request:\\n\\n    https://github.com/apache/cassandra-dtest/pull/37#discussion_r218264113\\n  \\n    --- Diff: repair_tests/incremental_repair_test.py ---\\n    @@ -918,3 +931,196 @@ def test_subrange(self):\\n             self.assertRepairedAndUnrepaired(node1, \\'ks\\')\\n             self.assertRepairedAndUnrepaired(node2, \\'ks\\')\\n             self.assertRepairedAndUnrepaired(node3, \\'ks\\')\\n    +\\n    +    @since(\\'4.0\\')\\n    +    def test_repaired_tracking_with_partition_deletes(self):\\n    +        \"\"\"\\n    +        check that when an tracking repaired data status following a digest mismatch,\\n    +        repaired data mismatches are marked as unconfirmed as we may skip sstables\\n    +        after the partition delete are encountered.\\n    +        @jira_ticket CASSANDRA-14145\\n    +        \"\"\"\\n    +        session, node1, node2 = self.setup_for_repaired_data_tracking()\\n    +        stmt = SimpleStatement(\"INSERT INTO ks.tbl (k, c, v) VALUES (%s, %s, %s)\")\\n    +        stmt.consistency_level = ConsistencyLevel.ALL\\n    +        for i in range(10):\\n    +            session.execute(stmt, (i, i, i))\\n    +\\n    +        for node in self.cluster.nodelist():\\n    +            node.flush()\\n    +            self.assertNoRepairedSSTables(node, \\'ks\\')\\n    +\\n    +        node1.repair(options=[\\'ks\\'])\\n    +        node2.stop(wait_other_notice=True)\\n    +\\n    +        session.execute(\"delete from ks.tbl where k = 5\")\\n    +\\n    +        node1.flush()\\n    +        node2.start(wait_other_notice=True)\\n    +\\n    +        # expect unconfirmed inconsistencies as the partition deletes cause some sstables to be skipped\\n    +        with JolokiaAgent(node1) as jmx:\\n    +            self.query_and_check_repaired_mismatches(jmx, session, \"SELECT * FROM ks.tbl WHERE k = 5\",\\n    +                                                     expect_unconfirmed_inconsistencies=True)\\n    +            self.query_and_check_repaired_mismatches(jmx, session, \"SELECT * FROM ks.tbl WHERE k = 5 AND c = 5\",\\n    +                                                     expect_unconfirmed_inconsistencies=True)\\n    +            # no digest reads for range queries so blocking read repair metric isn\\'t incremented\\n    +            # *all* sstables are read for partition ranges too, and as the repaired set is still in sync there should\\n    +            # be no inconsistencies\\n    +            self.query_and_check_repaired_mismatches(jmx, session, \"SELECT * FROM ks.tbl\", expect_read_repair=False)\\n    +\\n    +    @since(\\'4.0\\')\\n    +    def test_repaired_tracking_with_varying_sstable_sets(self):\\n    +        \"\"\"\\n    +        verify that repaired data digests are computed over the merged data for each replica\\n    +        and that the particular number of sstables on each doesn\\'t affect the comparisons\\n    +        both replicas start with the same repaired set, comprising 2 sstables. node1\\'s is\\n    +        then compacted and additional unrepaired data added (which overwrites some in the\\n    +        repaired set). We expect the repaired digests to still match as the tracking will\\n    +        force all sstables containing the partitions to be read\\n    +        there are two variants of this, for single partition slice & names reads and range reads\\n    +        @jira_ticket CASSANDRA-14145\\n    +        \"\"\"\\n    +        session, node1, node2 = self.setup_for_repaired_data_tracking()\\n    +        stmt = SimpleStatement(\"INSERT INTO ks.tbl (k, c, v) VALUES (%s, %s, %s)\")\\n    +        stmt.consistency_level = ConsistencyLevel.ALL\\n    +        for i in range(10):\\n    +            session.execute(stmt, (i, i, i))\\n    +\\n    +        for node in self.cluster.nodelist():\\n    +            node.flush()\\n    +\\n    +        for i in range(10,20):\\n    +            session.execute(stmt, (i, i, i))\\n    +\\n    +        for node in self.cluster.nodelist():\\n    +            node.flush()\\n    +            self.assertNoRepairedSSTables(node, \\'ks\\')\\n    +\\n    +        node1.repair(options=[\\'ks\\'])\\n    +        node2.stop(wait_other_notice=True)\\n    +\\n    +        session.execute(\"insert into ks.tbl (k, c, v) values (5, 5, 55)\")\\n    +        session.execute(\"insert into ks.tbl (k, c, v) values (15, 15, 155)\")\\n    +        node1.flush()\\n    +        node1.compact()\\n    +        node1.compact()\\n    +        node2.start(wait_other_notice=True)\\n    +\\n    +        #\\xa0we don\\'t expect any inconsistencies as all repaired data is read on both replicas\\n    +        with JolokiaAgent(node1) as jmx:\\n    +            self.query_and_check_repaired_mismatches(jmx, session, \"SELECT * FROM ks.tbl WHERE k = 5\")\\n    +            self.query_and_check_repaired_mismatches(jmx, session, \"SELECT * FROM ks.tbl WHERE k = 5 AND c = 5\")\\n    +            # no digest reads for range queries so read repair metric isn\\'t incremented\\n    +            self.query_and_check_repaired_mismatches(jmx, session, \"SELECT * FROM ks.tbl\", expect_read_repair=False)\\n    +\\n    +    @since(\\'4.0\\')\\n    +    def test_repaired_tracking_with_mismatching_replicas(self):\\n    +        \"\"\"\\n    +        there are two variants of this, for single partition slice & names reads and range reads\\n    +        @jira_ticket CASSANDRA-14145\\n    +        \"\"\"\\n    +        session, node1, node2 = self.setup_for_repaired_data_tracking()\\n    +        stmt = SimpleStatement(\"INSERT INTO ks.tbl (k, c, v) VALUES (%s, %s, %s)\")\\n    +        stmt.consistency_level = ConsistencyLevel.ALL\\n    +        for i in range(10):\\n    +            session.execute(stmt, (i, i, i))\\n    +\\n    +        for node in self.cluster.nodelist():\\n    +            node.flush()\\n    +\\n    +        for i in range(10,20):\\n    +            session.execute(stmt, (i, i, i))\\n    +\\n    +        for node in self.cluster.nodelist():\\n    +            node.flush()\\n    +            self.assertNoRepairedSSTables(node, \\'ks\\')\\n    +\\n    +        # stop node 2 and mark its sstables repaired\\n    +        node2.stop(wait_other_notice=True)\\n    +        node2.run_sstablerepairedset(keyspace=\\'ks\\')\\n    +        # before restarting node2 overwrite some data on node1 to trigger digest mismatches\\n    +        session.execute(\"insert into ks.tbl (k, c, v) values (5, 5, 55)\")\\n    +        node2.start(wait_for_binary_proto=True)\\n    +\\n    +        out1 = node1.run_sstablemetadata(keyspace=\\'ks\\').stdout\\n    +        out2 = node2.run_sstablemetadata(keyspace=\\'ks\\').stdout\\n    +\\n    +        # verify the repaired at times for the sstables on node1/node2\\n    +        assert all(t == 0 for t in [int(x) for x in [y.split(\\' \\')[0] for y in findall(\\'(?<=Repaired at: ).*\\', out1)]])\\n    +        assert all(t > 0 for t in [int(x) for x in [y.split(\\' \\')[0] for y in findall(\\'(?<=Repaired at: ).*\\', out2)]])\\n    +\\n    +        #\\xa0we expect inconsistencies due to sstables being marked repaired on one replica only\\n    +        # these are marked confirmed because no sessions are pending & all sstables are\\n    +        # skipped due to partition deletes\\n    +        with JolokiaAgent(node1) as jmx:\\n    +            self.query_and_check_repaired_mismatches(jmx, session, \"SELECT * FROM ks.tbl WHERE k = 5\",\\n    +                                                     expect_confirmed_inconsistencies=True)\\n    +            self.query_and_check_repaired_mismatches(jmx, session, \"SELECT * FROM ks.tbl WHERE k = 5 AND c = 5\",\\n    +                                                     expect_confirmed_inconsistencies=True)\\n    +            # no digest reads for range queries so read repair metric isn\\'t incremented\\n    +            self.query_and_check_repaired_mismatches(jmx, session, \"SELECT * FROM ks.tbl\", expect_read_repair=False)\\n    +\\n    +    def setup_for_repaired_data_tracking(self):\\n    +        self.fixture_dtest_setup.setup_overrides.cluster_options = ImmutableMapping({\\'hinted_handoff_enabled\\': \\'false\\',\\n    +                                                                                     \\'num_tokens\\': 1,\\n    +                                                                                     \\'commitlog_sync_period_in_ms\\': 500})\\n    +        self.fixture_dtest_setup.init_default_config()\\n    +        self.cluster.populate(2)\\n    +        node1, node2 = self.cluster.nodelist()\\n    +        remove_perf_disable_shared_mem(node1)  # necessary for jmx\\n    +        self.cluster.start()\\n    +\\n    +        session = self.patient_exclusive_cql_connection(node1)\\n    +        session.execute(\"CREATE KEYSPACE ks WITH REPLICATION={\\'class\\':\\'SimpleStrategy\\', \\'replication_factor\\': 2}\")\\n    +        session.execute(\"CREATE TABLE ks.tbl (k INT, c INT, v INT, PRIMARY KEY (k,c)) with read_repair=\\'NONE\\'\")\\n    +        return session, node1, node2\\n    +\\n    +    def query_and_check_repaired_mismatches(self, jmx, session, query,\\n    +                                            expect_read_repair=True,\\n    +                                            expect_unconfirmed_inconsistencies=False,\\n    +                                            expect_confirmed_inconsistencies=False):\\n    +\\n    +        rr_count = make_mbean(\\'metrics\\', type=\\'ReadRepair\\', name=\\'ReconcileRead\\')\\n    +        unconfirmed_count = make_mbean(\\'metrics\\', type=\\'Table,keyspace=ks\\', name=\\'RepairedDataInconsistenciesUnconfirmed,scope=tbl\\')\\n    +        confirmed_count = make_mbean(\\'metrics\\', type=\\'Table,keyspace=ks\\', name=\\'RepairedDataInconsistenciesConfirmed,scope=tbl\\')\\n    +\\n    +        rr_before = self.get_attribute_count(jmx, rr_count)\\n    +        uc_before = self.get_attribute_count(jmx, unconfirmed_count)\\n    +        cc_before = self.get_attribute_count(jmx, confirmed_count)\\n    +\\n    +        stmt = SimpleStatement(query)\\n    +        stmt.consistency_level = ConsistencyLevel.ALL\\n    +        session.execute(stmt)\\n    +\\n    +        rr_after = self.get_attribute_count(jmx, rr_count)\\n    +        uc_after = self.get_attribute_count(jmx, unconfirmed_count)\\n    +        cc_after = self.get_attribute_count(jmx, confirmed_count)\\n    +\\n    +        logger.debug(\"RR: {before}, {after}\".format(before=rr_before, after=rr_after))\\n    +        logger.debug(\"UI: {before}, {after}\".format(before=uc_before, after=uc_after))\\n    +        logger.debug(\"CI: {before}, {after}\".format(before=cc_before, after=cc_after))\\n    +\\n    +        if expect_read_repair:\\n    +            assert rr_after > rr_before\\n    +        else:\\n    +            assert rr_after == rr_before\\n    +\\n    +        if expect_unconfirmed_inconsistencies:\\n    +            assert uc_after > uc_before\\n    +        else:\\n    +            assert uc_after == uc_before\\n    +\\n    +        if expect_confirmed_inconsistencies:\\n    +            assert cc_after > cc_before\\n    +        else:\\n    +            assert cc_after == cc_before\\n    +\\n    +    def get_attribute_count(self, jmx, bean):\\n    +        # the MBean may not have been initialized, in which case Jolokia agent will return\\n    +        # a HTTP 404 response. If we receive such, we know that the count can only be 0\\n    +        if jmx.has_mbean(bean):\\n    +            # expect 0 digest mismatches\\n    --- End diff --\\n    \\n    should this comment be in the else branch? Or removed? \\n', 'Github user jrwest commented on a diff in the pull request:\\n\\n    https://github.com/apache/cassandra-dtest/pull/37#discussion_r218264028\\n  \\n    --- Diff: repair_tests/incremental_repair_test.py ---\\n    @@ -918,3 +931,196 @@ def test_subrange(self):\\n             self.assertRepairedAndUnrepaired(node1, \\'ks\\')\\n             self.assertRepairedAndUnrepaired(node2, \\'ks\\')\\n             self.assertRepairedAndUnrepaired(node3, \\'ks\\')\\n    +\\n    +    @since(\\'4.0\\')\\n    +    def test_repaired_tracking_with_partition_deletes(self):\\n    +        \"\"\"\\n    +        check that when an tracking repaired data status following a digest mismatch,\\n    +        repaired data mismatches are marked as unconfirmed as we may skip sstables\\n    +        after the partition delete are encountered.\\n    +        @jira_ticket CASSANDRA-14145\\n    +        \"\"\"\\n    +        session, node1, node2 = self.setup_for_repaired_data_tracking()\\n    +        stmt = SimpleStatement(\"INSERT INTO ks.tbl (k, c, v) VALUES (%s, %s, %s)\")\\n    +        stmt.consistency_level = ConsistencyLevel.ALL\\n    +        for i in range(10):\\n    +            session.execute(stmt, (i, i, i))\\n    +\\n    +        for node in self.cluster.nodelist():\\n    +            node.flush()\\n    +            self.assertNoRepairedSSTables(node, \\'ks\\')\\n    +\\n    +        node1.repair(options=[\\'ks\\'])\\n    +        node2.stop(wait_other_notice=True)\\n    +\\n    +        session.execute(\"delete from ks.tbl where k = 5\")\\n    +\\n    +        node1.flush()\\n    +        node2.start(wait_other_notice=True)\\n    +\\n    +        # expect unconfirmed inconsistencies as the partition deletes cause some sstables to be skipped\\n    +        with JolokiaAgent(node1) as jmx:\\n    +            self.query_and_check_repaired_mismatches(jmx, session, \"SELECT * FROM ks.tbl WHERE k = 5\",\\n    +                                                     expect_unconfirmed_inconsistencies=True)\\n    +            self.query_and_check_repaired_mismatches(jmx, session, \"SELECT * FROM ks.tbl WHERE k = 5 AND c = 5\",\\n    +                                                     e\n",
            "my_comment: repair_tests/incremental_repair_test.py ---\n",
            "    @@ -9183 +931196 @@ def test_subrange(self):\n",
            "             self.assertRepairedAndUnrepaired(node1 \\ks\\)\n",
            "             self.assertRepairedAndUnrepaired(node2 \\ks\\)\n",
            "             self.assertRepairedAndUnrepaired(node3 \\ks\\)\n",
            "    +\n",
            "    +    @since(\\4.0\\)\n",
            "    +    def test_repaired_tracking_with_partition_deletes(self):\n",
            "    +        \"\"\"\n",
            "    +        check that when an tracking repaired data status following a digest mismatch\n",
            "    +        repaired data mismatches are marked as unconfirmed as we may skip sstables\n",
            "    +        after the partition delete are encountered.\n",
            "    +        @jira_ticket CASSANDRA-14145\n",
            "    +        \"\"\"\n",
            "    +        session node1 node2 = self.setup_for_repaired_data_tracking()\n",
            "    +        stmt = SimpleStatement(\"INSERT INTO ks.tbl (k c v) VALUES (%s %s %s)\")\n",
            "    +        stmt.consistency_level = ConsistencyLevel.ALL\n",
            "    +        for i in range(10):\n",
            "    +            session.execute(stmt (i i i))\n",
            "    +\n",
            "    +        for node in self.cluster.nodelist():\n",
            "    +            node.flush()\n",
            "    +            self.assertNoRepairedSSTables(node \\ks\\)\n",
            "    +\n",
            "    +        node1.repair(options=[\\ks\\)\n",
            "    +        node2.stop(wait_other_notice=True)\n",
            "    +\n",
            "    +        session.execute(\"delete from ks.tbl where k = 5\")\n",
            "    +\n",
            "    +        node1.flush()\n",
            "    +        node2.start(wait_other_notice=True)\n",
            "    +\n",
            "    +        # expect unconfirmed inconsistencies as the partition deletes cause some sstables to be skipped\n",
            "    +        with JolokiaAgent(node1) as jmx:\n",
            "    +            self.query_and_check_repaired_mismatches(jmx session \"SELECT * FROM ks.tbl WHERE k = 5\"\n",
            "    +                                                     expect_unconfirmed_inconsistencies=True)\n",
            "    +            self.query_and_check_repaired_mismatches(jmx session \"SELECT * FROM ks.tbl WHERE k = 5 AND c = 5\"\n",
            "    +                                                     e\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14523\n",
            "issue_type: New Feature\n",
            "summary: Thread pool stats virtual table\n",
            "description: Expose the thread pools like in status logger/tpstats. Additionally be nice to include the scheduled executor pools that are currently unmonitored.\n",
            "\n",
            "{code:java}\n",
            "\n",
            "cqlsh> select * from system_views.thread_pools;\n",
            "\n",
            "\n",
            "\n",
            " thread_pool                      | active | active_max | completed | pending | tasks_blocked | total_blocked\n",
            "\n",
            "----------------------------------+--------+------------+-----------+---------+---------------+---------------\n",
            "\n",
            "               anti_entropy_stage |      0 |          1 |         0 |       0 |             0 |             0\n",
            "\n",
            "           cache_cleanup_executor |      0 |          1 |         0 |       0 |             0 |             0\n",
            "\n",
            "              compaction_executor |      0 |          4 |        41 |       0 |             0 |             0\n",
            "\n",
            "           counter_mutation_stage |      0 |         32 |         0 |       0 |             0 |             0\n",
            "\n",
            "                     gossip_stage |      0 |          1 |         0 |       0 |             0 |             0\n",
            "\n",
            "                 hints_dispatcher |      0 |          2 |         0 |       0 |             0 |             0\n",
            "\n",
            "          internal_response_stage |      0 |          8 |         0 |       0 |             0 |             0\n",
            "\n",
            "            memtable_flush_writer |      0 |          2 |         5 |       0 |             0 |             0\n",
            "\n",
            "              memtable_post_flush |      0 |          1 |        20 |       0 |             0 |             0\n",
            "\n",
            "          memtable_reclaim_memory |      0 |          1 |         5 |       0 |             0 |             0\n",
            "\n",
            "                  migration_stage |      0 |          1 |         0 |       0 |             0 |             0\n",
            "\n",
            "                       misc_stage |      0 |          1 |         0 |       0 |             0 |             0\n",
            "\n",
            "                   mutation_stage |      0 |         32 |       247 |       0 |             0 |             0\n",
            "\n",
            "        native_transport_requests |      1 |        128 |        28 |       0 |             0 |             0\n",
            "\n",
            "         pending_range_calculator |      0 |          1 |         2 |       0 |             0 |             0\n",
            "\n",
            " per_disk_memtable_flush_writer_0 |      0 |          2 |         5 |       0 |             0 |             0\n",
            "\n",
            "                read_repair_stage |      0 |          8 |         0 |       0 |             0 |             0\n",
            "\n",
            "                       read_stage |      0 |         32 |        13 |       0 |             0 |             0\n",
            "\n",
            "                      repair_task |      0 | 2147483647 |         0 |       0 |             0 |             0\n",
            "\n",
            "           request_response_stage |      0 |          8 |         0 |       0 |             0 |             0\n",
            "\n",
            "                          sampler |      0 |          1 |         0 |       0 |             0 |             0\n",
            "\n",
            "             scheduled_fast_tasks |      0 | 2147483647 |      1398 |       1 |             0 |             0\n",
            "\n",
            "              scheduled_heartbeat |      0 | 2147483647 |        14 |       1 |             0 |             0\n",
            "\n",
            "        scheduled_hotness_tracker |      0 | 2147483647 |         0 |       1 |             0 |             0\n",
            "\n",
            "     scheduled_non_periodic_tasks |      0 | 2147483647 |        10 |       0 |             0 |             0\n",
            "\n",
            "         scheduled_optional_tasks |      0 | 2147483647 |         5 |       8 |             0 |             0\n",
            "\n",
            "        scheduled_summary_builder |      0 | 2147483647 |         0 |       1 |             0 |             0\n",
            "\n",
            "                  scheduled_tasks |      0 | 2147483647 |       194 |      74 |             0 |             0\n",
            "\n",
            "       secondary_index_management |      0 |          1 |         0 |       0 |             0 |             0\n",
            "\n",
            "              validation_executor |      0 | 2147483647 |         0 |       0 |             0 |             0\n",
            "\n",
            "              view_build_executor |      0 |          1 |         0 |       0 |             0 |             0\n",
            "\n",
            "              view_mutation_stage |      0 |         32 |         0 |       0 |             0 |             0\n",
            "\n",
            "{code}\n",
            "architectural impact: NO\n",
            "comments: ['GitHub user clohfink opened a pull request:\\n\\n    https://github.com/apache/cassandra/pull/237\\n\\n    Virtual table of thread pool stats for CASSANDRA-14523\\n\\n    \\n\\nYou can merge this pull request into a Git repository by running:\\n\\n    $ git pull https://github.com/clohfink/cassandra virtual_tpstats\\n\\nAlternatively you can review and apply these changes as the patch at:\\n\\n    https://github.com/apache/cassandra/pull/237.patch\\n\\nTo close this pull request, make a commit to your master/trunk branch\\nwith (at least) the following in the commit message:\\n\\n    This closes #237\\n    \\n----\\ncommit 3d1d75e014b6262a4c37e9eb607559825a945d4d\\nAuthor: Chris Lohfink <clohfink@...>\\nDate:   2018-06-14T06:45:06Z\\n\\n    wip\\n\\ncommit 7b6762f9ddb4afcec458134dd32dbb46cccfcc3e\\nAuthor: Chris Lohfink <clohfink@...>\\nDate:   2018-06-14T19:09:26Z\\n\\n    wip\\n\\n----\\n', 'Thanks for the patch.\\r\\n\\r\\nI think we should take advantages of the virtual table patches to clean a bit the Metric classes. I pushed a commit [here|https://github.com/apache/cassandra/commit/7e8ec2f22c8d58351e72f550f811108fdc71a0a7] to remove the duplicated code in {{ThreadPoolMetrics}} and {{SEPMetrics}}.\\r\\n\\r\\nOne common problem to expose the metrics through virtual tables when the {{DataSet}} is build on demand is to be able to retrieve the set of metrics. Due to that the current patch has to fetch the metric values through {{JMX}} and change the visibility of some thread pools. To avoid that problem, we could keep a concurrent {{DataSet}} in memory and force the metric classes to add the column values to the tables. I have pushed a patch to add a concurrent {{DataSet}} [here|https://github.com/apache/cassandra/commit/98ff447ec580bf42cc7141a4a484f79ef7f664a9] and one to use it in {{ThreadPoolMetrics}} [here|https://github.com/apache/cassandra/commit/960563fdc865c9c06b66c4f29b161ffbb3d7213d]. I had too change a bit the {{DataSet}} API to be able to add the concurrent {{DataSet}} but the new API for building the {{DataSet}} can be used for both type of {{DataSets}}.\\r\\n\\r\\nAdding the metrics for the {{ScheduledThreadPool}} is a good idea. I just wonder if we should not simply add some real metrics for them. We could do that by replacing the {{ScheduledThreadPoolExecutor}} from {{SSTableReader}} by a {{DebuggableScheduledThreadPoolExecutor}} and by having the {{DebuggableScheduledThreadPoolExecutor}} class creating a metric for itself. The advantage of that approach is that it will also be possible to monitor those thread pools through {{nodetool}} and {{JMX}}. Ideally, I think it should be done in another ticket.\\r\\n\\r\\nI noticed that the patch reformat the name of the pools for exposing them in the virtual table. We should probably keep the original name as it is a bit confusing if other tools and logs display a different name.\\r\\n\\r\\nIt is not directly related to this patch but more a general remark about Virtual Tables: I wonder if we should not use a {{LocalPartitioner}} for them. Keeping the partition ordered by their partition key natural order is in my opinion an advantage for the virtual tables. ([~iamaleksey] do you see any problem with that?)\\r\\n\\r\\nI pushed some commits [here|https://github.com/apache/cassandra/compare/trunk...blerer:14523-trunk-review] to cover all the things I mentioned.\\r\\n\\r\\n\\xa0What do you think [~cnlwsu]?', \"If going with the dynamic virtual table i think that can be generic and creating the MetricTable shouldn't be necessary. Instead maybe a DynamicVirtualTable or something that does what the current MetricTable implementation is, then people can just build them inline without creating a parent class for each.\\r\\n\\r\\nbq.  replacing the ScheduledThreadPoolExecutor...\\r\\n\\r\\nAgreed. We can just remove all the scheduled timer executors and pull off in different ticket then to register metrics for tpstats etc as well. We wouldnt want to put it into DebuggableScheduledThreadPoolExecutor just like we dont put the metrics in DebuggableThreadPoolExecutor, but instead another layer higher and then just go through and change current scheduled pools to use that.\\r\\n\\r\\nbq. I noticed that the patch reformat the name of the pools for exposing them in the virtual table. We should probably keep the original name as it is a bit confusing if other tools and logs display a different name.\\r\\n\\r\\nI think we should use the names in cql naming convention, not camel case. This is not nodetool and we should look to making things that look and feel right and intuitive. I think we can safely assume people understand that MutationStage and mutation_stage are the same thing and it will not confuse them.\\r\\n\\r\\nbq.  I wonder if we should not use a LocalPartitioner\\r\\n\\r\\nFor the partitioner, {{result.addRow(..., result.newRow(...))}} is awkward naming... if going with this might need some naming or api changes as it doesnt really make much sense (clustering instead of newRow or {{result.newPartition(...).withClustering(...)}}). I think we can push the changing of the partitioner and change in api to another ticket as it doesnt have much to do with this.\\r\\n\\r\\nI can put together an updated PR in a bit\", '{quote}I think we should use the names in cql naming convention, not camel case. This is not nodetool and we should look to making things that look and feel right and intuitive. I think we can safely assume people understand that MutationStage and mutation_stage are the same thing and it will not confuse them.{quote}\\r\\n\\r\\nI was not only thinking to {{nodetool}}. The thread names appears in the log files, they also appears in all the Java tooling {{JMX}}, profilers, ... People will tend to search using those names which are the {{true}} names. cql naming convention do not apply there.', 'Removed scheduled thread pools and applied some of changes. I feel the dynamic tables and the local partitioner changes can be pulled off in different ticket as its a lot of refactoring things in way thats not necessary here. We can fetch the metrics through registry but I kinda like doing it same way as nodetool as it is pretty simple and low impact.\\r\\n\\r\\nIncluded change to make thread names camel case. It looks bad imho since we are not even constant in code, sometimes even using camel case with dashes:\\r\\n\\r\\n{code}\\r\\n thread_pool                      | active | active_max | completed | pending | tasks_blocked | total_blocked\\r\\n----------------------------------+--------+------------+-----------+---------+---------------+---------------\\r\\n          memtable_reclaim_memory |      0 |          1 |         1 |       0 |             0 |             0\\r\\n                      repair_task |      0 | 2147483647 |         0 |       0 |             0 |             0\\r\\n            memtable_flush_writer |      0 |          2 |         1 |       0 |             0 |             0\\r\\n           request_response_stage |      0 |          8 |         0 |       0 |             0 |             0\\r\\n                read_repair_stage |      0 |          8 |         0 |       0 |             0 |             0\\r\\n              compaction_executor |      0 |          2 |        47 |       0 |             0 |             0\\r\\n                   mutation_stage |      0 |         32 |         0 |       0 |             0 |             0\\r\\n                       read_stage |      0 |         32 |        12 |       0 |             0 |             0\\r\\n                          sampler |      0 |          1 |         0 |       0 |             0 |             0\\r\\n               anti_entropy_stage |      0 |          1 |         0 |       0 |             0 |             0\\r\\n per_disk_memtable_flush_writer_0 |      0 |          2 |         1 |       0 |             0 |             0\\r\\n           cache_cleanup_executor |      0 |          1 |         0 |       0 |             0 |             0\\r\\n              validation_executor |      0 | 2147483647 |         0 |       0 |             0 |             0\\r\\n                       misc_stage |      0 |          1 |         0 |       0 |             0 |             0\\r\\n           counter_mutation_stage |      0 |         32 |         0 |       0 |             0 |             0\\r\\n        native_transport_requests |      1 |        128 |        28 |       0 |             0 |             0\\r\\n          internal_response_stage |      0 |          8 |         0 |       0 |             0 |             0\\r\\n                  migration_stage |      0 |          1 |         0 |       0 |             0 |             0\\r\\n                     gossip_stage |      0 |          1 |         0 |       0 |             0 |             0\\r\\n              memtable_post_flush |      0 |          1 |         2 |       0 |             0 |             0\\r\\n         pending_range_calculator |      0 |          1 |         2 |       0 |             0 |             0\\r\\n              view_mutation_stage |      0 |         32 |         0 |       0 |             0 |             0\\r\\n                 hints_dispatcher |      0 |          2 |         0 |       0 |             0 |             0\\r\\n              view_build_executor |      0 |          1 |         0 |       0 |             0 |             0\\r\\n       secondary_index_management |      0 |          1 |         0 |       0 |             0 |             0\\r\\n\\r\\n(25 rows)\\r\\n\\r\\n\\r\\n thread_pool                  | active | active_max | completed | pending | tasks_blocked | total_blocked\\r\\n------------------------------+--------+------------+-----------+---------+---------------+---------------\\r\\n              HintsDispatcher |      0 |          2 |         0 |       0 |             0 |             0\\r\\n           CompactionExecutor |      0 |          2 |        47 |       0 |             0 |             0\\r\\n           ValidationExecutor |      0 | 2147483647 |         0 |       0 |             0 |             0\\r\\n                  GossipStage |      0 |          1 |         0 |       0 |             0 |             0\\r\\n         CounterMutationStage |      0 |         32 |         0 |       0 |             0 |             0\\r\\n                MutationStage |      0 |         32 |         0 |       0 |             0 |             0\\r\\n               MigrationStage |      0 |          1 |         0 |       0 |             0 |             0\\r\\n            ViewBuildExecutor |      0 |          1 |         0 |       0 |             0 |             0\\r\\n            ViewMutationStage |      0 |         32 |         0 |       0 |             0 |             0\\r\\n    Native-Transport-Requests |      1 |        128 |        48 |       0 |             0 |             0\\r\\n         CacheCleanupExecutor |      0 |          1 |         0 |       0 |             0 |             0\\r\\n         RequestResponseStage |      0 |          8 |         0 |       0 |             0 |             0\\r\\n        MemtableReclaimMemory |      0 |          1 |         1 |       0 |             0 |             0\\r\\n        InternalResponseStage |      0 |          8 |         0 |       0 |             0 |             0\\r\\n              ReadRepairStage |      0 |          8 |         0 |       0 |             0 |             0\\r\\n PerDiskMemtableFlushWriter_0 |      0 |          2 |         1 |       0 |             0 |             0\\r\\n       PendingRangeCalculator |      0 |          1 |         2 |       0 |             0 |             0\\r\\n            MemtablePostFlush |      0 |          1 |         2 |       0 |             0 |             0\\r\\n                    MiscStage |      0 |          1 |         0 |       0 |             0 |             0\\r\\n          MemtableFlushWriter |      0 |          2 |         1 |       0 |             0 |             0\\r\\n             AntiEntropyStage |      0 |          1 |         0 |       0 |             0 |             0\\r\\n                  Repair-Task |      0 | 2147483647 |         0 |       0 |             0 |             0\\r\\n                      Sampler |      0 |          1 |         0 |       0 |             0 |             0\\r\\n                    ReadStage |      0 |         32 |        13 |       0 |             0 |             0\\r\\n     SecondaryIndexManagement |      0 |          1 |         0 |       0 |             0 |             0\\r\\n{code}', \"bq. Keeping the partition ordered by their partition key natural order is in my opinion an advantage for the virtual tables.\\r\\n\\r\\nSure, sounds like a good idea. Should do it in a different ticket though (for other tables), but in this ticket for {{thread_pools}}.\\r\\n\\r\\nbq. I noticed that the patch reformat the name of the pools for exposing them in the virtual table. We should probably keep the original name as it is a bit confusing if other tools and logs display a different name.\\r\\n\\r\\nAgreed that we should leave the names untransformed by the virtual tables. I'm not against the idea of giving thread pools consistent names in 4.0, though, in a separate ticket.\\r\\n\\r\\nSeparately, I'm with [~cnlwsu] here re: simply using the existing {{DataSet}} here. We have a tiny number of thread pools and it's in my opinion not worth it, here, to complicate things with a semi-persistent implementation.\\r\\n\\r\\nThat said, what I'm not a huge fan of is having to query JMX for names, then building column values from metrics. It's not dissimilar to serializing things into maps then deserializing back for consumption by the virtual tables. We should be dealing with {{LocalAwareExecutorService}} instances directly instead of going through jmx, then through metrics. This is not some external code after all.\\r\\n\\r\\nThe closest we currently have is {{StageManager.stages}} map, but not every thread pool is in there. I wonder if we should either put the relevant thread pools that aren't currently in the stages map in there, or have a different registry somewhere for all executors.\\r\\n\\r\\nI started a branch [here|https://github.com/iamaleksey/cassandra/tree/14523-review] - so far with just some cleanups and nits, but I'm half-way through with coding up what I meant above. Will update the ticket once I'm done.\", \"[~cnlwsu] made most of the requested changes. I pushed a bit more [here|https://github.com/iamaleksey/cassandra/commits/14523-4.0]. It cleans up existing things a bit, makes them more consistent, and implements an override for single-partition-key reads, among other things.\\r\\n\\r\\nIt's going through CI now, [here|https://circleci.com/workflow-run/f3956836-29d7-4570-aef2-07fa2b6cae11]. If CI is happy, and you are ok with the latest changes, [~cnlwsu], I'll commit this version.\", 'Committed to trunk as [d5ae2ae481545b1fb2332b46013088f2f8cea636|https://github.com/apache/cassandra/commit/d5ae2ae481545b1fb2332b46013088f2f8cea636], thanks.']\n",
            "my_comment: simply using the existing {{DataSet}} here. We have a tiny number of thread pools and its in my opinion not worth it here to complicate things with a semi-persistent implementation.\n",
            "\n",
            "That said what Im not a huge fan of is having to query JMX for names then building column values from metrics. Its not dissimilar to serializing things into maps then deserializing back for consumption by the virtual tables. We should be dealing with {{LocalAwareExecutorService}} instances directly instead of going through jmx then through metrics. This is not some external code after all.\n",
            "\n",
            "The closest we currently have is {{StageManager.stages}} map but not every thread pool is in there. I wonder if we should either put the relevant thread pools that arent currently in the stages map in there or have a different registry somewhere for all executors.\n",
            "\n",
            "I started a branch [here|https://github.com/iamaleksey/cassandra/tree/14523-review] - so far with just some cleanups and nits but Im half-way through with coding up what I meant above. Will update the ticket once Im done.\" \"[~cnlwsu] made most of the requested changes. I pushed a bit more [here|https://github.com/iamaleksey/cassandra/commits/14523-4.0]. It cleans up existing things a bit makes them more consistent and implements an override for single-partition-key reads among other things.\n",
            "\n",
            "Its going through CI now [here|https://circleci.com/workflow-run/f3956836-29d7-4570-aef2-07fa2b6cae11]. If CI is happy and you are ok with the latest changes [~cnlwsu] Ill commit this version.\" Committed to trunk as [d5ae2ae481545b1fb2332b46013088f2f8cea636|https://github.com/apache/cassandra/commit/d5ae2ae481545b1fb2332b46013088f2f8cea636] thanks.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-15027\n",
            "issue_type: Bug\n",
            "summary: Handle IR prepare phase failures less race prone by waiting for all results\n",
            "description: Handling incremental repairs as a coordinator begins by sending a {{PrepareConsistentRequest}} message to all participants, which may also include the coordinator itself. Participants will run anti-compactions upon receiving such a message and report the result of the operation back to the coordinator.\n",
            "\n",
            "\n",
            "\n",
            "Once we receive a failure response from any of the participants, we fail-fast in {{CoordinatorSession.handlePrepareResponse()}}, which will in turn completes the {{prepareFuture}} that {{RepairRunnable}} is blocking on. Then the repair command will terminate with an error status, as expected.\n",
            "\n",
            "\n",
            "\n",
            "The issue is that in case the node will both be coordinator and participant, we may end up with a local session and submitted anti-compactions, which will be executed without any coordination with the coordinator session (on same node). This may result in situations where running repair commands right after another, may cause overlapping execution of anti-compactions that will cause the following (misleading) message to show up in the logs and will cause the repair to fail again:\n",
            "\n",
            " \"Prepare phase for incremental repair session %s has failed because it encountered intersecting sstables belonging to another incremental repair session (%s). This is by starting an incremental repair session before a previous one has completed. Check nodetool repair_admin for hung sessions and fix them.\"\n",
            "architectural impact: NO\n",
            "comments: ['* [\\xa0[trunk|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-15027] ][ [circleci|https://circleci.com/workflow-run/2b027f87-cf45-48ee-8eae-45a563701bc6] ]', 'Thanks [~spodxx@gmail.com]. I’ve extended your code so that in addition to waiting for other anti-compactions to complete, the coordinator also pro-actively cancels ongoing anti-compactions on the other participants. This avoids wasting time waiting for anti-compactions on other machines. The code does 3 things:\\r\\n * Adds a session state check to the\\xa0{{isStopRequested}} method in the anti-compaction iterator.\\r\\n * The coordinator now sends failure messages to all participants when it receives a failure message from one of them in the prepare phase. It does not mark these participants as having failed internally though, since that would cause the nodetool session to immediately complete. Instead, it waits until it’s received messages from all the other nodes.\\r\\n * The participants will now respond with a failed prepare message if the anti-compaction completes, but the session was failed in the mean time. This prevents a dead lock on the coordinator in the case where the participant received a failure message between the time the anti-compaction completes and the callback fires.\\r\\n\\r\\nLet me know what you think. If everything looks ok to you, I’m +1 on committing.\\r\\n\\r\\n[trunk|https://github.com/bdeggleston/cassandra/tree/15027-trunk]\\r\\n [circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15027-trunk]', \"Your updates look like valuable improvement over the initial\\xa0patch. I'm +1 in general\\xa0as for the changes, but also\\xa0fixed some additional minor issues and added a new tests:\\r\\n\\r\\n* [CASSANDRA-15027|https://github.com/spodkowinski/cassandra/commits/CASSANDRA-15027]\\r\\n* [https://circleci.com/workflow-run/2b444c33-a54c-46b5-9923-bcded8bcf465]\\r\\n\\r\\nPlease see comments with each commit in branch above for details.\\r\\n\\r\\nAlso happy to discuss any of the changes (most likely the last commit) in another jira, if you feel it's out of scope for this ticket.\\r\\n\\xa0\", \"Nice. Your follow on changes look good to me, I have 2 nits, but those can just be fixed on commit.\\r\\n\\r\\n* We should log the session id in compaction manager when an anti-compaction is cancelled (and probably when there's an error as well)\\r\\n* Some error handling should be added to the commit fixing the race between proposeFuture and hasFailure so nodetool doesn't hang if there's an error in the callback\\r\\n\\r\\nedit: proposed fixes [here|https://github.com/bdeggleston/cassandra/commit/02d7d9e09983db0d4661486b17adc375e17be24f]\", 'LGTM +1', 'Committed to trunk as\\xa09bde713ee8883f70d130efb6290ec0e6daea524f, thanks']\n",
            "my_comment: proposed fixes [here|https://github.com/bdeggleston/cassandra/commit/02d7d9e09983db0d4661486b17adc375e17be24f]\" LGTM +1 Committed to trunk as\\xa09bde713ee8883f70d130efb6290ec0e6daea524f thanks\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-15379\n",
            "issue_type: Improvement\n",
            "summary: Flush with fast compressors by default\n",
            "description: [~josnyder] and I have been testing out CASSANDRA-14482 (Zstd compression) on some of our most dense clusters and have been observing close to 50% reduction in footprint with Zstd on some of our workloads! Unfortunately though we have been running into an issue where the flush might take so long (Zstd is slower to compress than LZ4) that we can actually block the next flush and cause instability.\n",
            "\n",
            "\n",
            "\n",
            "Internally we are working around this with a very simple patch which flushes SSTables as the default compression strategy (LZ4) regardless of the table params. This is a simple solution but I think the ideal solution though might be for the flush compression strategy to be configurable separately from the table compression strategy (while defaulting to the same thing). Instead of adding yet another compression option to the yaml (like hints and commitlog) I was thinking of just adding it to the table parameters and then adding a {{default_table_parameters}} yaml option like:\n",
            "\n",
            "{noformat}\n",
            "\n",
            "\n",
            "\n",
            "# Default table properties to apply on freshly created tables. The currently supported defaults are:\n",
            "\n",
            "# * compression       : How are SSTables compressed in general (flush, compaction, etc ...)\n",
            "\n",
            "# * flush_compression : How are SSTables compressed as they flush\n",
            "\n",
            "# supported\n",
            "\n",
            "default_table_parameters:\n",
            "\n",
            "  compression:\n",
            "\n",
            "    class_name: 'LZ4Compressor'\n",
            "\n",
            "    parameters:\n",
            "\n",
            "      chunk_length_in_kb: 16\n",
            "\n",
            "  flush_compression:\n",
            "\n",
            "    class_name: 'LZ4Compressor'\n",
            "\n",
            "    parameters:\n",
            "\n",
            "      chunk_length_in_kb: 4\n",
            "\n",
            "{noformat}\n",
            "\n",
            "\n",
            "\n",
            "This would have the nice effect as well of giving our configuration a path forward to providing user specified defaults for table creation (so e.g. if a particular user wanted to use a different default chunk_length_in_kb they can do that).\n",
            "\n",
            "\n",
            "\n",
            "So the proposed (~mandatory) scope is:\n",
            "\n",
            "* Flush with a faster compression strategy\n",
            "\n",
            "\n",
            "\n",
            "I'd like to implement the following at the same time:\n",
            "\n",
            "* Per table flush compression configuration\n",
            "\n",
            "* Ability to default the table flush and compaction compression in the yaml.\n",
            "architectural impact: NO\n",
            "comments: [\"Makes sense.  We keep adding things to 4.0, but this seems to me to bundle along with other config cleanups, and is super minor.\\r\\n\\r\\nI'm not certain about the idea of putting default parameters in the yaml, though, as this is a feature we'll have to maintain despite making very little sense.  We've talked about introducing global and per-Keyspace defaults for tables, and I wonder if we should depend on that here.\", 'Yeah I understand it\\'s a bit late in the 4.0 cut, but I think we\\'ll have users running into this trying out the new Zstd compressor rather quickly (we ran into it on the first cluster we dropped it on).\\r\\n\\r\\nIf you\\'re not a fan of the defaults in yaml (I agree it\\'s not great) to reduce scope I could keep the change internal to C* entirely by adding a default method on\\xa0{{ICompressor}} such as:\\r\\n{noformat}\\r\\ndefault Set<String> unsuitableUseHints() {\\r\\n  return Collections.emptySet();\\r\\n}{noformat}\\r\\nThen the ZstdCompressor would yield a set with the string \"flush\" or something and the flush code path would just use the default compressor in that case.\\r\\n\\r\\n[~benedict] what do you think about this alternative?\\r\\n\\r\\n\\xa0', 'I am +1 on this idea. [~jolynch] would be happy to help review this.', 'In principle this seems reasonable to me, though since this is a temporary measure, and anyway because {{String}} are a bad way to communicate intent (particularly with a specific concept like \"unsuitability\"), perhaps just something like:\\r\\n\\r\\n{code}\\r\\ndefault boolean useOnMemtableFlush() { return true; }\\r\\n{code}\\r\\n\\r\\nOr alternatively\\r\\n{code}\\r\\ndefault ICompressor useOnMemtableFlush() { return this; }\\r\\n{code}', \"Alright, I made it so that Zstd, Deflate and LZ4HC (which compresses extremely slowly) now flush with LZ4 (fast) controlled via an EnumSet. Since I'm changing the ICompressor interface I figured it is more maintainable this way than having a somewhat arbitrary boolean switch.\\r\\n\\r\\nI also took the opportunity to add some more tests and improve the documentation as well. I tried to add some helpful documentation to help people pick compressors (I hear a lot of confusion about why we have Snappy and Deflate still around, so I tried to clarify in the documentation). I'll squash after review comments are integrated.\\r\\n\\r\\n||trunk||\\r\\n|[branch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-15379]|\\r\\n|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-15379.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-15379]|\\r\\n\\r\\nThe one failing unit test appears to be org.apache.cassandra.config.DatabaseDescriptorRefTest, which I thought was supposed to be fixed as part of CASSANDRA-15371, I'll double check tomorrow.\", \"What is your rationale for an {{EnumSet}} being more maintainable than a\\xa0member function? \\xa0As far as I understand we explicitly intend to retire this functionality, so planning for future uses seems counterproductive to me.\\r\\n\\r\\nIf we're adding per-table config for this, why are we blanket changing the behaviour for all relevant compressors? \\xa0This may well be surprising to users, and also seems to make the per-table config superfluous (or at least, only useful to restore the probably-assumed behaviour of using the same compressor for both flush and compaction)\\r\\n\\r\\n\\xa0\", 'My rationale for the {{EnumSet}} over a boolean member function is:\\r\\n # Versus the boolean function idea it doesn\\'t break the ICompressor abstraction and let compressors know that flushes exist. As in, it is very easy for an ICompressor author to claim to be good at {{FAST_COMPRESSION}} but probably can\\'t make the call if that should be used in flushes or other situations. I could have a {{isFastCompressor}} boolean function but given that {{ICompressor}} is a public API interface I think sets of capabilities will be more maintainable than a collection of boolean functions going forwards, especially if we start adding more capabilities (see #2).\\r\\n # If we go down the path of\\xa0_not_ making more knobs and just try to have the database figure out the best way to compress data for users this is easier to maintain long term since compressors can offer multiple types of hints to the database. For example the database might refuse to use slow compressors in flushes, commitlogs, etc or having compaction strategies opt into higher ratio compression strategies in higher \"levels\". If we do go down this path there are fewer interface changes (instead of adding and removing functions we just add ICompressor.Uses hints).\\r\\n # Versus the set of strings idea, it has compile time checks that are useful (which is the primary argument against sets of strings afaik).\\r\\n\\r\\nAfter thinking about this problem space more I\\'m no longer convinced that giving general users more knobs here is the right choice (the table properties). By using a\\xa0{{suitableUses}} hint the database can in the future 4.x releases internally optimize:\\r\\n * Flushes: \"get this data off my heap as fast as possible\". We don\\'t care about ratio (since the products will be re-compacted shortly) or decompression speed, only care about compression speed.\\r\\n * Commitlog: \"some compression is nice but get this data off my heap fast\". We mostly care about compression speed, but very minorly about ratio.\\r\\n * Compaction: \"The older the data the more compressed it should be\". We care a lot about decompression speed and ratio, but don\\'t want to pick expensive compressors at the high churn points (L0 in LCS, small tables in STCS, before the time window bucket in TWCS)\\r\\n\\r\\nThe interface still gives advanced users a backdoor (they extend the compressor they want to change the behavior of and change what capabilities it offers).\\r\\n\\r\\nedit: I pinged this ticket into [slack|https://the-asf.slack.com/archives/CK23JSY2K/p1572881897039500] to seek more feedback.', '[~djoshi]\\xa0per your feedback in slack I\\'ve added the ability for the user to control the flush via a yaml option while doing the right thing by default.\\r\\n\\r\\n||trunk||\\r\\n|[branch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-15379]|\\r\\n|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-15379.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-15379]|\\r\\n\\r\\nIn order to implement the \"don\\'t compress during the flush\" [option you suggested|https://the-asf.slack.com/archives/CK23JSY2K/p1572905922120300?thread_ts=1572905763.117000&cid=CK23JSY2K] I figured that the easiest was was to just implement the simple [NoopCompressor|https://github.com/apache/cassandra/commit/9030d8abcf593c06e85f549947ad41621d4776d1] everyone has been mentioning for years. I was having a hard time turning off compression at the level of abstraction BigTableWriter operates at since it doesn\\'t control that e.g. the compression offset file get\\'s written. This way even if you select \"none\" your flush is still protected by block level checksums. Separately it gives us a good path forward for mitigating CASSANDRA-12682 and CASSANDRA-9264 if we want it to I think.', \"Hi [~jolynch], thanks for the patch. I went over it and it looks generally good. On a high level the only concern I have is introducing a {{NoOpCompressor}} may lead to some performance issues compared to our current state. This is mainly due to Java JIT's inability to optimize megamorphic call sites. However, I think this is just a theory and we should try and validate it using an actual performance test. IMHO, the advantages that you have laid out would outweight a bit of performance penalty.\\r\\n\\r\\nOther than that, I had some code related feedback. It fixes the {{DatabaseDescriptorRefTest}} and also makes minor structural modifications for safety and clarity. I have illustrated in my branch [here|https://github.com/apache/cassandra/compare/trunk...dineshjoshi:CASSANDRA-15379-review?expand=1]. Please feel free to cherry pick the commits in your branch.\", 'Cool, took your changes and [rebased on trunk with a few fixups|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-15379]. Tests are running now.\\r\\n\\r\\nI am having some trouble with our performance integration suite for trunk right now, but should hopefully be able to run those performance tests on Monday.\\r\\n\\r\\nJust to  confirm you would like performance numbers for a write heavy test for baseline (trunk without my patch):\\r\\n\\r\\n* No compressor\\r\\n* LZ4 Compressor\\r\\n* Zstd Compressor\\r\\n\\r\\nAnd the following candidates:\\r\\n\\r\\n* No compressor\\r\\n* Noop compressor\\r\\n* LZ4 compressor\\r\\n* Zstd compressor', 'My main concern is the addition of the Noop compressor. So Noop vs No Compressor would be the minimal test case.', \"{quote}This is mainly due to Java JIT's inability to optimize megamorphic call sites. However, I think this is just a theory and we should try and validate it using an actual performance test.\\r\\n{quote}\\r\\nGoing off what [Shipilev|https://shipilev.net/jvm/anatomy-quarks/16-megamorphic-virtual-calls/] has to say [on the topic|https://shipilev.net/blog/2015/black-magic-method-dispatch/#_conclusion], seems like something we probably shouldn't lose too much sleep over, and definitely would want to benchmark if we were concerned.\", 'There are definitely situations where we might care, but compression is perhaps the archetypal \"doesn\\'t matter much\" scenario: we are dispatching large costly operations, so if we\\'re a few instructions slower in the process, it should be a rounding error.', \"Alright, finally fixed our internal trunk build so we can do performance validations again. I ran the following performance benchmark and the results are essentially identical for the default configuration (so testing _just_ the addition of the NoopCompressor on the\\xa0megamorphic call sites).\\r\\n\\r\\n*Experimental Setup:*\\r\\n\\r\\nA baseline and candidate cluster of EC2 machines running the following:\\r\\n * C* cluster: 3x3 (us-east-1 and eu-west-1) i3.2xlarge\\r\\n * Load cluster: 3 m5.2xlarge nodes running ndbench in us-east-1, generating a consistent load against the cluster\\r\\n * Baseline C* version: Latest trunk (b05fe7ab)\\r\\n * Candidate C* version: The proposed patch applied to the same version of trunk\\r\\n * Relevant system configuration: Ubuntu xenial running Linux 4.15, with kyber io scheduler (vs noop), 32 KiB readahead (vs 128), and tc-fq network qdisc (vs pfifo_fast)\\r\\n * Relevant JVM configuration: 12 GiB heap size\\r\\n\\r\\nIn all cases load is applied and then we wait for metrics to settle, especially things like pending compactions, read/write latencies, p99 latencies, etc ...\\r\\n\\r\\n*Defaults Benchmark:*\\r\\n * Load pattern: 1.2K wps and 1.2k rps at LOCAL_ONE consistency with a\\xa0 random load pattern.\\r\\n * Data sizing: 10 million partitions with 2 rows each of 10 columns, total size per partition of about 10 KiB of random data. ~100 GiB per node data size (replicated 6 ways)\\r\\n * Compaction settings: LCS with size=256MiB, fanout=20\\r\\n * Compression: LZ4 with 16 KiB block size\\xa0\\r\\n\\r\\n*Defaults Benchmark Results:*\\r\\n\\r\\nWe do not have data to support the hypothesis that the megamorphic call sites have become more expensive to the addition of the NoopCompressor.\\r\\n\\r\\n1. No significant change at the coordinator level (least relevant metric):\\xa0[^15379_coordinator_defaults.png]\\r\\n 2. No significant change at the replica level (most relevant metric): [^15379_replica_defaults.png]\\r\\n 3. No significant change at the system resource level (second most relevant metrics): [^15379_system_defaults.png]\\r\\n\\r\\nOur external flamegraphs exports appear to be broken, but I looked at them and they also show no noticeable difference (I'll work with our performance team to fix exports so I can share the data here).\\r\\n\\r\\n*Next steps for me:*\\r\\n * Squash, rebase, and re-run unit and dtests with latest trunk in preparation for commit\\r\\n * Run a benchmark of `ZstdCompressor` with and without the patch, we expect to see reduced CPU usage due to flushes. I will likely have to reduce the read/write throughput due to compactions taking a crazy amount of our on CPU time with this configuration.\", \"*Zstd Defaults Benchmark:*\\r\\n * Load pattern: 1.2K wps and 1.2k rps at LOCAL_ONE consistency with a\\xa0 random load pattern.\\r\\n * Data sizing: ~100 million partitions with 2 rows each of 10 columns, total size per partition of about 4 KiB of random data. ~120 GiB per node data size (replicated 6 ways)\\r\\n * Compaction settings: LCS with size=320MiB, fanout=20\\r\\n * Compression: Zstd with 16 KiB block size\\r\\n\\r\\nI had to tweak some settings to make compaction less of the overall trace (it was 50+% or more of the traces) which are hiding the flush behavior. Specifically I increased the size of the memtable before flush by increasing the {{memtable_cleanup_threshold}} setting from 0.11 to 0.5, which allowed flushes to get up to 1.4 GiB, and by setting compaction to defer as long as we can before doing the L0 -> L1 transition:\\r\\n{noformat}\\r\\ncompaction = {'class': 'LeveledCompactionStrategy', 'fanout_size': '20', 'max_threshold': '128', 'min_threshold': '32', 'sstable_size_in_mb': '320'}\\r\\ncompression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.ZstdCompressor'}\\r\\n{noformat}\\r\\nI would prefer to up fanout_size even more to defer compactions further, but with the increase in memtable size and increase in sstable size and fanout I was able to reduce the compaction load to where the cluster was stable (pending compactions not growing without bound) on both baseline and candidate\\xa0\\r\\n\\r\\n*Zstd Defaults Benchmark Results*:\\r\\n\\r\\nCandidate flushes were spaced about 4 minutes apart and took about 8 seconds to flush 1.4 GiB. Flamegraphs show 50% of on-cpu time in flush writer and ~45 in compression. [^15379_candidate_flush_trace.png]\\r\\n\\r\\nBaseline flushes were spaced about 4 minutes apart and took about 22 seconds to flush 1.4 GiB.\\xa0Flamegraphs show 20% of on-cpu time in flush writer and ~75 in compression.\\xa0 [^15379_baseline_flush_trace.png]\\r\\n\\r\\nNo significant change in coordinator level, replica level latency or system metrics. Some latencies were better on candidate some worse. [^15379_system_zstd_defaults.png] [^15379_coordinator_zstd_defaults.png] [^15379_replica_zstd_defaults.png]\\r\\n\\r\\nI think the main finding here is that already, with the cheapest zstd level, we are running closer to the flush interval than I'd like (if it takes longer to flush then the next time we flush, it's bad news bears for the cluster), and this is with a relatively small number of writes per second (~400 coordinator writes per second per node)\\r\\n\\r\\n*Next steps:*\\r\\n\\r\\nI've published a final squashed commit to:\\r\\n||trunk||\\r\\n|[657c39d4|https://github.com/jolynch/cassandra/commit/657c39d4aba0888c6db6a46d1b1febf899de9578]|\\r\\n|[branch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-15379-final]|\\r\\n|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-15379-final.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-15379-final]|\\r\\n\\r\\nThere appear to be a lot of failures in java8 runs that I'm pretty sure are unrelated to my change (unit tests and in-jvm dtests passed, along with long unit tests). I'll look into all the failures and make sure they're unrelated (on a related note I'm :( that trunk is so red again).\\r\\n\\r\\nI am now running a test with Zstd compression set to a block size of 256 KiB and level 10, which is how we typically run it in production for write mosty read rarely datasets such as trace data (for the significant reduction in disk space).\\xa0\", \"*Zstd Write Mostly Read Rarely Benchmark*:\\r\\n\\r\\nIn this test I configured Zstd the way we do in production for our write mostly read rarely (e.g. trace) datasets where Zstd really shines at getting the footprint down significantly (up to 50% in some cases). This benchmark simulates our production workloads for Zstd most accurately so far.\\r\\n * Load pattern: 3.6K wps and 1.2k rps at LOCAL_ONE consistency with a\\xa0 random load pattern.\\r\\n * Data sizing: ~50 million partitions with 2 rows each of 10 columns, total size per partition of about 4 KiB of random data. ~300 GiB per node data size (replicated 6 ways)\\r\\n * Compaction settings: STCS with min=8, max=32\\r\\n * Compression: Zstd level 10 with 256 KiB block size\\r\\n\\r\\n{noformat}\\r\\ncompaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '8'}\\r\\ncompression = {'chunk_length_in_kb': '256', 'class': 'org.apache.cassandra.io.compress.ZstdCompressor', 'compression_level': '10'}\\r\\n{noformat}\\r\\n*Zstd Write Mostly Read Rarely Benchmark Results*:\\r\\n\\r\\nThe candidate branch did significantly better in all aspects. Most importantly, the baseline cluster started falling infinitely behind and queueing/dropping mutations while candidate deferred the expensive work to compaction. Flamegraphs confirmed that the vast majority of our flusher thread on-cpu time was spent in zstd compression. Some data to support this conclusion:\\r\\n [^15379_request_queueing_zstd_level10.png]\\r\\n [^15379_message_drops_zstd_level10.png]\\r\\n [^15379_coordinator_zstd_level10.png]\\r\\n [^15379_flush_flamegraph_zstd_level10.png]\\r\\n [^15379_concurrent_flushes_zstd_level10.png]\\r\\n [^15379_backfill_duration_zstd_level10.png]\\r\\n [^15379_backfill_drops_zstd_level10.png]\\r\\n [^15379_backfill_queueing_zstd_level10.png]\\r\\n [^15379_backfill_zstd_level10.png]\\r\\n\\r\\nThis data clearly shows that baseline using zstd on the flush was so slow at flushing that it was unstable, like we observed in production at Netflix. The candidate version that flushed the data in LZ4 and then amortized the expensive compression to the compaction instead fared significantly better and remained relatively stable.\", 'Thank you for quantifying performance and clearly demonstrating the benefits. I am +1 on these changes.', \"Final commit with some quick fixes to the docs to make them a little clearer, test runs linked below.\\r\\n\\r\\n||trunk||\\r\\n|[063811c44|https://github.com/jolynch/cassandra/commit/063811c44f41996ee4903c92a95aa108e7ff7ad4]|\\r\\n|[branch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-15379-final]|\\r\\n|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-15379-final.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-15379-final]|\\r\\n\\r\\nAll unit tests and in-jvm dtests passed, a few dtest flakes on java8 and java11 that I'm pretty sure are unrelated (a transient replication dtest and two nodetool dtests).\\r\\n* test_refresh_size_estimates_clears_invalid_entries - nodetool_test.TestNodetool\\r\\n* test_optimized_primary_range_repair - transient_replication_test.TestTransientReplication\\r\\n* test_repaired_tracking_with_mismatching_replicas - repair_tests.incremental_repair_test.TestIncRepair\\r\\n\\r\\nAll appear to be unrelated failures.\\r\\n\\r\\n\", 'Committed as [9c1bbf3ac913f9bdf7a0e0922106804af42d2c1e|https://github.com/apache/cassandra/commit/9c1bbf3ac913f9bdf7a0e0922106804af42d2c1e]. Thank you [~djoshi] for the review.']\n",
            "my_comment: 10}\n",
            "{noformat}\n",
            "*Zstd Write Mostly Read Rarely Benchmark Results*:\n",
            "\n",
            "The candidate branch did significantly better in all aspects. Most importantly the baseline cluster started falling infinitely behind and queueing/dropping mutations while candidate deferred the expensive work to compaction. Flamegraphs confirmed that the vast majority of our flusher thread on-cpu time was spent in zstd compression. Some data to support this conclusion:\n",
            " [^15379_request_queueing_zstd_level10.png]\n",
            " [^15379_message_drops_zstd_level10.png]\n",
            " [^15379_coordinator_zstd_level10.png]\n",
            " [^15379_flush_flamegraph_zstd_level10.png]\n",
            " [^15379_concurrent_flushes_zstd_level10.png]\n",
            " [^15379_backfill_duration_zstd_level10.png]\n",
            " [^15379_backfill_drops_zstd_level10.png]\n",
            " [^15379_backfill_queueing_zstd_level10.png]\n",
            " [^15379_backfill_zstd_level10.png]\n",
            "\n",
            "This data clearly shows that baseline using zstd on the flush was so slow at flushing that it was unstable like we observed in production at Netflix. The candidate version that flushed the data in LZ4 and then amortized the expensive compression to the compaction instead fared significantly better and remained relatively stable.\" Thank you for quantifying performance and clearly demonstrating the benefits. I am +1 on these changes. \"Final commit with some quick fixes to the docs to make them a little clearer test runs linked below.\n",
            "\n",
            "||trunk||\n",
            "|[063811c44|https://github.com/jolynch/cassandra/commit/063811c44f41996ee4903c92a95aa108e7ff7ad4]|\n",
            "|[branch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-15379-final]|\n",
            "|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-15379-final.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-15379-final]|\n",
            "\n",
            "All unit tests and in-jvm dtests passed a few dtest flakes on java8 and java11 that Im pretty sure are unrelated (a transient replication dtest and two nodetool dtests).\n",
            "* test_refresh_size_estimates_clears_invalid_entries - nodetool_test.TestNodetool\n",
            "* test_optimized_primary_range_repair - transient_replication_test.TestTransientReplication\n",
            "* test_repaired_tracking_with_mismatching_replicas - repair_tests.incremental_repair_test.TestIncRepair\n",
            "\n",
            "All appear to be unrelated failures.\n",
            "\n",
            "\" Committed as [9c1bbf3ac913f9bdf7a0e0922106804af42d2c1e|https://github.com/apache/cassandra/commit/9c1bbf3ac913f9bdf7a0e0922106804af42d2c1e]. Thank you [~djoshi] for the review.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-16283\n",
            "issue_type: Bug\n",
            "summary: Incorrect output in \"nodetool status -r\"\n",
            "description: nodetool status -r not working well on C* 4,\n",
            "\n",
            " Version:\n",
            "\n",
            "{code:java}\n",
            "\n",
            "[root@foo001 ~]# nodetool version\n",
            "\n",
            "ReleaseVersion: 4.0-beta3\n",
            "\n",
            "{code}\n",
            "\n",
            "Without resolving:\n",
            "\n",
            "{code:java}\n",
            "\n",
            "[root@foo001 ~]# nodetool status\n",
            "\n",
            "Datacenter: V4CH\n",
            "\n",
            "================\n",
            "\n",
            "Status=Up/Down\n",
            "\n",
            "|/ State=Normal/Leaving/Joining/Moving\n",
            "\n",
            "--  Address Load    Tokens  Owns(effective) Host ID                            Rack\n",
            "\n",
            "UN  1.2.3.4 363.68 KiB  128     ?         92ae4c39-edb3-4e67-8623-b49fd8301b66 RAC1\n",
            "\n",
            "UN  1.2.3.5 109.71 KiB  128     ?         d80647a8-32b2-4a8f-8022-f5ae3ce8fbb2 RAC1\n",
            "\n",
            "{code}\n",
            "\n",
            "With resolving:\n",
            "\n",
            "{code:java}\n",
            "\n",
            "[root@foo001 ~]# nodetool status -r\n",
            "\n",
            "Datacenter: V4CH\n",
            "\n",
            "================\n",
            "\n",
            "Status=Up/Down\n",
            "\n",
            "|/ State=Normal/Leaving/Joining/Moving\n",
            "\n",
            "--  Address          Load  Tokens  Owns (effective)  Host ID  Rack\n",
            "\n",
            "?N  foo001.tab.com   ?     128     ?                          RAC1\n",
            "\n",
            "?N  foo002.tab.com   ?     128     ?                          RAC1\n",
            "\n",
            "{code}\n",
            "\n",
            "\n",
            "\n",
            "I only changed here IPs and hostnames.\n",
            "\n",
            "\n",
            "architectural impact: YES\n",
            "comments: ['See {{ToolRunner}} on how to test tooling #collaborating', 'Fixed via PR https://github.com/apache/cassandra/pull/845', 'Thank you [~wolfenhaut] :)', 'Thanks [~wolfenhaut]. Would you consider adding a test and moving it to \"Ready for Revidew\"?', 'Sure, and thanks!', 'Thanks for taking a look at this, and for the patch. I reviewed and left a few comments on the PR.\\r\\n\\r\\nWhile looking at this I think I noticed one other potential flaw around looking things up by endpoint (unrelated the resolved host lookup logic). It seems like the tool is never showing anything definitive for the \"Owns (effective)\" column. Resolved or not, the table always just produces \\'?\\'.', \"Did review it. Mostly aligned to what Adam mentioned + a minor. The only real concern is whether we tackle the 'Owns' issue here or on a new ticket.\", \"I personally don't see a reason to split out a new ticket, but I don't feel too strongly about it. To me it fits the description, and it's closely related in code. I can take a look at revising the patch if [~wolfenhaut] doesn't care to.\", \"[~wolfenhaut] Just checking to see if you're going to have time to get back to this review? If not, I have no problem carrying the work forward. Just wanted to check in first.\", '[~aholmber]\\r\\n\\r\\nBeen a little busy, feel free to run with this if you want...\\r\\n\\r\\nThanks for letting me help out!\\r\\n\\r\\n--scott\\xa0', 'Will do. Thanks for getting back on the matter, and thanks for getting it this far.', 'Updated branch with PR here:\\r\\nhttps://github.com/aholmberg/cassandra/pull/38\\r\\n\\r\\ntl;dr there are several maps in Status that were broken depending on whether host is resolved (-r) or port printing was enabled (-pp), and some were using different forms of string conversion. Now everything is normalized to always using the form \"<ip>:<port>\" for command internal maps. Port printing and DNS resolution only apply to printing the host string for table output.\\r\\n\\r\\nCI running:\\r\\nhttps://app.circleci.com/pipelines/github/aholmberg/cassandra?branch=CASSANDRA-16283', \"I see that the port is not fixed for unit tests, and that there is a jvm-dtest that codified the question marks. I'll address those test issues tomorrow.\", 'Thank you, [~aholmber]. Please let me know when you are done, I will be happy to review the patch.', \"The branch is updated. Full CI run finished revealing an in-jvm test I overlooked. Another limited run is still running on that tweak, but I think it's ready for review. Thanks in advance.\", 'Jenkins run pushed [here | [https://jenkins-cm4.apache.org/job/Cassandra-devbranch/333/#showFailuresLink].]\\r\\n\\r\\nReview in progress, thanks :)\\xa0', 'The patch looks good to me, I left just a few small comments [here|https://github.com/ekaterinadimitrova2/cassandra/commit/9e79a336bf5348ac6fae59dac7ffa60eb4c29bae]\\xa0- I created a new branch with squashed commits for myself while reviewing.\\r\\n\\r\\nI believe the two main things are CHANGES.txt entry is missing and the last in-jvm test [failing|https://jenkins-cm4.apache.org/job/Cassandra-devbranch/333/]\\xa0to be fixed.\\xa0', 'Thanks for the review. I pushed updates. I think CI looks good as well.', \"Thanks both [~wolfenhaut] and [~aholmber] for the patch!\\r\\nLGTM +1\\r\\nCircleCI has unrelated failures\\r\\n[~brandon.williams] can you review it as a second committer, please?\\r\\nI believe Berenguer's concerns were also addressed. We need only to move the CHANGES.txt update to the top on commit. :-)  \", \"+1\\r\\n\\r\\nbq. We need only to move the CHANGES.txt update to the top on commit.\\r\\n\\r\\nHistorically we've left it out of the patch, it's up to the committer to handle, and it often ends up conflicting too when in a patch.\", 'Patch rebased and squashed [here|https://github.com/ekaterinadimitrova2/cassandra/commit/f99faca3e62dad3c748673e306433ce242a31b92]\\r\\nCommit pending on final Jenkins run in progress [here|https://jenkins-cm4.apache.org/job/Cassandra-devbranch/352/]', 'Patch committed [here| https://github.com/apache/cassandra/commit/b61860c76e9cf1eebfb7d29dc4f4420955f62bb4]\\r\\n\\r\\nThank you!']\n",
            "my_comment: See {{ToolRunner}} on how to test tooling #collaborating Fixed via PR https://github.com/apache/cassandra/pull/845 Thank you [~wolfenhaut] :) Thanks [~wolfenhaut]. Would you consider adding a test and moving it to \"Ready for Revidew\"? Sure and thanks! Thanks for taking a look at this and for the patch. I reviewed and left a few comments on the PR.\n",
            "\n",
            "While looking at this I think I noticed one other potential flaw around looking things up by endpoint (unrelated the resolved host lookup logic). It seems like the tool is never showing anything definitive for the \"Owns (effective)\" column. Resolved or not the table always just produces \\?\\. \"Did review it. Mostly aligned to what Adam mentioned + a minor. The only real concern is whether we tackle the Owns issue here or on a new ticket.\" \"I personally dont see a reason to split out a new ticket but I dont feel too strongly about it. To me it fits the description and its closely related in code. I can take a look at revising the patch if [~wolfenhaut] doesnt care to.\" \"[~wolfenhaut] Just checking to see if youre going to have time to get back to this review? If not I have no problem carrying the work forward. Just wanted to check in first.\" [~aholmber]\n",
            "\n",
            "Been a little busy feel free to run with this if you want...\n",
            "\n",
            "Thanks for letting me help out!\n",
            "\n",
            "--scott\\xa0 Will do. Thanks for getting back on the matter and thanks for getting it this far. Updated branch with PR here:\n",
            "https://github.com/aholmberg/cassandra/pull/38\n",
            "\n",
            "tl;dr there are several maps in Status that were broken depending on whether host is resolved (-r) or port printing was enabled (-pp) and some were using different forms of string conversion. Now everything is normalized to always using the form \"<ip>:<port>\" for command internal maps. Port printing and DNS resolution only apply to printing the host string for table output.\n",
            "\n",
            "CI running:\n",
            "https://app.circleci.com/pipelines/github/aholmberg/cassandra?branch=CASSANDRA-16283 \"I see that the port is not fixed for unit tests and that there is a jvm-dtest that codified the question marks. Ill address those test issues tomorrow.\" Thank you [~aholmber]. Please let me know when you are done I will be happy to review the patch. \"The branch is updated. Full CI run finished revealing an in-jvm test I overlooked. Another limited run is still running on that tweak but I think its ready for review. Thanks in advance.\" Jenkins run pushed [here | [https://jenkins-cm4.apache.org/job/Cassandra-devbranch/333/#showFailuresLink].]\n",
            "\n",
            "Review in progress thanks :)\\xa0 The patch looks good to me I left just a few small comments [here|https://github.com/ekaterinadimitrova2/cassandra/commit/9e79a336bf5348ac6fae59dac7ffa60eb4c29bae]\\xa0- I created a new branch with squashed commits for myself while reviewing.\n",
            "\n",
            "I believe the two main things are CHANGES.txt entry is missing and the last in-jvm test [failing|https://jenkins-cm4.apache.org/job/Cassandra-devbranch/333/]\\xa0to be fixed.\\xa0 Thanks for the review. I pushed updates. I think CI looks good as well. \"Thanks both [~wolfenhaut] and [~aholmber] for the patch!\n",
            "LGTM +1\n",
            "CircleCI has unrelated failures\n",
            "[~brandon.williams] can you review it as a second committer please?\n",
            "I believe Berenguers concerns were also addressed. We need only to move the CHANGES.txt update to the top on commit. :-)  \" \"+1\n",
            "\n",
            "bq. We need only to move the CHANGES.txt update to the top on commit.\n",
            "\n",
            "Historically weve left it out of the patch its up to the committer to handle and it often ends up conflicting too when in a patch.\" Patch rebased and squashed [here|https://github.com/ekaterinadimitrova2/cassandra/commit/f99faca3e62dad3c748673e306433ce242a31b92]\n",
            "Commit pending on final Jenkins run in progress [here|https://jenkins-cm4.apache.org/job/Cassandra-devbranch/352/] Patch committed [here| https://github.com/apache/cassandra/commit/b61860c76e9cf1eebfb7d29dc4f4420955f62bb4]\n",
            "\n",
            "Thank you!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-16308\n",
            "issue_type: Improvement\n",
            "summary: Add droppable tombstone metrics to nodetool tablestats\n",
            "description: This is a useful metric to troubleshoot tombstone cleanup problems and is not currently exposed on table stats.\n",
            "\n",
            "\n",
            "\n",
            "While we're at it we should add the JMX metric to the table metrics documentation because it's currently missing.\n",
            "architectural impact: YES\n",
            "comments: ['I will pick this up.', 'Hi thanks for working on this ticket [~tejavadali]. Instructions on how to prepare a patch (ie. how to format commit messages) can be found [on this doc|https://cassandra.apache.org/_/development/patches.html].\\r\\n\\r\\nWhen you have a patch ready please submit a PR to the [github mirror|https://github.com/apache/cassandra] with this ticket number + short description (CASSANDRA-16308) and it will automagically link to this ticket and set the JIRA status to Patch Available. I can take a look and submit CI if it looks good to test.', 'LGTM, submitted CI:\\r\\n\\r\\n\\r\\n|[4.0|https://github.com/apache/cassandra/compare/cassandra-4.0...pauloricardomg:tejavadali/CASSANDRA-16308-4.0]|[tests|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1340/]|\\r\\n|[trunk|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:tejavadali/CASSANDRA-16308-trunk]|[tests|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1341/]|', 'LGTM too, test failures are unrelated, +1.']\n",
            "my_comment: I will pick this up. Hi thanks for working on this ticket [~tejavadali]. Instructions on how to prepare a patch (ie. how to format commit messages) can be found [on this doc|https://cassandra.apache.org/_/development/patches.html].\n",
            "\n",
            "When you have a patch ready please submit a PR to the [github mirror|https://github.com/apache/cassandra] with this ticket number + short description (CASSANDRA-16308) and it will automagically link to this ticket and set the JIRA status to Patch Available. I can take a look and submit CI if it looks good to test. LGTM submitted CI:\n",
            "\n",
            "\n",
            "|[4.0|https://github.com/apache/cassandra/compare/cassandra-4.0...pauloricardomg:tejavadali/CASSANDRA-16308-4.0]|[tests|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1340/]|\n",
            "|[trunk|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:tejavadali/CASSANDRA-16308-trunk]|[tests|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1341/]| LGTM too test failures are unrelated +1.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-17757\n",
            "issue_type: New Feature\n",
            "summary: A user should not be able to manually remove ephemeral snapshots\n",
            "description: in CASSANDRA-16911 we introduced the \"-e\" flag to nodetool listsnaphots which are returning ephemerals as well. An operator might try to remove these snapshots by hand. This should not be possible as these snapshots are there for repair to work on and manual removal breaks it. To be complete, these snapshots are removed as part of repair mechanism automatically, or they are removed on the next reboot upon node' start. They should never be removed by a human.\n",
            "architectural impact: YES\n",
            "comments: ['https://github.com/apache/cassandra/pull/1781', 'Hi [~paulo] , I would be delighted to have a review from you. It is rather straightforward patch. I had to parse manifests to see if a snapshot is ephemeral or not in order to skip it from deletion. SnapshotLoader seems to be best suitable for the job, I just accommodated it to my use case - from now on SnapshotLoader is able to list snapshots of some specific keyspace only. This is quite handy as we do not need to load all the snapshots when a user wants to clear snapshots for some keyspace only.', 'Looks mostly good. Added a few minor comments and created [this PR|https://github.com/instaclustr/cassandra/pull/47] to your branch with cosmetic suggestions. Let me know what do you think.', 'https://app.circleci.com/pipelines/github/instaclustr/cassandra/1214/workflows/341a96d4-d7b4-4ba4-b766-7c28465f41cb\\r\\nhttps://github.com/instaclustr/cassandra/tree/CASSANDRA-17757', 'LGTM, submitted CI:\\r\\n* https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1895/', 'I ran one more build here against current trunk https://ci-cassandra.apache.org/job/Cassandra-devbranch/1898/\\r\\n\\r\\nBased on these results I am going to merge it.', 'I have also run 300x circle on added junit test https://app.circleci.com/pipelines/github/instaclustr/cassandra/1232/workflows/bda47ced-ff64-428e-9398-5c293d7c00d8/jobs/4994', 'java 11 precommit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1232/workflows/0ee2b67a-28d3-496e-a372-da234292ec41\\r\\n\\r\\njava 8 precommit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1232/workflows/111cb22a-ae40-481f-8831-5d1af3c668bd']\n",
            "my_comment: https://github.com/apache/cassandra/pull/1781 Hi [~paulo]  I would be delighted to have a review from you. It is rather straightforward patch. I had to parse manifests to see if a snapshot is ephemeral or not in order to skip it from deletion. SnapshotLoader seems to be best suitable for the job I just accommodated it to my use case - from now on SnapshotLoader is able to list snapshots of some specific keyspace only. This is quite handy as we do not need to load all the snapshots when a user wants to clear snapshots for some keyspace only. Looks mostly good. Added a few minor comments and created [this PR|https://github.com/instaclustr/cassandra/pull/47] to your branch with cosmetic suggestions. Let me know what do you think. https://app.circleci.com/pipelines/github/instaclustr/cassandra/1214/workflows/341a96d4-d7b4-4ba4-b766-7c28465f41cb\n",
            "https://github.com/instaclustr/cassandra/tree/CASSANDRA-17757 LGTM submitted CI:\n",
            "* https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1895/ I ran one more build here against current trunk https://ci-cassandra.apache.org/job/Cassandra-devbranch/1898/\n",
            "\n",
            "Based on these results I am going to merge it. I have also run 300x circle on added junit test https://app.circleci.com/pipelines/github/instaclustr/cassandra/1232/workflows/bda47ced-ff64-428e-9398-5c293d7c00d8/jobs/4994 java 11 precommit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1232/workflows/0ee2b67a-28d3-496e-a372-da234292ec41\n",
            "\n",
            "java 8 precommit https://app.circleci.com/pipelines/github/instaclustr/cassandra/1232/workflows/111cb22a-ae40-481f-8831-5d1af3c668bd\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-17868\n",
            "issue_type: Improvement\n",
            "summary: Allow disabling hotness persistence, or tuning of rate limiter\n",
            "description: The persisting of the sstables hotness when there are 10s of thousands of sstables can have issues keeping up, and the rate limiter is hard coded. Another option may be nice to just completely disable the feature.\n",
            "\n",
            "\n",
            "\n",
            "When sstables super backed up (from repairs) the hotness tracking tends to cause the STCS in L0 to make poor decisions, always grabbing the large sstables and skipping the tiny sstables that would benefit more.\n",
            "architectural impact: YES\n",
            "comments: ['||Item|Link||\\r\\n|PR|[link|https://github.com/apache/cassandra/pull/1855]|\\r\\n|JDK8 CI|[link|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/295/workflows/c33ed2b0-5063-4707-ba65-075b38dd9361]|\\r\\n|JDK11 CI|[link|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/295/workflows/274b3677-f709-4e07-be40-a6ad4bc97f59]|', '+1\\r\\n\\r\\n(Dropped a couple tiny nits in the PR)', 'The added properties in config / dd have not been added into cassandra.yaml. ']\n",
            "my_comment: ||Item|Link||\n",
            "|PR|[link|https://github.com/apache/cassandra/pull/1855]|\n",
            "|JDK8 CI|[link|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/295/workflows/c33ed2b0-5063-4707-ba65-075b38dd9361]|\n",
            "|JDK11 CI|[link|https://app.circleci.com/pipelines/github/josh-mckenzie/cassandra/295/workflows/274b3677-f709-4e07-be40-a6ad4bc97f59]| +1\n",
            "\n",
            "(Dropped a couple tiny nits in the PR) The added properties in config / dd have not been added into cassandra.yaml. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2592\n",
            "issue_type: Bug\n",
            "summary: CQL greater-than and less-than operators (> and <) result in key ranges that are inclusive of the terms\n",
            "description: This affects range queries against keys, but not index queries.\n",
            "\n",
            "One possible solution: let the coordinator strip out the extra row in QueryProcessor.\n",
            "architectural impact: YES\n",
            "comments: ['I think this patch makes it so if I say \"LIMIT 10\" I might only get 8 back because one result got chopped off.  QP will need to request more than LIMIT to give back the right number.\\n\\nCan you add a test for this?', \"added a check which will prevent removing a first/last row if it wasn't start/end key. Tests for LIMIT added.\", \"Some issues around LIMIT. I've added more tests in v3 to demonstrate (but not fix) the problem.\", 'v3 with your tests included and insured to pass.', 'committed w/ minor changes', 'Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])\\n    ', '\\nThis blows up on queries that return no results.\\n\\n{noformat}\\njava.lang.IndexOutOfBoundsException: Index: 0, Size: 0\\n\\tat java.util.ArrayList.RangeCheck(ArrayList.java:547)\\n\\tat java.util.ArrayList.get(ArrayList.java:322)\\n\\tat org.apache.cassandra.cql.QueryProcessor.multiRangeSlice(QueryProcessor.java:194)\\n\\tat org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:534)\\n\\tat org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1131)\\n\\tat org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)\\n\\tat org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)\\n\\tat org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\\n\\tat java.lang.Thread.run(Thread.java:662)\\n{noformat}', 'I think Pavel\\'s patch is more correct since the result could come back with exactly one row equal to the start key on a \"KEY > X AND KEY < Y\" query.  Then we want to remove the extra row on the first check, w/o erroring out on the second.', \"you're right; +1.  committed.\", 'Integrated in Cassandra-0.8 #119 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/119/])\\n    properly handle empty result set\\n\\nPatch by Pavel Yaskevich; reviewed by eevans for CASSANDRA-2592\\n\\neevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1125622\\nFiles : \\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java\\n']\n",
            "my_comment: \n",
            "* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-10209\n",
            "issue_type: Bug\n",
            "summary: Missing role manager in cassandra.yaml causes unexpected behaviour\n",
            "description: On upgrading to 2.2+, if the new {{role_manager}} option is not added to {{cassandra.yaml}}, an instance of the default {{CassandraRoleManager}} is created during initialization of {{DatabaseDescriptor}}. This is a problem as the set of role options supported by {{CRM}} depends on the configured {{IAuthenticator}}, which at that point in time is always {{AllowAllAuthenticator}}.\n",
            "\n",
            "This StackOverflow post describes the problem; the configured authenticator is {{PasswordAuthenticator}}, the role manager should allow roles to be created using the {{PASSWORD}} option, but it does not.\n",
            "\n",
            "http://stackoverflow.com/questions/31820914/in-cassandra-2-2-unable-to-create-role-containing-password\n",
            "\n",
            "The simple workaround is to ensure that yaml contains the role manager option\n",
            "{code}\n",
            "role_manager: CassandraRoleManager\n",
            "{code}\n",
            "architectural impact: NO\n",
            "comments: ['Patches:\\n* [2.2 branch|https://github.com/beobal/cassandra/tree/10209-2.2]\\n* [3.0 branch|https://github.com/beobal/cassandra/tree/10209-3.0]\\n* [trunk branch|https://github.com/beobal/cassandra/tree/10209-trunk]\\n\\nCI Tests:\\n* [2.2 testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10209-2.2-testall/]\\n* [2.2 dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10209-2.2-dtest/]\\n* [3.0 testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10209-3.0-testall/]\\n* [3.0 dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10209-3.0-dtest/]\\n* [trunk testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10209-trunk-testall/]\\n* [trunk dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-10209-trunk-dtest/]', \"+1\\n\\nnit: I'd reverse the order of the comparison to avoid a double negative (!= .. else)\", 'committed with that tweak (since Sam is out this week)', 'This patch causes an NPE in CQLSSTableWriterClientTest. CI showed this commit as the regression, and I can repro - previous commit passes this test OK.\\n\\n{noformat}\\n    [junit] Testsuite: org.apache.cassandra.io.sstable.CQLSSTableWriterClientTest\\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.63 sec\\n    [junit] \\n    [junit] Testcase: testWriterInClientMode(org.apache.cassandra.io.sstable.CQLSSTableWriterClientTest):       Caused an ERROR\\n    [junit] null\\n    [junit] java.lang.ExceptionInInitializerError\\n    [junit]     at org.apache.cassandra.io.sstable.CQLSSTableWriter$Builder.getStatement(CQLSSTableWriter.java:496)\\n    [junit]     at org.apache.cassandra.io.sstable.CQLSSTableWriter$Builder.forTable(CQLSSTableWriter.java:351)\\n    [junit]     at org.apache.cassandra.io.sstable.CQLSSTableWriterClientTest.testWriterInClientMode(CQLSSTableWriterClientTest.java:74)\\n    [junit] Caused by: java.lang.NullPointerException\\n    [junit]     at org.apache.cassandra.service.ClientState.<clinit>(ClientState.java:69)\\n    [junit] \\n    [junit] \\n    [junit] Test org.apache.cassandra.io.sstable.CQLSSTableWriterClientTest FAILED\\n{noformat}', \"Reverted.\\n\\nCarl proposes this as a fix: https://github.com/carlyeks/cassandra/commit/204f7bdd8ea0a18d5c642cb7d42104749b82a62b\\n\\nWhich looks reasonable but I'm not sure what other resources should be excluded from client mode, and I'd rather not do it halfway.\", \"bq. I'm not sure what other resources should be excluded from client mode, and I'd rather not do it halfway.\\n\\nNot sure I completely follow; it isn't that those resources are excluded from client mode, rather that when *not* in client mode (and so cassandra.yaml isn't read), don't attempt to figure out which of the {{system_auth}} tables are not modifiable. Which seems reasonable, as if an IAuthenticator/IAuthorizer/IRoleManager is set in the yaml, those won't be correct anyway.\", \"Do you want to recommit with Carl's patch then?\", \"Sure, just wanted to check I wasn't misunderstanding what you meant\", \"Committed with [~carlyeks]'s additions as {{0c0f1ff1b1051627f38a8bf6cb0776241586dfce}}.\\nI notice that since the revert, {{UFTest.testTypesWithAndWithoutNulls}} has been failing with a timeout. This is doubly weird given it was just a revert, plus the same test is fine on 3.0 & trunk. I've also run being running {{UFTest}} in a loop locally and seen no errors in 65 runs. \\n\\n[~mshuler], any ideas about the above? Also, another oddity is that {{CQLSSTableWriterClientTest}} (where the offending test from 2.2 was moved to) never failed on 3.0 or trunk, even before the revert. I would have expected 3.0 to have hit the same NPE from [this build|http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_testall/90/], and indeed running the test locally with that revision fails in exactly the expected way. \\n\\n\"]\n",
            "my_comment: https://github.com/carlyeks/cassandra/commit/204f7bdd8ea0a18d5c642cb7d42104749b82a62b\n",
            "\n",
            "Which looks reasonable but Im not sure what other resources should be excluded from client mode and Id rather not do it halfway.\" \"bq. Im not sure what other resources should be excluded from client mode and Id rather not do it halfway.\n",
            "\n",
            "Not sure I completely follow; it isnt that those resources are excluded from client mode rather that when *not* in client mode (and so cassandra.yaml isnt read) dont attempt to figure out which of the {{system_auth}} tables are not modifiable. Which seems reasonable as if an IAuthenticator/IAuthorizer/IRoleManager is set in the yaml those wont be correct anyway.\" \"Do you want to recommit with Carls patch then?\" \"Sure just wanted to check I wasnt misunderstanding what you meant\" \"Committed with [~carlyeks]s additions as {{0c0f1ff1b1051627f38a8bf6cb0776241586dfce}}.\n",
            "I notice that since the revert {{UFTest.testTypesWithAndWithoutNulls}} has been failing with a timeout. This is doubly weird given it was just a revert plus the same test is fine on 3.0 & trunk. Ive also run being running {{UFTest}} in a loop locally and seen no errors in 65 runs. \n",
            "\n",
            "[~mshuler] any ideas about the above? Also another oddity is that {{CQLSSTableWriterClientTest}} (where the offending test from 2.2 was moved to) never failed on 3.0 or trunk even before the revert. I would have expected 3.0 to have hit the same NPE from [this build|http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_testall/90/] and indeed running the test locally with that revision fails in exactly the expected way. \n",
            "\n",
            "\"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-11038\n",
            "issue_type: Bug\n",
            "summary: Is node being restarted treated as node joining?\n",
            "description: Hi, \n",
            "What we found recently is that every time we restart a node, all other nodes in the cluster treat the restarted node as a new node joining and issue node joining notification to clients. We have traced the code path being hit when a peer node detected a restarted node:\n",
            "\n",
            "src/java/org/apache/cassandra/gms/Gossiper.java\n",
            "{code}\n",
            "    private void handleMajorStateChange(InetAddress ep, EndpointState epState)\n",
            "    {\n",
            "        if (!isDeadState(epState))\n",
            "        {\n",
            "            if (endpointStateMap.get(ep) != null)\n",
            "                logger.info(\"Node {} has restarted, now UP\", ep);\n",
            "            else\n",
            "                logger.info(\"Node {} is now part of the cluster\", ep);\n",
            "        }\n",
            "        if (logger.isTraceEnabled())\n",
            "            logger.trace(\"Adding endpoint state for \" + ep);\n",
            "        endpointStateMap.put(ep, epState);\n",
            "\n",
            "        // the node restarted: it is up to the subscriber to take whatever action is necessary\n",
            "        for (IEndpointStateChangeSubscriber subscriber : subscribers)\n",
            "            subscriber.onRestart(ep, epState);\n",
            "\n",
            "        if (!isDeadState(epState))\n",
            "            markAlive(ep, epState);\n",
            "        else\n",
            "        {\n",
            "            logger.debug(\"Not marking \" + ep + \" alive due to dead state\");\n",
            "            markDead(ep, epState);\n",
            "        }\n",
            "        for (IEndpointStateChangeSubscriber subscriber : subscribers)\n",
            "            subscriber.onJoin(ep, epState);\n",
            "    }\n",
            "\n",
            "{code}\n",
            "\n",
            "subscriber.onJoin(ep, epState) ends up with calling onJoinCluster in Server.java\n",
            "\n",
            "{code}\n",
            "src/java/org/apache/cassandra/transport/Server.java\n",
            "        public void onJoinCluster(InetAddress endpoint)\n",
            "        {\n",
            "server.connectionTracker.send(Event.TopologyChange.newNode(getRpcAddress(endpoint), server.socket.getPort()));\n",
            "        }\n",
            "{code}\n",
            "\n",
            "We have a full trace of code path and skip some intermedia function calls here for being brief. \n",
            "\n",
            "Upon receiving the node joining notification, clients would go and scan system peer table to fetch the latest topology information. Since we have tens of thousands of client connections, scans from all of them put an enormous load to our cluster. \n",
            "\n",
            "Although in the newer version of driver, client skips fetching peer table if the new node has already existed in local metadata, we are still curious why node being restarted is handled as node joining on server side? Did we hit a bug or this is the way supposed to be? Our old java driver version is 1.0.4 and cassandra version is 2.0.12.\n",
            "\n",
            "Thanks!\n",
            "\n",
            "architectural impact: NO\n",
            "comments: [\"If we do send a {{NEW_NODE}} notification for a simple restart, then that do qualify as a bug (though I'll changed the priority to minor since as you mentioned, modern drivers should easily ignore that). I'll note however than 2.0 is not supported anymore and that's too minor for 2.1, so we'll need to first check if that's still a thing in 2.2.\\n\", 'And this does seem to still be a thing since we have [a dtest|https://github.com/riptano/cassandra-dtest/blob/master/pushed_notifications_test.py#L155-L179] that actively assert that a {{NEW_NODE}} is send in that case (not sure why no red flags were raised when writing that test but that sound obviously wrong to me).', 'Not all drivers have that short circuit to avoid node refresh for known nodes.\\n\\nOne other thing I noticed related to this: in addition to status+topo events for starting existing nodes, we also receive both messages when adding a new node, in the same order (i.e. \"status up\" before \"topology_change new_node\").', \"Pushed branches with fixes for 2.2/3.0/3.7/trunk - though the fix merges forward cleanly except for conflicts where I've cleaned up imports. Basically, these preserve the existing behaviour of delivering both {{NEW_NODE}} and {{UP}} events when a node first joins the cluster & of delaying both until after the node becomes available for clients. The erroneous {{NEW_NODE}} when a known node is restarted has been removed. The tracking of pushed notifications in {{EventNotifier}} is still necessary at the moment (because [reasons|https://issues.apache.org/jira/browse/CASSANDRA-7816?focusedCommentId=14346387&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14346387]), but they will go away with CASSANDRA-9156. See CASSANDRA-11731 for some related discussion.\\n\\ndtest branch [here|https://github.com/beobal/cassandra-dtest/tree/11038]\\n\\n||branch||testall||dtest||\\n|[11038-2.2|https://github.com/beobal/cassandra/tree/11038-2.2]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-2.2-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-2.2-dtest]|\\n|[11038-3.0|https://github.com/beobal/cassandra/tree/11038-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.0-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.0-dtest]|\\n|[11038-3.7|https://github.com/beobal/cassandra/tree/11038-3.7]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.7-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.7-dtest]|\\n|[11038-trunk|https://github.com/beobal/cassandra/tree/11038-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-trunk-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-trunk-dtest]|\\n\\n(so far I've only kicked off CI for the 2.2 branch, just in case there's some problem I didn't run into locally, will kick off the other jobs when that finishes).\\n\\nedit: pushed an additional commit to the 2.2 branch as I forgot to switch to java 7 during dev and accidentally included an 8ism.\", \"See this dtest [pull request|https://github.com/riptano/cassandra-dtest/pull/983] for fixing the behavior of the restart node tests in pushed_notifications_test.py. You may also want to add a new dtest to check that NEW_NODE and REMOVED_NODE are sent when a node joins or leaves respectively, I don't think we have a test specific for this at the moment.\", 'thanks everyone for fixing this issue!', \"I've rebased (which should fix the dtest failures) and kicked off another set of CI runs. \\nFTR, the dtest jobs are using [this branch|https://github.com/beobal/cassandra-dtest/tree/11731] , which also includes [~Stefania]'s tests for CASSANDRA-11731.\\n\\n||branch||testall||dtest||\\n|[11038-2.2|https://github.com/beobal/cassandra/tree/11038-2.2]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-2.2-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-2.2-dtest]|\\n|[11038-3.0|https://github.com/beobal/cassandra/tree/11038-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.0-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.0-dtest]|\\n|[11038-3.7|https://github.com/beobal/cassandra/tree/11038-3.7]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.7-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.7-dtest]|\\n|[11038-trunk|https://github.com/beobal/cassandra/tree/11038-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-trunk-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-trunk-dtest]|\\n\", \"LGTM.\\n\\nPatches look good, testall runs look good. Looks like your dtests got branched at a bad time when there was a brief problem on the dtest branch - I rebased your dtest branch on master and pushed at [jkni/cassandra-dtest/11038|https://github.com/jkni/cassandra-dtest/tree/11038]. Because ccm has changes for latest trunk after the CDC merge, I've rebased your trunk branch and pushed it at [jkni/cassandra/11038-trunk-rebase|https://github.com/jkni/cassandra/tree/11038-trunk-rebase]. The rebase was clean so you can just rebase your trunk branch on commit. Dtest runs after this update are clean for [2.2|http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-11038-2.2-dtest/], [3.0|http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-11038-3.0-dtest/], and [trunk|http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-11038-trunk-dtest/].\\n\\nStefania has a PR open for her [CASSANDRA-11731] fixes [here|https://github.com/riptano/cassandra-dtest/pull/983]. I don't see a PR for your added test. Do you want to OK the 11731 PR and PR your added test on commit [~beobal], or do you want me to?\", \"Thanks! committed to 2.2 in {{142f358f6958695c4248acb94b89b64e95ccc609}} and merged to 3.0/trunk. \\n\\nI've also merged Stefania's dtest PR and opened a [second|https://github.com/riptano/cassandra-dtest/pull/1049] for my additional test.\"]\n",
            "my_comment: pushed an additional commit to the 2.2 branch as I forgot to switch to java 7 during dev and accidentally included an 8ism.\" \"See this dtest [pull request|https://github.com/riptano/cassandra-dtest/pull/983] for fixing the behavior of the restart node tests in pushed_notifications_test.py. You may also want to add a new dtest to check that NEW_NODE and REMOVED_NODE are sent when a node joins or leaves respectively I dont think we have a test specific for this at the moment.\" thanks everyone for fixing this issue! \"Ive rebased (which should fix the dtest failures) and kicked off another set of CI runs. \n",
            "FTR the dtest jobs are using [this branch|https://github.com/beobal/cassandra-dtest/tree/11731]  which also includes [~Stefania]s tests for CASSANDRA-11731.\n",
            "\n",
            "||branch||testall||dtest||\n",
            "|[11038-2.2|https://github.com/beobal/cassandra/tree/11038-2.2]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-2.2-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-2.2-dtest]|\n",
            "|[11038-3.0|https://github.com/beobal/cassandra/tree/11038-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.0-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.0-dtest]|\n",
            "|[11038-3.7|https://github.com/beobal/cassandra/tree/11038-3.7]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.7-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-3.7-dtest]|\n",
            "|[11038-trunk|https://github.com/beobal/cassandra/tree/11038-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-trunk-testall]|[dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-11038-trunk-dtest]|\n",
            "\" \"LGTM.\n",
            "\n",
            "Patches look good testall runs look good. Looks like your dtests got branched at a bad time when there was a brief problem on the dtest branch - I rebased your dtest branch on master and pushed at [jkni/cassandra-dtest/11038|https://github.com/jkni/cassandra-dtest/tree/11038]. Because ccm has changes for latest trunk after the CDC merge Ive rebased your trunk branch and pushed it at [jkni/cassandra/11038-trunk-rebase|https://github.com/jkni/cassandra/tree/11038-trunk-rebase]. The rebase was clean so you can just rebase your trunk branch on commit. Dtest runs after this update are clean for [2.2|http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-11038-2.2-dtest/] [3.0|http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-11038-3.0-dtest/] and [trunk|http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-11038-trunk-dtest/].\n",
            "\n",
            "Stefania has a PR open for her [CASSANDRA-11731] fixes [here|https://github.com/riptano/cassandra-dtest/pull/983]. I dont see a PR for your added test. Do you want to OK the 11731 PR and PR your added test on commit [~beobal] or do you want me to?\" \"Thanks! committed to 2.2 in {{142f358f6958695c4248acb94b89b64e95ccc609}} and merged to 3.0/trunk. \n",
            "\n",
            "Ive also merged Stefanias dtest PR and opened a [second|https://github.com/riptano/cassandra-dtest/pull/1049] for my additional test.\"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1128\n",
            "issue_type: Bug\n",
            "summary: sstable2json spews because it uses DatabaseDescriptor before loadSchemas() is called\n",
            "description: sstable2json depends on DatabaseDescriptor for ColumnFamily meta data.  DD requires loadSchemas() is called before the CFMD can be accesed.  nothing in the code path in sstable2json calls loadSchemas().\n",
            "architectural impact: NO\n",
            "comments: ['patch against -r948111', \"+1\\n\\nDD.loadSchemas() requires that system tables are present.  I wish there were a better way to do this, but I can't think of one.\", \"I'm going to hold off on committing this while I investigate some more.  There's got to be a better way.  Also, I want to make sure that sstableimport isn't broken in the same fundamental way.\", 'let me know if there is anything I can do to help...', 'Matthew, can you add this same fix to SSTableImport as well as a check immediately after the DD.loadSchemas() to verify that >0 non-system tables are defined?  The relevant call is DD.getNonSystemTables().', 'patch2 against r948964', 'committed with minor revisions.', 'Integrated in Cassandra #449 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/449/])\\n    have sstable import/export load schema from local storage. Patch by Matthew Dennis, reviewed by Gary Dusbabek. CASSANDRA-1128\\n']\n",
            "my_comment: patch against -r948111 \"+1\n",
            "\n",
            "DD.loadSchemas() requires that system tables are present.  I wish there were a better way to do this but I cant think of one.\" \"Im going to hold off on committing this while I investigate some more.  Theres got to be a better way.  Also I want to make sure that sstableimport isnt broken in the same fundamental way.\" let me know if there is anything I can do to help... Matthew can you add this same fix to SSTableImport as well as a check immediately after the DD.loadSchemas() to verify that >0 non-system tables are defined?  The relevant call is DD.getNonSystemTables(). patch2 against r948964 committed with minor revisions. Integrated in Cassandra #449 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/449/])\n",
            "    have sstable import/export load schema from local storage. Patch by Matthew Dennis reviewed by Gary Dusbabek. CASSANDRA-1128\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-124\n",
            "issue_type: Bug\n",
            "summary: NullPointerException in consistency manager after a failed node rejoins\n",
            "description: ERROR [CONSISTENCY-MANAGER:2] 2009-04-30 18:22:38,946 DebuggableThreadPoolExecutor.java (line 89) Error in ThreadPoolExecutor\n",
            "java.util.concurrent.ExecutionException: java.lang.NullPointerException\n",
            "        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)\n",
            "        at java.util.concurrent.FutureTask.get(FutureTask.java:83)\n",
            "        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:65)\n",
            "        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)\n",
            "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n",
            "        at java.lang.Thread.run(Thread.java:619)\n",
            "Caused by: java.lang.NullPointerException\n",
            "        at org.apache.cassandra.service.ConsistencyManager.run(ConsistencyManager.java:168)\n",
            "        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n",
            "        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n",
            "        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n",
            "        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n",
            "        ... 2 more\n",
            "\n",
            "Plus other similar ones.\n",
            "\n",
            "Config:\n",
            "\n",
            "    <ReplicationFactor>2</ReplicationFactor>\n",
            "    <Tables>\n",
            "        <Table Name=\"Messages\">\n",
            "            <ColumnFamily ColumnSort=\"Name\" Name=\"base\"/>\n",
            "            <ColumnFamily ColumnSort=\"Name\" Name=\"extra\"/>\n",
            "            <ColumnFamily ColumnSort=\"Time\" Name=\"StandardByTime1\"/>\n",
            "            <ColumnFamily ColumnSort=\"Time\" Name=\"StandardByTime2\"/>\n",
            "            <ColumnFamily ColumnType=\"Super\" ColumnSort=\"Name\" Name=\"Super1\"/>\n",
            "            <ColumnFamily ColumnType=\"Super\" ColumnSort=\"Name\" Name=\"Super2\"/>\n",
            "        </Table>\n",
            "    </Tables>\n",
            "\n",
            "\n",
            "I inserted some data using insert method on another node while one node had failed (of three), then brought the failed node back\n",
            "architectural impact: NO\n",
            "comments: [\"Shouldn't ConsistencyManager() constructor contain the following line?\\nthis.replicas_ = replicas_;\", \"FWIW this is not a regression, it's only showing up in the logs now after I added a change that logs exceptions from our executors instead of ignoring them.\", \"Oops, I take it back.  It's a regression from CASSANDRA-95.  nk11 is right, the constructor got broken.\", 'Integrated in Cassandra #59 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/59/])\\n    do not leave variables uninitialized in ConsistencyManager constructor.  fixes regression from #95.  patch by jbellis for \\n', 'looks fixed in my testing']\n",
            "my_comment: [\"Shouldnt ConsistencyManager() constructor contain the following line?\n",
            "this.replicas_ = replicas_;\" \"FWIW this is not a regression its only showing up in the logs now after I added a change that logs exceptions from our executors instead of ignoring them.\" \"Oops I take it back.  Its a regression from CASSANDRA-95.  nk11 is right the constructor got broken.\" Integrated in Cassandra #59 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/59/])\n",
            "    do not leave variables uninitialized in ConsistencyManager constructor.  fixes regression from #95.  patch by jbellis for \n",
            " looks fixed in my testing\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-12580\n",
            "issue_type: Bug\n",
            "summary: Fix merkle tree size calculation\n",
            "description: On CASSANDRA-5263 it was introduced dynamic merkle tree sizing based on estimated number of partitions as {{estimatedDepth = lg(numPartitions)}}, but on [CompactionManager.doValidationCompaction|https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1052] this is being calculated as:\n",
            "\n",
            "{{int depth = numPartitions > 0 ? (int) Math.min(Math.floor(Math.log(numPartitions)), 20) : 0;}}\n",
            "\n",
            "This is actually calculating {{ln(numPartitions)}} (base-e) instead of {{lg(numPartitions)}} (base-2), which causes merkle trees to lose resolution, what may result in overstreaming.\n",
            "architectural impact: NO\n",
            "comments: ['Attaching patch to fix the calculation formula to:\\n\\n{{int depth = numPartitions > 0 ? (int) Math.min(Math.ceil(Math.log(numPartitions) / Math.log(2)), 20) : 0;}}\\n\\nBesides fixing from {{ln}} to {{lg}}, this also changes the rounding formula from {{floor}} to  {{ceil}}, so we overestimate the depth rather than underestimate.\\n\\nI added a new test on {{ValidationTest}} that runs a validation compaction with N=128 and N=1500 keys and expect the merkle tree depth to be {{ceil(lg(N))}}. I also modified the other tests on this class to use a {{ListenableFuture}} ({{CompletableFuture}} on 3.0+) instead of {{SimpleCondition}}, since the JUnit assertions are not enforced in other threads.\\n\\n\\nPatch and tests available below:\\n||2.1||2.2||3.0||trunk||\\n|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-12580]|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-12580]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-12580]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-12580]|\\n|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.1-12580-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-12580-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-12580-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12580-testall/lastCompletedBuild/testReport/]|\\n|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.1-12580-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-12580-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-12580-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12580-dtest/lastCompletedBuild/testReport/]|\\n', 'Nice catch. Patch looks good to me.\\n', 'Committed to 2.2+ as {{c70ce6307da824529762ff40673642b6f86972aa}}.\\n(Skipped 2.1 because it is not critical at this point.)']\n",
            "my_comment: 0;}}\n",
            "\n",
            "Besides fixing from {{ln}} to {{lg}} this also changes the rounding formula from {{floor}} to  {{ceil}} so we overestimate the depth rather than underestimate.\n",
            "\n",
            "I added a new test on {{ValidationTest}} that runs a validation compaction with N=128 and N=1500 keys and expect the merkle tree depth to be {{ceil(lg(N))}}. I also modified the other tests on this class to use a {{ListenableFuture}} ({{CompletableFuture}} on 3.0+) instead of {{SimpleCondition}} since the JUnit assertions are not enforced in other threads.\n",
            "\n",
            "\n",
            "Patch and tests available below:\n",
            "||2.1||2.2||3.0||trunk||\n",
            "|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-12580]|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-12580]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-12580]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-12580]|\n",
            "|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.1-12580-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-12580-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-12580-testall/lastCompletedBuild/testReport/]|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12580-testall/lastCompletedBuild/testReport/]|\n",
            "|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.1-12580-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2.2-12580-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-12580-dtest/lastCompletedBuild/testReport/]|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-12580-dtest/lastCompletedBuild/testReport/]|\n",
            " Nice catch. Patch looks good to me.\n",
            " Committed to 2.2+ as {{c70ce6307da824529762ff40673642b6f86972aa}}.\n",
            "(Skipped 2.1 because it is not critical at this point.)\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-12717\n",
            "issue_type: Bug\n",
            "summary: IllegalArgumentException in CompactionTask\n",
            "description: When I was ran LargePartitionsTest.test_11_1G at trunk, I found that this test fails due to a java.lang.IllegalArgumentException during compaction.\n",
            "This exception apparently happens when the compaction merges a large (>2GB) partition.\n",
            "\n",
            "{noformat}\n",
            "DEBUG [COMMIT-LOG-ALLOCATOR] 2016-09-28 00:32:48,074 ?:? - No segments in reserve; creating a fresh one\n",
            "DEBUG [COMMIT-LOG-ALLOCATOR] 2016-09-28 00:32:48,437 ?:? - No segments in reserve; creating a fresh one\n",
            "WARN  [CompactionExecutor:14] 2016-09-28 00:32:48,463 ?:? - Writing large partition cql_test_keyspace/table_4:1000000000000000000000 (1.004GiB)\n",
            "ERROR [CompactionExecutor:14] 2016-09-28 00:32:49,734 ?:? - Fatal exception in thread Thread[CompactionExecutor:14,1,main]\n",
            "java.lang.IllegalArgumentException: Out of range: 2234434614\n",
            "        at com.google.common.primitives.Ints.checkedCast(Ints.java:91) ~[guava-18.0.jar:na]\n",
            "        at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:206) ~[main/:na]\n",
            "        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]\n",
            "        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85) ~[main/:na]\n",
            "        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61) ~[main/:na]\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:267) ~[main/:na]\n",
            "        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_77]\n",
            "        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_77]\n",
            "        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_77]\n",
            "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_77]\n",
            "        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_77]\n",
            "DEBUG [COMMIT-LOG-ALLOCATOR] 2016-09-28 00:32:49,909 ?:? - No segments in reserve; creating a fresh one\n",
            "DEBUG [COMMIT-LOG-ALLOCATOR] 2016-09-28 00:32:50,148 ?:? - No segments in reserve; creating a fresh one\n",
            "DEBUG [COMMIT-LOG-ALLOCATOR] 2016-09-28 00:32:50,385 ?:? - No segments in reserve; creating a fresh one\n",
            "DEBUG [COMMIT-LOG-ALLOCATOR] 2016-09-28 00:32:50,620 ?:? - No segments in reserve; creating a fresh one\n",
            "{noformat}\n",
            "\n",
            "{noformat}\n",
            "\n",
            "java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Out of range: 2540348821\n",
            "\n",
            "        at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:51)\n",
            "        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:393)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:695)\n",
            "        at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:2066)\n",
            "        at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:2061)\n",
            "        at org.apache.cassandra.cql3.CQLTester.compact(CQLTester.java:426)\n",
            "        at org.apache.cassandra.io.sstable.LargePartitionsTest.lambda$withPartitionSize$2(LargePartitionsTest.java:92)\n",
            "        at org.apache.cassandra.io.sstable.LargePartitionsTest.measured(LargePartitionsTest.java:50)\n",
            "        at org.apache.cassandra.io.sstable.LargePartitionsTest.withPartitionSize(LargePartitionsTest.java:90)\n",
            "        at org.apache.cassandra.io.sstable.LargePartitionsTest.test_11_1G(LargePartitionsTest.java:198)\n",
            "        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "        at java.lang.reflect.Method.invoke(Method.java:498)\n",
            "        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n",
            "        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n",
            "        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n",
            "        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n",
            "        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n",
            "        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n",
            "        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n",
            "        at com.intellij.junit4.JUnit4TestRunnerUtil$IgnoreIgnoredTestJUnit4ClassRunner.runChild(JUnit4TestRunnerUtil.java:358)\n",
            "        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)\n",
            "        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)\n",
            "        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)\n",
            "        at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)\n",
            "        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n",
            "        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n",
            "        at org.junit.runners.ParentRunner.run(ParentRunner.java:220)\n",
            "        at org.junit.runner.JUnitCore.run(JUnitCore.java:159)\n",
            "        at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:117)\n",
            "        at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:42)\n",
            "        at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:262)\n",
            "        at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:84)\n",
            "        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "        at java.lang.reflect.Method.invoke(Method.java:498)\n",
            "        at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)\n",
            "Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Out of range: 2540348821\n",
            "        at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
            "        at java.util.concurrent.FutureTask.get(FutureTask.java:192)\n",
            "        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:386)\n",
            "        ... 37 more\n",
            "Caused by: java.lang.IllegalArgumentException: Out of range: 2540348821\n",
            "        at com.google.common.primitives.Ints.checkedCast(Ints.java:91)\n",
            "        at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:206)\n",
            "        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)\n",
            "        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85)\n",
            "        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)\n",
            "        at org.apache.cassandra.db.compaction.CompactionManager$10.runMayThrow(CompactionManager.java:720)\n",
            "        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)\n",
            "        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
            "        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
            "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
            "        at java.lang.Thread.run(Thread.java:745)\n",
            "{noformat}\n",
            "\n",
            "architectural impact: NO\n",
            "comments: ['Patch is here. Could you please review this?\\n\\nFix IllegalArgumentException in CompactionTask\\nhttps://github.com/matope/cassandra/commit/d6c40dd3d4d95dba8b9c3f88de1015315e45990d', 'Took your patch and added the same fix to cleanup compaction. CI triggered.\\n\\n||cassandra-3.X|[branch|https://github.com/apache/cassandra/compare/cassandra-3.X...snazy:12717-3.x]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-12717-3.x-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-12717-3.x-dtest/lastSuccessfulBuild/]\\n', 'Thanks for the patch! CI looks good.\\n\\nCommitted as [433dd1c0ab77d296dafcc6c2079aa9445a6c1b2a|https://github.com/apache/cassandra/commit/433dd1c0ab77d296dafcc6c2079aa9445a6c1b2a] to [cassandra-3.X|https://github.com/apache/cassandra/tree/cassandra-3.X]\\n']\n",
            "my_comment: Patch is here. Could you please review this?\n",
            "\n",
            "Fix IllegalArgumentException in CompactionTask\n",
            "https://github.com/matope/cassandra/commit/d6c40dd3d4d95dba8b9c3f88de1015315e45990d Took your patch and added the same fix to cleanup compaction. CI triggered.\n",
            "\n",
            "||cassandra-3.X|[branch|https://github.com/apache/cassandra/compare/cassandra-3.X...snazy:12717-3.x]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-12717-3.x-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-12717-3.x-dtest/lastSuccessfulBuild/]\n",
            " Thanks for the patch! CI looks good.\n",
            "\n",
            "Committed as [433dd1c0ab77d296dafcc6c2079aa9445a6c1b2a|https://github.com/apache/cassandra/commit/433dd1c0ab77d296dafcc6c2079aa9445a6c1b2a] to [cassandra-3.X|https://github.com/apache/cassandra/tree/cassandra-3.X]\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-12786\n",
            "issue_type: Bug\n",
            "summary: Fix a bug in CASSANDRA-11005(Split consisten range movement flag)\n",
            "description: I missed a place in the code where we need to split this flag for bootstrap\n",
            "architectural impact: NO\n",
            "comments: [\"Patch merged cleanly from 2.2 -> trunk, assuming those are the branches you're targeting [~kohlisankalp] ? \\n\\nLooks pretty straight forward, but pushed to my github to kick off CI:\\n\\n| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-dtest/] |\\n| [3.X|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786-3.X] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.X-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.X-dtest/] |\\n| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786-3.0] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.0-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.0-dtest/] |\\n| [2.2|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786-2.2] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-2.2-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-2.2-dtest/] |\\n\", 'Yes..please commit it. ', 'lgtm, committed as {{28713778abe29c1d9120d2127354b7fd5ee8fff1}}']\n",
            "my_comment: [\"Patch merged cleanly from 2.2 -> trunk assuming those are the branches youre targeting [~kohlisankalp] ? \n",
            "\n",
            "Looks pretty straight forward but pushed to my github to kick off CI:\n",
            "\n",
            "| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-dtest/] |\n",
            "| [3.X|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786-3.X] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.X-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.X-dtest/] |\n",
            "| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786-3.0] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.0-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-3.0-dtest/] |\n",
            "| [2.2|https://github.com/jeffjirsa/cassandra/tree/cassandra-12786-2.2] | [utest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-2.2-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-12786-2.2-dtest/] |\n",
            "\" Yes..please commit it.  lgtm committed as {{28713778abe29c1d9120d2127354b7fd5ee8fff1}}\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-12956\n",
            "issue_type: Bug\n",
            "summary: CL is not replayed on custom 2i exception\n",
            "description: If during the node shutdown / drain the custom (non-cf) 2i throws an exception, CommitLog will get correctly preserved (segments won't get discarded because segment tracking is correct). \n",
            "\n",
            "However, when it gets replayed on node startup,  we're making a decision whether or not to replay the commit log. CL segment starts getting replayed, since there are non-discarded segments and during this process we're checking whether there every [individual mutation|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogReplayer.java#L215] in commit log is already committed or no. Information about the sstables is taken from [live sstables on disk|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogReplayer.java#L250-L256].\n",
            "\n",
            "architectural impact: NO\n",
            "comments: ['The problem is only present in Cassandra starting from 3.0. Versions before that will replay commit log despite the exception, possibly generating multiple indentical sstables.', \"Patch for {{3.0}} is quite different and is much bigger. Main problem is that there's no transactionality on the same level as in {{3.X}}. {{3.0}} memtables are flushed and renamed to non-tmp names, readers are returned. We need a bit better granularity, since after we may have to abort all the flushed sstables if 2i failed. I've changed it a bit in {{3.x}} fashion, although since we flush to just one sstable, I thought that extracting {{txn}} to the top level will not give us anything.\\n\\nBoth patches introduce the second latch. I'm usually not the biggest fan of two threads that have to wait for one another, but here the ordering is an issue. Problem is that post-flush executor is single-threaded (for ordering), and flush executor is multi-threaded, so we can't return future backed with that multi-threaded executor as it will break order. On the other hand, if we move 2i flush to flush executor, we'll have to sequentially wait for 2i, then all memtables. Current approach allows to keep these actions parallel. \\n\\nWe only need to synchronise the non-cf 2i flush with memtable holding data for current cf. All the cf-index memtables will be in sync with data one anyways since they're combined in the transaction. \\n\\n|[3.X|https://github.com/ifesdjeen/cassandra/tree/12956-3.X]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.X-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.X-dtest/]|\\n|[3.0|https://github.com/ifesdjeen/cassandra/tree/12956-3.0-v2]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.0-v2-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.0-v2-dtest/]|\\n|[trunk|https://github.com/ifesdjeen/cassandra/tree/12956-trunk]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-trunk-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-trunk-dtest/]|\\n\", '[~blambov] to review.', 'Looking at the 3.X patch, as flush threads waiting for post-flush waiting for flush threads is a recipe for disaster (poor performance and deadlocks in particular), I would much prefer the 2i flush to be done on a different thread. In particular, as the flush runnables doing the actual work proceed on their per-disk executors, the flush thread itself [here|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:12956-3.X#diff-98f5acb96aa6d684781936c141132e2aR1152] looks like a better candidate.\\n\\nCould the inverse of the current problem also cause issues (i.e. 2i flushing without the data being in sstables)? It appears that the 2i flush also needs to eventually become transactional -- doing the above would make that easier too.', 'Thanks for the review!\\n\\nI was also skeptical about two threads waiting for one another. Also, tried the approach you\\'ve suggested.\\nI hesitated mostly because it\\'d be blocking the flush thread (although you\\'re right that it\\'s going to be\\nwaiting for flushes anyways) and because {{flushMemtable}} is called from loop, so I wasn\\'t sure if it\\'s\\na good place.\\n\\nIn retrospect, I think your suggestion is much better than the previous version. I\\'ve re-implemented a patch\\nfor 3.0 as well, it got much simpler. Now, we do 2i flush before memtable flush during flush of \"data\" memtable\\n(first one). We could bring back changes that expose sstable writer and run 2i flush on the other\\nthread and commit sstable only on successful 2i flush, although since all cf memtables are flushed sequentially\\nanyways and it might be a bit out of scope of the bugfix I decided to leave it this way. Simply running 2i\\nflush in a different thread is not enough, as we need to ensure it\\'s in sync with \"data\" memtable flush.\\n\\nOrder of 2i/memtable flush does not matter, as for 2i it\\'s only important that data is present either in\\nsstable or in memtable. We can have the following situations: flush running (memtable is queried), flush\\nsuccessfull (sstable is queried), flush unsuccessful (memtable is queried), flush unsuccessful + node restarted\\n(CL will replay the data and it\\'ll be available from memtable again). So 3.0 patch relies on this behaviour.\\n\\n|[3.0|https://github.com/ifesdjeen/cassandra/tree/12956-3.0-v2]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.0-v2-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.0-v2-dtest/]|\\n|[3.X|https://github.com/ifesdjeen/cassandra/tree/12956-3.X]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.X-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.X-dtest/]|\\n|[trunk|https://github.com/ifesdjeen/cassandra/tree/12956-trunk]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-trunk-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-trunk-dtest/]|\\n', 'Thanks, much cleaner and safer indeed. One small issue and a nit:\\n- The [2i flush|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:12956-3.X#diff-98f5acb96aa6d684781936c141132e2aR1082] should only be done if {{truncate}} is false.\\n- The [barrier await|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:12956-3.X#diff-98f5acb96aa6d684781936c141132e2aR1130] is not necessary as the flush does not commence [until that has happened|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:12956-3.X#diff-98f5acb96aa6d684781936c141132e2aR1071].\\n', \"Great, thank you.\\nI've removed the duplicate {{await}}, thanks for catching that. {{truncate}} check for {{false}} is done in [flush memtable|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:12956-3.X#diff-98f5acb96aa6d684781936c141132e2aR1097].\\n\\nI've applied the change to all branches and did CI.\", 'Thanks, committed as 6f90e55e7e23cbe814a3232c8d1ec67f2ff2a537.', 'Thank you!', 'What are the fix versions for this?']\n",
            "my_comment: flush running (memtable is queried) flush\n",
            "successfull (sstable is queried) flush unsuccessful (memtable is queried) flush unsuccessful + node restarted\n",
            "(CL will replay the data and it\\ll be available from memtable again). So 3.0 patch relies on this behaviour.\n",
            "\n",
            "|[3.0|https://github.com/ifesdjeen/cassandra/tree/12956-3.0-v2]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.0-v2-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.0-v2-dtest/]|\n",
            "|[3.X|https://github.com/ifesdjeen/cassandra/tree/12956-3.X]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.X-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-3.X-dtest/]|\n",
            "|[trunk|https://github.com/ifesdjeen/cassandra/tree/12956-trunk]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-trunk-testall/]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12956-trunk-dtest/]|\n",
            " Thanks much cleaner and safer indeed. One small issue and a nit:\n",
            "- The [2i flush|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:12956-3.X#diff-98f5acb96aa6d684781936c141132e2aR1082] should only be done if {{truncate}} is false.\n",
            "- The [barrier await|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:12956-3.X#diff-98f5acb96aa6d684781936c141132e2aR1130] is not necessary as the flush does not commence [until that has happened|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:12956-3.X#diff-98f5acb96aa6d684781936c141132e2aR1071].\n",
            " \"Great thank you.\n",
            "Ive removed the duplicate {{await}} thanks for catching that. {{truncate}} check for {{false}} is done in [flush memtable|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:12956-3.X#diff-98f5acb96aa6d684781936c141132e2aR1097].\n",
            "\n",
            "Ive applied the change to all branches and did CI.\" Thanks committed as 6f90e55e7e23cbe814a3232c8d1ec67f2ff2a537. Thank you! What are the fix versions for this?\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-13526\n",
            "issue_type: Bug\n",
            "summary: nodetool cleanup on KS with no replicas should remove old data, not silently complete\n",
            "description: From the user list:\n",
            "\n",
            "https://lists.apache.org/thread.html/5d49cc6bbc6fd2e5f8b12f2308a3e24212a55afbb441af5cb8cd4167@%3Cuser.cassandra.apache.org%3E\n",
            "\n",
            "If you have a multi-dc cluster, but some keyspaces not replicated to a given DC, you'll be unable to run cleanup on those keyspaces in that DC, because [the cleanup code will see no ranges and exit early|https://github.com/apache/cassandra/blob/4cfaf85/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L427-L441]\n",
            "architectural impact: NO\n",
            "comments: ['The issue I am seeing on C* cluster with the below setup\\n\\nCassandra version : 2.1.16\\nDatacenters: 4 DC\\nRF: NetworkTopologyStrategy with 3 RF in each DC\\nKeyspaces: 50 keyspaces, few replicating to one DC and few replicating to multiple DC\\n\\n', '| branch | unit | [dtest|https://github.com/jasonstack/cassandra-dtest/commits/CASSANDRA-13526] |\\n| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526] |  [pass|https://circleci.com/gh/jasonstack/cassandra/182] | bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test known |\\n| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526-3.11]|  [pass|https://circleci.com/gh/jasonstack/cassandra/186] | pass |\\n| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526-3.0]|  [pass|https://circleci.com/gh/jasonstack/cassandra/181] | offline_tools_test.TestOfflineTools.sstableofflinerelevel_test  auth_test.TestAuth.system_auth_ks_is_alterable_test |\\n| [2.2|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526-2.2]|  [pass|https://circleci.com/gh/jasonstack/cassandra/185] |  ttl_test.TestTTL.collection_list_ttl_test |\\n\\nunit test all passed, some irrelevant dtests failed.\\n\\nwhen no local range && node has joined token ring,  clean up will remove all base local sstables.  ', '[~jjirsa] could you review ? thanks..', \"Thanks [~jasonstack] .  I glanced at it and it looked reasonable, though I'll do a more thorough review next week.\\n\\nSince it's a bug fix (and a pretty serious one at that), it seems like we should have patches for at least 3.0 and 3.11 , and perhaps even 2.1 and 2.2. Are you able to port your fix to 3.0 and 3.11? \\n\\nWe should also add a unit test to make sure we prevent this sort of regression again in the future.\\n\\n\\n\\n\", '[~jjirsa] thanks for reviewing. {{trunk}} was draft for review. I will prepare for older branches and more tests.', \"[~jasonstack] if you give me a few days I'll do a real review, and you can backport after that if it's easier\\n\", \"sure. it's not urgent.\", 'Patch looks good to me, dtest looks good as well, with two comments:\\n\\n1) New dtest repo is https://github.com/apache/cassandra-dtest\\n\\n2) You should remove dc1 from the replication strategy [here|https://github.com/riptano/cassandra-dtest/commit/15bf712988fb50ae29994da246dec186beff69bd#diff-9d7bd37d410a5598b9700b71476845ebR159] to be very explicit about what we expect to happen.\\n\\nWould you backport to 3.0 and 3.11 ? \\n', 'thanks for reviewing, I will back port to 3.0/3.11 this week.  I was stuck in other issues..', 'GitHub user jasonstack opened a pull request:\\n\\n    https://github.com/apache/cassandra-dtest/pull/1\\n\\n    CASSANDRA-13526: nodetool cleanup on KS with no replicas should remov…\\n\\n    JIRA: https://issues.apache.org/jira/browse/CASSANDRA-13526   pending for 2.2/3.0/3.11\\n\\nYou can merge this pull request into a Git repository by running:\\n\\n    $ git pull https://github.com/jasonstack/cassandra-dtest-1 CASSANDRA-13526\\n\\nAlternatively you can review and apply these changes as the patch at:\\n\\n    https://github.com/apache/cassandra-dtest/pull/1.patch\\n\\nTo close this pull request, make a commit to your master/trunk branch\\nwith (at least) the following in the commit message:\\n\\n    This closes #1\\n    \\n----\\ncommit 3c8877c0fa3eb998ed2ee9945ebb8d43687e65fa\\nAuthor: Zhao Yang <zhaoyangsingapore@gmail.com>\\nDate:   2017-07-20T03:18:18Z\\n\\n    CASSANDRA-13526: nodetool cleanup on KS with no replicas should remove old data, not silently complete\\n\\n----\\n', 'Github user jasonstack closed the pull request at:\\n\\n    https://github.com/apache/cassandra-dtest/pull/1\\n', '[~jjirsa] sorry for the delay, I updated the dtest result for 2.2/3.0/3.11/trunk, some irrelevant dtests failed. I skipped 2.1 since this is not critical.', \"Github user jeffjirsa commented on the issue:\\n\\n    https://github.com/apache/cassandra-dtest/pull/1\\n  \\n    (You could leave the PR open and I'll close it on merge with CASSANDRA-13526 )\\n\\n\", 'Github user jasonstack commented on the issue:\\n\\n    https://github.com/apache/cassandra-dtest/pull/1\\n  \\n    thanks..\\n', 'GitHub user jasonstack reopened a pull request:\\n\\n    https://github.com/apache/cassandra-dtest/pull/1\\n\\n    CASSANDRA-13526: nodetool cleanup on KS with no replicas should remov…\\n\\n    JIRA: https://issues.apache.org/jira/browse/CASSANDRA-13526   pending for 2.2/3.0/3.11\\n\\nYou can merge this pull request into a Git repository by running:\\n\\n    $ git pull https://github.com/jasonstack/cassandra-dtest CASSANDRA-13526\\n\\nAlternatively you can review and apply these changes as the patch at:\\n\\n    https://github.com/apache/cassandra-dtest/pull/1.patch\\n\\nTo close this pull request, make a commit to your master/trunk branch\\nwith (at least) the following in the commit message:\\n\\n    This closes #1\\n    \\n----\\ncommit ccb6e81451f3d9ca0d192c508beaeef0959e56fc\\nAuthor: Zhao Yang <zhaoyangsingapore@gmail.com>\\nDate:   2017-07-20T03:18:18Z\\n\\n    CASSANDRA-13526: nodetool cleanup on KS with no replicas should remove old data, not silently complete\\n\\n----\\n', \"Hi [~jasonstack] Really appreciate your patience in the time it's taken me to back to this. I hope to review it this weekend.\\r\\n\\r\\n[~krummas] / [~iamaleksey] - what do you folks think about versions here? 2.2 or 3.0? \\r\\n\\r\\n\", \"I'd probably go with 3.0+ only, but 2.2 is acceptable too.\", \"I've rebased your patch and I'm re-running CI, just because it took me so very long to review this patch.\\r\\n\\r\\nGenerally the patches look fine, but I don't understand why you're running this method twice [here|https://github.com/jasonstack/cassandra/commit/b51c46565adf0d765ac6ded831469a2eca2939d8#diff-ba6d3d8e296151fc283ef11ac4594e62R211] (and in the very similar helper below it)?\\r\\n\\r\\nI'm inclined to remove one of those calls. Other than that, marking as ready-to-commit, and I'll merge when CI finishes.\\r\\n\", \"[~jjirsa] it's a mistake in 3.11 PR.. thanks for the fix.\", 'Thank you so much for the patch and your patience. Committed to 3.0 as {{090f418831be4e4dace861fda380ee4ec27cec35}} and merged up, fixing the 3.11 test on the way.\\r\\n\\r\\n', 'Github user asfgit closed the pull request at:\\n\\n    https://github.com/apache/cassandra-dtest/pull/1\\n', 'Thanks for reviewing']\n",
            "my_comment: nodetool cleanup on KS with no replicas should remove old data not silently complete\n",
            "\n",
            "----\n",
            " \"Hi [~jasonstack] Really appreciate your patience in the time its taken me to back to this. I hope to review it this weekend.\n",
            "\n",
            "[~krummas] / [~iamaleksey] - what do you folks think about versions here? 2.2 or 3.0? \n",
            "\n",
            "\" \"Id probably go with 3.0+ only but 2.2 is acceptable too.\" \"Ive rebased your patch and Im re-running CI just because it took me so very long to review this patch.\n",
            "\n",
            "Generally the patches look fine but I dont understand why youre running this method twice [here|https://github.com/jasonstack/cassandra/commit/b51c46565adf0d765ac6ded831469a2eca2939d8#diff-ba6d3d8e296151fc283ef11ac4594e62R211] (and in the very similar helper below it)?\n",
            "\n",
            "Im inclined to remove one of those calls. Other than that marking as ready-to-commit and Ill merge when CI finishes.\n",
            "\" \"[~jjirsa] its a mistake in 3.11 PR.. thanks for the fix.\" Thank you so much for the patch and your patience. Committed to 3.0 as {{090f418831be4e4dace861fda380ee4ec27cec35}} and merged up fixing the 3.11 test on the way.\n",
            "\n",
            " Github user asfgit closed the pull request at:\n",
            "\n",
            "    https://github.com/apache/cassandra-dtest/pull/1\n",
            " Thanks for reviewing\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-13725\n",
            "issue_type: Bug\n",
            "summary: Secondary indexes are always rebuilt at startup\n",
            "description: Following CASSANDRA-10130, a bug has been introduced that causes a 2i to be rebuilt at startup, even if such index is already built.\n",
            "architectural impact: NO\n",
            "comments: ['GitHub user sbtourist opened a pull request:\\n\\n    https://github.com/apache/cassandra/pull/135\\n\\n    CASSANDRA-13725\\n\\n    \\n\\nYou can merge this pull request into a Git repository by running:\\n\\n    $ git pull https://github.com/sbtourist/cassandra CASSANDRA-13725\\n\\nAlternatively you can review and apply these changes as the patch at:\\n\\n    https://github.com/apache/cassandra/pull/135.patch\\n\\nTo close this pull request, make a commit to your master/trunk branch\\nwith (at least) the following in the commit message:\\n\\n    This closes #135\\n    \\n----\\ncommit 915382930a45244d439fd8407407322f6b5fa330\\nAuthor: Sergio Bossa <sergio.bossa@gmail.com>\\nDate:   2017-07-24T13:09:15Z\\n\\n    Indexes created during column family initialization should not be marked as \"not built\", to avoid rebuilding them needlessly.\\n\\n----\\n', 'This is caused by calling {{SIM#markIndexesBuilding}} when creating the index during column family initialization, which marks the index as \"not built\" and causes the index initialization task to rebuild it.\\n\\nGiven there\\'s no need to mark the index when a new column family is created (as the index will be \"not built\" by definition and there can\\'t be any concurrent indexing), we can just pass a boolean up to {{createIndex()}} to distinguish between index creation at different times, i.e. when a column family is [created|https://github.com/sbtourist/cassandra/blob/CASSANDRA-13725/src/java/org/apache/cassandra/db/Keyspace.java#L394] or [reloaded|https://github.com/sbtourist/cassandra/blob/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L129].\\n\\nSuch solution is implemented in the following patch, with a new dtest verifying it:\\n|[trunk|https://github.com/apache/cassandra/pull/135]|[dtest|https://github.com/apache/cassandra-dtest/pull/2]|\\n\\nTest runs are in progress on our internal CI and I will report results as soon as they\\'re ready.\\n', 'Both the patch and the dtest look good to me, and the CI results seem ok, +1.', 'Committed as [6e19e81db8e4c43bf5ef33308de1ae79916bb61c|https://github.com/apache/cassandra/commit/6e19e81db8e4c43bf5ef33308de1ae79916bb61c].', 'Dtests committed as [b724df80d3bbb55b6b41845633e3a9034116f3be|https://github.com/apache/cassandra-dtest/commit/b724df80d3bbb55b6b41845633e3a9034116f3be].']\n",
            "my_comment:   2017-07-24T13:09:15Z\n",
            "\n",
            "    Indexes created during column family initialization should not be marked as \"not built\" to avoid rebuilding them needlessly.\n",
            "\n",
            "----\n",
            " This is caused by calling {{SIM#markIndexesBuilding}} when creating the index during column family initialization which marks the index as \"not built\" and causes the index initialization task to rebuild it.\n",
            "\n",
            "Given there\\s no need to mark the index when a new column family is created (as the index will be \"not built\" by definition and there can\\t be any concurrent indexing) we can just pass a boolean up to {{createIndex()}} to distinguish between index creation at different times i.e. when a column family is [created|https://github.com/sbtourist/cassandra/blob/CASSANDRA-13725/src/java/org/apache/cassandra/db/Keyspace.java#L394] or [reloaded|https://github.com/sbtourist/cassandra/blob/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L129].\n",
            "\n",
            "Such solution is implemented in the following patch with a new dtest verifying it:\n",
            "|[trunk|https://github.com/apache/cassandra/pull/135]|[dtest|https://github.com/apache/cassandra-dtest/pull/2]|\n",
            "\n",
            "e ready.\n",
            " Both the patch and the dtest look good to me and the CI results seem ok +1. Committed as [6e19e81db8e4c43bf5ef33308de1ae79916bb61c|https://github.com/apache/cassandra/commit/6e19e81db8e4c43bf5ef33308de1ae79916bb61c]. Dtests committed as [b724df80d3bbb55b6b41845633e3a9034116f3be|https://github.com/apache/cassandra-dtest/commit/b724df80d3bbb55b6b41845633e3a9034116f3be].\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-13794\n",
            "issue_type: Bug\n",
            "summary: Fix short read protection logic for querying more rows\n",
            "description: Discovered by [~benedict] while reviewing CASSANDRA-13747:\n",
            "\n",
            "{quote}\n",
            "While reviewing I got a little suspicious of the modified line {{DataResolver}} :479, as it seemed that n and x were the wrong way around... and, reading the comment of intent directly above, and reproducing the calculation, they are indeed.\n",
            "\n",
            "This is probably a significant enough bug that it warrants its own ticket for record keeping, though I'm fairly agnostic on that decision.\n",
            "\n",
            "I'm a little concerned about our current short read behaviour, as right now it seems we should be requesting exactly one row, for any size of under-read, which could mean extremely poor performance in case of large under-reads.\n",
            "\n",
            "I would suggest that the outer unconditional {{Math.max}} is a bad idea, has been (poorly) insulating us from this error, and that we should first be asserting that the calculation yields a value >= 0 before setting to 1.\n",
            "{quote}\n",
            "architectural impact: NO\n",
            "comments: [\"Work in progress branch [here|https://github.com/iamaleksey/cassandra/tree/13794-3.0]. Currently missing (new) tests, but I want to get the underlying logic reviewed and approved, first. Would add coverage before committing it.\\n\\nA short summary of the issue: the code right now has two variables swapped, which ultimately results in us always fetching 1 extra row per short read protection requests, in a blocking manner, making it very inefficient. But upon closer look, there are some other inefficiencies here that can and should be addressed:\\n\\n1. One of our stop conditions is {{lastCount == counter.counted()}}. It's supposed to abort a short read if our previous attempt to fetch more rows yielded 0 extra rows. It's not incorrect, but is only a special case of the more general scenario: our previous attempt to fetch more extra rows yielded fewer results than we requested for. That would mean there is no more rows to fetch at that replica, and allows us to abort earlier and more frequently.\\n\\n2. Another of our stop conditions is {{!counter.isDoneForPartition()}}. Once again, it isn't incorrect, but it can be extended further. Due to the way {{isDoneForPartition()}} is defined ({{isDone() || rowInCurrentPartition >= perPartitionLimit}}) and because of that counter being counting-only, it is possible for us to have fetched enough rows total for other partitions short read retries previously to hit the global limit of rows in the counter. That would make {{isDone()}} return {{true}} always, and have {{isDoneForPartition()}} return false positives even if the partition currently processed only has a partition level deletion and/or tombstones. That can affect queries that set per partition limit explicitly or when running {{SELECT DISTINCT}} queries. Spotted that during CASSANDRA-13747 fixing.\\n\\n3. Once we've swapped {{x}} and {{n}} in {{moreContents()}} to fix the logic error, we'd still have some issues. In degenerate cases where we have some nodes missing a fresh partition deletion, for example, the formula would fetch *a lot* of rows {{n * (n - 1)}}, with {{n}} growing exponentially with every attempt.\\n\\nUpon closer inspection, the formula doesn't make 100% sense. It claims that we miss {{n - x}} rows - where {{n = counter.countedInCurrentPartition()}} and {{x = postReconciliationCounter.countedInCurrentPartition()}}, but the number we really miss is {{limit - postReconciliationCounter.counted()}} or {{perPartitionLimit - postReconciliationCounter.countedInCurrentPartition()}}. They might be the same on our first short read protection iteration, but will be diverging further and further with each request. In addition to that, it seems to assume a uniform distribution of tombstones (in the end result) rows in the source partition, which can't be true for most workloads.\\n\\nI couldn't come up with some ideal heuristic that covers all workloads, so I stuck to something safe that respects client paging limits but still attempts to minimise the # of requests we make by fetching (in most cases) more rows than is minimally necessary. I'm not completely sure about it, but I welcome any ideas on how to make it better. Either way, anything we do should be significantly more efficient than what we have now.\\n\\nI've also made some renames, refactorings, and moved a few things around to better understand the code myself, and make it clearer for future contributors - including future me. The most significant noticeable change is application of the per-response counter shift to {{mergeWithShortReadProtection()}} method, instead of overloading {{ShortReadRowProtection}} with responsibilities - I also like it to be next to the global counter creation, so you can see the contrast in arguments.\", 'Marking the ticket as {{Patch Available}}, despite its lack of (new) tests, so that it can be reviewed first. Tests will be committed with the rest of the code.', \"This patch is great (excepting a couple of extraneous edits).  Love the comments.\\n\\n+1\\n\\nI would suggest filing two follow-up tickets to address some short comings with this code path, but they're edge-case, and not straight-forward enough, to not block this merge.\\n\\n# For some extreme users, 16 rows could be a huge amount of data.  There should probably be some modulation of this lower bound based on known data sizes in the table, or the like.\\n# Conversely, in an overloaded cluster on which users are commonly performing fairly large-limit reads (say, 1k+ moderate sized rows) of larger partitions, we could find ourselves doubling the amount of work the cluster needs to do; during overload we can expect dropped writes, and a single missing row in any read would trigger a same-sized read.  This could iteratively compound the overload. \\n The best solutions to this problem are probably non-trivial, though a simplish approach might be to use exponential growth, bounded on both sides by a minimum and maximum (perhaps similarly determined from the known data size distribution) - with the query limit being used as the first value if it is small enough.\\n\\nI _would_ say that (2) is no worse than the status-quo, given the per-request overheads are probably greater than the per-datum overheads in a typical cluster, but CASSANDRA-12872 suggests we haven't been incurring the full overheads of SRP, so we cannot claim that.  I still think it is reasonable to address this in a follow-up ticket, however.\", \"Committed to 3.0 as [f93e6e3401c343dec74687d8b079b5697813ab28|https://github.com/apache/cassandra/commit/f93e6e3401c343dec74687d8b079b5697813ab28] and merged with 3.11 and trunk.\\n\\nCircle run for 3.0 [here|https://circleci.com/gh/iamaleksey/cassandra/39] has two completely unrelated {{CommitLogSegmentManagerTest}} failures, and [dtest run|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/314/testReport/] here is mostly failures to git clone.\\n\\nThe passing tests include the 3 new dtests added since this JIRA was created. My initial plan was to cover it with proper unit tests, too - similar to read repair tests we have - but doing it properly has proven to be too time consuming. In addition to the tests we have, I did a lot of manual testing (which uncovered a couple more issues - not affecting my branch). But more unit test coverage will be added later - we've budgeted a significant chunk of time on {{DataResolver}} testing alone.\\n\\nFollow up JIRAs I'll file soonish. Thanks for the review! \"]\n",
            "my_comment: our previous attempt to fetch more extra rows yielded fewer results than we requested for. That would mean there is no more rows to fetch at that replica and allows us to abort earlier and more frequently.\n",
            "\n",
            "2. Another of our stop conditions is {{!counter.isDoneForPartition()}}. Once again it isnt incorrect but it can be extended further. Due to the way {{isDoneForPartition()}} is defined ({{isDone() || rowInCurrentPartition >= perPartitionLimit}}) and because of that counter being counting-only it is possible for us to have fetched enough rows total for other partitions short read retries previously to hit the global limit of rows in the counter. That would make {{isDone()}} return {{true}} always and have {{isDoneForPartition()}} return false positives even if the partition currently processed only has a partition level deletion and/or tombstones. That can affect queries that set per partition limit explicitly or when running {{SELECT DISTINCT}} queries. Spotted that during CASSANDRA-13747 fixing.\n",
            "\n",
            "3. Once weve swapped {{x}} and {{n}} in {{moreContents()}} to fix the logic error wed still have some issues. In degenerate cases where we have some nodes missing a fresh partition deletion for example the formula would fetch *a lot* of rows {{n * (n - 1)}} with {{n}} growing exponentially with every attempt.\n",
            "\n",
            "Upon closer inspection the formula doesnt make 100% sense. It claims that we miss {{n - x}} rows - where {{n = counter.countedInCurrentPartition()}} and {{x = postReconciliationCounter.countedInCurrentPartition()}} but the number we really miss is {{limit - postReconciliationCounter.counted()}} or {{perPartitionLimit - postReconciliationCounter.countedInCurrentPartition()}}. They might be the same on our first short read protection iteration but will be diverging further and further with each request. In addition to that it seems to assume a uniform distribution of tombstones (in the end result) rows in the source partition which cant be true for most workloads.\n",
            "\n",
            "I couldnt come up with some ideal heuristic that covers all workloads so I stuck to something safe that respects client paging limits but still attempts to minimise the # of requests we make by fetching (in most cases) more rows than is minimally necessary. Im not completely sure about it but I welcome any ideas on how to make it better. Either way anything we do should be significantly more efficient than what we have now.\n",
            "\n",
            "Ive also made some renames refactorings and moved a few things around to better understand the code myself and make it clearer for future contributors - including future me. The most significant noticeable change is application of the per-response counter shift to {{mergeWithShortReadProtection()}} method instead of overloading {{ShortReadRowProtection}} with responsibilities - I also like it to be next to the global counter creation so you can see the contrast in arguments.\" Marking the ticket as {{Patch Available}} despite its lack of (new) tests so that it can be reviewed first. Tests will be committed with the rest of the code. \"This patch is great (excepting a couple of extraneous edits).  Love the comments.\n",
            "\n",
            "+1\n",
            "\n",
            "I would suggest filing two follow-up tickets to address some short comings with this code path but theyre edge-case and not straight-forward enough to not block this merge.\n",
            "\n",
            "# For some extreme users 16 rows could be a huge amount of data.  There should probably be some modulation of this lower bound based on known data sizes in the table or the like.\n",
            "# Conversely in an overloaded cluster on which users are commonly performing fairly large-limit reads (say 1k+ moderate sized rows) of larger partitions we could find ourselves doubling the amount of work the cluster needs to do; during overload we can expect dropped writes and a single missing row in any read would trigger a same-sized read.  This could iteratively compound the overload. \n",
            " The best solutions to this problem are probably non-trivial though a simplish approach might be to use exponential growth bounded on both sides by a minimum and maximum (perhaps similarly determined from the known data size distribution) - with the query limit being used as the first value if it is small enough.\n",
            "\n",
            "I _would_ say that (2) is no worse than the status-quo given the per-request overheads are probably greater than the per-datum overheads in a typical cluster but CASSANDRA-12872 suggests we havent been incurring the full overheads of SRP so we cannot claim that.  I still think it is reasonable to address this in a follow-up ticket however.\" \"Committed to 3.0 as [f93e6e3401c343dec74687d8b079b5697813ab28|https://github.com/apache/cassandra/commit/f93e6e3401c343dec74687d8b079b5697813ab28] and merged with 3.11 and trunk.\n",
            "\n",
            "Circle run for 3.0 [here|https://circleci.com/gh/iamaleksey/cassandra/39] has two completely unrelated {{CommitLogSegmentManagerTest}} failures and [dtest run|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/314/testReport/] here is mostly failures to git clone.\n",
            "\n",
            "The passing tests include the 3 new dtests added since this JIRA was created. My initial plan was to cover it with proper unit tests too - similar to read repair tests we have - but doing it properly has proven to be too time consuming. In addition to the tests we have I did a lot of manual testing (which uncovered a couple more issues - not affecting my branch). But more unit test coverage will be added later - weve budgeted a significant chunk of time on {{DataResolver}} testing alone.\n",
            "\n",
            "Follow up JIRAs Ill file soonish. Thanks for the review! \"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14010\n",
            "issue_type: Bug\n",
            "summary: Fix SStable ordering by max timestamp in SinglePartitionReadCommand\n",
            "description: We have a test environment were we drop and create keyspaces and tables several times within a short time frame. Since upgrading from 3.11.0 to 3.11.1, we are seeing a lot of create statements failing. See the logs below:\n",
            "\n",
            "{code:java}\n",
            "\n",
            "2017-11-13T14:29:20.037986449Z WARN Directory /tmp/ramdisk/commitlog doesn't exist\n",
            "\n",
            "2017-11-13T14:29:20.038009590Z WARN Directory /tmp/ramdisk/saved_caches doesn't exist\n",
            "\n",
            "2017-11-13T14:29:20.094337265Z INFO Initialized prepared statement caches with 10 MB (native) and 10 MB (Thrift)\n",
            "\n",
            "2017-11-13T14:29:20.805946340Z INFO Initializing system.IndexInfo\n",
            "\n",
            "2017-11-13T14:29:21.934686905Z INFO Initializing system.batches\n",
            "\n",
            "2017-11-13T14:29:21.973914733Z INFO Initializing system.paxos\n",
            "\n",
            "2017-11-13T14:29:21.994550268Z INFO Initializing system.local\n",
            "\n",
            "2017-11-13T14:29:22.014097194Z INFO Initializing system.peers\n",
            "\n",
            "2017-11-13T14:29:22.124211254Z INFO Initializing system.peer_events\n",
            "\n",
            "2017-11-13T14:29:22.153966833Z INFO Initializing system.range_xfers\n",
            "\n",
            "2017-11-13T14:29:22.174097334Z INFO Initializing system.compaction_history\n",
            "\n",
            "2017-11-13T14:29:22.194259920Z INFO Initializing system.sstable_activity\n",
            "\n",
            "2017-11-13T14:29:22.210178271Z INFO Initializing system.size_estimates\n",
            "\n",
            "2017-11-13T14:29:22.223836992Z INFO Initializing system.available_ranges\n",
            "\n",
            "2017-11-13T14:29:22.237854207Z INFO Initializing system.transferred_ranges\n",
            "\n",
            "2017-11-13T14:29:22.253995621Z INFO Initializing system.views_builds_in_progress\n",
            "\n",
            "2017-11-13T14:29:22.264052481Z INFO Initializing system.built_views\n",
            "\n",
            "2017-11-13T14:29:22.283334779Z INFO Initializing system.hints\n",
            "\n",
            "2017-11-13T14:29:22.304110311Z INFO Initializing system.batchlog\n",
            "\n",
            "2017-11-13T14:29:22.318031950Z INFO Initializing system.prepared_statements\n",
            "\n",
            "2017-11-13T14:29:22.326547917Z INFO Initializing system.schema_keyspaces\n",
            "\n",
            "2017-11-13T14:29:22.337097407Z INFO Initializing system.schema_columnfamilies\n",
            "\n",
            "2017-11-13T14:29:22.354082675Z INFO Initializing system.schema_columns\n",
            "\n",
            "2017-11-13T14:29:22.384179063Z INFO Initializing system.schema_triggers\n",
            "\n",
            "2017-11-13T14:29:22.394222027Z INFO Initializing system.schema_usertypes\n",
            "\n",
            "2017-11-13T14:29:22.414199833Z INFO Initializing system.schema_functions\n",
            "\n",
            "2017-11-13T14:29:22.427205182Z INFO Initializing system.schema_aggregates\n",
            "\n",
            "2017-11-13T14:29:22.427228345Z INFO Not submitting build tasks for views in keyspace system as storage service is not initialized\n",
            "\n",
            "2017-11-13T14:29:22.652838866Z INFO Scheduling approximate time-check task with a precision of 10 milliseconds\n",
            "\n",
            "2017-11-13T14:29:22.732862906Z INFO Initializing system_schema.keyspaces\n",
            "\n",
            "2017-11-13T14:29:22.746598744Z INFO Initializing system_schema.tables\n",
            "\n",
            "2017-11-13T14:29:22.759649011Z INFO Initializing system_schema.columns\n",
            "\n",
            "2017-11-13T14:29:22.766245435Z INFO Initializing system_schema.triggers\n",
            "\n",
            "2017-11-13T14:29:22.778716809Z INFO Initializing system_schema.dropped_columns\n",
            "\n",
            "2017-11-13T14:29:22.791369819Z INFO Initializing system_schema.views\n",
            "\n",
            "2017-11-13T14:29:22.839141724Z INFO Initializing system_schema.types\n",
            "\n",
            "2017-11-13T14:29:22.852911976Z INFO Initializing system_schema.functions\n",
            "\n",
            "2017-11-13T14:29:22.852938112Z INFO Initializing system_schema.aggregates\n",
            "\n",
            "2017-11-13T14:29:22.869348526Z INFO Initializing system_schema.indexes\n",
            "\n",
            "2017-11-13T14:29:22.874178682Z INFO Not submitting build tasks for views in keyspace system_schema as storage service is not initialized\n",
            "\n",
            "2017-11-13T14:29:23.700250435Z INFO Initializing key cache with capacity of 25 MBs.\n",
            "\n",
            "2017-11-13T14:29:23.724357053Z INFO Initializing row cache with capacity of 0 MBs\n",
            "\n",
            "2017-11-13T14:29:23.724383599Z INFO Initializing counter cache with capacity of 12 MBs\n",
            "\n",
            "2017-11-13T14:29:23.724386906Z INFO Scheduling counter cache save to every 7200 seconds (going to save all keys).\n",
            "\n",
            "2017-11-13T14:29:23.984408710Z INFO Populating token metadata from system tables\n",
            "\n",
            "2017-11-13T14:29:24.032687075Z INFO Global buffer pool is enabled, when pool is exhausted (max is 125.000MiB) it will allocate on heap\n",
            "\n",
            "2017-11-13T14:29:24.214123695Z INFO Token metadata:\n",
            "\n",
            "2017-11-13T14:29:24.304218769Z INFO Completed loading (14 ms; 8 keys) KeyCache cache\n",
            "\n",
            "2017-11-13T14:29:24.363978406Z INFO No commitlog files found; skipping replay\n",
            "\n",
            "2017-11-13T14:29:24.364005238Z INFO Populating token metadata from system tables\n",
            "\n",
            "2017-11-13T14:29:24.394408476Z INFO Token metadata:\n",
            "\n",
            "2017-11-13T14:29:24.709411652Z INFO Preloaded 0 prepared statements\n",
            "\n",
            "2017-11-13T14:29:24.719332880Z INFO Cassandra version: 3.11.1\n",
            "\n",
            "2017-11-13T14:29:24.719355969Z INFO Thrift API version: 20.1.0\n",
            "\n",
            "2017-11-13T14:29:24.719359443Z INFO CQL supported versions: 3.4.4 (default: 3.4.4)\n",
            "\n",
            "2017-11-13T14:29:24.719362103Z INFO Native protocol supported versions: 3/v3, 4/v4, 5/v5-beta (default: 4/v4)\n",
            "\n",
            "2017-11-13T14:29:24.766102400Z INFO Initializing index summary manager with a memory pool size of 25 MB and a resize interval of 60 minutes\n",
            "\n",
            "2017-11-13T14:29:24.778800183Z INFO Starting Messaging Service on /172.17.0.2:7000 (eth0)\n",
            "\n",
            "2017-11-13T14:29:24.783832188Z WARN No host ID found, created 62452b7c-33ae-40e6-859c-1d7c803aaea8 (Note: This should happen exactly once per node).\n",
            "\n",
            "2017-11-13T14:29:24.897281778Z INFO Loading persisted ring state\n",
            "\n",
            "2017-11-13T14:29:24.904217782Z INFO Starting up server gossip\n",
            "\n",
            "2017-11-13T14:29:25.003802973Z INFO This node will not auto bootstrap because it is configured to be a seed node.\n",
            "\n",
            "2017-11-13T14:29:25.047674499Z INFO Generated random tokens. tokens are [-6736304773851341012, 3437071596424929702, 4372058337604769145, -306854781937968525, -4419476154597297006, 4339837665480866486, 2052026232731139893, -5761537575805252593, -4477540978357776290, 6263754683045286998, 3670054894619378302, -4326549778810780939, 7187409938161102814, 7030537377703307755, -2757270254308154659, -1953637968902719055, -7235425703069930259, 7123794193321014835, 349308827967095711, 997472983569031481, 992257140226393205, -4045122629441468253, 4149955653388319941, -3690032393349188278, 3528068129562283633, -5057394127379238561, -4944743272177354946, 1371473468273321389, -2771267888257678908, -2379074055482922854, 8800628062632970014, 6016352719444925532, -6458243637210081043, -7131512441131507433, -6135681286390467242, -7886878247827491401, -3964432859204941604, -7124853795154335905, 4536647221115220987, 4518363137218750861, -3945920538919881061, -8569890499152898728, -2228677668104169495, -4004623128783039030, -6849460601197629451, -1787645289665343374, -9004089114738085395, -8444847561386064840, -7719025430480017932, -5020575591450775929, -3535144847803187721, 7252524597471726426, -2582131369519057623, 3737595811793840609, -7248797595897252845, -7065188032269288840, -6731826791431802176, -2970075663731571587, -2619987499373344925, -2698285069650269138, -8589822844420136511, 2658120945314344720, -3710290429036098141, 134530136452862749, 3703742438909992913, 3460544540911930621, 8673891706698173777, 2853177281247015813, 13977464647778584, 2404057737490125388, -6759648287860184451, 744453319830059045, -688104893800828924, 3356383003502762348, 9054641886966810357, 2317130729058165506, -5810663910204725460, 2577132949237273515, 6326216055185945365, 1376570278575995967, 8758101809469842945, -2892126907778256351, -1716283861287440286, 3040640159143123724, 4243935966006505554, -6827972097309863039, 3055912546894309570, -3992773844369808712, -4717007910267923035, -846198401308205724, -3924870907185309086, 1746803312676010060, 6821355560067598541, -5786385588878319458, 3085551110635941848, 7832310180114101987, -9149254679798945822, 3124836728424468300, -100875121723899324, -7606007094353527325, 270256410769436649, -3016541299722946307, 6864985654287583845, 8465468836551135602, 7372808321676939792, -2815261206329145311, -2044219183173664775, -5342853768228072396, 3636940711408324184, -2772742494800447004, -8420993393273439531, -1530882172522252534, 8236427746033013128, -8939749738449264357, -571957476330656311, 6462994120934510138, -2744633996286755268, 1001793370994802364, 6170004027360887596, 383603396273760626, 184737756504479596, -4799447088893889554, 1038205033737034383, 2078124248957773983, -5177819727898656480, 1588469358432181111, 2476693400197902714, 246839957213783595, -7804622995667946321, 3516202677463047183, 7649126752776473673, -3286662198144050257, 2592926684883421936, 6953901594207876325, 8920684239689152479, -2427878301857439455, -6527468054932471540, -4117125961852289967, -2833593154725933249, 2548273043767381234, -814886098184093796, -1113961241682560435, -8364806058670744019, -86067309810855914, -7325813350040495905, -2651532619332818109, -3028501296208600216, 2638649530375347897, -3870517833780069551, 3770751443844709295, -7272035856681375921, -6750394828506790417, 3368553496734537183, 8516129492713951191, 4435960977618718666, 638690551817702460, -7462842134093200053, -7312636473795422279, 3825550639500258186, -490674188267611204, 8488259904981422083, 4436678791994058329, 5971819389544487212, 5777643219857256454, 6295906877222880293, -6635403410495817577, -7125973103119231247, 2275471188158109929, -6554337501188391642, -4759608795508681126, -7655250005358224912, 9106670136441382451, -9080117178764089351, 5094764588972879219, -3599769156391426161, 6116955962236377408, -1734768840951819839, 7826627278264825770, -2624139016757063818, -4122417151587476614, -6757251857390630385, 2099124804383862824, -3162332634454027278, 4826222794133551270, 9122652158513265055, 1734656138981660315, 972980826344778639, -1746779194020635548, -3426944282250211269, -3857828063692993065, 1895243495321867610, -8828035583443240909, -4705856469629722102, -8519546521146945353, -2150150551733933931, 8281585304878501119, -2775028105733898661, 2087277989579187052, -4016777313261130077, 2747128117959922334, -1398884803916585873, 7188260080368469340, -3880993098463994199, 3574665846011083154, 5260683239918360122, 5817587463499837044, 38978473621576635, 2680910834841463710, 6083561971466189055, 7236937177408808074, -3600112532662592989, -4559800196660261967, 8276688045060113438, 5496539762676760591, -2999626688519766687, 8917068693185637310, 2348378561310644717, 7605443413072783308, 5729359499569394810, -782345069306605591, 1165004403533704355, -8301882560002322767, 2008499890787626408, -6211027251975593898, 7406423735628820605, -3204398339633370684, -7917412446164112725, -106645076087724250, -1186720400780396653, -8676089669972641821, -1970508303671183113, -7283082875075535628, -3469652138221449481, -3310949358194646693, 6449384223770405185, -3602652844861890703, -7845236015467185307, -4548809972889727666, -8898627491921139823, 5187965699546741544, 295363921125698104, -8013235493809339368, -6747271362503076577, 1102625310233591704, -2543233385033476145, -6197912327393001665, 118165474822979356, -4838870266722406438, -5797141823778124932, -1506683916229985698, 9139710449103348665, -1571612701117454805, 8031141543284728427, 8472337544063987034, 3222463867738580103, 8210687258187437204]\n",
            "\n",
            "2017-11-13T14:29:25.092248590Z INFO Create new Keyspace: KeyspaceMetadata{name=system_traces, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=2}}, tables=[org.apache.cassandra.config.CFMetaData@3bc5ed95[cfId=c5e99f16-8677-3914-b17e-960613512345,ksName=system_traces,cfName=sessions,flags=[COMPOUND],params=TableParams{comment=tracing sessions, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=0, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [client command coordinator duration request started_at parameters]],partitionKeyColumns=[session_id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.UUIDType,columnMetadata=[client, command, session_id, coordinator, request, started_at, duration, parameters],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@1a296ffd[cfId=8826e8e9-e16a-3728-8753-3bc1fc713c25,ksName=system_traces,cfName=events,flags=[COMPOUND],params=TableParams{comment=tracing events, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=0, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.TimeUUIDType),partitionColumns=[[] | [activity source source_elapsed thread]],partitionKeyColumns=[session_id],clusteringColumns=[event_id],keyValidator=org.apache.cassandra.db.marshal.UUIDType,columnMetadata=[activity, event_id, session_id, source, thread, source_elapsed],droppedColumns={},triggers=[],indexes=[]]], views=[], functions=[], types=[]}\n",
            "\n",
            "2017-11-13T14:29:25.394141160Z INFO Not submitting build tasks for views in keyspace system_traces as storage service is not initialized\n",
            "\n",
            "2017-11-13T14:29:25.408584506Z INFO Initializing system_traces.events\n",
            "\n",
            "2017-11-13T14:29:25.424314845Z INFO Initializing system_traces.sessions\n",
            "\n",
            "2017-11-13T14:29:25.483133136Z INFO Create new Keyspace: KeyspaceMetadata{name=system_distributed, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=3}}, tables=[org.apache.cassandra.config.CFMetaData@2884b38b[cfId=759fffad-624b-3181-80ee-fa9a52d1f627,ksName=system_distributed,cfName=repair_history,flags=[COMPOUND],params=TableParams{comment=Repair history, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.TimeUUIDType),partitionColumns=[[] | [coordinator exception_message exception_stacktrace finished_at parent_id range_begin range_end started_at status participants]],partitionKeyColumns=[keyspace_name, columnfamily_name],clusteringColumns=[id],keyValidator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type),columnMetadata=[status, id, coordinator, finished_at, participants, exception_stacktrace, parent_id, range_end, range_begin, exception_message, keyspace_name, started_at, columnfamily_name],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@7fcc80b2[cfId=deabd734-b99d-3b9c-92e5-fd92eb5abf14,ksName=system_distributed,cfName=parent_repair_history,flags=[COMPOUND],params=TableParams{comment=Repair history, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [exception_message exception_stacktrace finished_at keyspace_name started_at columnfamily_names options requested_ranges successful_ranges]],partitionKeyColumns=[parent_id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.TimeUUIDType,columnMetadata=[requested_ranges, exception_message, keyspace_name, successful_ranges, started_at, finished_at, options, exception_stacktrace, parent_id, columnfamily_names],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@7e500004[cfId=5582b59f-8e4e-35e1-b913-3acada51eb04,ksName=system_distributed,cfName=view_build_status,flags=[COMPOUND],params=TableParams{comment=Materialized View build status, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UUIDType),partitionColumns=[[] | [status]],partitionKeyColumns=[keyspace_name, view_name],clusteringColumns=[host_id],keyValidator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type),columnMetadata=[view_name, status, keyspace_name, host_id],droppedColumns={},triggers=[],indexes=[]]], views=[], functions=[], types=[]}\n",
            "\n",
            "2017-11-13T14:29:25.598604284Z INFO Not submitting build tasks for views in keyspace system_distributed as storage service is not initialized\n",
            "\n",
            "2017-11-13T14:29:25.602132560Z INFO Initializing system_distributed.parent_repair_history\n",
            "\n",
            "2017-11-13T14:29:25.624580018Z INFO Initializing system_distributed.repair_history\n",
            "\n",
            "2017-11-13T14:29:25.624605811Z INFO Initializing system_distributed.view_build_status\n",
            "\n",
            "2017-11-13T14:29:25.682205208Z INFO JOINING: Finish joining ring\n",
            "\n",
            "2017-11-13T14:29:25.808448539Z INFO Create new Keyspace: KeyspaceMetadata{name=system_auth, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[org.apache.cassandra.config.CFMetaData@3c28c0da[cfId=5bc52802-de25-35ed-aeab-188eecebb090,ksName=system_auth,cfName=roles,flags=[COMPOUND],params=TableParams{comment=role definitions, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [can_login is_superuser salted_hash member_of]],partitionKeyColumns=[role],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[salted_hash, member_of, role, can_login, is_superuser],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@2e0f771e[cfId=0ecdaa87-f8fb-3e60-88d1-74fb36fe5c0d,ksName=system_auth,cfName=role_members,flags=[COMPOUND],params=TableParams{comment=role memberships lookup table, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UTF8Type),partitionColumns=[[] | []],partitionKeyColumns=[role],clusteringColumns=[member],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[role, member],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@4fabdebb[cfId=3afbe79f-2194-31a7-add7-f5ab90d8ec9c,ksName=system_auth,cfName=role_permissions,flags=[COMPOUND],params=TableParams{comment=permissions granted to db roles, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UTF8Type),partitionColumns=[[] | [permissions]],partitionKeyColumns=[role],clusteringColumns=[resource],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[role, resource, permissions],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@7103b8de[cfId=5f2fbdad-91f1-3946-bd25-d5da3a5c35ec,ksName=system_auth,cfName=resource_role_permissons_index,flags=[COMPOUND],params=TableParams{comment=index of db roles with permissions granted on a resource, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UTF8Type),partitionColumns=[[] | []],partitionKeyColumns=[resource],clusteringColumns=[role],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[resource, role],droppedColumns={},triggers=[],indexes=[]]], views=[], functions=[], types=[]}\n",
            "\n",
            "2017-11-13T14:29:25.934019252Z INFO Not submitting build tasks for views in keyspace system_auth as storage service is not initialized\n",
            "\n",
            "2017-11-13T14:29:25.953887674Z INFO Initializing system_auth.resource_role_permissons_index\n",
            "\n",
            "2017-11-13T14:29:25.957358898Z INFO Initializing system_auth.role_members\n",
            "\n",
            "2017-11-13T14:29:25.967935061Z INFO Initializing system_auth.role_permissions\n",
            "\n",
            "2017-11-13T14:29:25.995449692Z INFO Initializing system_auth.roles\n",
            "\n",
            "2017-11-13T14:29:26.193856408Z INFO Netty using native Epoll event loop\n",
            "\n",
            "2017-11-13T14:29:26.247676724Z INFO Using Netty Version: [netty-buffer=netty-buffer-4.0.44.Final.452812a, netty-codec=netty-codec-4.0.44.Final.452812a, netty-codec-haproxy=netty-codec-haproxy-4.0.44.Final.452812a, netty-codec-http=netty-codec-http-4.0.44.Final.452812a, netty-codec-socks=netty-codec-socks-4.0.44.Final.452812a, netty-common=netty-common-4.0.44.Final.452812a, netty-handler=netty-handler-4.0.44.Final.452812a, netty-tcnative=netty-tcnative-1.1.33.Fork26.142ecbb, netty-transport=netty-transport-4.0.44.Final.452812a, netty-transport-native-epoll=netty-transport-native-epoll-4.0.44.Final.452812a, netty-transport-rxtx=netty-transport-rxtx-4.0.44.Final.452812a, netty-transport-sctp=netty-transport-sctp-4.0.44.Final.452812a, netty-transport-udt=netty-transport-udt-4.0.44.Final.452812a]\n",
            "\n",
            "2017-11-13T14:29:26.247705469Z INFO Starting listening for CQL clients on /0.0.0.0:9042 (unencrypted)...\n",
            "\n",
            "2017-11-13T14:29:26.309591159Z INFO Not starting RPC server as requested. Use JMX (StorageService->startRPCServer()) or nodetool (enablethrift) to start it\n",
            "\n",
            "2017-11-13T14:29:36.275846037Z INFO Created default superuser role 'cassandra'\n",
            "\n",
            "2017-11-13T14:29:40.333918591Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}\n",
            "\n",
            "2017-11-13T14:29:40.434399612Z INFO Create new table: org.apache.cassandra.config.CFMetaData@c74a94b[cfId=1572b410-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]\n",
            "\n",
            "2017-11-13T14:29:40.566922871Z INFO Initializing my_keyspace.schema_version\n",
            "\n",
            "2017-11-13T14:29:42.719380089Z INFO Drop Keyspace 'my_keyspace'\n",
            "\n",
            "2017-11-13T14:29:43.124510221Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}\n",
            "\n",
            "2017-11-13T14:29:43.243928493Z INFO Create new table: org.apache.cassandra.config.CFMetaData@1a0616e9[cfId=171e8f50-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]\n",
            "\n",
            "2017-11-13T14:29:43.284700491Z INFO Initializing my_keyspace.schema_version\n",
            "\n",
            "2017-11-13T14:29:44.706916652Z INFO Drop Keyspace 'my_keyspace'\n",
            "\n",
            "2017-11-13T14:29:44.924446999Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}\n",
            "\n",
            "2017-11-13T14:29:44.993983743Z INFO Create new table: org.apache.cassandra.config.CFMetaData@7338ccab[cfId=182996b0-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]\n",
            "\n",
            "2017-11-13T14:29:45.078407254Z INFO Initializing my_keyspace.schema_version\n",
            "\n",
            "2017-11-13T14:29:46.244137923Z INFO Drop Keyspace 'my_keyspace'\n",
            "\n",
            "2017-11-13T14:29:46.500351100Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}\n",
            "\n",
            "2017-11-13T14:29:46.575419551Z INFO Create new table: org.apache.cassandra.config.CFMetaData@229f3694[cfId=191b97d0-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]\n",
            "\n",
            "2017-11-13T14:29:46.617101680Z ERROR Unexpected error during query\n",
            "\n",
            "2017-11-13T14:29:46.617126436Z java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException\n",
            "\n",
            "2017-11-13T14:29:46.617130194Z at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:404) ~[apache-cassandra-3.11.1.jar:3.11.1]\n",
            "\n",
            "2017-11-13T14:29:46.617133358Z at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:549) ~[apache-cassandra-3.11.1.jar:3.11.1]\n",
            "\n",
            "2017-11-13T14:29:46.617135966Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:356) ~[apache-cassandra-3.11.1.jar:3.11.1]\n",
            "\n",
            "2017-11-13T14:29:46.617138576Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:341) ~[apache-cassandra-3.11.1.jar:3.11.1]\n",
            "\n",
            "2017-11-13T14:29:46.617141018Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:321) ~[apache-cassandra-3.11.1.jar:3.11.1]\n",
            "\n",
            "2017-11-13T14:29:46.617143454Z at org.apache.cassandra.cql3.statements.CreateTableStatement.announceMigration(CreateTableStatement.java:89) ~[apache-cassandra-3.11.1.jar:3.11.1]\n",
            "\n",
            "2017-11-13T14:29:46.617145953Z at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:93) ~[apache-cassandra-3.11.1.jar:3.11.1]\n",
            "\n",
            "2017-11-13T14:29:46.617148372Z at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:224) ~[apache-cassandra-3.11.1.jar:3.11.1]\n",
            "\n",
            "2017-11-13T14:29:46.617150806Z at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:255) ~[apache-cassandra-3.11.1.jar:3.11.1]\n",
            "\n",
            "2017-11-13T14:29:46.617153201Z at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:240) ~[apache-cassandra-3.11.1.jar:3.11.1]\n",
            "\n",
            "2017-11-13T14:29:46.617155595Z at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.1.jar:3.11.1]\n",
            "\n",
            "2017-11-13T14:29:46.617157962Z at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.1.jar:3.11.1]\n",
            "\n",
            "2017-11-13T14:29:46.617160377Z at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.1.jar:3.11.1]\n",
            "\n",
            "2017-11-13T14:29:46.617162787Z at io.netty.channel.SimpleChannelInboundHandler.\n",
            "architectural impact: NO\n",
            "comments: [\"I had a look at this and assumed that org.apache.cassandra.schema.SchemaKeyspace#fetchKeyspaceParams() was just not getting any rows returned when it queried the system_schema.keyspaces table. In fact in my testing it was getting a row returned but it only contained the primary key and null's for both other columns. I didn't dig too deep into this but it doesn't seem like it should happen, it's probably worth someone with a more intimate knowledge of the read path taking a look. Also on my machine I could only trigger this exception with multiple clients looping CREATE/DROP commands and it was still relatively rare.  \\r\\n\", \"I just saw this on some tests today as well.  The issue seems to be that the drop is happening concurrently with tables being initialized:\\r\\n\\r\\n{quote}\\r\\n2017-11-13T14:29:40.566922871Z INFO Initializing my_keyspace.schema_version\\r\\n2017-11-13T14:29:42.719380089Z INFO Drop Keyspace 'my_keyspace'\\r\\n2017-11-13T14:29:43.124510221Z INFO Create new Keyspace: \\r\\n{quote}\", '| patch  |  test  | dtest|\\r\\n| [3.0 |https://github.com/apache/cassandra/compare/cassandra-3.0...jasonstack:CASSANDRA-14010-3.0?expand=1 ] |\\r\\n| [3.11 |https://github.com/apache/cassandra/compare/cassandra-3.11...jasonstack:CASSANDRA-14010-3.11?expand=1 ] |\\r\\n| [trunk | https://github.com/apache/cassandra/compare/trunk...jasonstack:CASANDRA-14010-trunk?expand=1] |\\r\\n\\r\\n\\r\\nIt turns out that the query in {{fetchKeyspaceParams()}} gets incomplete data from memtable.\\r\\n\\r\\n{code}\\r\\nprocess:\\r\\n  0. drop ks with ts1 \\r\\n  1. apply create ks mutation with t2 (t2>t1)\\r\\n  2. flush memtables including \"system_schema.keyspaces\" table\\r\\n  3. select keyspace_name from \"system_schema.keyspaces\" table in {{fetchKeyspaceOnly()}} causing \"defragmenting\" (at the end of SPRC.queryMemtableAndSSTablesInTimestampOrder()) to insert the selected data into memtable\\r\\n  4. select * from \"system_schema.keyspaces\" table in {{fetchKeyspaceParams()}} getting incomplete data(row with liveness of t2 and deletion of t1, no regular columns) from memtable. First sstable\\'s maxtimestamp is smaller than memtable data\\'s deletion time(drop ks time, t1) because sstables are sorted by maxTS in ascending order and other newer sstables are skipped...\\r\\n\\r\\nThe correct order is descending to eliminate older sstables.\\r\\n{code}\\r\\n\\r\\nThe patch is to make sure sstables are compared with max-timestamp in descending order...\\r\\n\\r\\nThe reason that it only happened on 3.11 is related to {{queriedColumn in ColumnFilter}} and value skipping added in 3.x.  (a bit complex...)\\r\\n\\r\\nWhen no non-pk column is selected, the {{queried}} columns in ColumnFilter.builder will be initialized as empty, thus when processing the query in #3, unselected columns(eg. durable_wirtes, replication) are skipped in Cell.Serializer: helper.canSkipValue().\\r\\n\\r\\nBut in trunk, due to CASSANDRA-7396, when no non-pk column is selected, the {{queried}} columns in ColumnFilter.builder will be initialized as null, thus unselected columns are not skipped, later put into memtable. (lost the benefit of value skipping)', 'If we ignore the complexity of defragmenting, columnfilter, etc... It can be reproduced easily:\\r\\n{code:title=reproduce}\\r\\n        createTable(\"CREATE TABLE %s (k1 int, v1 int, v2 int, PRIMARY KEY (k1))\");\\r\\n        ColumnFamilyStore cfs = ColumnFamilyStore.getIfExists(keyspace(), currentTable());\\r\\n        cfs.disableAutoCompaction();\\r\\n\\r\\n        execute(\"INSERT INTO %s(k1,v1,v2) VALUES(1,1,1)  USING TIMESTAMP 5\");\\r\\n        cfs.forceBlockingFlush();\\r\\n\\r\\n        execute(\"INSERT INTO %s(k1,v1,v2) VALUES(1,1,2)  USING TIMESTAMP 8\");\\r\\n        cfs.forceBlockingFlush();\\r\\n\\r\\n        execute(\"INSERT INTO %s(k1) VALUES(1)  USING TIMESTAMP 7\");\\r\\n        // deletion 6 shadow sstable-1 with ts=5 ...\\r\\n        execute(\"DELETE FROM %s USING TIMESTAMP 6 WHERE k1 = 1\");\\r\\n\\r\\n        assertRows(execute(\"SELECT * FROM %s WHERE k1=1\"), row(1, 1, 2));\\r\\n{code}\\r\\n', 'CI looks good..\\r\\n\\r\\n', 'Thanks for the patch. The fix looks good :-)\\r\\nSmall nit: The unit test can be simplified by using {{disableCompaction()}} instead of {{cfs.disableAutoCompaction()}} and {{flush()}} instead of {{cfs.forceBlockingFlush()}}. ', '{{SSTableReader.maxTimestampComparator}} is used in LCS:\\r\\n\\r\\n{code}\\r\\n                if (candidates.size() > MAX_COMPACTING_L0)\\r\\n                {\\r\\n                    // limit to only the MAX_COMPACTING_L0 oldest candidates\\r\\n                    candidates = new HashSet<>(ageSortedSSTables(candidates).subList(0, MAX_COMPACTING_L0));\\r\\n                    break;\\r\\n                }\\r\\n...\\r\\n\\r\\n    private List<SSTableReader> ageSortedSSTables(Collection<SSTableReader> candidates)\\r\\n    {\\r\\n        List<SSTableReader> ageSortedCandidates = new ArrayList<>(candidates);\\r\\n        Collections.sort(ageSortedCandidates, SSTableReader.maxTimestampComparator);\\r\\n        return ageSortedCandidates;\\r\\n    }\\r\\n\\r\\n{code}\\r\\n\\r\\nChanging it to be oldest first violates at least the comment and the intent. Probably need to introduce a new {{Comparator<SSTableReader>}} like {{maxTimestampComparatorDescending}}\\r\\n\\r\\n', '[~jjirsa] CASSANDRA-13776 accidentally changed the definition of the maxTimestampComparator while trying to simplify code.\\r\\nFrom CASSANDRA-13776:\\r\\n\\r\\n{code}\\r\\n-    public static final Comparator<SSTableReader> maxTimestampComparator = new Comparator<SSTableReader>()\\r\\n -    {\\r\\n -        public int compare(SSTableReader o1, SSTableReader o2)\\r\\n -        {\\r\\n -            long ts1 = o1.getMaxTimestamp();\\r\\n -            long ts2 = o2.getMaxTimestamp();\\r\\n -            return (ts1 > ts2 ? -1 : (ts1 == ts2 ? 0 : 1));\\r\\n -        }\\r\\n -    };\\r\\n +    public static final Comparator<SSTableReader> maxTimestampComparator = (o1, o2) -> Long.compare(o1.getMaxTimestamp(), o2.getMaxTimestamp());\\r\\n{code}\\r\\n\\r\\nThis is just putting it back like it was before the CASSANDRA-13776.  So this is how it has worked up until 13776 went in.', 'Ok so LCS is wrong, created  CASSANDRA-14099 to follow up there.\\r\\n', 'Thanks for the review.. fix nits and restarted CI.  CI looks good.', 'Committed as {{a9225f90e205a7c2b24a4ad4a32d0961067005b0}} to cassandra-3.0 and merged up to cassandra-3.11 and trunk. Thanks!']\n",
            "my_comment: 1));\n",
            " -        }\n",
            " -    };\n",
            " +    public static final Comparator<SSTableReader> maxTimestampComparator = (o1 o2) -> Long.compare(o1.getMaxTimestamp() o2.getMaxTimestamp());\n",
            "{code}\n",
            "\n",
            "This is just putting it back like it was before the CASSANDRA-13776.  So this is how it has worked up until 13776 went in. Ok so LCS is wrong created  CASSANDRA-14099 to follow up there.\n",
            " Thanks for the review.. fix nits and restarted CI.  CI looks good. Committed as {{a9225f90e205a7c2b24a4ad4a32d0961067005b0}} to cassandra-3.0 and merged up to cassandra-3.11 and trunk. Thanks!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14141\n",
            "issue_type: Improvement\n",
            "summary: Enable CDC unittest\n",
            "description: Follow up for CASSANDRA-14066\n",
            "\n",
            "2 CDC unittests are skipped in the normal test run, it has to be {{$ ant test-cdc}} to run the cdc test.\n",
            "\n",
            "\n",
            "\n",
            "The fix enables them in normal {{$ ant test}}\n",
            "architectural impact: NO\n",
            "comments: ['Here is the patch, please review:\\r\\n| Branch | uTest |\\r\\n| [14141-3.11|https://github.com/cooldoger/cassandra/tree/14141-3.11] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14141-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14141-3.11] |\\r\\n| [14141-trunk|https://github.com/cooldoger/cassandra/tree/14141-trunk] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14141-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14141-trunk] |\\r\\n\\r\\nTested with both {{$ ant test}} and {{$ ant test-cdc}}.', '+1. committed as sha {{5133526733f7ff24062acb5aa57fcfff050ac424}}\\r\\n\\r\\nThanks, Jay!', \"Just a quick note: I wouldn't remove or stop running the {{test-cdc}} target on ci. The reason I kept these completely separate was to fully exercise the rest of the testing infrastructure with cdc enabled to confirm no regression / data loss in the CommitLog from the changed cdc code. Running the unit tests specific to cdc on a regular {{ant test}} run is fine as a smoke test but shouldn't serve as a replacement for running the entire test suite against the changed logic.\", \"bq. Just a quick note: I wouldn't remove or stop running the test-cdc target on ci. The reason I kept these completely separate was to fully exercise the rest of the testing infrastructure with cdc enabled to confirm no regression / data loss in the CommitLog from the changed cdc code. Running the unit tests specific to cdc on a regular ant test run is fine as a smoke test but shouldn't serve as a replacement for running the entire test suite against the changed logic.\\r\\n+1\", 'Yup, 100% agree, [~JoshuaMcKenzie] .', \"This doesn't work in CircleCI as of when it was merged here 0ac5a17d3aac49f678f8b1ec4c934c8a0e7367ab\\r\\n\\r\\n{noformat}\\r\\n    [junit] Testcase: testRetainLinkOnDiscardCDC(org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest):\\tCaused an ERROR\\r\\n    [junit] Rejecting mutation to keyspace cql_test_keyspace. Free up space in build/test/cassandra/cdc_raw:0 by processing CDC logs.\\r\\n    [junit] org.apache.cassandra.exceptions.CDCWriteException: Rejecting mutation to keyspace cql_test_keyspace. Free up space in build/test/cassandra/cdc_raw:0 by processing CDC logs.\\r\\n    [junit] \\tat org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDC.throwIfForbidden(CommitLogSegmentManagerCDC.java:136)\\r\\n    [junit] \\tat org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDC.allocate(CommitLogSegmentManagerCDC.java:108)\\r\\n    [junit] \\tat org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:272)\\r\\n    [junit] \\tat org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:603)\\r\\n    [junit] \\tat org.apache.cassandra.db.Keyspace.apply(Keyspace.java:480)\\r\\n    [junit] \\tat org.apache.cassandra.db.Mutation.apply(Mutation.java:194)\\r\\n    [junit] \\tat org.apache.cassandra.db.Mutation.apply(Mutation.java:199)\\r\\n    [junit] \\tat org.apache.cassandra.db.Mutation.apply(Mutation.java:208)\\r\\n    [junit] \\tat org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest.testRetainLinkOnDiscardCDC(CommitLogSegmentManagerCDCTest.java:256)\\r\\n    [junit]\\r\\n    [junit]\\r\\n    [junit] Testcase: testCompletedFlag(org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest):\\tFAILED\\r\\n    [junit] Index file not written: build/test/cassandra/cdc_raw:0/CommitLog-7-1517000670440_cdc.idx\\r\\n    [junit] junit.framework.AssertionFailedError: Index file not written: build/test/cassandra/cdc_raw:0/CommitLog-7-1517000670440_cdc.idx\\r\\n    [junit] \\tat org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest.testCompletedFlag(CommitLogSegmentManagerCDCTest.java:210)\\r\\n{noformat}\\r\\n\\r\\nI'm going to try it with the circle.yml on trunk just to see what is up. I am wondering if this test requires more disk space to pass? \", \"I think this may be disk space related. We suddenly lost access to the larger volume in CircleCI that is supposed to be mounted in the container but it's no longer appearing.\", \"Nope not disk space. Also passes when run manually from the container. Reliably fails every time when it's run automatically. \\r\\n\\r\\nThis just seems to be generally flaky on both my laptop and CircleCI.\"]\n",
            "my_comment: build/test/cassandra/cdc_raw:0/CommitLog-7-1517000670440_cdc.idx\n",
            "    [junit] \\tat org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest.testCompletedFlag(CommitLogSegmentManagerCDCTest.java:210)\n",
            "{noformat}\n",
            "\n",
            "Im going to try it with the circle.yml on trunk just to see what is up. I am wondering if this test requires more disk space to pass? \" \"I think this may be disk space related. We suddenly lost access to the larger volume in CircleCI that is supposed to be mounted in the container but its no longer appearing.\" \"Nope not disk space. Also passes when run manually from the container. Reliably fails every time when its run automatically. \n",
            "\n",
            "This just seems to be generally flaky on both my laptop and CircleCI.\"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14319\n",
            "issue_type: Improvement\n",
            "summary: nodetool rebuild from DC lets you pass invalid datacenters \n",
            "description: If you pass an invalid datacenter to nodetool rebuild, you'll get an error like this:\n",
            "\n",
            "\n",
            "\n",
            "{code}\n",
            "\n",
            "Unable to find sufficient sources for streaming range (3074457345618258602,-9223372036854775808] in keyspace system_distributed\n",
            "\n",
            "{code}\n",
            "\n",
            "\n",
            "\n",
            "Unfortunately, this is a rabbit hole of frustration if you are using caps for your DC names and you pass in a lowercase DC name, or you just typo the DC.  \n",
            "\n",
            "\n",
            "\n",
            "Let's do the following:\n",
            "\n",
            "\n",
            "\n",
            "# Check the DC name that's passed in against the list of DCs we know about\n",
            "\n",
            "# If we don't find it, let's output a reasonable error, and list all the DCs someone could put in.\n",
            "\n",
            "# Ideally we indicate which keyspaces are set to replicate to this DC and which aren't\n",
            "architectural impact: NO\n",
            "comments: [\"Hi [~rustyrazorblade],\\r\\n\\r\\nAre you looking for something like below when we provide invalid datacenter as an option to rebuild?\\r\\n{code:java}\\r\\n$ bin/nodetool rebuild no_dc\\r\\nnodetool: Provided datacenter: no_dc is not a valid datacenter, available datacenters are: datacenter1, datacenter2\\r\\nSee 'nodetool help' or 'nodetool help <command>'.\\r\\n$ \\r\\n{code}\\r\\nAnd for\\r\\n{quote}3. Ideally, we indicate which keyspaces are set to replicate to this DC and which aren't\\r\\n{quote}\\r\\nAre you referring {{this}} as the datacenter from where the {{rebuild}} command is being executed from or the one which is provided as an option (e.g., {{no_dc}} in above example)? if it is the later, invalid dc would not have any keyspaces, so what is expected for #3 in this scenario?\\r\\n\\r\\n\\xa0\\r\\n\\r\\nI am interested in working on this ticket. Can you assign this ticket to me?\\xa0\", 'Hi [~rustyrazorblade],\\r\\n\\r\\nI took a stab at it and implemented #1 and #2 from the list above, attached the patch with this implementation. Waiting for more information on #3. ', \"I'm not sure #3 is something that we can do as you pointed out, but I'm +1 on what you have.\", 'The fix looks good to me. I think it would be good to have some test to verify the behavior. \\r\\n[~vinaykumarcse] could you add a test to your patch? ', '[~vinaykumarcse] do you plan to finish this? Am I ok to assign this to myself? It seems like this patch is abandoned.', 'Assigning to myself. [~vinaykumarcse] feel free to take it from me if you feel like it though.', 'I believe the logic should be moved a little bit up so we fail earlier, in StorageService.rebuild method before anything is actually executed where all other checks are.\\r\\n\\r\\nhttps://github.com/apache/cassandra/pull/2309', 'trunk [https://github.com/apache/cassandra/pull/2309]\\r\\nj11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2185/workflows/c0b0e974-fdb1-4410-a180-fc8890c9a7e5]\\r\\nj8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2185/workflows/4fc8308c-3f07-4b41-9ad3-3a20a547c0e9]\\r\\n\\r\\n4.1 [https://github.com/apache/cassandra/pull/2323]\\r\\nj11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2194/workflows/e6aee8b6-95e7-446f-879d-a66bb4275255]\\r\\nj8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2194/workflows/bae89aec-bbe4-4705-bcdf-fc4fdecbfe3a]\\r\\n\\r\\n4.0 [https://github.com/apache/cassandra/pull/2324]\\r\\nj11 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2195/workflows/4e9e290d-9ada-4aa7-a2a9-68d66b1961fe\\r\\nj8 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2195/workflows/87cb1f9a-b71e-414b-b270-81d934390ab0\\r\\n\\r\\n3.11 [https://github.com/apache/cassandra/pull/2325]\\r\\nj8 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2196/workflows/7d692de2-3f73-4247-b4ec-fb8f6031c681\\r\\n\\r\\n3.0 [https://github.com/apache/cassandra/pull/2332]\\r\\nj8 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2197/workflows/831b7247-b5aa-4402-b9e9-9748602240f5', '[~brandon.williams] [~blerer] could you please review? Thank you.', '+1 from me.', \"This seems to set {{isRebuilding}} to false when validating the input or failing to {{isRebuilding.compareAndSet}} as part of the {{try/catch}} that is introduced. Just because this thread is not able to proceed with rebuild doesn't mean another thread isn't still proceeding.\\r\\n\\r\\nI think the new {{try/catch}} should be removed and the {{isRebuilding.compareAndSet}} should be in the existing {{try/catch}} that sets {{isRebuilding}} to {{false}} if actually doing the rebuild fails.\", 'Ah, good catch, I will take a closer look in a moment. We will need a new ticket for this though.\\n\\n\\nSent from ProtonMail mobile\\n\\n\\n\\n\\\\', \"What I said about moving the {{isRebuilding.compareAndSet}} into the {{try/catch}} doesn't make sense. It should be outside the {{try/catch}} but after all the up front validation is complete.\", '[~aweisberg] you mean it like this? https://github.com/apache/cassandra/blob/fe004912a829151a4ac3c3ec0d267fc74938953e/src/java/org/apache/cassandra/service/StorageService.java#L1309-L1328\\r\\n\\r\\nhttps://github.com/apache/cassandra/pull/2637', 'I prepared the patches in CASSANDRA-18803']\n",
            "my_comment: datacenter1 datacenter2\n",
            "See nodetool help or nodetool help <command>.\n",
            "$ \n",
            "{code}\n",
            "And for\n",
            "{quote}3. Ideally we indicate which keyspaces are set to replicate to this DC and which arent\n",
            "{quote}\n",
            "Are you referring {{this}} as the datacenter from where the {{rebuild}} command is being executed from or the one which is provided as an option (e.g. {{no_dc}} in above example)? if it is the later invalid dc would not have any keyspaces so what is expected for #3 in this scenario?\n",
            "\n",
            "\\xa0\n",
            "\n",
            "I am interested in working on this ticket. Can you assign this ticket to me?\\xa0\" Hi [~rustyrazorblade]\n",
            "\n",
            "I took a stab at it and implemented #1 and #2 from the list above attached the patch with this implementation. Waiting for more information on #3.  \"Im not sure #3 is something that we can do as you pointed out but Im +1 on what you have.\" The fix looks good to me. I think it would be good to have some test to verify the behavior. \n",
            "[~vinaykumarcse] could you add a test to your patch?  [~vinaykumarcse] do you plan to finish this? Am I ok to assign this to myself? It seems like this patch is abandoned. Assigning to myself. [~vinaykumarcse] feel free to take it from me if you feel like it though. I believe the logic should be moved a little bit up so we fail earlier in StorageService.rebuild method before anything is actually executed where all other checks are.\n",
            "\n",
            "https://github.com/apache/cassandra/pull/2309 trunk [https://github.com/apache/cassandra/pull/2309]\n",
            "j11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2185/workflows/c0b0e974-fdb1-4410-a180-fc8890c9a7e5]\n",
            "j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2185/workflows/4fc8308c-3f07-4b41-9ad3-3a20a547c0e9]\n",
            "\n",
            "4.1 [https://github.com/apache/cassandra/pull/2323]\n",
            "j11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2194/workflows/e6aee8b6-95e7-446f-879d-a66bb4275255]\n",
            "j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2194/workflows/bae89aec-bbe4-4705-bcdf-fc4fdecbfe3a]\n",
            "\n",
            "4.0 [https://github.com/apache/cassandra/pull/2324]\n",
            "j11 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2195/workflows/4e9e290d-9ada-4aa7-a2a9-68d66b1961fe\n",
            "j8 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2195/workflows/87cb1f9a-b71e-414b-b270-81d934390ab0\n",
            "\n",
            "3.11 [https://github.com/apache/cassandra/pull/2325]\n",
            "j8 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2196/workflows/7d692de2-3f73-4247-b4ec-fb8f6031c681\n",
            "\n",
            "3.0 [https://github.com/apache/cassandra/pull/2332]\n",
            "j8 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2197/workflows/831b7247-b5aa-4402-b9e9-9748602240f5 [~brandon.williams] [~blerer] could you please review? Thank you. +1 from me. \"This seems to set {{isRebuilding}} to false when validating the input or failing to {{isRebuilding.compareAndSet}} as part of the {{try/catch}} that is introduced. Just because this thread is not able to proceed with rebuild doesnt mean another thread isnt still proceeding.\n",
            "\n",
            "I think the new {{try/catch}} should be removed and the {{isRebuilding.compareAndSet}} should be in the existing {{try/catch}} that sets {{isRebuilding}} to {{false}} if actually doing the rebuild fails.\" Ah good catch I will take a closer look in a moment. We will need a new ticket for this though.\n",
            "\n",
            "\n",
            "Sent from ProtonMail mobile\n",
            "\n",
            "\n",
            "\n",
            "\\\\ \"What I said about moving the {{isRebuilding.compareAndSet}} into the {{try/catch}} doesnt make sense. It should be outside the {{try/catch}} but after all the up front validation is complete.\" [~aweisberg] you mean it like this? https://github.com/apache/cassandra/blob/fe004912a829151a4ac3c3ec0d267fc74938953e/src/java/org/apache/cassandra/service/StorageService.java#L1309-L1328\n",
            "\n",
            "https://github.com/apache/cassandra/pull/2637 I prepared the patches in CASSANDRA-18803\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14411\n",
            "issue_type: Bug\n",
            "summary: Use Bounds instead of Range to represent sstable first/last token when checking how to anticompact sstables\n",
            "description: There is currently a chance of missing marking a token as repaired due to the fact that we use Range which are (a, b] to represent first/last token in sstables instead of Bounds which are [a, b].\n",
            "architectural impact: NO\n",
            "comments: ['https://github.com/krummas/cassandra/commits/marcuse/14411\\r\\n\\r\\ntests:\\r\\nhttps://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14411', 'minimal patches for 2.2 -> 3.11: \\r\\nhttps://github.com/krummas/cassandra/commits/marcuse/14411-2.2\\r\\nhttps://github.com/krummas/cassandra/commits/marcuse/14411-3.0\\r\\nhttps://github.com/krummas/cassandra/commits/marcuse/14411-3.11\\r\\n\\r\\ntests for 3.11: https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14411-3%2E11', 'circle seems to be down, but +1 assuming tests look good', 'committed as {{334dca9aa825e6d353aa04fd97016ac1077ff132}}, thanks!']\n",
            "my_comment: https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14411-3%2E11 circle seems to be down but +1 assuming tests look good committed as {{334dca9aa825e6d353aa04fd97016ac1077ff132}} thanks!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14647\n",
            "issue_type: Bug\n",
            "summary: Reading cardinality from Statistics.db failed\n",
            "description: There is some issue with sstable metadata which is visible in system.log, the messages says:\n",
            "\n",
            "{noformat}\n",
            "\n",
            "WARN  [Thread-6] 2018-07-25 07:12:47,928 SSTableReader.java:249 - Reading cardinality from Statistics.db failed for /opt/data/disk5/data/keyspace/table/mc-big-Data.db.{noformat}\n",
            "\n",
            "Although there is no such file. \n",
            "\n",
            "\n",
            "\n",
            "The message has appeared after i've changed the compaction strategy from SizeTiered to Leveled. Compaction strategy has been changed region by region (total 3 regions) and it has coincided with the double client write traffic increase.\n",
            "\n",
            " I have tried to run nodetool scrub to rebuilt the sstable, but that does not fix the issue.\n",
            "\n",
            "\n",
            "\n",
            "So very hard to define the steps to reproduce, probably it will be:\n",
            "\n",
            " # run stress tool with write traffic\n",
            "\n",
            " # under load change compaction strategy from SireTiered to Leveled for the bunch of hosts\n",
            "\n",
            " # add more write traffic\n",
            "\n",
            "\n",
            "\n",
            "Reading the code it is said that if this metadata is broken, then \"estimating the keys will be done using index summary\". \n",
            "\n",
            " [https://github.com/apache/cassandra/blob/cassandra-3.0.17/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java#L247]\n",
            "\n",
            "  \n",
            "architectural impact: NO\n",
            "comments: ['Hey Vitali, thanks for the report. [~krummas] any idea what is going on here? Would changing compaction strategies cause this issue?', '[~nezdali] could you post logs?', 'This is not due to STCS -> LCS. I have the same behavior on one cluster with LCS and heavy writes.\\xa0STCS has never been configured on it.', 'Looks like this can happen when the table metric `EstimatedPartitionCount` is [queried |https://github.com/apache/cassandra/blob/d049c6b9b4af4f663aac2bf90d860c3b0c20684a/src/java/org/apache/cassandra/metrics/TableMetrics.java#L307] - it grabs the CANONICAL sstables without referencing them, so if there are very many sstables some might get compacted away while calculating the partition count and we get this warning\\r\\n\\r\\nIf this is the case it is not really a problem (other than the annoying warn message in the log files)\\r\\n\\r\\n[~rha] could you verify that you are querying this metric?\\r\\n\\r\\n[~rha] / [~nezdali] could you pause querying this metric and check if the error stops appearing?\\r\\n\\r\\nThanks for providing the logs over email btw [~nezdali]', 'Thanks Marcus. Stopped quering\\xa0EstimatedPartitionCount metric from on 1 node.', 'Unfortunately this did no help, the failed cardinality message is still in the log.', \"I do query partition count through Datadog JMX agent, so it happens all the time.\\xa0I'll try to disabled it - although it didn't work for [~nezdali]\", \"After removing {{EstimatedPartitionCount}} from Datadog's cassandra.yaml on one node, I don't see any warning for 4 days.\\r\\n\\r\\n[~nezdali] can you double check that your change was effective (e.g. monitoring service restarted, etc.)?\\xa0\", \"there is an alias for the metric called {{EstimatedRowCount}} which should also be disabled\\r\\n\\r\\nI'll work on a fix\", 'Tested again, and there are no more warning in the log for the whole day. Turned out that monitoring system is querying all keys via jmx and then filters them based on the configuration file.', '[~krummas] I have created a patch for this and uploaded to this ticket\\xa0 [^14647-trunk-1.patch]', \"The {{TableMetrics}} change looks good to me (grabbing refs to the sstables before checking key count), but I don't think we need the {{SSTableReader.refAndGetApproximateKeyCount}} part - in the cases this is called we always already have a ref to the sstable.\", '[~krummas] Updated the patch.\\xa0\\r\\n||Branch||CircleCI||\\r\\n|[14647-trunk|https://github.com/apache/cassandra/compare/trunk...nvharikrishna:14647-trunk]|[link|https://circleci.com/gh/nvharikrishna/cassandra/3#tests/containers/2]|', '+1\\r\\n\\r\\nre-ran the tests and failures look unrelated;\\r\\n\\r\\nhttps://circleci.com/workflow-run/264bb5cb-fd18-4ea2-8b1d-768d9ab96d94', 'sorry for the delay on this - committed as {{ded62076e7fdfd1cfdcf96447489ea607ca796a0}} to trunk']\n",
            "my_comment: Hey Vitali thanks for the report. [~krummas] any idea what is going on here? Would changing compaction strategies cause this issue? [~nezdali] could you post logs? This is not due to STCS -> LCS. I have the same behavior on one cluster with LCS and heavy writes.\\xa0STCS has never been configured on it. Looks like this can happen when the table metric `EstimatedPartitionCount` is [queried |https://github.com/apache/cassandra/blob/d049c6b9b4af4f663aac2bf90d860c3b0c20684a/src/java/org/apache/cassandra/metrics/TableMetrics.java#L307] - it grabs the CANONICAL sstables without referencing them so if there are very many sstables some might get compacted away while calculating the partition count and we get this warning\n",
            "\n",
            "If this is the case it is not really a problem (other than the annoying warn message in the log files)\n",
            "\n",
            "[~rha] could you verify that you are querying this metric?\n",
            "\n",
            "[~rha] / [~nezdali] could you pause querying this metric and check if the error stops appearing?\n",
            "\n",
            "Thanks for providing the logs over email btw [~nezdali] Thanks Marcus. Stopped quering\\xa0EstimatedPartitionCount metric from on 1 node. Unfortunately this did no help the failed cardinality message is still in the log. \"I do query partition count through Datadog JMX agent so it happens all the time.\\xa0Ill try to disabled it - although it didnt work for [~nezdali]\" \"After removing {{EstimatedPartitionCount}} from Datadogs cassandra.yaml on one node I dont see any warning for 4 days.\n",
            "\n",
            "[~nezdali] can you double check that your change was effective (e.g. monitoring service restarted etc.)?\\xa0\" \"there is an alias for the metric called {{EstimatedRowCount}} which should also be disabled\n",
            "\n",
            "Ill work on a fix\" Tested again and there are no more warning in the log for the whole day. Turned out that monitoring system is querying all keys via jmx and then filters them based on the configuration file. [~krummas] I have created a patch for this and uploaded to this ticket\\xa0 [^14647-trunk-1.patch] \"The {{TableMetrics}} change looks good to me (grabbing refs to the sstables before checking key count) but I dont think we need the {{SSTableReader.refAndGetApproximateKeyCount}} part - in the cases this is called we always already have a ref to the sstable.\" [~krummas] Updated the patch.\\xa0\n",
            "||Branch||CircleCI||\n",
            "|[14647-trunk|https://github.com/apache/cassandra/compare/trunk...nvharikrishna:14647-trunk]|[link|https://circleci.com/gh/nvharikrishna/cassandra/3#tests/containers/2]| +1\n",
            "\n",
            "re-ran the tests and failures look unrelated;\n",
            "\n",
            "https://circleci.com/workflow-run/264bb5cb-fd18-4ea2-8b1d-768d9ab96d94 sorry for the delay on this - committed as {{ded62076e7fdfd1cfdcf96447489ea607ca796a0}} to trunk\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-14705\n",
            "issue_type: Improvement\n",
            "summary: ReplicaLayout follow-up\n",
            "description: Clarify the new {{ReplicaLayout}} code, separating it into ReplicaPlan (for what we want to do) and {{ReplicaLayout}} (for what we know about the cluster), with well defined semantics (and comments in the rare cases those semantics are weird)\n",
            "\n",
            "\n",
            "\n",
            "Found and fixed some bugs:\n",
            "\n",
            " * {{commitPaxos}} was using only live nodes, when needed to include down\n",
            "\n",
            " * We were not writing to pending transient replicas\n",
            "\n",
            " * On write, we were not hinting to full nodes with transient replication enabled (since we filtered to {{liveOnly}}, in order to include our transient replicas above {{blockFor}})\n",
            "\n",
            " * If we speculated, in {{maybeSendAdditionalReads}} (in read repair) we would only consult the same node we had speculated too. This also applied to {{maybeSendAdditionalWrites}} - and this issue was also true pre-TR.\n",
            "\n",
            " * Transient->Full movements mishandled consistency level upgrade\n",
            "\n",
            " ** While we need to treat a transitioning node as ‘full’ for writes, so that it can safely begin serving full data requests once it has finished, we cannot maintain it in the ‘pending’ collection else we will also increase our consistency requirements by a node that doesn’t exist.\n",
            "architectural impact: NO\n",
            "comments: ['[trunk|https://github.com/belliottsmith/cassandra/tree/14705]  [CI|https://circleci.com/workflow-run/4f121d12-86b5-48c6-a8e2-b78597ef47c2]', 'I\\'ve tried to split the patch into digestible chunks.  \\r\\n\\r\\nThe initial two commits are just to get most of the changes that touch a lot of files out of the way, to reduce the cognitive burden of review.  They consist of only a handful of IDE refactors (and a fairly safe find/replace of replicaLayout->replicaPlan).  You can probably ignore these commits almost entirely.\\r\\n\\r\\nThe \"main refactor\" commit has the guts of the work.\\r\\n* Introduce {{ReplicaPlan}}, made up of\\r\\n** two {{ReplicaLayout}}, for the {{live}} and {{liveAndDown}} replicas (maybe we will want to lazily build a separate {{downOnly}} in future, but I\\'ve kept it simple for now)\\r\\n** two {{Endpoints}}, containing the candidate replicas and those we intend to contact; there are now comments clearly defining what the contents of each are in any given scenario.  It\\'s probably the case there were some bugs introduced wrt totalEndpoints before this, but I didn\\'t waste time corroborating.\\r\\n* There are now separate {{ReplicaPlan}} and {{ReplicaLayout}} for each of read/write and token/range, and we enforce only the correct type be provided to each recipient.  {{ReplicaLayout}} only supports {{natural()}} for read operations, so it is impossible to invoke {{pending()}} here, or think you have {{pending()}} nodes when you do not.  <*Open question*: should we disable {{all()}} for read queries, and only support it for writes?  Haven\\'t tried, and might get a little ugly, but might be conceptually cleaner>\\r\\n* {{ReplicaCollection}}\\r\\n** I have caved and implemented a {{count}} method, because there is none in {{Iterables}}.   I\\'m actually wavering on the review feedback I gave to [~aweisberg]\\'s original version of this patch (to not implement all the {{Iterables}} method equivalents), for two reasons: we now use these _extensively_, and we also have only one implementation, so it could be very cheap to implement e.g. a {{filterLazily}} and {{\\\\{any,all,none\\\\}Match}}\\r\\n** I have removed {{select()}} because the correct logic did not benefit from this abstraction.  I have, however, renamed the {{keep()}} method in {{Endpoints}} that re-orders the contents to {{select()}}, to make clear the distinction of semantics.\\r\\n\\r\\nThen there are a few follow up commits to either fix bugs or perform some follow up clean ups.\\r\\n\\r\\nLooking forward to your feedback.', 'FYI, my fix for {{maybeSendAdditionalReads}} is incomplete, I will follow up with it on Monday, but it will be very minor.', \"[~ifesdjeen] that branch you linked to in your PR is the wrong one, it's 14705\", '[~aweisberg] sorry, it was right until branch was renamed and switched)\\r\\n\\r\\nThank you for the patch. Some comments, all rather minor:\\r\\n\\r\\n  * we could rename {{liveOnly}} to {{live}} unless \"only\" here bears \\r\\n  * {{allUncontactedCandidates}} could be just {{uncontactedCandidates}} \\r\\n  * {{ForTokenWrite}} can be just {{ForWrite}} since we\\'ve removed {{ForRangeWrite}}, similarly in {{forTokenWriteLiveAndDown}} and {{forTokenWrite}}\\r\\n  * we can avoid iterating in {{ReplicaPlan#forWrite}} checking for {{isTransient}} and short-circuit it for non-transient keyspace with {{CL#hasTransient}}.\\r\\n * in {{EndpointsFor(Range|Token)}} we have checks for range/token so they matched for concatenated ranges. Should we apply same checks when constructing layouts?\\r\\n  * I know it was same before the patch and we can do it in a separate one, but {{isSufficientReplicasForRead}} and {{assureSufficientReplicas}} share the logic which potentially makes it prone to modifications just in one place and not the other. Looks like we could combine the two. In same methods, naming is either liveReplicas or allLive for same argument. \\r\\n  * {{assureSufficientReplicas}} might need word \"live\" in it\\r\\n  * {{ReplicaCount}} can now use the new {{count}} method\\r\\n  * {{ReplicaPlans#maybeMerge}} is currently checking for {{isSufficientReplicasForRead}} and then proceeds with filtering the nodes, unlike token reads, which filter and then check. Since this is the only place {{isSufficientReplicasForRead}}, this might be an opportunity for consolidation.\\r\\n\\r\\nAnd some that we\\'ve discussed offline for history:\\r\\n  * looks like you\\'re half a step away from removing all the logic from {{ConsistencyLevel}}, which is great. Theoretically, all static methods from there could be moved to {{ReplicaPlan}} and things like \"filterForQuery\" could have stronger invariants, such as {{ForRead<Endpoints<?>>}}, same is true for {{isSufficientReplicasForRead/write}}, especially is applicable for write since we have to pass {{Endpoints}} separately there even though they\\'re technically in the same bucket of {{ForWrite}}.\\r\\n  * we\\'ve discussed it offline and it\\'s probably worth to do in a follow-up ticket, but collecting replicas and {{ReplicaLayouts}} in general might be worth to cover with tests. They\\'re tested indirectly, however mostly through dtests. Maybe flushing out some things, especially related to pending nodes (which are generally hard to test), might be good candidates there.', 'Thanks.\\xa0 I vacillated over a number of these naming decisions myself, so I\\'m happy to accept your suggestions. \\xa0I\\'ll respond to just a few points:\\r\\n{quote}we can avoid iterating in {{ReplicaPlan#forWrite}} checking for {{isTransient}} and short-circuit it for non-transient keyspace with {{CL#hasTransient}}.\\r\\n{quote}\\r\\nI\\'m not sure we can - this would be a race condition on the Keyspace. \\xa0Very rare, admittedly, but worth avoiding. Perhaps we could cache the value of this inside of Endpoints, though, and introduce a {{hasTransient()}}\\xa0method?\\xa0 Ideally, we will later cache the {{ReplicaLayout}}\\xa0(or perhaps even {{ReplicaPlan}}) at which point this would short-circuit this work for all queries.\\r\\n{quote}in {{EndpointsFor(Range|Token)}} we have checks for range/token so they matched for concatenated ranges. Should we apply same checks when constructing layouts?\\r\\n{quote}\\r\\nNot quite sure what you mean here? \\xa0That, for an AbstractBounds, we fully cover it? \\xa0I did wonder about this, but there\\'s actually potential race conditions in the code where we generate our ranges to query, and actually query them, where we might fail this check. \\xa0But this race condition is inherent to the way we do range ownerships anyway, so not a lot of point in killing queries today, I reckon. \\xa0This obviously doesn\\'t apply to Token, since they\\xa0are the unit by which range ownerships can move.\\r\\n{quote}I know it was same before the patch and we can do it in a separate one, but {{isSufficientReplicasForRead}} and {{assureSufficientReplicas}} share the logic which potentially makes it prone to modifications just in one place and not the other. Looks like we could combine the two. In same methods, naming is either liveReplicas or allLive for same argument.\\r\\n{quote}\\r\\nI agree, but at present it would be a bit painful to modify, because {{assureSufficientReplicas}}\\xa0throws exceptions that utilise the computations. \\xa0I guess we could\\xa0have a shared method that accepts a boolean indicating if we should throw, if you like? \\xa0My intention was that all of this would be cleaned up in a later Jira, where we might address the overall management of \\'sufficiency\\' - as this is dotted all over resolvers, write handlers, CL, read-repair...\\r\\n{quote}{{ReplicaCount}} can now use the new {{count}} method\\r\\n{quote}\\r\\nI don\\'t follow? \\xa0Do you mean {{ReplicaCollection.count()}} - if so, this only returns an int, and we need two ints...\\r\\n{quote}{{ReplicaPlans#maybeMerge}} is currently checking for {{isSufficientReplicasForRead}} and then proceeds with filtering the nodes, unlike token reads, which filter and then check. Since this is the only place {{isSufficientReplicasForRead}}, this might be an opportunity for consolidation.\\r\\n{quote}\\r\\nThese checks are a little\\xa0different - we\\'re testing to see if two\\xa0sufficient plans can be merged into one, so a quick check to confirm sufficiency is, well, sufficient. \\xa0We aren\\'t then performing a later check to assert sufficiency, because if they aren\\'t we just use the two separate\\xa0plans. \\xa0That said, we could make the\\xa0structure of the code similar, by checking after construction - but I assume the point of this\\xa0was originally to avoid generating more garbage than necessary.\\r\\n{quote}looks like you\\'re half a step away from removing all the logic from {{ConsistencyLevel}}, which is great. Theoretically, all static methods from there could be moved to {{ReplicaPlan}} and things like \"filterForQuery\" could have stronger invariants, such as {{ForRead<Endpoints<?>>}}, same is true for {{isSufficientReplicasForRead/write}}, especially is applicable for write since we have to pass {{Endpoints}} separately there even though they\\'re technically in the same bucket of {{ForWrite}}.\\r\\n{quote}\\r\\nAgreed entirely, CL should ideally return to being mostly just an enum. \\xa0Since most of its\\xa0logic is for tracking sufficiency (either in the is/assert checks, or in resolvers, write handlers, etc.), when we address that we can move all of the logic out of CL. \\xa0I also already have a follow-up bug fix that moves {{filterForQuery}}\\xa0into {{ReplicaPlans}}\\r\\n{quote}we\\'ve discussed it offline and it\\'s probably worth to do in a follow-up ticket, but collecting replicas and {{ReplicaLayouts}} in general might be worth to cover with tests. They\\'re tested indirectly, however mostly through dtests. Maybe flushing out some things, especially related to pending nodes (which are generally hard to test), might be good candidates there.\\r\\n{quote}\\r\\nI will try to rustle up some unit\\xa0tests for sure, though you may be right that a follow-up ticket is best, so we can get this committed ASAP.', \"bq. I'm not sure we can - this would be a race condition on the Keyspace.\\r\\n\\r\\nTrue, I haven't thought of it. We should probably try to make replication factor immutable for the time of query and attached to replica plan at some point in future.\\r\\n\\r\\nbq. Not quite sure what you mean here?  That, for an AbstractBounds, we fully cover it?\\r\\n\\r\\nI mostly meant something like [this|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/locator/EndpointsForRange.java#L94-L96]. Maybe it's actually good to fail queries where we have collected half the replicas from one token metadata and the other one from other? Not sure.\\r\\n\\r\\nbq.  I guess we could have a shared method that accepts a boolean indicating if we should throw, if you like? \\r\\n\\r\\nWe could also leave it for later, really. It's rather minor and I'm not very worried about it right now, just thought it's worth persisting it.\\r\\n\\r\\nbq. ReplicaCount can now use the new count method\\r\\n\\r\\nSorry, I was thinking about something different, let's leave it as is.\\r\\n\\r\\nbq.  so a quick check to confirm sufficiency is, well, sufficient. \\r\\n\\r\\nRight, agreed. Maybe if we used a boolean for check and threw if it returned {{false}}, we could still consolidate those, but this is also not a huge win.\", \"{quote}Maybe it's actually good to fail queries where we have collected half the replicas from one token metadata and the other one from other? Not sure.\\r\\n{quote}\\r\\nYeah, not sure either, but since we're already exposed (generally) to range ownership races, it's _probably_\\xa0better to hold off until we fix those across the board IMO, to least surprise our users.\", 'Thank you for the changes & discussions. \\r\\n\\r\\n+1, LGTM.', 'These two are outdated so you will need to click view outdated, but there is https://github.com/apache/cassandra/pull/265#pullrequestreview-153925151 and https://github.com/apache/cassandra/pull/265#pullrequestreview-153914279 which both seem to not be responded to.\\r\\n\\r\\nThe first is the one that really matters because I think not understanding why there are multiple plans is an issue.', \"Not loving the GitHub comments experience at the moment. \\xa0I've responded to the comments I could find.\", \"If you could remove that one code comment, maybe clean it up to only refer to the one ReplicaPlan and do something about the other TODOs that need to either be done, removed, or JIRAed. That's a good policy for TODOs right?\\r\\n\\r\\n+1 after that.\\r\\n\\r\\n\", \"So, my personal view on TODOs is that they can be much more useful left inline with their context, for when you or somebody else next comes to touch the surrounding code\\xa0- whether in the intended JIRA, or\\xa0another related one. \\xa0They're also a bit specific for a JIRA, they're more guidance for what to consider when\\xa0shaping your next solution.\\r\\n\\r\\nThey're like documentation, only they document inadequacy, things for the next author to watch out for, and a suggestion that (if time permits)\\xa0they\\xa0resolve them at the same time. \\xa0\\r\\n\\r\\nThere are multiple\\xa0JIRA filed already for these code paths, any of which might hopefully\\xa0encompass refactoring / fixing these TODOs.\\r\\n{quote}If you could remove that one code comment, maybe clean it up to only refer to the one ReplicaPlan\\r\\n{quote}\\r\\nWhich one? \\xa0The one I saw was already missing from the code, hence the comment being\\xa0out of date for the PR (and my missing it) on GitHub UI.\", \"[This is what I was talking about|https://github.com/apache/cassandra/pull/265/files#diff-0246c72855070863c2fdbee6d97f494dR63]. Couldn't this refer to the CL from the parameter ReplicaPlan? Can you remove the TODO?\", \"Ah, thanks. \\xa0I must have gone amiss somehow with the summary\\xa0GitHub UI. \\xa0It does look like we can just use the provided replica plan safely, so I've removed the CL parameters.\", \"So, I've pushed a squashed and rebased version [here|https://github.com/belliottsmith/cassandra/tree/14705-rebase], however I'm considering on last modification - Alex's most recent patches mean we can\\xa0perhaps get rid of the ReplicaPlan.Shared concept. \\xa0I may try, and see how it turns out. \\xa0Previously we needed to construct the readRepair upfront, but now we could construct it on demand. \\xa0It might be that it is a useful abstraction anyway,\\xa0since we at least make it clear that these are modified. \\xa0But it would actually localise more the scope of who\\xa0can see what modifications to a single class.\", '+1 unless you want to do more changes as part of this.', 'This got merged as [047bcd7ad171d6a4aa89128c5e6c6ed5f012b1c0|https://github.com/apache/cassandra/commit/047bcd7ad171d6a4aa89128c5e6c6ed5f012b1c0]?\\r\\n\\r\\nIs this issue ready to resolve?', 'Yes, thanks.']\n",
            "my_comment: we now use these _extensively_ and we also have only one implementation so it could be very cheap to implement e.g. a {{filterLazily}} and {{\\\\{anyallnone\\\\}Match}}\n",
            "** I have removed {{select()}} because the correct logic did not benefit from this abstraction.  I have however renamed the {{keep()}} method in {{Endpoints}} that re-orders the contents to {{select()}} to make clear the distinction of semantics.\n",
            "\n",
            "Then there are a few follow up commits to either fix bugs or perform some follow up clean ups.\n",
            "\n",
            "Looking forward to your feedback. FYI my fix for {{maybeSendAdditionalReads}} is incomplete I will follow up with it on Monday but it will be very minor. \"[~ifesdjeen] that branch you linked to in your PR is the wrong one its 14705\" [~aweisberg] sorry it was right until branch was renamed and switched)\n",
            "\n",
            "Thank you for the patch. Some comments all rather minor:\n",
            "\n",
            "  * we could rename {{liveOnly}} to {{live}} unless \"only\" here bears \n",
            "  * {{allUncontactedCandidates}} could be just {{uncontactedCandidates}} \n",
            "  * {{ForTokenWrite}} can be just {{ForWrite}} since we\\ve removed {{ForRangeWrite}} similarly in {{forTokenWriteLiveAndDown}} and {{forTokenWrite}}\n",
            "  * we can avoid iterating in {{ReplicaPlan#forWrite}} checking for {{isTransient}} and short-circuit it for non-transient keyspace with {{CL#hasTransient}}.\n",
            " * in {{EndpointsFor(Range|Token)}} we have checks for range/token so they matched for concatenated ranges. Should we apply same checks when constructing layouts?\n",
            "  * I know it was same before the patch and we can do it in a separate one but {{isSufficientReplicasForRead}} and {{assureSufficientReplicas}} share the logic which potentially makes it prone to modifications just in one place and not the other. Looks like we could combine the two. In same methods naming is either liveReplicas or allLive for same argument. \n",
            "  * {{assureSufficientReplicas}} might need word \"live\" in it\n",
            "  * {{ReplicaCount}} can now use the new {{count}} method\n",
            "  * {{ReplicaPlans#maybeMerge}} is currently checking for {{isSufficientReplicasForRead}} and then proceeds with filtering the nodes unlike token reads which filter and then check. Since this is the only place {{isSufficientReplicasForRead}} this might be an opportunity for consolidation.\n",
            "\n",
            "And some that we\\ve discussed offline for history:\n",
            "e technically in the same bucket of {{ForWrite}}.\n",
            "e tested indirectly however mostly through dtests. Maybe flushing out some things especially related to pending nodes (which are generally hard to test) might be good candidates there. Thanks.\\xa0 I vacillated over a number of these naming decisions myself so I\\m happy to accept your suggestions. \\xa0I\\ll respond to just a few points:\n",
            "{quote}we can avoid iterating in {{ReplicaPlan#forWrite}} checking for {{isTransient}} and short-circuit it for non-transient keyspace with {{CL#hasTransient}}.\n",
            "{quote}\n",
            "I\\m not sure we can - this would be a race condition on the Keyspace. \\xa0Very rare admittedly but worth avoiding. Perhaps we could cache the value of this inside of Endpoints though and introduce a {{hasTransient()}}\\xa0method?\\xa0 Ideally we will later cache the {{ReplicaLayout}}\\xa0(or perhaps even {{ReplicaPlan}}) at which point this would short-circuit this work for all queries.\n",
            "{quote}in {{EndpointsFor(Range|Token)}} we have checks for range/token so they matched for concatenated ranges. Should we apply same checks when constructing layouts?\n",
            "{quote}\n",
            "Not quite sure what you mean here? \\xa0That for an AbstractBounds we fully cover it? \\xa0I did wonder about this but there\\s actually potential race conditions in the code where we generate our ranges to query and actually query them where we might fail this check. \\xa0But this race condition is inherent to the way we do range ownerships anyway so not a lot of point in killing queries today I reckon. \\xa0This obviously doesn\\t apply to Token since they\\xa0are the unit by which range ownerships can move.\n",
            "{quote}I know it was same before the patch and we can do it in a separate one but {{isSufficientReplicasForRead}} and {{assureSufficientReplicas}} share the logic which potentially makes it prone to modifications just in one place and not the other. Looks like we could combine the two. In same methods naming is either liveReplicas or allLive for same argument.\n",
            "{quote}\n",
            "I agree but at present it would be a bit painful to modify because {{assureSufficientReplicas}}\\xa0throws exceptions that utilise the computations. \\xa0I guess we could\\xa0have a shared method that accepts a boolean indicating if we should throw if you like? \\xa0My intention was that all of this would be cleaned up in a later Jira where we might address the overall management of \\sufficiency\\ - as this is dotted all over resolvers write handlers CL read-repair...\n",
            "{quote}{{ReplicaCount}} can now use the new {{count}} method\n",
            "{quote}\n",
            "I don\\t follow? \\xa0Do you mean {{ReplicaCollection.count()}} - if so this only returns an int and we need two ints...\n",
            "{quote}{{ReplicaPlans#maybeMerge}} is currently checking for {{isSufficientReplicasForRead}} and then proceeds with filtering the nodes unlike token reads which filter and then check. Since this is the only place {{isSufficientReplicasForRead}} this might be an opportunity for consolidation.\n",
            "{quote}\n",
            "e testing to see if two\\xa0sufficient plans can be merged into one so a quick check to confirm sufficiency is well sufficient. \\xa0We aren\\t then performing a later check to assert sufficiency because if they aren\\t we just use the two separate\\xa0plans. \\xa0That said we could make the\\xa0structure of the code similar by checking after construction - but I assume the point of this\\xa0was originally to avoid generating more garbage than necessary.\n",
            "e technically in the same bucket of {{ForWrite}}.\n",
            "{quote}\n",
            "Agreed entirely CL should ideally return to being mostly just an enum. \\xa0Since most of its\\xa0logic is for tracking sufficiency (either in the is/assert checks or in resolvers write handlers etc.) when we address that we can move all of the logic out of CL. \\xa0I also already have a follow-up bug fix that moves {{filterForQuery}}\\xa0into {{ReplicaPlans}}\n",
            "e tested indirectly however mostly through dtests. Maybe flushing out some things especially related to pending nodes (which are generally hard to test) might be good candidates there.\n",
            "{quote}\n",
            "I will try to rustle up some unit\\xa0tests for sure though you may be right that a follow-up ticket is best so we can get this committed ASAP. \"bq. Im not sure we can - this would be a race condition on the Keyspace.\n",
            "\n",
            "True I havent thought of it. We should probably try to make replication factor immutable for the time of query and attached to replica plan at some point in future.\n",
            "\n",
            "bq. Not quite sure what you mean here?  That for an AbstractBounds we fully cover it?\n",
            "\n",
            "I mostly meant something like [this|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/locator/EndpointsForRange.java#L94-L96]. Maybe its actually good to fail queries where we have collected half the replicas from one token metadata and the other one from other? Not sure.\n",
            "\n",
            "bq.  I guess we could have a shared method that accepts a boolean indicating if we should throw if you like? \n",
            "\n",
            "We could also leave it for later really. Its rather minor and Im not very worried about it right now just thought its worth persisting it.\n",
            "\n",
            "bq. ReplicaCount can now use the new count method\n",
            "\n",
            "Sorry I was thinking about something different lets leave it as is.\n",
            "\n",
            "bq.  so a quick check to confirm sufficiency is well sufficient. \n",
            "\n",
            "Right agreed. Maybe if we used a boolean for check and threw if it returned {{false}} we could still consolidate those but this is also not a huge win.\" \"{quote}Maybe its actually good to fail queries where we have collected half the replicas from one token metadata and the other one from other? Not sure.\n",
            "{quote}\n",
            "Yeah not sure either but since were already exposed (generally) to range ownership races its _probably_\\xa0better to hold off until we fix those across the board IMO to least surprise our users.\" Thank you for the changes & discussions. \n",
            "\n",
            "+1 LGTM. These two are outdated so you will need to click view outdated but there is https://github.com/apache/cassandra/pull/265#pullrequestreview-153925151 and https://github.com/apache/cassandra/pull/265#pullrequestreview-153914279 which both seem to not be responded to.\n",
            "\n",
            "The first is the one that really matters because I think not understanding why there are multiple plans is an issue. \"Not loving the GitHub comments experience at the moment. \\xa0Ive responded to the comments I could find.\" \"If you could remove that one code comment maybe clean it up to only refer to the one ReplicaPlan and do something about the other TODOs that need to either be done removed or JIRAed. Thats a good policy for TODOs right?\n",
            "\n",
            "+1 after that.\n",
            "\n",
            "\" \"So my personal view on TODOs is that they can be much more useful left inline with their context for when you or somebody else next comes to touch the surrounding code\\xa0- whether in the intended JIRA or\\xa0another related one. \\xa0Theyre also a bit specific for a JIRA theyre more guidance for what to consider when\\xa0shaping your next solution.\n",
            "\n",
            "Theyre like documentation only they document inadequacy things for the next author to watch out for and a suggestion that (if time permits)\\xa0they\\xa0resolve them at the same time. \\xa0\n",
            "\n",
            "There are multiple\\xa0JIRA filed already for these code paths any of which might hopefully\\xa0encompass refactoring / fixing these TODOs.\n",
            "{quote}If you could remove that one code comment maybe clean it up to only refer to the one ReplicaPlan\n",
            "{quote}\n",
            "Which one? \\xa0The one I saw was already missing from the code hence the comment being\\xa0out of date for the PR (and my missing it) on GitHub UI.\" \"[This is what I was talking about|https://github.com/apache/cassandra/pull/265/files#diff-0246c72855070863c2fdbee6d97f494dR63]. Couldnt this refer to the CL from the parameter ReplicaPlan? Can you remove the TODO?\" \"Ah thanks. \\xa0I must have gone amiss somehow with the summary\\xa0GitHub UI. \\xa0It does look like we can just use the provided replica plan safely so Ive removed the CL parameters.\" \"So Ive pushed a squashed and rebased version [here|https://github.com/belliottsmith/cassandra/tree/14705-rebase] however Im considering on last modification - Alexs most recent patches mean we can\\xa0perhaps get rid of the ReplicaPlan.Shared concept. \\xa0I may try and see how it turns out. \\xa0Previously we needed to construct the readRepair upfront but now we could construct it on demand. \\xa0It might be that it is a useful abstraction anyway\\xa0since we at least make it clear that these are modified. \\xa0But it would actually localise more the scope of who\\xa0can see what modifications to a single class.\" +1 unless you want to do more changes as part of this. This got merged as [047bcd7ad171d6a4aa89128c5e6c6ed5f012b1c0|https://github.com/apache/cassandra/commit/047bcd7ad171d6a4aa89128c5e6c6ed5f012b1c0]?\n",
            "\n",
            "Is this issue ready to resolve? Yes thanks.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-16077\n",
            "issue_type: Improvement\n",
            "summary: Only allow strings to be passed to JMX authentication\n",
            "description: It doesn't make sense to allow other object types.\n",
            "architectural impact: NO\n",
            "comments: ['CI:\\r\\n* trunk\\r\\n** https://ci-cassandra.apache.org/job/Cassandra-devbranch/285/\\r\\n* 3.11\\r\\n** https://ci-cassandra.apache.org/job/Cassandra-devbranch/284/\\r\\n* 3.0\\r\\n** https://ci-cassandra.apache.org/job/Cassandra-devbranch/289/\\r\\n* 2.2\\r\\n** https://ci-cassandra.apache.org/job/Cassandra-devbranch/288/', 'Committed as [63f4da90c3c51d230c535265786dbc7a33c1ace9|https://github.com/apache/cassandra/commit/63f4da90c3c51d230c535265786dbc7a33c1ace9]']\n",
            "my_comment: CI:\n",
            "* trunk\n",
            "** https://ci-cassandra.apache.org/job/Cassandra-devbranch/285/\n",
            "* 3.11\n",
            "** https://ci-cassandra.apache.org/job/Cassandra-devbranch/284/\n",
            "* 3.0\n",
            "** https://ci-cassandra.apache.org/job/Cassandra-devbranch/289/\n",
            "* 2.2\n",
            "** https://ci-cassandra.apache.org/job/Cassandra-devbranch/288/ Committed as [63f4da90c3c51d230c535265786dbc7a33c1ace9|https://github.com/apache/cassandra/commit/63f4da90c3c51d230c535265786dbc7a33c1ace9]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-16780\n",
            "issue_type: Improvement\n",
            "summary: Log when writing many tombstones to a partition\n",
            "description: Log when writing many tombstones to a partition like we do when writing a large partition\n",
            "architectural impact: NO\n",
            "comments: ['https://github.com/krummas/cassandra/commits/marcuse/16780\\r\\n\\r\\nhttps://app.circleci.com/pipelines/github/krummas/cassandra?branch=marcuse%2F16780', 'I think we should add this to the yaml with comments explaining it too.', 'yep, good point, pushed a fix', '+1', \"I don't think we can technically put this in 4.0.x unless it's considered a bug, unfortunately.\", 'committed, thanks']\n",
            "my_comment: https://github.com/krummas/cassandra/commits/marcuse/16780\n",
            "\n",
            "https://app.circleci.com/pipelines/github/krummas/cassandra?branch=marcuse%2F16780 I think we should add this to the yaml with comments explaining it too. yep good point pushed a fix +1 \"I dont think we can technically put this in 4.0.x unless its considered a bug unfortunately.\" committed thanks\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-16932\n",
            "issue_type: Bug\n",
            "summary: Gossip Fixes\n",
            "description: Testing with CEP-10 discovered faults with gossip where status updates may be processed in an order that invalidates their application. These fixes are necessary for simulation to run correctly, but also potentially affect gossip time to settle.\n",
            "architectural impact: NO\n",
            "comments: ['Patch [here|https://github.com/belliottsmith/cassandra/tree/16932-trunk]']\n",
            "my_comment: Patch [here|https://github.com/belliottsmith/cassandra/tree/16932-trunk]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-173\n",
            "issue_type: Improvement\n",
            "summary: add getPendingTasks to CFSMBean\n",
            "description: need to add an atomicint and inc/decr it whenever we acquire memtableLock\n",
            "architectural impact: NO\n",
            "comments: ['rebased patch as 0001-CASSANDRA-173-added-CFS-pending-tasks-JMX-attr.txt', 'patch 001-CASSANDRA-173-added-CFS-pending-tasks-JMX-attr.txt looks good to me\\n\\n-Arin', 'committed', 'Integrated in Cassandra #132 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/132/])\\n    added CFS pending tasks JMX attribute\\n\\nPatch by eevans; reviewed by Arin Sarkissian for \\n']\n",
            "my_comment: rebased patch as 0001-CASSANDRA-173-added-CFS-pending-tasks-JMX-attr.txt patch 001-CASSANDRA-173-added-CFS-pending-tasks-JMX-attr.txt looks good to me\n",
            "\n",
            "-Arin committed Integrated in Cassandra #132 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/132/])\n",
            "    added CFS pending tasks JMX attribute\n",
            "\n",
            "Patch by eevans; reviewed by Arin Sarkissian for \n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-17808\n",
            "issue_type: Improvement\n",
            "summary: Optionally avoid hint transfer during decommission\n",
            "description: Both because they aren’t strictly necessary to maintain consistency, and because throttling induced by their rate-limiter (see {{hinted_handoff_throttle}}) may stall progress, transferring hints during decommission (specifically unbootstrap) rather than just pausing, disabling, and truncating them probably doesn’t make sense. The only other concern would be the BatchLog, which nominally depends on hint delivery to maintain its \"guarantees\". However, during BatchLog replay on unbootstrap, {{ReplayingBatch}} ignores batches older the gcgs anyway.\n",
            "\n",
            "\n",
            "\n",
            "Here's a proposal from [~aleksey] that might strike a reasonable balance:\n",
            "\n",
            "\n",
            "\n",
            "1.) We continue to transfer hints by default during decommission, but at a higher rate. We could, for instance, stop having {{DispatchHintsTask}} divide its effective rate by the number of nodes in the cluster.\n",
            "\n",
            "\n",
            "\n",
            "{noformat}\n",
            "\n",
            "int nodesCount = Math.max(1, StorageService.instance.getTokenMetadata().getAllEndpoints().size() - 1);\n",
            "\n",
            "double throttleInBytes = DatabaseDescriptor.getHintedHandoffThrottleInKiB() * 1024.0 / nodesCount;\n",
            "\n",
            "this.rateLimiter = RateLimiter.create(throttleInBytes == 0 ? Double.MAX_VALUE : throttleInBytes);\n",
            "\n",
            "{noformat}\n",
            "\n",
            "\n",
            "\n",
            "2.) We provide an option to simply avoid transferring hints during unbootstrap. Even this would only take the BatchLog from \"best effort\" to \"slightly less effort\" ;)\n",
            "architectural impact: NO\n",
            "comments: [\"I may try to address CASSANDRA-16679 while I'm in this corner of the codebase...\", \"Did a 300-iteration run of {{HintedHandoffAddRemoveNodesTest}} w/ some minor fixes to make checking for hint totals more tolerant: https://app.circleci.com/pipelines/github/maedhroz/cassandra?branch=CASSANDRA-17808\\r\\n\\r\\nThings look clean, but I'll probably do another run after making the rest of the changes I need to make for this issue...\", 'Reading back in CASSANDRA-16679, unfortunately the CircleCI logs are gone but I noticed we were hitting the following issue:\\r\\n{code:java}\\r\\nIf I remove node 4 never to get back up, I still can see in the logs of node 2 that node 4 goes down and then a connection is established between node 2 and node4 and node 4 is marked immediately up in the logs of node 2...{code}\\r\\nDid this got resolved somewhere down the road? :D\\xa0That would be awesome', 'Looks like I have, in fact, found the cause of the flakes in CASSANDRA-16679. Moving over to fix that issue, then will rebase and finalize this.', 'Going to try to get this into review, now that CASSANDRA-16679 is committed...', '[~stefan.miklosovic] Not sure if you were interested in reviewing further, but [the PR is ready|https://github.com/apache/cassandra/pull/1835].', '+1', '+1', 'Committed as https://github.com/apache/cassandra/commit/d6aee7e08c658db9d394a6b7e3e27791b4d6854f', \"[~maedhroz], [~smiklosovic] \\xa0Hello, can we also do the same exercise and cherry-pick this issue as we did for the virtual tables running queries? I think this is a helpful flag for users.\\r\\n\\r\\nI have prepared changes here:\\r\\nhttps://github.com/apache/cassandra/pull/2448\\r\\n\\r\\nThe test results on Jenkins:\\r\\n[https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2522/]\\r\\n\\r\\nAnd again, I'm a bit limited with getting CircleCi results for this, so help is needed :)\", '[~maedhroz] are you ok with backporting to 4.1? How is this ad-hoc addition of features done? We just agree on that?']\n",
            "my_comment: https://app.circleci.com/pipelines/github/maedhroz/cassandra?branch=CASSANDRA-17808\n",
            "\n",
            "Things look clean but Ill probably do another run after making the rest of the changes I need to make for this issue...\" Reading back in CASSANDRA-16679 unfortunately the CircleCI logs are gone but I noticed we were hitting the following issue:\n",
            "{code:java}\n",
            "If I remove node 4 never to get back up I still can see in the logs of node 2 that node 4 goes down and then a connection is established between node 2 and node4 and node 4 is marked immediately up in the logs of node 2...{code}\n",
            "Did this got resolved somewhere down the road? :D\\xa0That would be awesome Looks like I have in fact found the cause of the flakes in CASSANDRA-16679. Moving over to fix that issue then will rebase and finalize this. Going to try to get this into review now that CASSANDRA-16679 is committed... [~stefan.miklosovic] Not sure if you were interested in reviewing further but [the PR is ready|https://github.com/apache/cassandra/pull/1835]. +1 +1 Committed as https://github.com/apache/cassandra/commit/d6aee7e08c658db9d394a6b7e3e27791b4d6854f \"[~maedhroz] [~smiklosovic] \\xa0Hello can we also do the same exercise and cherry-pick this issue as we did for the virtual tables running queries? I think this is a helpful flag for users.\n",
            "\n",
            "I have prepared changes here:\n",
            "https://github.com/apache/cassandra/pull/2448\n",
            "\n",
            "The test results on Jenkins:\n",
            "[https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2522/]\n",
            "\n",
            "And again Im a bit limited with getting CircleCi results for this so help is needed :)\" [~maedhroz] are you ok with backporting to 4.1? How is this ad-hoc addition of features done? We just agree on that?\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-1789\n",
            "issue_type: Improvement\n",
            "summary: Clean up (and make sane) key/row cache loading logspam\n",
            "description: //Start\n",
            " INFO 19:18:03,362 Heap size: 1935147008/1994063872\n",
            " INFO 19:18:03,366 JNA not found. Native methods will be disabled.\n",
            " INFO 19:18:03,376 Loading settings from file:/home/hermes/work/c/cass7/conf/cassandra.yaml\n",
            " INFO 19:18:03,533 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap\n",
            " INFO 19:18:03,612 Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1291079883612.log\n",
            "\n",
            "//Keycache loading\n",
            " *INFO 19:18:03,659 read 0 from saved key cache*\n",
            " *INFO 19:18:03,663 read 0 from saved key cache*\n",
            " *INFO 19:18:03,664 read 0 from saved key cache*\n",
            " *INFO 19:18:03,666 read 0 from saved key cache*\n",
            " *INFO 19:18:03,668 read 0 from saved key cache*\n",
            "\n",
            "//Rowcache loading\n",
            " *INFO 19:18:03,671 loading row cache for LocationInfo of system*\n",
            " *INFO 19:18:03,671 completed loading (0 ms; 0 keys)  row cache for LocationInfo of system*\n",
            " *INFO 19:18:03,672 loading row cache for HintsColumnFamily of system*\n",
            " *INFO 19:18:03,672 completed loading (0 ms; 0 keys)  row cache for HintsColumnFamily of system*\n",
            " *INFO 19:18:03,673 loading row cache for Migrations of system*\n",
            " *INFO 19:18:03,673 completed loading (0 ms; 0 keys)  row cache for Migrations of system*\n",
            " *INFO 19:18:03,676 loading row cache for Schema of system*\n",
            " *INFO 19:18:03,676 completed loading (0 ms; 0 keys)  row cache for Schema of system*\n",
            " *INFO 19:18:03,676 loading row cache for IndexInfo of system*\n",
            " *INFO 19:18:03,677 completed loading (0 ms; 0 keys)  row cache for IndexInfo of system*\n",
            "\n",
            "//The rest\n",
            " INFO 19:18:03,730 Couldn't detect any schema definitions in local storage.\n",
            " INFO 19:18:03,731 Found table data in data directories. Consider using JMX to call org.apache.cassandra.service.StorageService.loadSchemaFromYaml().\n",
            " INFO 19:18:03,735 No commitlog files found; skipping replay\n",
            " INFO 19:18:03,783 Upgrading to 0.7. Purging hints if there are any. Old hints will be snapshotted.\n",
            " INFO 19:18:03,786 Cassandra version: 0.7.0-rc1-SNAPSHOT\n",
            " INFO 19:18:03,786 Thrift API version: 19.4.0\n",
            " INFO 19:18:03,795 Loading persisted ring state\n",
            " INFO 19:18:03,796 Starting up server gossip\n",
            " INFO 19:18:03,803 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1291079883612.log', position=700)\n",
            " INFO 19:18:03,804 Enqueuing flush of Memtable-LocationInfo@1249086728(227 bytes, 4 operations)\n",
            " INFO 19:18:03,805 Writing Memtable-LocationInfo@1249086728(227 bytes, 4 operations)\n",
            " INFO 19:18:03,992 Completed flushing /var/lib/cassandra/data/system/LocationInfo-e-1-Data.db (473 bytes)\n",
            " WARN 19:18:04,058 Generated random token 109302658160365096146210744235544448283. Random tokens will result in an unbalanced ring; see http://wiki.apache.org/cassandra/Operations\n",
            " INFO 19:18:04,059 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1291079883612.log', position=996)\n",
            " INFO 19:18:04,060 Enqueuing flush of Memtable-LocationInfo@1940835386(53 bytes, 2 operations)\n",
            " INFO 19:18:04,060 Writing Memtable-LocationInfo@1940835386(53 bytes, 2 operations)\n",
            " INFO 19:18:04,258 Completed flushing /var/lib/cassandra/data/system/LocationInfo-e-2-Data.db (301 bytes)\n",
            " INFO 19:18:04,269 Will not load MX4J, mx4j-tools.jar is not in the classpath\n",
            " INFO 19:18:04,301 Binding thrift service to localhost/127.0.0.1:9160\n",
            " INFO 19:18:04,304 Using TFramedTransport with a max frame size of 15728640 bytes.\n",
            " INFO 19:18:04,307 Listening for thrift clients...\n",
            "\n",
            "The logging here is annoying (and a bit schizophrenic).\n",
            "Either the keycache loading logging should include as much info as the rowcache loading (time duration, CF/KS names) or it should be a much smaller snippet for both.\n",
            "The best fix would probably be the line:\n",
            " *INFO XX:XX:XX,XXX completed loading (time; keys) row/key cache for CF in KS.*\n",
            " ...which would be a log line per CF per saved key/row cache (with more logging on error). \n",
            "\n",
            "I don't know if logging that \"0 rows (key or row cache) successfully loaded\" is worth it either, but I could be swayed by an argument for this.\n",
            "architectural impact: NO\n",
            "comments: ['saner logging', 'backported to 0.6 and committed', 'Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])\\n    ']\n",
            "my_comment: saner logging backported to 0.6 and committed Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])\n",
            "    \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-17904\n",
            "issue_type: Improvement\n",
            "summary: Consider to not warn about deprecated properties in logs when the value is not deprecated\n",
            "description: When there is an initialisation of database descriptor for tools, for example via \"Util.initDatabaseDescriptor()\", it will eventually buble up to \"YamlConfigurationLoader.check\" where this is logged:\n",
            "\n",
            "{code}\n",
            "\n",
            "if (!deprecationWarnings.isEmpty())\n",
            "\n",
            "logger.warn(\"{} parameters have been deprecated. They have new names and/or value format; For more information, please refer to NEWS.txt\", deprecationWarnings);\n",
            "\n",
            "{code}\n",
            "\n",
            "\n",
            "\n",
            "For example, I saw this log:\n",
            "\n",
            "\n",
            "\n",
            "{code}\n",
            "\n",
            "WARN  09:07:42,486 [key_cache_save_period, counter_cache_save_period, row_cache_save_period] parameters have been deprecated. They have new names and/or value format; For more information, please refer to NEWS.txt\n",
            "\n",
            "{code}\n",
            "\n",
            "\n",
            "\n",
            "The \"problems\" I see are two:\n",
            "\n",
            "1) it pollutes the console for tool commands, when a tool needs to initialise DD, at the very beginning of the output.\n",
            "\n",
            "2) When you look closely, for example at key_cache_save_period, by default, in cassandra.yaml, it has value \"4h\". My question is: why is it necessary to mark that as deprecated when the value is already in new format? In other words, I would log that warning only in case that the expected value of a property is not in the new format. But when it already is, why do we need to inform a user about that?\n",
            "\n",
            "\n",
            "\n",
            "I think it would require to take some extra care for cases like these in YamlConfigurationLoader.getProperty to not add deprecated properties if their value is already new.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "architectural impact: NO\n",
            "comments: ['Kindly pinging [~e.dimitrova] to raise awareness.', \"Thanks [~smiklosovic], those three are special case where only the value format was changed and their names were kept (the only 3 which had proper names and no unit as part of the name... ), that's why your valid observation - they emit deprecation warning even with the value being in the new format. You made me wonder whether it was worth it to special case them at all, I will take a look tomorrow.\\xa0\", \"Slept on it and had some discussion with [~marcuse]\\xa0and [~dcapwell]\\xa0about those three parameters as I was wondering whether we missed to take some action around them when we added the new flags - _allow_duplicate_config_keys_ and _allow_new_old_config_keys_ in\\xa0CASSANDRA-17379\\r\\n\\r\\nThe key names haven't been changed so we technically didn't deprecate the three properties but we added an option to be able to add value both in numeric and numeric+unit format.\\xa0\\r\\n\\r\\nThe fix should be to make those not deprecated in the Replaces annotation. I will add it soon. Also, I plan to add quick additional note in the config docs to remind people that _allow_duplicate_config_keys_\\xa0is the only way to not be able to add that property more than once with both formats; Those three are a special case that is already mentioned in the docs but I think it will be nice to stress on them when talking about the flags.\\r\\n\\r\\n\\xa0I will push a patch soon, thanks\", 'The same patch was added for both [4.1|https://github.com/ekaterinadimitrova2/cassandra/tree/17904-4.1]\\xa0and [trunk|https://github.com/ekaterinadimitrova2/cassandra/tree/17904-trunk].\\r\\n\\r\\nI tested manually to see that the warnings are not presented anymore on startup. I can push sanity check CI pre-commit when\\xa0[~smiklosovic]\\xa0or any other committer reviewer approves the change.\\xa0', 'tested and works fine, +1 on good build.', 'Thank you for the quick review.\\r\\n\\r\\nCI currently running, I will check the results later:\\r\\n * 4.1 - [j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1928/workflows/3f9da019-1301-41b7-85c4-0767ff2dbd8f], [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1928/workflows/bff4ff51-cece-47b5-9d3c-ffe06363fabc]\\r\\n * trunk – [j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1929/workflows/f70dd874-0b32-4028-9e9e-40e4f739b436], [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1929/workflows/8c22347c-4b63-488d-98d6-5c0a81bc47b7]', 'Both failures known and already reported:\\r\\n\\r\\nCASSANDRA-16861\\xa0- test_compression_cql_options\\r\\n\\r\\nCASSANDRA-17005\\xa0- test_multiple_repair\\r\\n\\r\\nStarting commit soon', 'Committed, thanks:\\r\\n\\r\\nd8bbeb9e39..4c85c6a403\\xa0 cassandra-4.1 -> cassandra-4.1\\r\\n\\r\\n\\xa0\\xa0 e89b214d06..d80d934ed2\\xa0 trunk -> trunk']\n",
            "my_comment: Kindly pinging [~e.dimitrova] to raise awareness. \"Thanks [~smiklosovic] those three are special case where only the value format was changed and their names were kept (the only 3 which had proper names and no unit as part of the name... ) thats why your valid observation - they emit deprecation warning even with the value being in the new format. You made me wonder whether it was worth it to special case them at all I will take a look tomorrow.\\xa0\" \"Slept on it and had some discussion with [~marcuse]\\xa0and [~dcapwell]\\xa0about those three parameters as I was wondering whether we missed to take some action around them when we added the new flags - _allow_duplicate_config_keys_ and _allow_new_old_config_keys_ in\\xa0CASSANDRA-17379\n",
            "\n",
            "The key names havent been changed so we technically didnt deprecate the three properties but we added an option to be able to add value both in numeric and numeric+unit format.\\xa0\n",
            "\n",
            "The fix should be to make those not deprecated in the Replaces annotation. I will add it soon. Also I plan to add quick additional note in the config docs to remind people that _allow_duplicate_config_keys_\\xa0is the only way to not be able to add that property more than once with both formats; Those three are a special case that is already mentioned in the docs but I think it will be nice to stress on them when talking about the flags.\n",
            "\n",
            "\\xa0I will push a patch soon thanks\" The same patch was added for both [4.1|https://github.com/ekaterinadimitrova2/cassandra/tree/17904-4.1]\\xa0and [trunk|https://github.com/ekaterinadimitrova2/cassandra/tree/17904-trunk].\n",
            "\n",
            "I tested manually to see that the warnings are not presented anymore on startup. I can push sanity check CI pre-commit when\\xa0[~smiklosovic]\\xa0or any other committer reviewer approves the change.\\xa0 tested and works fine +1 on good build. Thank you for the quick review.\n",
            "\n",
            "CI currently running I will check the results later:\n",
            " * 4.1 - [j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1928/workflows/3f9da019-1301-41b7-85c4-0767ff2dbd8f] [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1928/workflows/bff4ff51-cece-47b5-9d3c-ffe06363fabc]\n",
            " * trunk – [j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1929/workflows/f70dd874-0b32-4028-9e9e-40e4f739b436] [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1929/workflows/8c22347c-4b63-488d-98d6-5c0a81bc47b7] Both failures known and already reported:\n",
            "\n",
            "CASSANDRA-16861\\xa0- test_compression_cql_options\n",
            "\n",
            "CASSANDRA-17005\\xa0- test_multiple_repair\n",
            "\n",
            "Starting commit soon Committed thanks:\n",
            "\n",
            "d8bbeb9e39..4c85c6a403\\xa0 cassandra-4.1 -> cassandra-4.1\n",
            "\n",
            "\\xa0\\xa0 e89b214d06..d80d934ed2\\xa0 trunk -> trunk\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-18219\n",
            "issue_type: Improvement\n",
            "summary: Warning message on aggregation queries doesn't specify table name or aggregate type\n",
            "description: The existing aggregation query warning messages in cassandra.log from SelectStatement.java:\n",
            "\n",
            "\n",
            "\n",
            "     _'[WARN] Aggregation query used without partition key'_\n",
            "\n",
            "\n",
            "\n",
            "     {_}'{_}{_}[WARN]{_} _Aggregation query used on multiple partition keys (IN restriction)'_\n",
            "\n",
            "\n",
            "\n",
            "are missing the helpful details which would assist users in quickly identifying the offending query. The table name and type of aggregation would be useful additions in the WARN message.\n",
            "\n",
            "\n",
            "\n",
            "In addition, following the example of the slow query logger and printing out the query string at DEBUG level would be especially helpful.\n",
            "architectural impact: NO\n",
            "comments: ['Thank you for reporting this! I will take a closer look if we could propagate the information you need. What you mean under the \"type of aggregation\"? \\r\\n\\r\\nI am not sure what slow query logger is. I think you meant \"no spamming logger\", we can probably print this in debug via that logger if you find it useful.', '[~bschoeni] this is what I have. (1)\\r\\n\\r\\nI am little bit confused by emitting warning in CQL(SH) but we are logging it on debug level in the logs. I would say these levels should be probably equal but at the same time there is not anything like \"debug level\" in CQLSH so this might be probably just fine.\\r\\n\\r\\n[~blerer] what do you think?\\r\\n\\r\\n(1) https://github.com/apache/cassandra/pull/2128', 'https://github.com/apache/cassandra/pull/2128/files', '[~smiklosovic] Thanks for the lightning fast fix! Yes, by DEBUG level I was referring to the NoSpamLogger which writes to debug.log.\\r\\n{quote}{{INFO [ScheduledTasks:1] 2016-08-10 16:56:50,209 NoSpamLogger.java:91 - Some operations were slow, details available at debug level (debug.log)}}\\r\\n{quote}\\r\\nThis issue is only about cassandra.log and debug.log, not CQLSH.\\xa0\\xa0To be most useful, the logger would log the CQL query text, not just repeat what WARN already logs. I.e. in <...> and probably without the query execution time:\\r\\n{quote}DEBUG [ScheduledTasks:1] 2016-08-10 16:56:50,211 MonitoringTask.java:173 - 3 operations were slow in the last 4998 msecs:\\r\\n<SELECT * FROM ks.test2 LIMIT 5000>, time 3026 msec - slow timeout 30 msec\\r\\n<SELECT * FROM ks.test2 WHERE id = 1 LIMIT 5000>, was slow 2 times: avg/min/max 330/325/335 msec - slow timeout 30 msec\\r\\n<SELECT * FROM ks.test2 WHERE token(id) > 0 LIMIT 5000>, time 1449 msec - slow timeout 30 msec\\r\\n{quote}', 'The idea behind the warn is to send the warning back to the client performing the query. Of course they can choose to ignore it which is why we also log the message to the logs.\\r\\nAdding the table name make total sense from the log point of view.\\r\\n[~bschoeni] I imagine that if you want to find the problematic query your intention is to ask the user to stop running that query. Would it not make sense at this point to had some Guardrails to block those queries rather than introducing more logging?', 'Yes you are right we probably do not need that in cql warn, this warning is propagated to drivers anyway (as far as I know) and users executing queries know what query they executed so we do not need to repeat the information about table and keyspace because they are the ones executing the query and they know it. So I will remove modifications about cqlsh warn.\\r\\n\\r\\nThe current PR (1) logs this via NoSpamLogger:\\r\\n{code:java}\\r\\nDEBUG [Native-Transport-Requests-1] 2023-02-02 11:23:47,086 NoSpamLogger.java:105 - Aggregation query used without partition key on table ks.tb, aggregation type: AGGREGATE_EVERYTHING, query: <SELECT c1, c2 FROM ks.tb>\\r\\n{code}\\r\\nI am working on the fix because it should write \"max(c1), max(c2)\" ....', '[~blerer]\\xa0 we could do guardrails as well, sure. By default it would be on warn and if turned on it would fail the query. We have done similar logic in CASSANDRA-18042.\\r\\n\\r\\n[~bschoeni] would be guardrail viable option for\\xa0 you? Do we want to emit warnings or failures or we just enable / disable these queries?\\r\\n\\r\\nThe PR for simply logging it is here:\\r\\n\\r\\n[https://github.com/apache/cassandra/pull/2128/files]\\r\\n\\r\\n\\xa0I will wait until there is a decision whether a guardrail is not better option before implementing that instead.', \"[~blerer] [~smiklosovic] both application code and ad hoc queries can generate these WARN messages.\\xa0 Ad hoc may be something like 'select count *'.\\xa0 We wouldn't want to block those and, in any event, if they're too slow they'll time out.\\xa0\\r\\n\\r\\nThe situations where the existing warning is a problem is with application code.\\xa0 There may be multiple application teams using the same database, and when the aggregation warn message is seen in the log, it's difficult to tie it back to the code or even know which application ran the query.\\xa0 Multiple, unrelated applications may share the same database using different keyspaces, and the existing WARN message doesn't mention the keyspace.\\r\\n\\r\\nWe usually have to turn on settraceprobability to capture the queries.\\xa0 Having just the keyspace, table and aggregation type in the cassandra.log would solve 90% of the issues.\\r\\n\\r\\nThe main issue is {+}traceability{+}, we're not trying to outright prevent these queries. \\xa0I'm not sure if guardrails in 4.1+ would be a better option here, it may be worth further discussion.\\xa0 But it would be nice to fix the WARN message with Stefan's PR in 4.0.x.\", 'Given the fact that it is quite difficult to differentiate between \"select count \" and \"select max(a), max(b) from ks.tb\", or, in other words, between aggregation queries which people do not want to forbid in general and those which are causing problems on the application level, I would go with simple logging only as in the patch instead of guardrail.\\r\\n\\r\\nWe can also print user as well, like this:\\r\\n{code:java}\\r\\nAggregation query used without partition key on table ks.tb, aggregation type: AGGREGATE_EVERYTHING, query: <SELECT system.max(c1), system.sum(c2) FROM ks.tb>, user: cassandra\\r\\n{code}\\r\\nWhen user is anonymous, it would be\\r\\n{code:java}\\r\\nAggregation query used without partition key on table ks.tb, aggregation type: AGGREGATE_EVERYTHING, query: <SELECT system.max(c1), system.sum(c2) FROM ks.tb>, user: anonymous\\r\\n{code}\\r\\nWe can also log remote address for that matter if it helps you.\\r\\n\\r\\nHowever, one little problem I see is that if we start to print user as well, why do we do it {_}only here{_}? The output would start to become inconsistent with other logging output for which we do not print user yet.\\r\\n\\r\\nFor this reason, unless we start to put users everywhere, I would omit user from the query and we would just log it without it.\\r\\n\\r\\nbtw I think this is a new feature, not a bug. So backporting this to 4.0.x will likely not happen. Only to trunk. We are fixing bugs only for anything lower than trunk so you would need to upgrade to 4.2 to see this in action, I guess.', '+1 on simple logging\\r\\n\\r\\nWould be nice to have the keyspace+table+type in 4.0.x/4.1.\\xa0 Is it worth considering a separate bug for just that simple two line fix (excluding the debug logging)?', '[~blerer]\\xa0 would you mind to review my patch? I think the guardrail is not necessary here.\\r\\n\\r\\nI will try to gather more info whether we can put this to 4.0.x too.', 'They are improving logging recently in CASSANDRA-18184 so I think this might to 4.0.x up as well.', \"I don't see any reason to not put this in 4.0.x.\", '4.0.x should get it as well imo', '[~bschoeni]\\xa0 is it fine to log it without the actual query? There is a bug in cql dumping if you look into the PR (cc [~blerer] )', '[~smiklosovic] yes, just the WARN level info works fine.', '[~bschoeni]\\xa0 warn or debug? Now it is \"debug\". Making it \"warn\" would simplify the PR even more as we would not need to introduce DEBUG level for NoSpamLogger.', 'I made it super simple - it is logged on WARN and without a query.\\r\\n\\r\\n4.0 [https://github.com/instaclustr/cassandra/tree/CASSANDRA-18219-4.0]\\r\\n4.1 [https://github.com/instaclustr/cassandra/tree/CASSANDRA-18219-4.1]\\r\\ntrunk [https://github.com/instaclustr/cassandra/tree/CASSANDRA-18219]\\r\\n\\r\\nThis is so tiny patch I am not sure building all tests make sense. I have built j8 and j11 for 4.0 branch here\\r\\n\\r\\n4.0\\r\\n\\r\\nj11 https://app.circleci.com/pipelines/github/instaclustr/cassandra/1857/workflows/62bd1cc8-ff4c-4eda-bb85-7370be2e4d13\\r\\n\\r\\nj8 https://app.circleci.com/pipelines/github/instaclustr/cassandra/1857/workflows/c3244884-d48d-4ebb-a8f6-d259bb64f2db\\r\\n\\r\\n4.1\\r\\n\\r\\nj8 https://app.circleci.com/pipelines/github/instaclustr/cassandra/1859/workflows/7a73972f-25a4-4fa2-b3ac-9ebf591177c4\\r\\n\\r\\nj11 https://app.circleci.com/pipelines/github/instaclustr/cassandra/1859/workflows/14becd41-44cf-4526-b265-893eece64faa\\r\\n\\r\\n[~bereng] [~brandon.williams] [~blerer] anybody to +1 this, please?', \"None of the CI links did run any tests iiuc? Also trunk CI links are missing. In terms of the review itself it's just a few log lines, not much I can say about it.\", '[~bereng] yeah, that is what I wrote above: This is so tiny patch I am not sure building all tests make sense. I have built j8 and j11 for 4.0 branch here\\r\\n\\r\\nDo we want to run 5h of CI just for this? That seems like overkill. I ve just compiled that.', \"Let's see if circle complies one more time.\\r\\n\\r\\n||Branch||CI||\\r\\n|[4.0|https://github.com/driftx/cassandra/tree/CASSANDRA-18219-4.0]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/855/workflows/ef784b41-4d20-426c-892f-fd7f48137394], [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/855/workflows/d575b5f3-d99b-48eb-a012-f73e0c263cd7]|\\r\\n|[4.1|https://github.com/driftx/cassandra/tree/CASSANDRA-18219-4.1]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/856/workflows/ab2c31f2-d80a-47ce-9645-7d16bad8b4e1], [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/856/workflows/75338b20-1ad7-479d-bbee-59f674b0cbb8]|\\r\\n|[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-18219-trunk]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/858/workflows/c402f0c3-f010-443e-8cd3-b0181487cc37], [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/858/workflows/8e85b273-c2b4-4508-9a3c-7147938b1b0f]|\\r\\n\\r\\n\", \"Yeah we have to run CI I am afraid. There are some dtests that parse log files that could break iirc i.e. In any case thx [~brandon.williams] . I'm +1 unless sbdy else objects.\", 'Everything looks good here, +1.']\n",
            "my_comment: This is so tiny patch I am not sure building all tests make sense. I have built j8 and j11 for 4.0 branch here\n",
            "\n",
            "Do we want to run 5h of CI just for this? That seems like overkill. I ve just compiled that. \"Lets see if circle complies one more time.\n",
            "\n",
            "||Branch||CI||\n",
            "|[4.0|https://github.com/driftx/cassandra/tree/CASSANDRA-18219-4.0]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/855/workflows/ef784b41-4d20-426c-892f-fd7f48137394] [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/855/workflows/d575b5f3-d99b-48eb-a012-f73e0c263cd7]|\n",
            "|[4.1|https://github.com/driftx/cassandra/tree/CASSANDRA-18219-4.1]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/856/workflows/ab2c31f2-d80a-47ce-9645-7d16bad8b4e1] [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/856/workflows/75338b20-1ad7-479d-bbee-59f674b0cbb8]|\n",
            "|[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-18219-trunk]|[j8|https://app.circleci.com/pipelines/github/driftx/cassandra/858/workflows/c402f0c3-f010-443e-8cd3-b0181487cc37] [j11|https://app.circleci.com/pipelines/github/driftx/cassandra/858/workflows/8e85b273-c2b4-4508-9a3c-7147938b1b0f]|\n",
            "\n",
            "\" \"Yeah we have to run CI I am afraid. There are some dtests that parse log files that could break iirc i.e. In any case thx [~brandon.williams] . Im +1 unless sbdy else objects.\" Everything looks good here +1.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-18697\n",
            "issue_type: Bug\n",
            "summary: Skip ColumnFamilyStore#topPartitions initialization when client or tool mode\n",
            "description: In {{org.apache.cassandra.db.ColumnFamilyStore}} the {{topPartitions}} is initialized when the keyspace is not a system keyspace. However, when running the cassandra library as client mode or tool mode, the initialization also happens. However, {{TopPartitionTracker}} performs queries to the {{system}} keyspace, which might not be available in most of the cases. For that reason, we should skip initialization of {{topPartitions}} when running on client mode or tool mode.\n",
            "\n",
            "\n",
            "\n",
            "In utilities and external libraries, this can produce a warning that is displayed with a stack trace. This warning can be misleading for end users, and can cause confusion. But more importantly, the initialization of {{topPartitions}} is not required in this mode.\n",
            "\n",
            "\n",
            "\n",
            "The warning is similar to this:\n",
            "\n",
            "{code:java}\n",
            "\n",
            "    WARN org.apache.cassandra.db.SystemKeyspace: Could not load stored top SIZES partitions for ...\n",
            "\n",
            "    org.apache.cassandra.db.KeyspaceNotDefinedException: keyspace system does not exist\n",
            "\n",
            "            at org.apache.cassandra.schema.Schema.validateTable(Schema.java:xxx) ~[?:?]\n",
            "\n",
            "            at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:xxx) ~[?:?]\n",
            "\n",
            "            at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:xxx) ~[?:?]\n",
            "\n",
            "            at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:xxx) ~[?:?]\n",
            "\n",
            "            at org.apache.cassandra.cql3.QueryProcessor.parseAndPrepare(QueryProcessor.java:xxx) ~[?:?]\n",
            "\n",
            "            ... {code}\n",
            "architectural impact: NO\n",
            "comments: ['CI: [https://app.circleci.com/pipelines/github/frankgh/cassandra/124/workflows/5f9bd478-41d2-43dc-988a-cb2bb0199560]', '+1 on the patch. ', '4.1 PR: [https://github.com/apache/cassandra/pull/2535]', '+1, thanks for the patch.', 'Preparing to commit\\r\\n||Branch||CI||\\r\\n|[4.1|https://github.com/yifan-c/cassandra/tree/r/upstream/pr2535-4.1]|[CI|https://app.circleci.com/pipelines/github/yifan-c/cassandra?branch=r%2Fupstream%2Fpr2535-4.1]|\\r\\n|[trunk|https://github.com/yifan-c/cassandra/tree/r/upstream/pr2519-trunk]|[CI|https://app.circleci.com/pipelines/github/yifan-c/cassandra?branch=r%2Fupstream%2Fpr2519-trunk]|\\r\\n\\r\\n-- Update --\\r\\n\\r\\nBoth CI runs look green. There are some _known_ test failures due to \"failed to create test certs\" on trunk. They are to be fixed in another ticket.\\xa0', 'Committed [9c796dfb2|https://github.com/apache/cassandra/commit/9c796dfb272daa3ce57a2dc5cbeadd9273e1ac72] into Cassandra-4.1 and merged up to trunk. ']\n",
            "my_comment: [https://github.com/apache/cassandra/pull/2535] +1 thanks for the patch. Preparing to commit\n",
            "||Branch||CI||\n",
            "|[4.1|https://github.com/yifan-c/cassandra/tree/r/upstream/pr2535-4.1]|[CI|https://app.circleci.com/pipelines/github/yifan-c/cassandra?branch=r%2Fupstream%2Fpr2535-4.1]|\n",
            "|[trunk|https://github.com/yifan-c/cassandra/tree/r/upstream/pr2519-trunk]|[CI|https://app.circleci.com/pipelines/github/yifan-c/cassandra?branch=r%2Fupstream%2Fpr2519-trunk]|\n",
            "\n",
            "-- Update --\n",
            "\n",
            "Both CI runs look green. There are some _known_ test failures due to \"failed to create test certs\" on trunk. They are to be fixed in another ticket.\\xa0 Committed [9c796dfb2|https://github.com/apache/cassandra/commit/9c796dfb272daa3ce57a2dc5cbeadd9273e1ac72] into Cassandra-4.1 and merged up to trunk. \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-18803\n",
            "issue_type: Bug\n",
            "summary: Refactor validation logic in StorageService.rebuild\n",
            "description: This is a follow-up ticket of CASSANDRA-14319\n",
            "architectural impact: NO\n",
            "comments: ['[~aweisberg] would you mind to take a look? Super easy. On your +1 I ll do all the builds for 6 branches.', '+1 TY', '[3.0|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3035/workflows/7f55fcda-1cf0-43db-8471-ebd54be87c9e]\\r\\n\\r\\n[3.11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3034/workflows/e10a709e-97e1-4795-b4ca-7e3aac3253cd]\\r\\n\\r\\n[4.0 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/f2e2c30f-6d37-48cc-874a-104f34f67b50]\\r\\n[4.0 j8|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/af8cc33a-69ca-4164-800f-3b049a8ac6da]\\r\\n\\r\\n[4.1 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5297fd1a-f33b-4a17-b56d-d0522b65c95b]\\r\\n[4.1 j8|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5626cfef-a6a7-453c-9abf-43c30678ec41]\\r\\n\\r\\n[5.0 j17|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/70cf2550-e278-426a-a63b-533c25c4eccf]\\r\\n[5.0 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/2a5dcdc5-17f9-4311-9aeb-60fcea5787d9]\\r\\n\\r\\nThese builds are technically not same as what is in the current branches. I just moved one check outside of try-catch to add it logically where it belongs (where other checks are) and I moved logging into try catch (was outside of it).']\n",
            "my_comment: [~aweisberg] would you mind to take a look? Super easy. On your +1 I ll do all the builds for 6 branches. +1 TY [3.0|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3035/workflows/7f55fcda-1cf0-43db-8471-ebd54be87c9e]\n",
            "\n",
            "[3.11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3034/workflows/e10a709e-97e1-4795-b4ca-7e3aac3253cd]\n",
            "\n",
            "[4.0 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/f2e2c30f-6d37-48cc-874a-104f34f67b50]\n",
            "[4.0 j8|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3033/workflows/af8cc33a-69ca-4164-800f-3b049a8ac6da]\n",
            "\n",
            "[4.1 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5297fd1a-f33b-4a17-b56d-d0522b65c95b]\n",
            "[4.1 j8|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3036/workflows/5626cfef-a6a7-453c-9abf-43c30678ec41]\n",
            "\n",
            "[5.0 j17|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/70cf2550-e278-426a-a63b-533c25c4eccf]\n",
            "[5.0 j11|https://app.circleci.com/pipelines/github/instaclustr/cassandra/3037/workflows/2a5dcdc5-17f9-4311-9aeb-60fcea5787d9]\n",
            "\n",
            "These builds are technically not same as what is in the current branches. I just moved one check outside of try-catch to add it logically where it belongs (where other checks are) and I moved logging into try catch (was outside of it).\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2227\n",
            "issue_type: Improvement\n",
            "summary: add cache loading to row/key cache tests\n",
            "description: None\n",
            "architectural impact: NO\n",
            "comments: [\"don't use thread.sleep() waiting for the future to be done, just call future.get()\\n\\nI wouldn't bother reading the cache directly from disk and checking the contents, but rather force a save (the way you do) and then force C* to completely load the cache as if it was opening the table for the first time and then checking the size/contents via the actual cache in use.  The patch implements this at the end of the test anyway, so just assert there is nothing in the cache, then create the CFS and verify the cache size/contents.  Do something similar for the row cache.\\n\\notherwise the patch looks good, but I would like to see these changes before it's committed.\\n\", 'attached v3:\\n\\n1) consolidates the insert and read (to populate cache) code\\n2) compares cached values against expected values instead of just checking if the key exists\\n3) exposes the cache from CFS directly instead of requiring a round about path through a reader to get a reference\\n\\nother than the above changes, patch looks good', '+1, looks good to me, i just wanted to avoid changes in the main code base with my v2, thanks!', 'Committed.', 'I did my best to merge this to trunk but testRowCacheLoad is broken.  I committed it anyway since the rest of the merge was a bunch of work thanks to counters -- can you check out current trunk and post a fix?', 'np :)', 'committed, thanks!', 'Integrated in Cassandra-0.7 #399 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/399/])\\n    ']\n",
            "my_comment: [\"dont use thread.sleep() waiting for the future to be done just call future.get()\n",
            "\n",
            "I wouldnt bother reading the cache directly from disk and checking the contents but rather force a save (the way you do) and then force C* to completely load the cache as if it was opening the table for the first time and then checking the size/contents via the actual cache in use.  The patch implements this at the end of the test anyway so just assert there is nothing in the cache then create the CFS and verify the cache size/contents.  Do something similar for the row cache.\n",
            "\n",
            "otherwise the patch looks good but I would like to see these changes before its committed.\n",
            "\" attached v3:\n",
            "\n",
            "1) consolidates the insert and read (to populate cache) code\n",
            "2) compares cached values against expected values instead of just checking if the key exists\n",
            "3) exposes the cache from CFS directly instead of requiring a round about path through a reader to get a reference\n",
            "\n",
            "other than the above changes patch looks good +1 looks good to me i just wanted to avoid changes in the main code base with my v2 thanks! Committed. I did my best to merge this to trunk but testRowCacheLoad is broken.  I committed it anyway since the rest of the merge was a bunch of work thanks to counters -- can you check out current trunk and post a fix? np :) committed thanks! Integrated in Cassandra-0.7 #399 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/399/])\n",
            "    \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2759\n",
            "issue_type: Bug\n",
            "summary: Scrub could lose increments and replicate that loss\n",
            "description: If scrub cannot 'repair' a corrupted row, it will skip it. On node A, if the row contains some sub-count for A id, those will be lost forever since A is the source of truth on it's current id. We should thus renew node A id when that happens to avoid this (not unlike we do in cleanup).\n",
            "architectural impact: NO\n",
            "comments: [\"Attached patch against 0.8.\\n\\nThe patch also add a new startup option to renew the node id on startup. This could be useful if someone lose one of it's sstable (because of a bad disk for instance) and don't want to fully decommission that node.\\n\\nThis could arguably be splitted in another ticket though.\", 'what is \"renewing a node id?\"', \"It's picking a new UUID for the current node to use for new counter increment.\\n\\nThe problem is that on a given node we store deltas for it's current nodeId (to avoid synchronized read-before-write, but I'm starting to wonder is that was the smartest ever). Anyway, if scrub skips a row, it may skip some of those deltas. Let's say at first there is no increments coming for this row for A as 'first distinguished replica'. So far we are still kind of good, because on a read (with CL > ONE) the result coming from A will have a 'version' for it's own sub-count smaller that the one on the other replica, so we will us the sub-count on those replica and return the correct value.\\n\\nHowever, as soon as A acknowledge new increments for this row, it will start inserting new deltas while he is not intrinsically up to date. Which will result in an definitive undercount.\\n\\nThe goal of renewing the node id of A is to make sure that second part never happen (because after the renew A will add new deltas as A', not A anymore).\\n\\nAnyway, now that I've plugged the brain this patch doesn't really works because A will never be repaired by the other nodes of it's now inconsistent value.\\n\\nSo I have no clue how to actually fix that.\", \"It may be that the best short fix here is to make scrub *not* skipping row on counter column families (though CASSANDRA-2614 would change that to 'never ever skipping row') and just throw a RuntimeException.\", \"bq. make scrub not skip rows on counter column families\\n\\n+1\\n\\nbq. CASSANDRA-2614 would change that to 'never ever skipping row'\\n\\nOnly if you actually did have a counter in the column_metadata, right?\", 'Attaching patch to simply re-throw the exception instead of skipping the row for counter column families.\\n\\nbq. Only if you actually did have a counter in the column_metadata, right?\\n\\nright.', '+1\\n\\ncan you add a link to this issue in the \"dangerous\" comment?', 'Committed with suggested comment update.', 'Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])\\n    ']\n",
            "my_comment: [\"Attached patch against 0.8.\n",
            "\n",
            "The patch also add a new startup option to renew the node id on startup. This could be useful if someone lose one of its sstable (because of a bad disk for instance) and dont want to fully decommission that node.\n",
            "\n",
            "This could arguably be splitted in another ticket though.\" what is \"renewing a node id?\" \"Its picking a new UUID for the current node to use for new counter increment.\n",
            "\n",
            "The problem is that on a given node we store deltas for its current nodeId (to avoid synchronized read-before-write but Im starting to wonder is that was the smartest ever). Anyway if scrub skips a row it may skip some of those deltas. Lets say at first there is no increments coming for this row for A as first distinguished replica. So far we are still kind of good because on a read (with CL > ONE) the result coming from A will have a version for its own sub-count smaller that the one on the other replica so we will us the sub-count on those replica and return the correct value.\n",
            "\n",
            "However as soon as A acknowledge new increments for this row it will start inserting new deltas while he is not intrinsically up to date. Which will result in an definitive undercount.\n",
            "\n",
            "The goal of renewing the node id of A is to make sure that second part never happen (because after the renew A will add new deltas as A not A anymore).\n",
            "\n",
            "Anyway now that Ive plugged the brain this patch doesnt really works because A will never be repaired by the other nodes of its now inconsistent value.\n",
            "\n",
            "So I have no clue how to actually fix that.\" \"It may be that the best short fix here is to make scrub *not* skipping row on counter column families (though CASSANDRA-2614 would change that to never ever skipping row) and just throw a RuntimeException.\" \"bq. make scrub not skip rows on counter column families\n",
            "\n",
            "+1\n",
            "\n",
            "bq. CASSANDRA-2614 would change that to never ever skipping row\n",
            "\n",
            "Only if you actually did have a counter in the column_metadata right?\" Attaching patch to simply re-throw the exception instead of skipping the row for counter column families.\n",
            "\n",
            "bq. Only if you actually did have a counter in the column_metadata right?\n",
            "\n",
            "right. +1\n",
            "\n",
            "can you add a link to this issue in the \"dangerous\" comment? Committed with suggested comment update. Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])\n",
            "    \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2941\n",
            "issue_type: Improvement\n",
            "summary: Expose number of rpc timeouts for individual hosts metric via jmx\n",
            "description: We have a total number timeouts for each node. It's better for monitoring to break down this total number into number of timeouts per host that this node tried to connect to.\n",
            "architectural impact: NO\n",
            "comments: ['expose the number of timeouts per host.\\nexpose the delta of this metric.\\n', \"- does not apply to 0.8 for me\\n- i don't see anything to prevent dropping timeouts b/c of race in timeoutreporter.apply.  (using NBHM + replace would fix this)\\n- if you did the recentTimeouts create first, then the timeouts put, you wouldn't have to special case recent == null later\", \"patch are old, need to rebase. i'll do it.\\n\\ntimeoutreporter.apply is only called in one thread, right? In expireMap, a timerTask will be created to monitor the cache, yes/no ?\\n\\nIf the previous is true, the reason for me to do it this way is that I only do 'write' operation to the hashmap in one thread so that we will not corrupt the data structure. Although get**** is called from multi threaded, only 'read' operations of hashmap is performed so we don't need 'lock' here. I think this is the reason I try not to create an atomicLong and insert into hashmap.\", 'rebased my patch.', \"bq. timeoutreporter.apply is only called in one thread, right?\\n\\nyou're right, that should be fine.\\n\\nbq. rebased my patch.\\n\\nWhat did you rebase against?  There have been no commits in the meantime but it does not apply to 0.8 head.\", 'Sorry for the confusion. This patch worked against the current trunk.', \"bq. Although get**** is called from multi threaded, only 'read' operations of hashmap is performed so we don't need 'lock' here.\\n\\nActually, we still need to establish a happens-before for the read, or we have no guarantees that the JMX thread will ever see the updates made by the timeout reporter.  So we could either use a Map of AtomicLong or a ConcurrentMap of Long.\", 'I am not clear why we have no guarantees that the JMX thread will ever see the updates made by the timeout reporter. The current structure is that, if there is a timeout happened, apply() will be called for it and if this is the first time for a certain IP address, an atomicLong will be created for it. Since this is the first time for this IP address to time out, it is natural not to see its updates before. From then on, it will get updated whenever JMX threads call getRecent***(). Maybe I miss something here?', \"I'm talking about timeoutsPerHost, not recentTimeoutsPerHost.  since tPH is a plain HashMap there is no happens-before relationship between the updates and the reads.\", 'change timeoutsPerHost to use AtomicLong.\\nre-structure the code of getRecentTimeoutPerHost()', 'committed, thanks!', 'Integrated in Cassandra-0.8 #289 (See [https://builds.apache.org/job/Cassandra-0.8/289/])\\n    expose rpc timeouts per host in MessagingServiceMBean\\npatch by Melvin Wang; reviewed by jbellis for CASSANDRA-2941\\n\\njbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1160449\\nFiles : \\n* /cassandra/branches/cassandra-0.8/CHANGES.txt\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingServiceMBean.java\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java\\n', \"for the records, this patch had the following lines:\\n{noformat}\\n        AtomicLong c = timeoutsPerHost.get(ip);\\n        if (c == null)\\n            c = timeoutsPerHost.put(ip, new AtomicLong());\\n        c.incrementAndGet();\\n{noformat}\\nwhich are a guaranteed NPE.\\nI've fixed that directly though (in r1164068).\", \"Ah, my bad. Thinking of python's dictionary while I did this :) Sorry.\", 'Integrated in Cassandra-0.8 #310 (See [https://builds.apache.org/job/Cassandra-0.8/310/])\\n    Fix typo introduced by CASSANDRA-2941\\n\\nslebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1164068\\nFiles : \\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java\\n']\n",
            "my_comment: \n",
            "* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-2975\n",
            "issue_type: Improvement\n",
            "summary: Upgrade MurmurHash to version 3\n",
            "description: MurmurHash version 3 was finalized on June 3. It provides an enormous speedup and increased robustness over version 2, which is implemented in Cassandra. Information here:\n",
            "\n",
            "http://code.google.com/p/smhasher/\n",
            "\n",
            "The reference implementation is here:\n",
            "http://code.google.com/p/smhasher/source/browse/trunk/MurmurHash3.cpp?spec=svn136&r=136\n",
            "\n",
            "I have already done the work to port the (public domain) reference implementation to Java in the MurmurHash class and updated the BloomFilter class to use the new implementation:\n",
            "\n",
            "https://github.com/lindauer/cassandra/commit/cea6068a4a3e5d7d9509335394f9ef3350d37e93\n",
            "\n",
            "Apart from the faster hash time, the new version only requires one call to hash() rather than 2, since it returns 128 bits of hash instead of 64.\n",
            "architectural impact: NO\n",
            "comments: ['So v3 is faster to compute a 128bit hash, than v2 is to compute a 64bit one?', 'Surprising, but yes. It\\'s dramatically faster. The MurmurHash author reports a 50% speedup over v2 at http://code.google.com/p/smhasher/wiki/MurmurHash3. I ran my own simple benchmark on the Java version comparing the existing MurmurHash.hash64() function to the MurmurHash.hash3_x64_128() I added and found an even larger advantage. The improvement is so huge that I wonder a little bit if there isn\\'t a flaw in my test, but here it is:\\n\\n{code:java}\\nstart = System.currentTimeMillis();\\nlong[] reta = {0, 0};\\nByteBuffer buf = strToByteBuffer(key);\\nfor (int i=0; i<cnt; i++)\\n{\\n  buf.clear();\\n  reta = MurmurHash.hash3_x64_128(buf, 0, key.length(), (int) reta[0]);\\n}\\nend = System.currentTimeMillis();\\nSystem.err.println(\"Ran v3 \" + cnt + \" times in \" + (end - start) + \" ms.\");\\n{code}\\n\\nSimilarly for v2.\\n\\nOutput:\\n{code}\\nRan v2 100000000 times in 19993 ms.\\nRan v3 100000000 times in 3104 ms.\\n{code}\\n\\nFWIW, I also ran some tests where I generated random strings and seeds and submitted them to both the reference implementation and the Java port and found no differences.', 'Interesting.  Sounds like a free lunch. :)\\n\\nBesides speed, we\\'d need to make sure Murmur3 gives us as good a hash distribution as Murmur2, so our bloom filter false positive rate doesn\\'t go up -- see BloomFilterTest, which runs with \"ant long-test\". ', \"Also, it's not quite as simple as ripping out M2 and replacing with M3 -- we need to continue to support M2 for compatibility with old data files.  Look at uses of Descriptor.hasStringsInBloomFilter for a similar change.\", \"The test didn't seem to produce any errors:\\n\\n{code}\\n[junit] Testsuite: org.apache.cassandra.utils.LongBloomFilterTest\\n[junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 75.354 sec\\n[junit] \\n[junit] Testsuite: org.apache.cassandra.utils.LongLegacyBloomFilterTest\\n[junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 76.545 sec\\n{code}\\n\\nI'll have to familiarize myself more with Cassandra before dealing with the other issue you pointed out. I've just been using the BloomFilter class and only have a very general understanding of the overall system.\", 'Can you dig a little deeper and compare actual FP rates?\\n\\nlogging/printing fpratio before\\n\\n{code}\\n        assert fp_ratio < 1.03 : fp_ratio;\\n{code}\\n\\nshould be fine.', 'Summary:\\n\\n{code}\\nMean FP rates for version 2:\\nLongBloomFilterTest: 0.997967059178744\\nLongLegacyBloomFilterTest: 0.997908061594203\\n\\nMean FP rates for version 3:\\nLongBloomFilterTest: 0.998045621980676\\nLongLegacyBloomFilterTest: 0.998863888888889\\n{code}\\n\\n\\nDetails:\\n\\n{code}\\nVersion 2:\\n     [echo] running long tests\\n    [junit] WARNING: multiple versions of ant detected in path for junit \\n    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class\\n    [junit]      and jar:file:/Users/jbl/git/cassandra/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class\\n    [junit] Testsuite: org.apache.cassandra.utils.LongBloomFilterTest\\n    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 106.213 sec\\n    [junit] \\n    [junit] ------------- Standard Error -----------------\\n    [junit] fp_ratio = 0.9973043478260869\\n    [junit] fp_ratio = 0.9965793478260869\\n    [junit] fp_ratio = 0.9996123188405797\\n    [junit] fp_ratio = 1.0004746376811595\\n    [junit] fp_ratio = 0.998409420289855\\n    [junit] fp_ratio = 0.9920978260869565\\n    [junit] fp_ratio = 0.9979420289855072\\n    [junit] fp_ratio = 0.9940797101449276\\n    [junit] fp_ratio = 0.9983913043478261\\n    [junit] fp_ratio = 1.0006159420289855\\n    [junit] fp_ratio = 1.0000362318840579\\n    [junit] fp_ratio = 1.0000615942028985\\n    [junit] ------------- ---------------- ---------------\\nmean = 0.997967059178744\\n\\n    [junit] Testsuite: org.apache.cassandra.utils.LongLegacyBloomFilterTest\\n    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 61.721 sec\\n    [junit] \\n    [junit] ------------- Standard Error -----------------\\n    [junit] fp_ratio = 0.998095652173913\\n    [junit] fp_ratio = 0.9982576086956522\\n    [junit] fp_ratio = 0.999159420289855\\n    [junit] fp_ratio = 1.0001340579710145\\n    [junit] fp_ratio = 1.0011557971014493\\n    [junit] fp_ratio = 0.9967717391304348\\n    [junit] fp_ratio = 0.9955978260869566\\n    [junit] fp_ratio = 0.9989673913043479\\n    [junit] fp_ratio = 0.9966231884057971\\n    [junit] fp_ratio = 0.9973514492753623\\n    [junit] fp_ratio = 0.9969855072463768\\n    [junit] fp_ratio = 0.9957971014492754\\n    [junit] ------------- ---------------- ---------------\\nmean      = 0.997908061594203\\n\\n\\nVersion 3:\\n     [echo] running long tests\\n    [junit] WARNING: multiple versions of ant detected in path for junit \\n    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class\\n    [junit]      and jar:file:/Users/jbl/git/cassandra/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class\\n    [junit] Testsuite: org.apache.cassandra.utils.LongBloomFilterTest\\n    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 75.994 sec\\n    [junit] \\n    [junit] ------------- Standard Error -----------------\\n    [junit] fp_ratio = 0.9986532608695652\\n    [junit] fp_ratio = 0.997158695652174\\n    [junit] fp_ratio = 0.9995797101449275\\n    [junit] fp_ratio = 0.9995\\n    [junit] fp_ratio = 0.9984565217391305\\n    [junit] fp_ratio = 0.9987101449275362\\n    [junit] fp_ratio = 0.9979528985507247\\n    [junit] fp_ratio = 0.9998224637681159\\n    [junit] fp_ratio = 0.9938876811594203\\n    [junit] fp_ratio = 0.9993623188405797\\n    [junit] fp_ratio = 0.9953369565217391\\n    [junit] fp_ratio = 0.9981268115942029\\n    [junit] ------------- ---------------- ---------------\\nmean      = 0.998045621980676\\n\\n    [junit] Testsuite: org.apache.cassandra.utils.LongLegacyBloomFilterTest\\n    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 60.999 sec\\n    [junit] \\n    [junit] ------------- Standard Error -----------------\\n    [junit] fp_ratio = 0.998095652173913\\n    [junit] fp_ratio = 0.9983760869565217\\n    [junit] fp_ratio = 0.9993043478260869\\n    [junit] fp_ratio = 0.9996159420289855\\n    [junit] fp_ratio = 0.9980217391304348\\n    [junit] fp_ratio = 1.0016920289855074\\n    [junit] fp_ratio = 0.9953623188405797\\n    [junit] fp_ratio = 0.9968188405797102\\n    [junit] fp_ratio = 0.9947173913043478\\n    [junit] fp_ratio = 1.000695652173913\\n    [junit] fp_ratio = 1.0030760869565218\\n    [junit] fp_ratio = 1.0005905797101449\\n    [junit] ------------- ---------------- ---------------\\nmean      = 0.998863888888889\\n{code}', \"You weren't kidding about compatibility with old data files not being simple. It actually turned out to be fairly major surgery. The original changes just to support Mumur3 are here:\\n\\nhttps://github.com/lindauer/cassandra/commit/cea6068a4a3e5d7d9509335394f9ef3350d37e93\\n\\nThe additional proposed changes to support backward compatibility are at:\\n\\nhttps://github.com/lindauer/cassandra/commit/9d7479675752a07732f434b307be6642d8b3e85f\\n\\nI can't say I'm completely satisfied with these changes. It feels like we should unify with LegacyBloomFilter now that there are 3 versions. It also feels like all of the places where a serializer is selected based on a Descriptor version/flag could be moved under one roof, where callers just pass the Descriptor and it returns the correct serializer instance. But, not being too familiar with Cassandra, I was trying to be minimally invasive for fear of breaking something.\\n\\nAll of the tests pass, but I haven't added any tests, such as making sure that old files can still be read in. Like I said, I'm not very familiar with Cassandra, so you should review these changes carefully. (I'm sure you would anyway.)\\n\", \"Stu said he could review this since they're particularly interested in bloom filter performance.\", 'First of all - can you please rebase both with latest trunk and attach them to JIRA?\\n\\nWhat I see from first look (about patch for backward compatibility):\\n  \\n  - I think we should extract interface from BloomFilter class and make BF a factory as now we have Murmur{2,3}BloomFilter classes \\n  - Needs a test for compatibility with old SSTables (which are using Murmur2BF)\\n  - minor note: comment about new SSTable version is missing at the top of the Descriptor class\\n\\nAs soon as you attach files in here I will apply and play with them and maybe find other problems.\\n\\n\\n', 'Murmur3 changes', 'Murmur3->Murmur2 backwards compatibility', '* sstable.Descriptor\\n** Add a description for the version \\'i\\' bump\\n** Could you rename \"usesOldBloomFilter\" to \"uses32BitBloomFilter\"? I know you didn\\'t add it, but with 3 filter-related bools, it\\'s time to get more descriptive\\n* sstable.IndexHelper\\n** Rather than 2 booleans being passed to defreezeBloomFilter, maybe just pass the Descriptor\\n* sstable.SSTableReader\\n** In .loadBloomFilter(), could the .uses*BloomFilter checks move into a factory function on one of the bloom filter classes, or in a deserializer?\\n\\nOther than those nits, this looks great: thanks Brian!\\n\\n(the committer for this patch should probably generate a new legacy-sstables entry for version \\'h\\': see LegacySSTableTest)', 'Has this fix been finalized? ', \"No, sorry. I'll try to get to Stu's requests soon.\", 'In the attached patch, in MurmurHash.hash3_x64_128(), in the main loop:\\n\\n{noformat}\\n        for(int i = 0; i < nblocks; i++)\\n        {\\n            int i_8 = i << 4;\\n        ...\\n{noformat}\\n\\n{{i_8}} is assigned but not used, as far as I can see?', \"Does anyone have any data on what the typical key size is (i.e. the average input size for the hash)?\\n\\nI have a couple of optimisations for the MurmurHash3 implementation that I think give another 10-40% speedup, particularly for smaller values (e.g. 30% speedup for buffer lengths under 256 bytes) and no worse for large values (tens of KB). These results were on a AMD Phenom II X6 1055T @ 2.80 GHz, under 64-bit Windows 7, Java 1.6.0_27.\\n\\nFirstly, inline the {{rotl64}} calls , e.g. so that\\n{noformat}\\nk1 = rotl64(k1, 31);\\n{noformat}\\nbecomes\\n{noformat}\\nk1 = (k1 << 31) | (k1 >>> 33);\\n{noformat}\\n\\n-Secondly, rather than a large {{switch-case}} to handle the 'tail', use nested {{if-else}} to form a simple binary search. Particularly for relatively small inputs, handling the 'tail' is a significant part of the computation. E.g:-\\n\\n{noformat}\\nint ln = length & 15;\\nif (ln > 8)\\n  {\\n     if (ln > 12)\\n       {\\n          // etc for cases 13 - 15\\n       }\\n     else\\n       {\\n          // cases 11 and 12\\n       }\\n\\n  }\\nelse\\n  {\\n     // etc for cases 1-7\\n  }\\n{noformat}\\n\\nWill try to post a proper benchmark when I've tidied it up (run out of time today!) so anyone interested can try it on other hardware...\\n\\n-This latter optimisation is pretty verbose and ugly to look at - it _might_ be just as fast, and much more concise, to lookup the offsets and shifts from an array, and deal with the special cases 1 and 9 as, well, special cases - but haven't benchmarked this alternative yet...-\", 'Our typical key size is under 20 bytes.', 'I really must learn not to post stuff late at night :-(.  The \"optimisation\" of the {{switch-case}} breaks the algorithm because it relies on the fall-through behaviour of {{switch-case}} in C and Java. Oh well. The inlining only speeds things up a few percent, but might be worthwhile if others see the same improvement on other hardware.', 'Really, JIT doesn\\'t inline the rot164?  Did you \"warm up\" the JVM before timing things?  http://stackoverflow.com/questions/504103/how-do-i-write-a-correct-micro-benchmark-in-java', 'It appears not! Benchmark is attached so folks can try it on other hardware (no point if the advantage is specific to my particular machine!) Will post my results below:', 'The benchmark does several rounds of warmup for each iteration (i.e. for each buffer size from 1 to 32 bytes). \\n\\nIt reduces the number of iterations as the input buffer size grows, so that each run processes a similar number of bytes - though this is probably irrelevant since the performance seems fairly constant with respect to buffer size.\\n\\n{noformat}\\nRunning test for buffer lengths from 1 to 32\\n         *|    Ratio: 0.96 for keylength 1 iterations=100000000\\n         *|    Ratio: 0.95 for keylength 2 iterations=50000000\\n         *|    Ratio: 0.95 for keylength 3 iterations=33333333\\n         *|    Ratio: 0.96 for keylength 4 iterations=25000000\\n         *|    Ratio: 0.94 for keylength 5 iterations=20000000\\n         *|    Ratio: 0.94 for keylength 6 iterations=16666666\\n         *|    Ratio: 0.96 for keylength 7 iterations=14285714\\n         *|    Ratio: 0.93 for keylength 8 iterations=12500000\\n        * |    Ratio: 0.89 for keylength 9 iterations=11111111\\n         *|    Ratio: 0.93 for keylength 10 iterations=10000000\\n         *|    Ratio: 0.95 for keylength 11 iterations=9090909\\n         *|    Ratio: 0.95 for keylength 12 iterations=8333333\\n         *|    Ratio: 0.93 for keylength 13 iterations=7692307\\n         *|    Ratio: 0.90 for keylength 14 iterations=7142857\\n         *|    Ratio: 0.95 for keylength 15 iterations=6666666\\n        * |    Ratio: 0.86 for keylength 16 iterations=6250000\\n        * |    Ratio: 0.87 for keylength 17 iterations=5882352\\n         *|    Ratio: 0.91 for keylength 18 iterations=5555555\\n        * |    Ratio: 0.83 for keylength 19 iterations=5263157\\n        * |    Ratio: 0.83 for keylength 20 iterations=5000000\\n        * |    Ratio: 0.80 for keylength 21 iterations=4761904\\n        * |    Ratio: 0.88 for keylength 22 iterations=4545454\\n         *|    Ratio: 0.91 for keylength 23 iterations=4347826\\n         *|    Ratio: 0.91 for keylength 24 iterations=4166666\\n        * |    Ratio: 0.88 for keylength 25 iterations=4000000\\n         *|    Ratio: 0.92 for keylength 26 iterations=3846153\\n        * |    Ratio: 0.85 for keylength 27 iterations=3703703\\n        * |    Ratio: 0.88 for keylength 28 iterations=3571428\\n        * |    Ratio: 0.88 for keylength 29 iterations=3448275\\n        * |    Ratio: 0.89 for keylength 30 iterations=3333333\\n         *|    Ratio: 0.92 for keylength 31 iterations=3225806\\n--------\\nOld (ms): 18938\\nNew (ms): 17470\\nOverall ratio: 0.9224838948146583\\n{noformat}\\n\\ni.e. 8% improvement on average.\\n\\n', 'Have just repeated the benchmark with HeapByteBuffer as input rather than DirectByteBuffer (using ByteBuffer allocate() rather than allocateDirect()), and the performance improvement seems to almost vanish.\\n\\nThe input to MurmurHash within Cassandra seems to be a HeapByteBuffer (based on adding a println to the existing MurmurHash2 hash64() method), so the inlining is probably of no benefit in practice.', \"Vijay, would you mind picking this up by addressing Stu's feedback to Brian's patchset?\", 'Will do\\n\\nSent from my iPhone\\n\\n\\n', 'Thanks, Vijay.', 'Attached is the refactor which includes fixes as per the suggestions. Added a factory to make adding newer hashesh easier but left the Legacy alone but it will be fairly trivial and more cleaner if we want to refactor a little more. Let me know thanks! Tests passed and the long test shows significant improvement Thanks Brian!', 'updating the patch because old one missed the new files created.', 'This patch is for murmur partitioner also or it is just faster bloom filter?', 'The latter.  CASSANDRA-3772 is open to try a Murmur-based partitioner.', 'Vijay, can you please rebase?', 'Done! unit tests and functional tests works fine.', '+1 with following nit - LazilyCompactedRowTest modifications are redundant and my additions patch which adds \"clear()\" method to the Filter class and fixes problem in the LongBloomFilterTest.', 'Committed with cleaned white spaces and added license headers to the new files.']\n",
            "my_comment: 0.9224838948146583\n",
            "{noformat}\n",
            "\n",
            "i.e. 8% improvement on average.\n",
            "\n",
            " Have just repeated the benchmark with HeapByteBuffer as input rather than DirectByteBuffer (using ByteBuffer allocate() rather than allocateDirect()) and the performance improvement seems to almost vanish.\n",
            "\n",
            "The input to MurmurHash within Cassandra seems to be a HeapByteBuffer (based on adding a println to the existing MurmurHash2 hash64() method) so the inlining is probably of no benefit in practice. \"Vijay would you mind picking this up by addressing Stus feedback to Brians patchset?\" Will do\n",
            "\n",
            "Sent from my iPhone\n",
            "\n",
            "\n",
            " Thanks Vijay. Attached is the refactor which includes fixes as per the suggestions. Added a factory to make adding newer hashesh easier but left the Legacy alone but it will be fairly trivial and more cleaner if we want to refactor a little more. Let me know thanks! Tests passed and the long test shows significant improvement Thanks Brian! updating the patch because old one missed the new files created. This patch is for murmur partitioner also or it is just faster bloom filter? The latter.  CASSANDRA-3772 is open to try a Murmur-based partitioner. Vijay can you please rebase? Done! unit tests and functional tests works fine. +1 with following nit - LazilyCompactedRowTest modifications are redundant and my additions patch which adds \"clear()\" method to the Filter class and fixes problem in the LongBloomFilterTest. Committed with cleaned white spaces and added license headers to the new files.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-298\n",
            "issue_type: Task\n",
            "summary: check for common PEBCAKs in startup\n",
            "description: listenaddress=0.0.0.0 is one\n",
            "\n",
            "any others?\n",
            "\n",
            "let's abort and link to the FAQ or other docs as necessary\n",
            "architectural impact: NO\n",
            "comments: ['http://en.wikipedia.org/wiki/Pebcak', \"adds checks for listenaddress.\\n\\nno suggestions for other checks have been forthcoming so let's put this ticket out of its misery.\", 'Looks good. +1', 'committed', 'Integrated in Cassandra #264 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/264/])\\n    check for listenaddress misconfiguration\\npatch by jbellis; reviewed by gdusbabek for \\n']\n",
            "my_comment: http://en.wikipedia.org/wiki/Pebcak \"adds checks for listenaddress.\n",
            "\n",
            "no suggestions for other checks have been forthcoming so lets put this ticket out of its misery.\" Looks good. +1 committed Integrated in Cassandra #264 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/264/])\n",
            "    check for listenaddress misconfiguration\n",
            "patch by jbellis; reviewed by gdusbabek for \n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-303\n",
            "issue_type: Improvement\n",
            "summary: allow get_slice to operate on SC subcolumns\n",
            "description: again, post CASSANDRA-185\n",
            "architectural impact: NO\n",
            "comments: ['Currently, there is no index at subcolumn level. So, do we want to just add scan-based get_slice or do we want to add subcolumn index? The latter further complicates the storage format.', 'for this ticket i will be happy to just say \"we still assume supercolumns have to fit in memory\"\\n\\njust want to expose the slice api at that level, since get_columns_since goes away w/ CASSANDRA-185', '    add get_slice for supercolumn, tests.  add back updated TimeSortTest.  we need to pass gcBefore to the filters so Slice can count correctly.\\n', 'All tests pass, looks good. +1', 'committed']\n",
            "my_comment: Currently there is no index at subcolumn level. So do we want to just add scan-based get_slice or do we want to add subcolumn index? The latter further complicates the storage format. for this ticket i will be happy to just say \"we still assume supercolumns have to fit in memory\"\n",
            "\n",
            "just want to expose the slice api at that level since get_columns_since goes away w/ CASSANDRA-185     add get_slice for supercolumn tests.  add back updated TimeSortTest.  we need to pass gcBefore to the filters so Slice can count correctly.\n",
            " All tests pass looks good. +1 committed\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3032\n",
            "issue_type: Task\n",
            "summary: clean up KSMetadata, CFMetadata\n",
            "description: There are too many conversion methods between Thrift and Avro and Native, which is a potential source of bugs.\n",
            "architectural impact: NO\n",
            "comments: ['Standardizes on to{Avro,Thrift} and from{Avro,Thrift} instance methods.  redundant methods are removed.', 'updated to fix regression in CFMD.fromThrift that was causing CliTest to fail', 'Needs rebase', 'rebased', 'Integrated in Cassandra-0.8 #293 (See [https://builds.apache.org/job/Cassandra-0.8/293/])\\n    Fix NPE in describe_ring with a mixed cluster.\\nPatch by brandonwilliams, reviewed by jbellis for CASSANDRA-3032\\n\\nbrandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1161189\\nFiles : \\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java\\n', 'Committed with minor changes and fixes + CFMetaDataTest', 'Integrated in Cassandra #1045 (See [https://builds.apache.org/job/Cassandra/1045/])\\n    Clean up KSMetadata, CFMetadata from unnecessary Thrift<->Avro conversion methods\\npatch by Jonathan Ellis and Pavel Yaskevich; reviewed by Pavel Yaskevich for CASSANDRA-3032\\n\\nxedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1161230\\nFiles : \\n* /cassandra/trunk/test/unit/org/apache/cassandra/config/DatabaseDescriptorTest.java\\n* /cassandra/trunk/test/unit/org/apache/cassandra/config/ColumnDefinitionTest.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/config/ColumnDefinition.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/cql/DropIndexStatement.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/UpdateColumnFamily.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/Migration.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/cql/QueryProcessor.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/AddKeyspace.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/thrift/CassandraServer.java\\n* /cassandra/trunk/test/unit/org/apache/cassandra/thrift/ThriftValidationTest.java\\n* /cassandra/trunk/CHANGES.txt\\n* /cassandra/trunk/src/java/org/apache/cassandra/db/DefsTable.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/cql/AlterTableStatement.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/config/KSMetaData.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/UpdateKeyspace.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/AddColumnFamily.java\\n* /cassandra/trunk/test/unit/org/apache/cassandra/config/CFMetaDataTest.java\\n* /cassandra/trunk/test/unit/org/apache/cassandra/db/DefsTest.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/config/CFMetaData.java\\n* /cassandra/trunk/test/unit/org/apache/cassandra/SchemaLoader.java\\n']\n",
            "my_comment: \n",
            "* /cassandra/trunk/test/unit/org/apache/cassandra/config/DatabaseDescriptorTest.java\n",
            "* /cassandra/trunk/test/unit/org/apache/cassandra/config/ColumnDefinitionTest.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/config/ColumnDefinition.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/cql/DropIndexStatement.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/UpdateColumnFamily.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/Migration.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/cql/QueryProcessor.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/AddKeyspace.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/thrift/CassandraServer.java\n",
            "* /cassandra/trunk/test/unit/org/apache/cassandra/thrift/ThriftValidationTest.java\n",
            "* /cassandra/trunk/CHANGES.txt\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/db/DefsTable.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/cql/AlterTableStatement.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/config/KSMetaData.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/UpdateKeyspace.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/AddColumnFamily.java\n",
            "* /cassandra/trunk/test/unit/org/apache/cassandra/config/CFMetaDataTest.java\n",
            "* /cassandra/trunk/test/unit/org/apache/cassandra/db/DefsTest.java\n",
            "* /cassandra/trunk/src/java/org/apache/cassandra/config/CFMetaData.java\n",
            "* /cassandra/trunk/test/unit/org/apache/cassandra/SchemaLoader.java\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-34\n",
            "issue_type: Bug\n",
            "summary: Hinted handoff rows never get deleted\n",
            "description: from the list: \"after the hints are delivered, the hinted keys are deleted from the hinted CF only, but not from the application CF.\"\n",
            "\n",
            "Prashant verified that this is a bug that can't be fixed until deletes are fully working.\n",
            "\n",
            "Note: when we fix this, see if we can do so w/o compromising the immediate-GC of the hinted CF keys.\n",
            "architectural impact: NO\n",
            "comments: ['Sending out hinted data needs correct RowMutation support.', 'Attach a fix. About the patch.\\n\\n1. Make sendMessage blocking.\\n2. Delete the rows in application CF after hinted data is sent. To do that, we need to collect the largest timestamp among columns in a CF and then delete the CF with the largest timestamp.\\n3. When a column is in a deleted CF and their timestamps are the same, the rule is that the deleted CF wins. This rule is needed for step 2 above. Change CFStore.removeDeleted according to this rule.', 'Patch v2. Added some description on hinted data gets delivered.', 'any idea what the purpose of this code in runHints is?  why flush if nothing changed?\\n\\n            \\tif(hintedColumnFamily == null)\\n            \\t{\\n                    columnFamilyStore_.forceFlush();\\n            \\t\\treturn;\\n            \\t}\\n', 'also, now that we have range queries, it seems that a normal CF would be a better fit for this than a single super CF with keys as supercolumns.  i guess that is a separate issue.', \"committed the sendMessage fix.  I've reworked the rest substantially in two parts.\\n\\n01 is just refactoring / cleanup.\\n\\n02 includes your fix to deleteKey, and also:\\n A make hint generation include a real timestamp so we can do meaningful deletes\\n B call removeDeleted on the data we read locally to purge tombstones\\n C because of (B) any supercolumn w/o subcolumns simply won't exist so we know we can skip re-deleting the endpoint data.  so deleteKey becomes deleteHintedData.\\n D because deleted data is not immediately purged, increased the scheduled interval fro 20min to 60 to reduce the load of scanning the hint CF.\\n\\n(for another ticket: since all this is purely local data and not subject to read repair we should find a way to GC it immediately post-delete.)\\n\\nhow does this look to you?\", 'Integrated in Cassandra #57 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/57/])\\n    make sendMessage only return true after ack by recipient.\\npatch by Jun Rao; reviewed by jbellis for \\n', 'created CASSANDRA-128 for improvments beyond the scope of 0.3', \"Looked at the new patch. Here are some comments.\\n\\n1. Move the comments above sendMessage to the beginning of class.\\n\\n2. There is compilation error in MinorCompactionManager because of the removed HintedHandOff class.\\n\\n3. Your new code deletes a column using the same timestamp as the column to be deleted. This is a more fundamental question. If a column has a non-delete entry and a deleted entry with the same timestamp, which one wins? I don't think that we can rely on the ordering of the insertion/deletion. This is because the insertion and the deletion can end up in different SSTables. During compaction, FileStruct is sorted only by keys. Therefore, columns from different SSTables with the same key can come in arbitrary order. One solution is to modify CF.addColumn such that a deletion always wins when the column timestamp is the same. Not sure if there is any other implications, though.\\n\", \"4. It's probably worthwhile to make intervalInMins_ in HHM configurable.\", 'patch to make tombstones have higher precedence than non- for same timestamp in Column (and SuperColumn, to be consistent w/ C and CF)', \"(incorporated changes for comments (1) and (2) into my patchset, not bothering resubmitting those unless you really want 'em)\", 'noted comment (4) on CASSANDRA-128', 'Comments for the new patch.\\n1. In CFStore.removeDeleted(), we should add comments to explain how we resolve the conflicts among CF, SC, and C when the timestamps are the same. As time goes, we are likely to forget those decisions that we have made.\\n\\nOther than that, the patch looks fine to me.\\n', 'done and committed.', 'Integrated in Cassandra #63 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/63/])\\n    ']\n",
            "my_comment: since all this is purely local data and not subject to read repair we should find a way to GC it immediately post-delete.)\n",
            "\n",
            "how does this look to you?\" Integrated in Cassandra #57 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/57/])\n",
            "    make sendMessage only return true after ack by recipient.\n",
            "patch by Jun Rao; reviewed by jbellis for \n",
            " created CASSANDRA-128 for improvments beyond the scope of 0.3 \"Looked at the new patch. Here are some comments.\n",
            "\n",
            "1. Move the comments above sendMessage to the beginning of class.\n",
            "\n",
            "2. There is compilation error in MinorCompactionManager because of the removed HintedHandOff class.\n",
            "\n",
            "3. Your new code deletes a column using the same timestamp as the column to be deleted. This is a more fundamental question. If a column has a non-delete entry and a deleted entry with the same timestamp which one wins? I dont think that we can rely on the ordering of the insertion/deletion. This is because the insertion and the deletion can end up in different SSTables. During compaction FileStruct is sorted only by keys. Therefore columns from different SSTables with the same key can come in arbitrary order. One solution is to modify CF.addColumn such that a deletion always wins when the column timestamp is the same. Not sure if there is any other implications though.\n",
            "\" \"4. Its probably worthwhile to make intervalInMins_ in HHM configurable.\" patch to make tombstones have higher precedence than non- for same timestamp in Column (and SuperColumn to be consistent w/ C and CF) \"(incorporated changes for comments (1) and (2) into my patchset not bothering resubmitting those unless you really want em)\" noted comment (4) on CASSANDRA-128 Comments for the new patch.\n",
            "1. In CFStore.removeDeleted() we should add comments to explain how we resolve the conflicts among CF SC and C when the timestamps are the same. As time goes we are likely to forget those decisions that we have made.\n",
            "\n",
            "Other than that the patch looks fine to me.\n",
            " done and committed. Integrated in Cassandra #63 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/63/])\n",
            "    \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3489\n",
            "issue_type: Bug\n",
            "summary: EncryptionOptions should be instantiated\n",
            "description: As the title says, otherwise you get an NPE when the options are missing from the yaml.  It's included in my second patch on CASSANDRA-3045 and is a one line fix.\n",
            "architectural impact: NO\n",
            "comments: ['There\\'s a bunch of \"if encryption options is null then ignore it\" special cases already, if you\\'re going to instantiate a default instead then let\\'s get rid of those.\\n\\nMay also need to be applied to 0.8 unless aforesaid special cases cover everything.', 'I could only find the special case added the first time we fixed this back in 0.8 for CASSANDRA-3007.  Attached patch removes that and instantiates the default instead.', \"Hmm.  I thought the other place was OTC, but that's going to NPE in the current code base.  So +1 for this patch.\", \"(Checked, and 0.8 OTC does have the null check.  So we're good there.)\", 'Committed.']\n",
            "e going to instantiate a default instead then let\\s get rid of those.\n",
            "\n",
            "May also need to be applied to 0.8 unless aforesaid special cases cover everything. I could only find the special case added the first time we fixed this back in 0.8 for CASSANDRA-3007.  Attached patch removes that and instantiates the default instead. \"Hmm.  I thought the other place was OTC but thats going to NPE in the current code base.  So +1 for this patch.\" \"(Checked and 0.8 OTC does have the null check.  So were good there.)\" Committed.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3577\n",
            "issue_type: Bug\n",
            "summary: TimeoutException When using QuorumEach or ALL consistency on Multi-DC\n",
            "description: Currently we have \n",
            "1) StorageProxy.sendMessages() sending messages to the first node in the other DC...  \n",
            "2) A node in the other DC will remove the ForwardHeader and sendRR (Adding a MessageID to the Queue).\n",
            "3) The receiving node receives the mutation, updates and sends the response to the Original Co-ordinator.\n",
            "4) Co-Ordinator now checks for the MessageID (which it never had)\n",
            "\n",
            "All the Quorum_Each updates fail in the co-ordinator, this issue started showing up after CASSANDRA-3472 the code was introduced in CASSANDRA-2138 .\n",
            "\n",
            "Simple Fix is to remove the optimization in 0.8 and fix it in 1.x because it seems to me like it needs a change to the Message service version.\n",
            "\n",
            "Possible Solution: We might want send the message ID's to be used by the all the nodes in other DC (Which is currently generated by the node which receives the Forward request see: (2) ).\n",
            "architectural impact: NO\n",
            "comments: ['All we need to do is forward with the original id, no?  Patch attached to do that.', '(Patch is against 0.8.)', 'But when the Co-Ordinator receives the response with the message ID the message is already removed because ResponseVerbHandler does\\nMessagingService.instance().removeRegisteredCallback(id);\\nWe wont have the ID there.\\n\\n\\n\\n', \"You're right, we switched to using unique message IDs per target in CASSANDRA-2058 so that we can track timeouts for the dynamic snitch, so my patch won't work.\\n\\nI agree that pre-generating extra IDs on the coordinator is the easiest fix, and also that we should just disable this behavior in 0.8 (which was the case until CASSANDRA-3472 anyway).\", 'removing mutation optimization for .8, i will work on the update to 1.1 shortly. Thanks!', 'committed .8 patch w/ comment pointing to this issue', 'Integrated in Cassandra-0.8 #409 (See [https://builds.apache.org/job/Cassandra-0.8/409/])\\n    remove nonlocal DC write optimization\\npatch by Vijay; reviewed by jbellis for CASSANDRA-3577\\n\\njbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1210902\\nFiles : \\n* /cassandra/branches/cassandra-0.8/CHANGES.txt\\n* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java\\n', 'Testing took some additional time, This patch is on 1.1 with an updated MessagingService.version to handle both older version and new version mutations.', \"Patch doesn't apply to latest trunk for me, can you rebase?\", 'Sorry, Rebased to the the trunk. Thanks!', 'v3 attached.  Some cleanup of StorageProxy, switches to FastBAIS, and does a version check on the receiving side as well as the sending (since we do have released versions in the wild sending out \"bad\" FORWARD_HEADERs).', '+1 Thanks!', 'committed', 'Integrated in Cassandra #1249 (See [https://builds.apache.org/job/Cassandra/1249/])\\n    multi-dc replication optimization supporting CL > ONE\\npatch by Vijay and jbellis for CASSANDRA-3577\\n\\njbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1212088\\nFiles : \\n* /cassandra/trunk/CHANGES.txt\\n* /cassandra/trunk/src/java/org/apache/cassandra/db/RowMutationVerbHandler.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java\\n* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java\\n', 'This can actually cause the more subtle problem of CASSANDRA-3585: Node A (DC1) sends a write to node B (DC2), which forwards to node C (DC2).  Node C replies to node A with the message ID it received from node B.  If the message generation on A and B is far enough apart, then A will not have a callback for the reply and all you will see happen is the write timeout (at CL > ONE).  But if A *does* have a callback (for a different operation) waiting, then A will try to apply the mutation response to that callback, which (if the callback is for a read) will result in the error see in that ticket.']\n",
            "my_comment: Node A (DC1) sends a write to node B (DC2) which forwards to node C (DC2).  Node C replies to node A with the message ID it received from node B.  If the message generation on A and B is far enough apart then A will not have a callback for the reply and all you will see happen is the write timeout (at CL > ONE).  But if A *does* have a callback (for a different operation) waiting then A will try to apply the mutation response to that callback which (if the callback is for a read) will result in the error see in that ticket.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-359\n",
            "issue_type: Bug\n",
            "summary: CFS readStats_ and diskReadStats_ are missing\n",
            "description: None\n",
            "architectural impact: NO\n",
            "comments: [\"shouldn't we also get rid of getReadDiskHits from the mbean + implementation per our irc discussion?\", 'Removed diskReadStats.\\n', 'committed', 'Integrated in Cassandra #165 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/165/])\\n    add back read latency stats for CFS.getColumnFamily.  patch by Sammy Yu; reviewed by jbellis for \\n']\n",
            "my_comment: [\"shouldnt we also get rid of getReadDiskHits from the mbean + implementation per our irc discussion?\" Removed diskReadStats.\n",
            " committed Integrated in Cassandra #165 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/165/])\n",
            "    add back read latency stats for CFS.getColumnFamily.  patch by Sammy Yu; reviewed by jbellis for \n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3690\n",
            "issue_type: Bug\n",
            "summary: Streaming CommitLog backup\n",
            "description: Problems with the current SST backups\n",
            "1) The current backup doesn't allow us to restore point in time (within a SST)\n",
            "2) Current SST implementation needs the backup to read from the filesystem and hence additional IO during the normal operational Disks\n",
            "3) in 1.0 we have removed the flush interval and size when the flush will be triggered per CF, \n",
            "          For some use cases where there is less writes it becomes increasingly difficult to time it right.\n",
            "4) Use cases which needs BI which are external (Non cassandra), needs the data in regular intervals than waiting for longer or unpredictable intervals.\n",
            "\n",
            "Disadvantages of the new solution\n",
            "1) Over head in processing the mutations during the recover phase.\n",
            "2) More complicated solution than just copying the file to the archive.\n",
            "\n",
            "Additional advantages:\n",
            "Online and offline restore.\n",
            "Close to live incremental backup.\n",
            "\n",
            "Note: If the listener agent gets restarted, it is the agents responsibility to Stream the files missed or incomplete.\n",
            "\n",
            "There are 3 Options in the initial implementation:\n",
            "1) Backup -> Once a socket is connected we will switch the commit log and send new updates via the socket.\n",
            "2) Stream -> will take the absolute path of the file and will read the file and send the updates via the socket.\n",
            "3) Restore -> this will get the serialized bytes and apply's the mutation.\n",
            "\n",
            "Side NOTE: (Not related to this patch as such) The agent which will take incremental backup is planned to be open sourced soon (Name: Priam).\n",
            "architectural impact: NO\n",
            "comments: [\"The socket business sounds complicated.  CASSANDRA-1602 is a lot more straightforward, I'd recommend starting with that.\", 'Hi Jonathan, But there is additional IO which the server has to do to copy the archive logs to a different location (not locally)... \\nWhile streaming the Commit log back to the server we have to copy it first and then read it back which is also a over head in recovery. \\n\\nSomething like copying the data to S3 in amazon and copying right back for the node for recovery. (this backup will also be used for test cluster refresh for prod data and BI which is completely a different system)\\nRecovery in most case are loose of instance or the whole cluster (Virtual machines).', \"0001 => Adds a configuration so we can avoid recycling in case some one wants to copy the files across to another location like a archive logs\\n0002 => Adds CommitLogListener, implementation can recive the updates to the commitlogs.\\n0003 => helper JMX in case the user wants to query the active CL's\\n0004 => this can go to the tools folder/we dont need to commit it to the core.\", 'bq. But there is additional IO which the server has to do to copy the archive logs to a different location (not locally)... While streaming the Commit log back to the server we have to copy it first and then read it back which is also a over head in recovery.\\n\\nWhat if you mounted the archive logs via s3fs?', \"We spent some time looking into s3fs, and ran into problems between fuse (which s3fs depends on) and mmap. I put together a small java app to simulate writing to local disc vs. s3fs using mmapp'ed and non-mmap'ed files. (Note most of my java sample code is based on CommitLogSegment's implementation.) We found the non-mmap'ed files wrote to s3fs without a hitch, but writing to the mmap'ed S3 mount failed. The failure was different between OpenJDK 6 vs. Sun JDK, but they were both SIGBUS errors. Also, I ran the tests on my local ubuntu box running Linux 3.0.0-12, fuse version (fusermount --version) at 2.8.4. \\n\\nAt this point, it seems like s3fs isn't as viable as one would hope.\\n\\n\\n\\n\", 'That does rule out using s3fs in read/write mode, but I imagine that would be a pretty bad idea from a latency standpoint anyway.  But in the context of just mounting log files for replay/recover, CommitLog uses RandomAccessReader which is ordinary buffered i/o.', 'Starting to think... What we really want is a Async Triggers CASSANDRA-1311 which listens for all the updates + a way to restore the data with mutation before starting the node. In someways thats what the original patch was trying to do will it make sense to merge these two efforts?', \"I'm skeptical of trying to do this on top of triggers.  First, CASSANDRA-1311 seems to lean towards coordinator-level triggers rather than replica-level.  Second, I don't think it makes sense for a Trigger-level API to deal with raw bytes, which would mean losing efficiency from having to re-serialize RowMutations.\\n\\nI like the postgresql approach: http://www.postgresql.org/docs/9.1/static/continuous-archiving.html -- briefly, you configure an {{archive_command}} that tells it how you want it to copy full log segments off-server when full, and set up a recovery.conf file when you want to recover, which includes a {{restore_command}} that is the inverse of the archive command.\\n\\nThe main difference is that postgresql's default wal segment size is 16MB, which gives them a finer resolution than our 128MB.  I can't think of a reason we can't lower ours, though.\", 'Hi Jonathan,\\n\\nAttached patch does exactly what we discussed here. Its almost the same as PostgreSQL :) \\n\\nIn addition we can start the node with -Dcassandra.join_ring=false and then use JMX to restore files one by one via JMX.\\n\\nPlz let me know.', \"I don't see any uses of CommitLogRecover outside of CommitLog, which makes me think this is an unrelated refactoring.  Is that correct?  If so, let's split that out of this ticket.\", 'Hi Jonathan, The refactor is mainly for the following:\\n\\n>>> In addition we can start the node with -Dcassandra.join_ring=false and then use JMX to restore files one by one via JMX.\\nThis allows us to start the recovery process before all the files are downloaded from S3/External source. I can do that it in another ticket, let me know. Thanks!', \"I've made a bunch of minor changes and pushed to https://github.com/jbellis/cassandra/branches/3690-v3.\\n\\nI noticed that we need to wait for the archive to finish whether we end up recycling or not.  Seems to me it would be simpler to continue to always recycle, but (as we have here) wait for the archive first.  So archive can copy off to s3 or whatever directly, instead of ln somewhere else as an intermediate step.  Total i/o will be lower and commitlog will create extra segments if needed in the meantime.\\n\\nMaybe we should also have a restore_list_segments command as well, so we can query s3 (again for instance) directly and have restore_command pull from there, rather than requiring a local directory?\", 'Also: would be nice to get rid of the new Thread / busywait archive dance.  If we used an ExecutorService instead, we could add the Future to the segment and just say segment.waitForArchive(), no looping.', 'Hi Jonathan, Attached patch incorporates all the recommended changes.... except\\n\\n>>> Maybe we should also have a restore_list_segments command as well, so we can query s3 (again for instance) directly and have restore_command pull from there, rather than requiring a local directory?\\nIMHO. It might be better if we have a streaming API to list and stream the data in... otherwise we need have to download to the local FS anyways, So it will be better to incrementally download and use the JMX to restore the files independently (example: A external agent), that may be a simple solution for now..... If the user has a NFS mount it will work even better all he needs to do is to \"ln -s\" location and he is done :)\\n\\nPlz note that i also removed the requirement to turn off recycling for backup (as recommended), but i left that as configurable because it will good to have unique names in the backup sometimes so we dont overwrite :)', 'bq. it will good to have unique names in the backup sometimes so we dont overwrite \\n\\nIf you think about it, \"target filename\" is just a suggestion... you\\'d be free to ignore it and generate a different filename (incorporating timestamp for instance, or even a uuid) in the archive script.', 'Hi Jonathan, Agree, would you like to remove the configuration to disable recycle (which this patch added)? let me know... Thanks!', \"I do think we should make recycling always-on; it's a non-negligible performance win and so far we don't have a use case that requires disabling it.\", 'Hi Jonathan, \\nv5 removes the recycle related changes and\\nAdded 2 JMX (getActiveSegmentNames and getArchivingSegmentNames)\\n\\n(list all files) - (getActiveSegmentNames) will provide a view of orphan files which failed archiving...', \"v6 attached.  The primary changes made are fixes to the Future logic: the only way you'll get a null Future back is if no archive tack was submitted; if it errors out, you'll get an ExecutionException when you call get(), but never a null.\\n\\nUpdated getArchivingSegmentNames javadoc to emphasize that it does NOT include failed archive attempts. Not sure if this is what was intended.\", '+1', 'committed w/ some final improvements to yaml comments']\n",
            "my_comment: the only way youll get a null Future back is if no archive tack was submitted; if it errors out youll get an ExecutionException when you call get() but never a null.\n",
            "\n",
            "Updated getArchivingSegmentNames javadoc to emphasize that it does NOT include failed archive attempts. Not sure if this is what was intended.\" +1 committed w/ some final improvements to yaml comments\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3708\n",
            "issue_type: Sub-task\n",
            "summary: Support \"composite prefix\" tombstones\n",
            "description: None\n",
            "architectural impact: NO\n",
            "comments: [\"@Vijay: are you actively working on this? I'd keen to give it a shot myself unless you do.\", 'I just started looking into this hence plz go ahead.', \"I've pushed a first version of this at https://github.com/pcmanus/cassandra/commits/3708.\\n\\nIt's an inefficient version in that range tombstones are stored where the row deletion infos is stored today. In other words, we deserialize all those range tombstones (for a row) every time. Thus this won't scale to more than a few tombstones. I have plans to improve that, but in the meantime this version felt like a good first step. In particular, outside of not scaling, it is fully functional and an improved version will build on this anyway. As such, feedbak on this would already be useful.\\n\\nBasically, what this does is abstracting the deletion details inside the (existing) DeletionInfo class, to then adds support for range tombstones (stored with an interval tree) to that class. Amongst various details, this change the serialized form of a ColumnFamily, and thus required to push the message versioning down to the ColumnFamilySerializer.\\n\\nUnit tests are passing but I've also put a simple at https://github.com/pcmanus/cassandra-dtest/commits/3708_tests. Note that only CQL3 has support for those range tombstones.\\n\", \"I've updated my branch at https://github.com/pcmanus/cassandra/commits/3708 to add efficient on-disk handling of the new range tombstones.\\n\\nThe idea is that we don't want to have to read every range tombstone for each query, but only the ones corresponding to the columns queried. For that, the idea is to write the range tombstone along with the columns themselves. So the basic principal of the patch is that if we have a range tombstone RT[x, y] deleting all columns between x and y, we write a tombstone marker on disk before column x. Of course in practice that's more complicated because we want to be sure to read that tombstone even if we read only say y. To ensure that, such tombstone marker is repeated at the beginning of every column block (index block) the range covers (the code is smart enough to not repeat a marker that is superseded by other ones so there won't be a lot of such repeated marker at the beginning of each block in practice).\\n\\nNote that those tombstone marker are only specific for the on-disk format (in memory we use an interval tree), which has 2 consequences for the patch:\\n# the on-disk format now diverges a little bit from the wire format. So the code separates (hopefullly) cleanly serialization functions that deal with on-disk format from the others. I don't think it's a bad idea to have that distinction anyway since we don't want to break the wire protocol but it's ok to change the on-disk one.\\n# on-disk column iterators (SSTable{Slice,Name}Iterator) have to handle those tombstone markers that are not columns per-se. I.e, after having read them from disk we want to store them in the interval tree of the ColumnFamily object, not as an IColumn in the ColumnFamily map. To do this distinction, the code introduces an interface called OnDiskAtom, which represent basically either a column or a range tombstone. And the sstable iterators return those OnDiskAtom which are then ultimately added correctly to the resulting ColumnFamily object. I do think this is the clean way to handle this, but this is responsible for quite a bit of code diffs.\\n\\nI'll also note that both those changes should be useful for CASSANDRA-4180 too to handle the end-of-row marker described in that issue.\\n\\nNow I admit this patch is not a small one, but unit tests are passing and there is a few basic tests at https://github.com/pcmanus/cassandra-dtest/commits/3708_tests.\\n\\nLastly, I'll add that the support for this by CQL3 is minimal as of this patch. We only allow what is basically the equivalent of the 'delete a whole super column' behavior. But it would be simple to allow for more generic use of range tombstones, i.e to allow stuff like:\\n{noformat}\\nDELETE FROM test WHERE k=0 AND c > 3 and c <= 10\\n{noformat}\\nBut the patch is big enough that we can see that later.\\n\", \"I've rebased the patches to https://github.com/pcmanus/cassandra/commits/3708-2. I also changed a bit what was in which patch to hopefully make it easier to review, but I don't know how much I succeed (some changes in the earlier patches really make sense only with the following ones).\", \"I checked out Sylvain's latest branch and overall it looked good to me. New on disk format works fine with compaction and streaming, as well as RowMutations among nodes.\\n\\nOne thing that doesn't work though is the following:\\nI created table as in dtest for 3708,\\n\\n{code:sql}\\nCREATE TABLE test1 (\\n  k int,\\n  c1 int,\\n  c2 int,\\n  v1 int,\\n  v2 int,\\n  PRIMARY KEY (k, c1, c2)\\n);\\n{code}\\n\\nand insert several rows.\\nDeleting using following statement works as expected in dtest:\\n\\n{code:sql}\\nDELETE FROM test1 WHERE k = 0 AND c1 = 0;\\n{code}\\n\\nBut following delete statement doesn't work.\\n\\n{code:sql}\\nDELETE FROM test1 WHERE k = 0 AND c1 = 0 AND c2 = 0;\\n{code}\\n\\nAlthough specifying which columns to delete does work.\\n\\n{code:sql}\\nDELETE v1, v2 FROM test1 WHERE k = 0 AND c1 = 0 AND c2 = 0;\\n{code}\\n\\nFrom the log, I did't see any DeletionInfo in RowMutation for the first (not working) statement.\\n\\n{code}\\nDEBUG [Thrift:4] 2012-05-17 16:27:46,515 StorageProxy.java (line 174) Mutations/ConsistencyLevel are [RowMutation(keyspace='Keyspace1', key='00000000', modifications=[ColumnFamily(test1 [])])]/ONE\\n{code}\\n\\nI think this is just the matter of DeleteStatement. Other than that, LGTM.\", \"You're right, DeleteStatement wasn't handling this case correctly. I pushed the simple fix to the 3708-2 branch.\", '+1', 'Committed, thanks', 'There is a bug in DeletionInfo.\\n{code}\\npublic int dataSize()\\n    {\\n        int size = TypeSizes.NATIVE.sizeof(topLevel.markedForDeleteAt);\\n        for (RangeTombstone r : ranges)\\n            size += r.data.markedForDeleteAt;\\n        return size;\\n    }\\n{code}\\n\\n1) We should do TypeSizes.NATIVE.sizeof(r.data.markedForDeleteAt)\\n2) We should also calculate the type sizes for the range tombstones.', \"Right, I've been arguably a little too hasty with my rebase-before-commit. Fixed in commit 5ab69b6, thanks.\", 'looks like there is another bug in the following code.\\n\\nAbstractType.onDiskAtomComparator\\n{code}\\n                        int comp2 = AbstractType.this.compare(t1.max, t1.max);\\n                        if (comp2 == 0)\\n                            return t1.data.compareTo(t2.data);\\n                        else\\n                            return comp2;\\n{code}', 'Fixed, thanks for spotting it.', 'IntervalTree:254\\n\\n{code}\\nif (comparator == null)\\n  Collections.sort(allEndpoints, comparator);\\nelse\\n  Collections.sort((List<Comparable>)allEndpoints);\\n{code}', 'Good catch. Fixed in b4e47bca8c80de488, thanks Daniel.', 'Dont want to reopen again because its not a bug but maybe an improvement ...\\n\\nIt seems that searchInternal is always descending into the tree even when there are no chances to find a result anymore.\\nUnless I missing something a break early test could help (unit tests pass and hit the break early test):\\n\\n{code}\\nvoid searchInternal(Interval<C, D> searchInterval, List<D> results)\\n{\\n    if (comparePoints(searchInterval.max, low) < 0 ||\\n        comparePoints(searchInterval.min, high) > 0) return;\\n{code}\\n', 'lgtm, committed']\n",
            "my_comment: ranges)\n",
            "            size += r.data.markedForDeleteAt;\n",
            "        return size;\n",
            "    }\n",
            "{code}\n",
            "\n",
            "1) We should do TypeSizes.NATIVE.sizeof(r.data.markedForDeleteAt)\n",
            "2) We should also calculate the type sizes for the range tombstones. \"Right Ive been arguably a little too hasty with my rebase-before-commit. Fixed in commit 5ab69b6 thanks.\" looks like there is another bug in the following code.\n",
            "\n",
            "AbstractType.onDiskAtomComparator\n",
            "{code}\n",
            "                        int comp2 = AbstractType.this.compare(t1.max t1.max);\n",
            "                        if (comp2 == 0)\n",
            "                            return t1.data.compareTo(t2.data);\n",
            "                        else\n",
            "                            return comp2;\n",
            "{code} Fixed thanks for spotting it. IntervalTree:254\n",
            "\n",
            "{code}\n",
            "if (comparator == null)\n",
            "  Collections.sort(allEndpoints comparator);\n",
            "else\n",
            "  Collections.sort((List<Comparable>)allEndpoints);\n",
            "{code} Good catch. Fixed in b4e47bca8c80de488 thanks Daniel. Dont want to reopen again because its not a bug but maybe an improvement ...\n",
            "\n",
            "It seems that searchInternal is always descending into the tree even when there are no chances to find a result anymore.\n",
            "Unless I missing something a break early test could help (unit tests pass and hit the break early test):\n",
            "\n",
            "{code}\n",
            "void searchInternal(Interval<C D> searchInterval List<D> results)\n",
            "{\n",
            "    if (comparePoints(searchInterval.max low) < 0 ||\n",
            "        comparePoints(searchInterval.min high) > 0) return;\n",
            "{code}\n",
            " lgtm committed\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3743\n",
            "issue_type: Improvement\n",
            "summary: Lower memory consumption used by index sampling\n",
            "description: currently j.o.a.c.io.sstable.indexsummary is implemented as ArrayList of KeyPosition (RowPosition key, long offset)i propose to change it to:\n",
            "\n",
            "RowPosition keys[]\n",
            "long offsets[]\n",
            "\n",
            "and use standard binary search on it. This will lower number of java objects used per entry from 2 (KeyPosition + RowPosition) to 1 (RowPosition).\n",
            "\n",
            "For building these arrays convenient ArrayList class can be used and then call to .toArray() on it.\n",
            "\n",
            "This is very important because index sampling uses a lot of memory on nodes with billions rows\n",
            "architectural impact: NO\n",
            "comments: ['Its all about nuking KeyPosition class', 'This is a good idea.  Are you going to take a stab at it, Radim?', 'i will load cassandra into Eclipse and check how many times is IndexSummary referenced', 'I am working on it now', 'patch is against-1.0. This was expected to go into 1.0-branch. Its small change KeyPosition was referenced by just one other class', '+1 except coding style:\\n\\n- you should use 4 spaces instead of tab\\n- always place { and } on new line', 'Are you going to fix these codestyle errors? its just few lines', 'Please submit an updated patch against 1.1, Radim.', 'Wondering if we can use DecoratedKey[] instead of ArrayList in the attached patch... Just my 2 cents (I noticed this while doing another patch).', 'has java builtin binary search operating on array[] ?', \"There's Arrays.binarySearch.  But do we always know how many positions we have before performing the sampling?  If not you could just use AL.trimToSize at the end to avoid wasted space.  With that the difference b/t AL and array are negligible here.\", 'new patch attached lgtm.', \"Really?  I'm getting 7 of 11 hunks failing to apply against current trunk.\", 'I thought patch is for 1.0 branch.\\nIs it going to trunk?', 'Yes, we should really be keeping 1.0 for bugfixes at this point.  1.1 freeze is in less than a week.', '>>> But do we always know how many positions we have before performing the sampling?\\nI think we do while doing the flush... not a big deal i just noticed long position[] hence was wondering why not DecoratedKeys[]\\n\\n>>> With that the difference b/t AL and array are negligible here.\\nAgreed!', 'i have no plans to use 1.1, lets close ticket.', \"Rebased Radim's patch for trunk.\", 'Slight update attached to use positions.trimToSize instead of leaving an \"orphan\" arraylist around.  As mentioned above, the size difference is negligible.\\n\\nQuestion: is the getKeySamples transform still necessary, or can we just cast the keys collection?\\n\\nOr could we just make it a List of DK in the first place instead of RP?', 'bq. is the getKeySamples transform still necessary, or can we just cast the keys collection? Or could we just make it a List of DK in the first place instead of RP?\\n\\nI think it is better just make a list of DK since IndexSummary.addEntry accepts only DK. Attached v2.', 'committed, thanks!']\n",
            "my_comment: is the getKeySamples transform still necessary or can we just cast the keys collection?\n",
            "\n",
            "Or could we just make it a List of DK in the first place instead of RP? bq. is the getKeySamples transform still necessary or can we just cast the keys collection? Or could we just make it a List of DK in the first place instead of RP?\n",
            "\n",
            "I think it is better just make a list of DK since IndexSummary.addEntry accepts only DK. Attached v2. committed thanks!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3762\n",
            "issue_type: Improvement\n",
            "summary: AutoSaving KeyCache and System load time improvements.\n",
            "description: CASSANDRA-2392 saves the index summary to the disk... but when we have saved cache we will still scan through the index to get the data out.\n",
            "\n",
            "We might be able to separate this from SSTR.load and let it load the index summary, once all the SST's are loaded we might be able to check the bloomfilter and do a random IO on fewer Index's to populate the KeyCache.\n",
            "architectural impact: NO\n",
            "comments: ['This patch will read the keys from the cache and sort it, so the index scan can be less impact full on the MMaped index. \\n\\nTest (Not an extensive test but from my laptop):\\n100 Keys to be loaded into cache.\\n\\n7G data file and 15M index (long type keys)\\nbefore the patch:\\n/var/log/cassandra/system.log:DEBUG [SSTableBatchOpen:4] 2012-01-23 15:10:40,825 SSTableReader.java (line 196) INDEX LOAD TIME for /var/lib/cassandra/data/Keyspace2/Standard3/Keyspace2-Standard3-hc-2777: 850 ms.\\nafter this patch:\\n/var/log/cassandra/system.log:DEBUG [SSTableBatchOpen:4] 2012-01-23 15:10:59,128 SSTableReader.java (line 196) INDEX LOAD TIME for /var/lib/cassandra/data/Keyspace2/Standard3/Keyspace2-Standard3-hc-2777: 177 ms.', \"With this patch we trade whole sequential primary_index read for random I/O with SSTableReader.getPosition() only for amount saved keys. Can you extend key cache, let's make it 75% of the keys, and run your test again? I think the closer key cache size will get to actual number of keys the worse will performance get...\", \"I dont think it is as bad as it looks.... We aren't doing a lot of random IO because with this patch Keys are sorted and we will read the same blocks often and if it is mmapped it will get the most advantage. Also most of the work load, the keys will not be from the same SST's and 75% of the keys falling into a SST is not that common IMO (If they do they have a bigger problem because all their reads are going to be loger and longer) the load time increases if we have a lot of data in the disk.\\nI got around 180ms for 3K keys and thats far is the memory in my laptop :)\\n\\nThe other option is to redesign keycache and save the Index location when we store the keys and then look it up and to fault fill the data which are not in the cache via (getPosition).... Makes sense?\", \"I mention this because the problem in the original ticket was with rolling restarts taking too much time on index summary computation (read going though whole PrimaryIndex for every SSTable out there), so imagine situation when you have few hundreds of SSTables each with key cache in the different parts of the primary index this means if you go with getPosition() calls you will have a lot of random I/O (meaning you will have to seek deeper and deeper into the primary index file which means slower data access even in mmap mode) on each of those and I'm not sure if it's really better than reading primary index sequentially especially knowing that you have already read all of the index/data positions from the Summary component. I propose you do the test with many SSTables and compare system load times (don't forget to drop page cache between tests with `sync; echo 3 > /proc/sys/vm/drop_caches`).\\n\\nBy the way, I forgot to ask you if you dropped page cache before running second test? if you didn't that would pretty much explain such a dramatic improvement in the load time...\", '>>> By the way, I forgot to ask you if you dropped page cache before running second test?\\nNo it is basically a average of multiple runs.... :) Without any additional writes... start and immediatly stop and compare the logs... both tests are the same. Again it is on my laptop.', 'bq. No it is basically a average of multiple runs....  Without any additional writes... start and immediatly stop and compare the logs... both tests are the same. Again it is on my laptop.\\n\\nWhich means the more you run the more data you get cached which affects the results, I would suggest you to drop cache every time you run each of the tests to get cleaner load time values when any I/O is involved.', 'That applies to the sequential index scans too (the variables are same both are in-memory it is just 15MB index :) )... looks like i have to do more extensive test than this one but i am not sure how much is an optimal number i will try loading the keycache over night with stress tool.', \"Bigger test with much spread out data showed some mixed results.... Tested on M24XL nodes in AWS, 8M Keys inserted via Stress tool 3 times to get distribution of the keys into the SST's\\n\\n\\nTrunk shows:\\nDEBUG [SSTableBatchOpen:1] 2012-01-25 09:48:28,098 SSTableReader.java (line 193) INDEX LOAD TIME for /mnt/data/cassandra070/data/system/LocationInfo/system-LocationInfo-hc-17: 16 ms.\\nDEBUG [SSTableBatchOpen:6] 2012-01-25 09:48:47,699 SSTableReader.java (line 193) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-637: 4289 ms.\\nDEBUG [SSTableBatchOpen:3] 2012-01-25 09:48:51,541 SSTableReader.java (line 193) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-607: 8136 ms.\\nDEBUG [SSTableBatchOpen:5] 2012-01-25 09:48:52,910 SSTableReader.java (line 193) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-429: 9504 ms.\\nDEBUG [SSTableBatchOpen:1] 2012-01-25 09:49:07,725 SSTableReader.java (line 193) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-611: 24321 ms.\\nDEBUG [SSTableBatchOpen:7] 2012-01-25 09:49:29,760 SSTableReader.java (line 193) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-534: 46348 ms.\\nDEBUG [SSTableBatchOpen:8] 2012-01-25 09:49:34,972 SSTableReader.java (line 193) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-462: 51560 ms.\\nDEBUG [SSTableBatchOpen:2] 2012-01-25 09:49:35,893 SSTableReader.java (line 193) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-636: 52488 ms.\\nDEBUG [SSTableBatchOpen:4] 2012-01-25 09:49:45,671 SSTableReader.java (line 193) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-635: 62258 ms\\n\\nJMX key cache size:\\n01/25/2012 09:50:18 +0000 org.archive.jmx.Client KeyCacheSize: 104857584\\n\\nAfter this patch:\\nDEBUG [SSTableBatchOpen:6] 2012-01-25 18:22:33,343 SSTableReader.java (line 195) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-637: 5731 ms.\\nDEBUG [SSTableBatchOpen:6] 2012-01-25 18:22:33,344 SSTableReader.java (line 198) key cache contains 2/2184533 keys\\nDEBUG [SSTableBatchOpen:3] 2012-01-25 18:22:34,291 SSTableReader.java (line 195) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-607: 6685 ms.\\nDEBUG [SSTableBatchOpen:3] 2012-01-25 18:22:34,291 SSTableReader.java (line 198) key cache contains 7619/2184533 keys\\nDEBUG [SSTableBatchOpen:1] 2012-01-25 18:22:37,144 SSTableReader.java (line 195) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-611: 9541 ms.\\nDEBUG [SSTableBatchOpen:1] 2012-01-25 18:22:37,145 SSTableReader.java (line 198) key cache contains 45172/2184533 keys\\nDEBUG [SSTableBatchOpen:5] 2012-01-25 18:22:37,275 SSTableReader.java (line 195) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-429: 9664 ms.\\nDEBUG [SSTableBatchOpen:5] 2012-01-25 18:22:37,276 SSTableReader.java (line 198) key cache contains 48914/2184533 keys\\nDEBUG [SSTableBatchOpen:2] 2012-01-25 18:22:42,224 SSTableReader.java (line 195) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-636: 14620 ms.\\nDEBUG [SSTableBatchOpen:2] 2012-01-25 18:22:42,224 SSTableReader.java (line 198) key cache contains 170841/2184533 keys\\nDEBUG [SSTableBatchOpen:4] 2012-01-25 18:22:45,053 SSTableReader.java (line 195) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-635: 17442 ms.\\nDEBUG [SSTableBatchOpen:4] 2012-01-25 18:22:45,053 SSTableReader.java (line 198) key cache contains 241841/2184533 keys\\nDEBUG [SSTableBatchOpen:8] 2012-01-25 18:23:01,720 SSTableReader.java (line 195) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-462: 34107 ms.\\nDEBUG [SSTableBatchOpen:8] 2012-01-25 18:23:01,720 SSTableReader.java (line 198) key cache contains 689699/2184533 keys\\nDEBUG [SSTableBatchOpen:7] 2012-01-25 18:24:19,975 SSTableReader.java (line 195) INDEX LOAD TIME for /mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-534: 112363 ms.\\nDEBUG [SSTableBatchOpen:7] 2012-01-25 18:24:19,975 SSTableReader.java (line 198) key cache contains 2184533/2184533 keys\\n\\nSizes of the index:\\n1068\\t/mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-637-Index.db\\n7852\\t/mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-429-Index.db\\n20044\\t/mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-607-Index.db\\n84600\\t/mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-611-Index.db\\n188560\\t/mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-534-Index.db\\n212204\\t/mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-636-Index.db\\n217292\\t/mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-462-Index.db\\n315988\\t/mnt/data/cassandra070/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-635-Index.db\\n\\n\\nIf we want to see the optimal solution for all the use cases i think we have to go for the alternative where we can save the Keycache position to the disk and read it back and what ever is missing let it fault fill. Agree?\", 'Can you please update your comment with \"key cache contains x/y keys\" to the results on the current trunk but like you did for \"After this patch results\"? Btw, did you drop the page cache using `sync; echo 3 > /proc/sys/vm/drop_caches` before running \"After time patch\" tests?', '>>> did you drop the page cache\\nYes (I always do that except when i am testing in my laptop). \\n\\n>>> Can you please update your comment with \"key cache contains x/y keys\"\\nWhy would that matter? Trunk reads the index even if there is 0/0 keys to be loaded.... but for this test the comments for both the load is the same.... (because all i did was kept restarting in between dropping the cache). The above comments are a sample of multiple restarts... They are relative to my environment.', \"bq. Why would that matter? Trunk reads the index even if there is 0/0 keys to be loaded.... but for this test the comments for both the load is the same.... (because all i did was kept restarting in between dropping the cache). The above comments are a sample of multiple restarts... They are relative to my environment.\\n\\nBecause even if it reads the whole index when don't know how key cache pre-load affects performance. Because with your results we can clearly see the thing I was talking about - after this patch, key cache pre-load directly degrades performance the bigger it gets. \", 'I agree :) \\n\\nCopying from my previous comments:\\n\"If we want to see the optimal solution for all the use cases i think we have to go for the alternative where we can save the Keycache position to the disk and read it back and what ever is missing let it fault fill. Agree?\"\\n\\n\\n\"what ever is missing\" => i mean we might have gotten a new SST\\'s which where created from the point we saved the keycache and the restart... those can be fault filled.', 'bq. If we want to see the optimal solution for all the use cases i think we have to go for the alternative where we can save the Keycache position to the disk and read it back and what ever is missing let it fault fill. Agree?\\n\\nI need to think about this option.', \"bq. With this patch we trade whole sequential primary_index read for random I/O with SSTableReader.getPosition() only for amount saved keys.\\n\\nI thought Vijay said this sorts the cache first.  In which case we're really doing seq i/o, we're just skipping parts that don't have any keys.  Right?\\n\\nbq. If we want to see the optimal solution for all the use cases i think we have to go for the alternative where we can save the Keycache position to the disk and read it back and what ever is missing let it fault fill.\\n\\nI like this idea.  If you have a lot of rows (i.e., a large index) then this is the only thing that's going to save you from doing a lot of i/o.  Even with seq i/o, reading a small cache will be much faster than scanning a large index.\\n\\nThe only downside I see is the question of how much churn your sstables will experience between save, and load.  If you have a small data set that is constantly being overwritten for instance, you could basically invalidate the whole cache.  But, it's quite possible that just reducing cache save period is adequate to address this.  So I think we should give this a try.\", \"It seems like saving cache's data positions (in combination with SSTable index summaries) to the disk to make it independent from the sstable loading is only viable solution we have.\", 'Attached patch does the suggested.\\n\\nNOTE:\\n1) We will take an additional cache hit while saving the data into the cache.\\n2) Older SSTable formats will not have cache saved (without promoted index) after the upgrade.\\n3) User has to drop the old Caches during the upgrade.\\n\\nWe should update the Upgrade notes.', \"We can't require caches drop from users if they want to update to the newer version, imagine if users had a big caches and they would expect them to be warmed up to serve traffic from the beginning (good user experience on system start-up). \\n\\nWe should support current (old cached keys are saved) and new (key + position in the SSTable) schemas equally. Also I don't think that (de-)serializer logic, as you did it, is really needed because we would have sufficient data inside of the cache file to pre-load cache with one simple algorithm (in pseudo-code): \\n\\nFor *KeyCache*:\\n{code}\\nwhile (!eof(cache_file)) {\\n   cache_entry = deserialize_entry(cache_file)\\n\\n   # we don't want to load into the cache entry where \\n   # SSTable to which descriptor corresponds does not exist (compacted, deleted)\\n   if (exists(cache_entry.key.descriptor))\\n      cache.put(cache_entry)\\n}\\n{code}\\n\\nFor *RowCache*:\\n{code}\\nwhile (!eof(cache_file)) {\\n   # as simple as data because we presume that we did cache save as part of the shutdown *(which we should do)*\\n   cache.put(deserialize_entry(cache_file))\\n}\\n{code}\\n\\nIt could be as simple as that because we know how to (de-)serialize RowIndexEntry (that KeyCache uses) as well as ColumnFamily (that RowCache uses), note that we would write/read and (de-)serialize both caches with sequential I/O.\\n\", \">>> We can't require caches drop from users if they want to update to the newer version, imagine if users had a big caches and they would expect them to be warmed up to serve traffic from the beginning (good user experience on system start-up).\\nWe can add versioning and support the older cache files and the newer ones we can deserialize an additional byte about the promoted keys. \\nI was not sure if it is worth it because moving forward we are going to let the keycache fault fill for the newer SST's, If we feel strongly about not dropping the keys i can add logic to it.\\n\\n>>> Also I don't think that (de-)serializer logic, is really needed because we would have sufficient data inside of the cache file to pre-load cache with one simple algorithm (in pseudo-code)\\nIn the current way Descriptor is just a reference and if you want to construct it is an additional overhead :), and in the row cache we dont have the values of the row cache and we have to query ColumnFamilyStore.\", 'I think Pavel is right that we need to give users who rely on the warm cache some kind of solution.\\n\\nOne alternative is, we could provide a \"cache converter\" tool that fills in an old-style cache, with position information from the index.  In other words basically combining an old-style startup, with a new-style cache save.', \"Attached patch doesn't have any of the above limitations added the version (very basic, as the real versioning is controlled by the SSTable itself).\", \"Why don't you serialize data (payload) of the caches, which is the main goal of this ticket? CacheSerializer should serialize the *whole* cache entry - key + payload, that way we would minimize the time taken for processing on deserialize (aka when caches are loaded) and ColumnFamilyStore wouldn't be involved in the deserialization process fo the updated cache and you won't have to make methods such as getTopLevelColumns public. Also you forgot to force cache flush upon system shutdown as I wrote in my previous comment.\", \">>> Why don't you serialize data (payload) of the caches\\nWell the payloads can be in GB's when using Serialized cache and when you are trying to load the old cache you will see inconsistencies between the cache save.\\nWe cannot rely on shutdown hooks to make the data consistent because (http://docs.oracle.com/javase/1.5.0/docs/api/java/lang/Runtime.html#addShutdownHook(java.lang.Thread)) OS has timeouts on the shutdown and it doesnt help it when there are power failures. The main problem with the RowCache Values are that they are mutable and Keycache values are immutable.\\n\\nThe main pain is during startup is keycache, but i dont see why it will be a pain to load the rowcache.\\n\\n\", \"Agreed on both counts -- I don't think this approach works for row cache, nor do I think row cache is as important to optimize.\", 'I can agree on this point but on the other hand if we have big row cache and would read it using CF that would create a ton of random I/O as well as rearrange key cache that we have preloaded before...', \"Agreed, but that's what you're signing up for by enabling row cache saving (same as today).  Which is one reason it defaults to disabled.\", 'Indeed but by giving it additional disk space to save payload you could make I/O sequential and make it harmless to key cache. I was thinking saving row cache into different files for keys and payload so in event of shutdown we could just persist keys and on startup we could check timestamps of the files and decide if we need to load it from CF or it is possible to read payload from save...', \"That still doesn't grantee that we wont have inconsistent row caches. \\n\\nhttp://stackoverflow.com/questions/8171646/when-shutdown-hooks-break-bad\\nkill -9 is a normal way to shut down a Cassandra for us so far.\", \"You should probably move to the SIGINT (or SIGTERM) for a graceful shutdown not even for caches but for other services as well (e.g. CASSANDRA-3936). But I'm aware of that problem and I think that if row cache is hopelessly outdated we don't really want to load it the first place (in current situation or new) because it would be just a waste of time...\", \"Let's move the row cache discussion to a separate ticket and keep this one focused on key cache as the title says.\", 'Ok, currently I see following problems in the v3 \\n\\n  - KeyCacheSerializer.serializingSize(...) method uses AVERAGE_KEY_CACHE_ROW_SIZE where it should use an actual size of the serialized value.\\n  - in RowCacheTest.java deleted assert should be moved back because we really want to test the amount of read rows properly.\\n  - AutoSavingCache.loadSaved(...) method should return if exception occurred while loading old style cache.\\n  - in the AutoSavingCache.loadSaved(...) deleted debug output should be returned because it\\'s very useful for diagnostic purposes.\\n  - we should probably make CacheSerialize.serialize method to return size of the serialized data, would be useful upon cache writes instead of explicitly calling serializedSize\\n  - in the ColumnFamilyStore we really should check if we want to load the cache using (caching == Caching.NONE || caching == Caching.ROWS_ONLY). Right now that check if removed and the cache load is called even if there caching was disabled on the given ColumnFamily, the same also applies for the row cache.\\n  - the following code could be changed (otherwise we would have ifs each time we add a new element):\\n\\n*From*\\n{noformat}\\nif (version == null)\\n    return new File(conf.saved_caches_directory + File.separator + ksName + \"-\" + cfName + \"-\" + cacheType);\\nelse\\n    return new File(conf.saved_caches_directory + File.separator + ksName + \"-\" + cfName + \"-\" + cacheType + \"-\" + version +\".db\");\\n{noformat}\\n\\n*To*\\n{noformat}\\n   return new File(conf.saved_caches_directory + File.separator + ksName + \"-\" + cfName + \"-\" + cacheType + \"-\" + ((version != null) ? version + \".db\" : \"\"));\\n{noformat}', \"I dont know why i decided to delete the debug messages :)\\n\\n>>> KeyCacheSerializer.serializingSize(...) method uses AVERAGE_KEY_CACHE_ROW_SIZE where it should use an actual size of the serialized value.\\nIf we get the real value, this will cause a cache Hit and another cache hit for saving the value. We just need a estimate and we dont need to be accurate IMHO. Added documentation for the same.\\n\\n>>> in RowCacheTest.java deleted assert should be moved back because we really want to test the amount of read rows properly.\\nFixed.\\n\\n>>> AutoSavingCache.loadSaved(...) method should return if exception occurred while loading old style cache.\\nIt does :)\\n\\n>>> in the AutoSavingCache.loadSaved(...) deleted debug output should be returned because it's very useful for diagnostic purposes.\\nFixed\\n\\n>>> in the ColumnFamilyStore we really should check if we want to load the cache using (caching == Caching.NONE || caching == Caching.ROWS_ONLY). Right now that check if removed and the cache load is called even if there caching was disabled on the given ColumnFamily, the same also applies for the row cache.\\nIt is harmless but Fixed!\\n\\n>>> the following code could be changed (otherwise we would have ifs each time we add a new element):\\nDone!\", 'bq. If we get the real value, this will cause a cache Hit and another cache hit for saving the value. We just need a estimate and we dont need to be accurate IMHO. Added documentation for the same.\\n\\nIt means that the method don\\'t return what what it should and should be renamed to *estimate*SerializedSize but as we would to the same for all of the rows in the cache I don\\'t see how it would harm LRU if we do the real calculation.\\n\\nbq. AutoSavingCache.loadSaved(...) method should return if exception occurred while loading old style cache. - It does :)\\n\\nIt does not :) Just uses \"logger.warn(String.format(\"error reading saved cache %s\", path.getAbsolutePath()), e);\" and would go on with processing, it also should have the same error message as the next \"new\" case.\\n\\nbq. we should probably make CacheSerialize.serialize method to return size of the serialized data, would be useful upon cache writes instead of explicitly calling serializedSize\\n\\nWhat about this one?\\n\\n', \">>> It means that the method don't return what what it should and should be renamed to *estimate*SerializedSize but as we would to the same for all of the rows in the cache I don't see how it would harm LRU if we do the real calculation.\\nIf you are getting hot n keys, it will be making hot keys even more hotter.... renamed.\\n\\n>>> It does not\\nthe idea is that the newer cache will delete the older one :) so it is a no op after that.... anyways fixed it just for the sake of it :)\\n\\n>>> What about this one?\\nAs we are using estimate not the real value we need to use the serializedSize.... Thats how it was earlier too :)\", \"Hmm, the size business is a bit tricky.\\n\\nI agree that having serialize() return the bytes written is cleaner than retaining a separate [estimated]serializedSize method.  (We could let AutoSavingCache wrap its output in a CountingOutputStream that just records the length of what its wrapped stream is given.)  *However*, since we're pretty much stuck w/ an estimate for estimatedTotalBytes anyway, it feels strange to count the same data two different ways.\\n\\nBut, if we stick with the current design, we're doing *three* get() calls on each key -- not sequentially either, first we get() everything to estimate total, then we get() each in turn twice to serialize and add in size to progress.\\n\\nOne alternative would be to use an entry set instead of key set in the saving, but CLHM doesn't expose this.\\n\\nWhat if we redefined the problem a bit?  Instead of having totalBytes/bytesComplete in CompactionInfo, we could have long total/complete and String units.  Then we could just return progress in terms of key count for cache saving.\", '>>> What if we redefined the problem a bit? Instead of having totalBytes/bytesComplete in CompactionInfo, we could have long total/complete and String units. Then we could just return progress in terms of key count for cache saving.\\n\\nDone! \\nBTW: In addition there is a minor fix for NPE on CompactionInfo.getID().toString()', 'LGTM, +1\\n\\nNit:\\n\\n{{public CompactionInfo(UUID id, OperationType tasktype, long bytesComplete, long totalBytes, String unit)}}\\n\\nwould be better w/ parameters renamed to\\n\\n{{public CompactionInfo(UUID id, OperationType tasktype, long complete, long total, String unit)}}', '+1 with Jonathan comment and also I think it would be great to add CompactionInfo(OperationType, long complete, long total, String unit) constructor so we don\\'t have to pass \"null\" as UUID argument (hide implementation detail).', 'Committed with the fix! Thanks']\n",
            "my_comment: In addition there is a minor fix for NPE on CompactionInfo.getID().toString() LGTM +1\n",
            "\n",
            "Nit:\n",
            "\n",
            "{{public CompactionInfo(UUID id OperationType tasktype long bytesComplete long totalBytes String unit)}}\n",
            "\n",
            "would be better w/ parameters renamed to\n",
            "\n",
            "{{public CompactionInfo(UUID id OperationType tasktype long complete long total String unit)}} +1 with Jonathan comment and also I think it would be great to add CompactionInfo(OperationType long complete long total String unit) constructor so we don\\t have to pass \"null\" as UUID argument (hide implementation detail). Committed with the fix! Thanks\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3862\n",
            "issue_type: Bug\n",
            "summary: RowCache misses Updates\n",
            "description: While performing stress tests to find any race problems for CASSANDRA-2864 I guess I (re-)found one for the standard on-heap row cache.\n",
            "\n",
            "During my stress test I hava lots of threads running with some of them only reading other writing and re-reading the value.\n",
            "\n",
            "This seems to happen:\n",
            "\n",
            "- Reader tries to read row A for the first time doing a getTopLevelColumns\n",
            "- Row A which is not in the cache yet is updated by Writer. The row is not eagerly read during write (because we want fast writes) so the writer cannot perform a cache update\n",
            "- Reader puts the row in the cache which is now missing the update\n",
            "\n",
            "I already asked this some time ago on the mailing list but unfortunately didn't dig after I got no answer since I assumed that I just missed something. In a way I still do but haven't found any locking mechanism that makes sure that this should not happen.\n",
            "\n",
            "The problem can be reproduced with every run of my stress test. When I restart the server the expected column is there. It's just missing from the cache.\n",
            "\n",
            "To test I have created a patch that merges memtables with the row cache. With the patch the problem is gone.\n",
            "\n",
            "I can also reproduce in 0.8. Haven't checked 1.1 but I haven't found any relevant change their either so I assume the same aplies there.\n",
            "architectural impact: NO\n",
            "comments: [\"Dunno if there's a better way to do it...\", \"I believe you are absolutely right that this is a bug.\\n\\nUnfortunately I don't think including the memtables during cache reads really solves it. If you miss an update, it won't ever get added to the cached row, but the update itself will be flushed at some point and thus not be in any memtable anymore.\\n\\nOne partial solution I see could be that when a read 'reads for caching', it starts adding some sentinel object in the cache for the given row key. That sentinel would need to be an actual (empty) row but marked with the fact it's only a sentinel. When a write look if the row is cache, if it's a sentinel we would add the write to the sentinel. Once the read returns and we actually put the row in cache, we would it (atomically) with the content of the sentinel. A read that check the cache and see a sentinel would just skip the cache (and would not put it's result into the cache). Adapting that to the serializingCache is trivial.\\n\\nUnfortunately, this is not perfect because this would screw counters. Though I guess for counters we could do the same thing as we would do for the serializingCache, i.e, if a read that 'reads for caching' see that the sentinel is not empty, we would just not cache the result (i.e, a row would be cache only if we are sure no write were done concurrently to the read).\", \"bq.  If you miss an update, it won't ever get added to the cached row, but the update itself will be flushed at some point and thus not be in any memtable anymore.\\n\\nVery true ...\\n\\nHow about adopting the strategy we apply with CASSANDRA-2864:\\n\\n- Writers dont update the cache at all\\n- Readers merge cache with memtables\\n- Upon flush merge memtables with cache\", \"{quote}\\nHow about adopting the strategy we apply with CASSANDRA-2864:\\n\\n* Writers dont update the cache at all\\n* Readers merge cache with memtables\\n* Upon flush merge memtables with cache\\n{quote}\\nThe problem with that is that I don't see how we can make that work for counters at all. I also think it would be nice not having to merge on reads if we can avoid it (even if it's in-memory, it still uses CPU).\\n\\nAs a side note, I also suspect it's not bulletproof in theory, as a memtable could be fully flushed while a 'read to be cached' happens and with a bad timing during that, we could still miss an update. Of course, that kind of timing have almost no chance to happen. But in the case where a user triggers a flush manually, a memtable with only a handful of columns could be flushed very quickly, and I suspect the behavior could be observed. However unlikely that is, it'd be better if we can fix this problem once and for all.\\n\\nI'll probably give a shot to my 'sentinel' proposal described above, I don't think it's too much code.\", \"Hmokay ... don't want to abuse Jira as an educational forum but maybe as a reward for the bugreport :-) ... are you saying that a reader could see a memtable view where flushing memtables are gone (flushed) and sstables don't contain the flushed memtables?\\n\\nIf that's the case than yes the cache would lose an update. But that what also imply that a read could miss an update without caching being in place at all no?\\n\\nOtherwise (and that's how I read the code) given that the memtable switch will only happen after the merge the reader will read all updates because they are either in (flushing) memtables or in sstables and the cache will be in fact valid.\\n\\n\", 'To be precise, what I\\'m saying is that (at least in theory) the following scenario would be possible:\\n* A read-for-cache read the memtables grabing updates\\n* then it start reading the sstables\\n* while the previous happens, a new update arrives. The memtable is then flushed and happens to be fully flushed *before* our read-for-cache completes.\\n\\nIn that case, the new update won\\'t be part of the cached row (ever) because during the flush (when we would merge the memtable to the cache) the row was not in the cache yet. That may seem far fetched but consider a simple implementation of you proposition, where the \\'upon flush merge memtables with cache\\' phase happens in the same loop over rows that is used for flushing. It is actually possible for a new write to be \"flushed\" within a few milliseconds of being received by the node: if the update triggers the memtable threshold *and* sorts at the very beginning of the memtable. But don\\'t get me wrong, it would probably be possible to deal with that problem, but it feels a bit complicated and error prone.\\n\\n', 'Patch attached with my \"sentinel\" idea (The patch is against 1.1 currently). I *think* this fixes the problem, and this deal with counters.', \"Just had a look at it and maybe I got it wrong but:\\n\\nCFS.getRawCachedRow returns null for a sentinel and CFS.updateRowCache calls this.\\nIsn't updateRowCache supposed to add changes to the sentinel so that cacheRow can detect the race?\", \"You're right, thanks for catching that. Attached v2 fixed this (I realized that when we hit the cache during range_slice queries we don't update the statistics, which I'm not sure is what we want, but it's unrelated to that issue so haven't changed it).\", 'Looks to me like we might be able to simplify things by splitting the \"initialize row cache\" code (which can assume the cache is empty, and does not need a filter) out from the \"look up a cached row and cache it if it is not present\" method.\\n\\nNit: although not perfect, IMO \"getRawCachedRow\" is a better method name than \"getCachedRowNoStats\" -- the important thing to convey is that we\\'re only inspecting the cache\\'s contents, not changing them.', \"Attaching v3. This mostly fix a but of the previous version where sentinels were not handled correctly in cacheRow(). I've also switch back to getRawCachedRow.\\nI'm not fully sure what you proposed to split exactly, but v3 does split cacheRow() in the hope of increasing clarity. \", \"Attached cleanup patch that applies on top of v3.  Most of the changes are adding docstrings/comments and cleaning up typos.\\n\\nA minor change to the code was to make cacheRow take just cfId and filter, removing the redundant filter.key as a parameter.\\n\\nI also renamed cacheRow to getThroughCache.  Still not 100% happy with that, but my goal is to make the distinction between readAndCache more obvious.\\n\\nFinally, I've modified the logic in invalidateCachedRow according to the reasoning in this comment:\\n\\n{noformat}\\n.       // This method is used to (1) drop obsolete entries from a copying cache after the row in question was updated\\n        // and to (2) make sure we're not wasting cache space on rows that don't exist anymore post-compaction.\\n        // Sentinels complicate this because it means we've caught a read thread in the process of loading\\n        // the cache, and we don't know (in case 2) if it will do so with rows from before the compaction or after,\\n        // so we need to loop until the load completes.\\n{noformat}\\n\\n(I also negated the loop condition, which looked like an oversight.)\", \"The cleanup lgtm.\\n\\nFor the change to invalidateCacheRow however, I wonder if it's worth it. By waiting when we found a sentinel, we may have writes waiting on a read to complete, which could involve a non negligible latency spike. On the other side, if we just leave the sentinel in that case, the only risk we take is that a read may end up putting tombstone in the cache that are already expired. But it doesn't seem like a big deal, especially given that it will very rarely happen.\\n\\nBut in any case, you're right about negating the loop condition. \", \"bq. By waiting when we found a sentinel, we may have writes waiting on a read to complete, which could involve a non negligible latency spike\\n\\nYou're right, that's a worse negative than leaving tombstones in the cache.  I'm fine with changing it back if you update the comments accordingly. :)\", \"Actually then handling of the copying patch by the preceding patches is wrong.  When a put arrives and there is a sentinel, the patch does not add the put to the sentinel correctly. But thinking about it, for the copying cache, we should avoid having writes check the current value in the cache, because that have a non-negligible performance impact. What we should do is let invalidate actually invalidate sentinels. The only problem we're faced with if we do that, is that when a read-for-caching returns, it must make sure his own sentinel hasn't been invalidated. And in particular it must be careful of the case where the sentinel has been invalidated and another read has set another sentinel.\\n\\nAnyway, attaching a v4 (that include the comments cleanups) that choose that strategy instead (and thus is (hopefully) not buggy even in the copying cache case). Note that it means that reads must be able to identify sentinels uniquely (not based on the content), so the code assign a unique ID to sentinel and use that for comparison.\\n\", \"Attached v5 with a simpler approach: for serializing cache, getThroughCache does a classic CAS loop with a sentinel vs the write's invalidate.\\n\\nv5 also adds a containsCachedRow method to CFS so that callers that don't care about the value don't force a deserialize in the serializing cache case.\", \"I don't think v5 works. All sentinels are empty CF, so all sentinels will be equal (in SerializingCache.contentsEqual()). Which means we can have the following sequence of actions:\\n* a read r1 comes, the cache is empty, it sets sentinel s1 and start reading from disk\\n* a write w comes and invalidate s1.\\n* a read r2 comes, the cache is (now) empty, it sets sentinel s2 and start reading from disk\\n* r1 finish reading from disk having missed w. It'll do the replace, but since all sentinel are equals this will succeed (even though the current sentinel is the one of the second read) and we'll end up having missed w.\\n\\nThat's the reason of the sentinel IDs of v4.\", \"v6 pulls RCS out to a separate file and adds a uuid version and equals/hashcode methods.  SerializingCacheProvider uses a custom CF serializer that is RCS-aware.  SerializingCache.replace is simplified to use RCS.equals.  CAS loop is extended to non-serializing cache: since cache/write race is extremely rare, I'd rather take the occasional re-read penalty, than increase the overhead of every row in the cache by making them RCS objects permanently.\", \"Remarks on v6:\\n* Since we don't add stuffs to the sentinel, it has no reason to be a subclass of ColumnFamily. We should probably create a CachedRow class extended by both Sentinel (that would really just be an identifier, no metadata needed) and ColumnFamily and use that as cache values. It'll be cleaner and more importantly more type safe (a cache lookup won't be able to ignore by mistake that it could get a sentinel).\\n* Not adding stuffs to the sentinel also mean that in getThroughCache the counter special case is not needed anymore.\\n* In getThroughCache, if we fail to replace the sentinel, I think we should still better return the data rather than looping and re-reading. Better let the next client read cache the data than getting a crappy latency on the current read.\\n* Is it really an improvement to use UUIDs (over an AtomicLong)? I have nothing against UUID per se but it takes twice the space (and we serialize them) and without having benchmarked it, I'm willing to bet are much faster to generate. And let's be honest, the risk of overflow with an AtomicLong is science-fiction (or to be precise, at 1 millions sentinels created per seconds (which is *way* more than we'll ever see), you'd need more than 100,000 year of uptime to overflow).\\n\", \"v7 attached.\\n\\nbq. Since we don't add stuffs to the sentinel, it has no reason to be a subclass of ColumnFamily\\n\\nTrue, but when I tried this I ended up with a LOT of casting cache values to CF.  I think it might be the lesser of evils the way it is.\\n\\nbq. the counter special case is not needed anymore\\n\\nUpdated.\\n\\nbq. if we fail to replace the sentinel, I think we should still better return the data rather than looping and re-reading\\n\\nMakes sense, updated.\\n\\nbq. Is it really an improvement to use UUIDs (over an AtomicLong)? \\n\\nI'd rather have the reduced contention on instantiation than the 8 bytes of space (during the sentinel lifetime -- this goes away once the sentinel is replaced by the data CF).\", '* In SerializingCache, remove misses a \"not\" in the while condition (this date back from one of my earlier patch). We don\\'t need that new remove method anymore though so it\\'s probably as simple to just remove it from the patch.\\n* In CFS.getThroughCache, the following line\\n{noformat}\\nboolean sentinelSuccess = !CacheService.instance.rowCache.putIfAbsent(key, sentinel);\\n{noformat}\\nshould not be negated.\\n* Also in CFS.getThroughCache, we won\\'t remove the sentinel if there is an exception during the read. It\\'s not a big deal but it doesn\\'t cost much to prevent it from happening.\\n\\nbq. True, but when I tried this I ended up with a LOT of casting cache values to CF. I think it might be the lesser of evils the way it is.\\n\\nIn that case, I think I wouldn\\'t mind too much casts and I would prefer getting the type safety of knowing that a method that take a ColumnFamily can\\'t ever get a sentinel (and to make it explicit when you need to care about sentinel or not), but that\\'s a bit subjective. There would also be some small wins like the fact we wouldn\\'t need to save the cfId when serializing a sentinel.\\n\\nbq. I\\'d rather have the reduced contention on instantiation than the 8 bytes of space\\n\\nMy point was that the UUID don\\'t reduce contention. UUID.randomUUID() uses SecureRandom.nextBytes() that is synchronized (and thus likely entail a much bigger degradation in face of contention than an AtomicLong) and probably a bit CPU intensive. For reference, I did a quick micro-benchmark having 50 threads generating 10,000 ids simultaneously using both methods, using an AtomicLong is two orders of magnitude faster.\\n', 'v8 attached w/ long sentinel and IRowCacheEntry.', 'v8 lgtm mostly except for the 3 remarks at the beginning of my previous comment. Added simple patch on top of v8 that does the proposed modifications.', \"shouldn't {{if (data == null)}} in the finally block be {{if (sentinelSuccess && data == null)}} ?\", \"Oups, you're right. Patch updated.\", '+1', 'Committed, thanks', 'Integrated in Cassandra #1646 (See [https://builds.apache.org/job/Cassandra/1646/])\\n    restore pre-CASSANDRA-3862 approach to removing expired tombstones during compaction (Revision fbb5ec0374e1a5f1b24680f1604b6e9201fb535f)\\nfix build - re-add CompactionController.removeDeletedInCache for commit fbb5ec0374e1a5f1b24680f1604b6e9201fb535f restore pre-CASSANDRA-3862 approach to removing expired tombstones during compaction (Revision 086c06ad7fb211de6be877c3c1ea2ee4f86c6d7e)\\n\\n     Result = ABORTED\\njbellis : \\nFiles : \\n* src/java/org/apache/cassandra/db/compaction/CompactionIterable.java\\n* CHANGES.txt\\n\\ndbrosius : \\nFiles : \\n* src/java/org/apache/cassandra/db/compaction/CompactionController.java\\n']\n",
            "my_comment: \n",
            "* src/java/org/apache/cassandra/db/compaction/CompactionController.java\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3931\n",
            "issue_type: Bug\n",
            "summary: gossipers notion of schema differs from reality as reported by the nodes in question\n",
            "description: On a 1.1 cluster we happened to notice that {{nodetool gossipinfo | grep SCHEMA}} reported disagreement:\n",
            "\n",
            "{code}\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:b0d7bab7-c13c-37d9-9adb-8ab8a5b7215d\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:bcdbd318-82df-3518-89e3-6b72227b3f66\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:bcdbd318-82df-3518-89e3-6b72227b3f66\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f\n",
            "{code}\n",
            "\n",
            "However, the result of a thrift {{describe_ring}} on the cluster claims they all agree and that {{b0d7bab7-c13c-37d9-9adb-8ab8a5b7215d}} is the schema they have.\n",
            "\n",
            "The schemas seem to \"actually\" propagate; e.g. dropping a keyspace actually drops the keyspace.\n",
            "architectural impact: NO\n",
            "comments: [\"I forgot to mention that {{describe_ring}} is using a piece of code that actually sends messages to other nodes asking for their schema, while {{gossipinfo}} is giving the information contained in the Gossiper's endpoint state map.\\n\\nGiven that the schema seems to *actually* be propagated, I suspect this is only a gossiping propagation bug but that is not confirmed.\", \"Hmm, does hinted handoff work in this state?  I ask because we've had this problem before and addressed it there:\\n\\n{code}\\n        waited = 0;\\n        // then wait for the correct schema version.\\n        // usually we use DD.getDefsVersion, which checks the local schema uuid as stored in the system table.\\n        // here we check the one in gossip instead; this serves as a canary to warn us if we introduce a bug that\\n        // causes the two to diverge (see CASSANDRA-2946)\\n{code}\", 'Patch to update gossip when merging remote versions.', 'v2 replaces all the one-off calls of passiveAnnounce by introducing updateVersionAndAnnounce in Schema.', \"+1 with nit: change in Migration is unnecessary because passiveAnnounce get's called as part of Migration.announce() routine so we don't need to change apply() behavior.\", 'Committed w/Migration.apply change removed.', \"@Brandon For the record, we had HH turned off so I don't know.\", 'FYI the fixes for this issue introduced issue CASSANDRA-5025.']\n",
            "my_comment: change in Migration is unnecessary because passiveAnnounce gets called as part of Migration.announce() routine so we dont need to change apply() behavior.\" Committed w/Migration.apply change removed. \"@Brandon For the record we had HH turned off so I dont know.\" FYI the fixes for this issue introduced issue CASSANDRA-5025.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-399\n",
            "issue_type: Bug\n",
            "summary: Consisteny Level of ZERO blocks for ack on Commit Log\n",
            "description: If consistency level is set to ZERO and endpoint is local, clients must wait for a write to the commit log. We need to remove this special case, and just send through MessagingService.getMessagingInstance().sendOneWay.\n",
            "architectural impact: NO\n",
            "comments: ['I remember we put this in to get limited read-your-writes consistency for a session. (Log + rm.apply()  before ack.)\\nSee CASSANDRA-132.\\n\\nThere are cases when this is not the right thing to do (eg skip logging during a load operation), but for a normal client, read-your-writes is way easier to deal with than pure eventual consistency. \\n\\n\\n\\n\\n\\n', \"Committed before I saw Sandeep's objection.  But insert -- the method that handles ConsistencyLevel.ZERO -- is the wrong place to do this.  If you want any blocking you need to use ONE or higher, that's how it's supposed to work.\", '(which goes to the separate insertBlocking method.)', 'to clarify: I am +1 on adding a CASSANDRA-132 special case for insertBlocking, but -1 on having it in insert.  My bad for not catching that in review originally.  Should we re-open 132?', 'I agree that I think this special case should live in insertBlocking. ', \"I'm +1 on the patch, but we should re-open 132. \\nCurrent insertBlocking=1 is not the same as session level read-your-writes.\\nI guess we could just special-case this write for insertBlocking=1. I'd rather not introduce a new ConsistencyLevel.\\n\\n\\nConsistencyLevel.ZERO should help stress test MessagingService... I remember with the old codepath writes would get queued up in the MessagingService and we ended up with a few lost writes. Should be interesting to see if those problems pop up again.\\n\\n\\n\\n\", 'Integrated in Cassandra #182 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/182/])\\n    r/m special case of local destination when writing with ConsistencyLevel.ZERO, since it causes blocking for commitlog.  (MessagingService still optimizes out the network write/read.)  Patch by Chris Goffinet; reviewed by jbellis for \\n']\n",
            "my_comment: I am +1 on adding a CASSANDRA-132 special case for insertBlocking but -1 on having it in insert.  My bad for not catching that in review originally.  Should we re-open 132? I agree that I think this special case should live in insertBlocking.  \"Im +1 on the patch but we should re-open 132. \n",
            "Current insertBlocking=1 is not the same as session level read-your-writes.\n",
            "I guess we could just special-case this write for insertBlocking=1. Id rather not introduce a new ConsistencyLevel.\n",
            "\n",
            "\n",
            "ConsistencyLevel.ZERO should help stress test MessagingService... I remember with the old codepath writes would get queued up in the MessagingService and we ended up with a few lost writes. Should be interesting to see if those problems pop up again.\n",
            "\n",
            "\n",
            "\n",
            "\" Integrated in Cassandra #182 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/182/])\n",
            "    r/m special case of local destination when writing with ConsistencyLevel.ZERO since it causes blocking for commitlog.  (MessagingService still optimizes out the network write/read.)  Patch by Chris Goffinet; reviewed by jbellis for \n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-3997\n",
            "issue_type: Improvement\n",
            "summary: Make SerializingCache Memory Pluggable\n",
            "description: Serializing cache uses native malloc and free by making FM pluggable, users will have a choice of gcc malloc, TCMalloc or JEMalloc as needed. \n",
            "Initial tests shows less fragmentation in JEMalloc but the only issue with it is that (both TCMalloc and JEMalloc) are kind of single threaded (at-least they crash in my test otherwise).\n",
            "architectural impact: NO\n",
            "comments: ['Attached is the test classes used for the test.\\n\\nResults on CentOS:\\n\\n{noformat}\\n[vijay_tcasstest@vijay_tcass-i-a91ee8cd ~]$ /etc/alternatives/jre_1.7.0/bin/java -Djava.library.path=/usr/local/lib/ -cp jna.jar:/apps/nfcassandra_server/lib/*:. com.sun.jna.MallocAllocator 50000 2000000\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   26049380   45638840          0     169116     996172\\n-/+ buffers/cache:   24884092   46804128\\nSwap:            0          0          0\\n**** Starting Test! ****\\nTotal bytes read: 101422934016\\nTime taken: 25407\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   31981924   39706296          0     169116     996312\\n-/+ buffers/cache:   30816496   40871724\\nSwap:            0          0          0\\n**** ending Test!**** \\n[vijay_tcasstest@vijay_tcass-i-a91ee8cd ~]$ export LD_LIBRARY_PATH=/usr/local/lib/\\n[vijay_tcasstest@vijay_tcass-i-a91ee8cd ~]$ /etc/alternatives/jre_1.7.0/bin/java -Djava.library.path=/usr/local/lib/ -cp jna.jar:/apps/nfcassandra_server/lib/*:. com.sun.jna.TCMallocAllocator 50000 2000000\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   26054620   45633600          0     169128     996228\\n-/+ buffers/cache:   24889264   46798956\\nSwap:            0          0          0\\n**** Starting Test! ****\\nTotal bytes read: 101304894464\\nTime taken: 46387\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   28535136   43153084          0     169128     996436\\n-/+ buffers/cache:   27369572   44318648\\nSwap:            0          0          0\\n**** ending Test!**** \\n[vijay_tcasstest@vijay_tcass-i-a91ee8cd ~]$ export LD_LIBRARY_PATH=~/jemalloc-2.2.5/lib/ \\n[vijay_tcasstest@vijay_tcass-i-a91ee8cd ~]$ /etc/alternatives/jre_1.7.0/bin/java -Djava.library.path=~/jemalloc-2.2.5/lib/ -cp jna.jar:/apps/nfcassandra_server/lib/*:. com.sun.jna.JEMallocAllocator 50000 2000000\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   26060604   45627616          0     169128     996300\\n-/+ buffers/cache:   24895176   46793044\\nSwap:            0          0          0\\n**** Starting Test! ****\\nTotal bytes read: 101321734144\\nTime taken: 29937\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   28472436   43215784          0     169128     996440\\n-/+ buffers/cache:   27306868   44381352\\nSwap:            0          0          0\\n**** ending Test!**** \\n[vijay_tcasstest@vijay_tcass-i-a91ee8cd ~]$ \\n\\n{noformat}\\n\\nThe test shows around 4 GB savings. The test was on 101321734144 bytes (101 GB each). The test use CLHM to hold on to the objects and release them when the capacity is reached (5K)', \"I don't understand the goal here.  What is the value in allowing the use of malloc implementations that cause segfaults? :)\", 'Does this preserve the ability to use Unsafe instead of a JNA-backed malloc?', \"Hi Jonathan, The good thing is that it saves us from the Memory Fragmentation which we have seen with the native malloc's after it runs for a prolonge period of time. If the user wants to use a different implementation they can use it. JEMAlloc hasn't segfault in any of my tests I think it is better to use we still need to do more tests per sure.\\n\\n>>> Does this preserve the ability to use Unsafe instead of a JNA-backed malloc?\\nNo, the ticket just makes it pluggable so any other implementation is possible. We dont need to build the *.so/dll's for every environment we come across and the unsafe can be default :)\", 'bq. JEMAlloc hasn\\'t segfault in any of my tests \\n\\nWhat did you mean then by \"both TCMalloc and JEMalloc are kind of single threaded (at-least they crash in my test otherwise)?\"', 'Ohhh sorry for the confusion. \\nJEMAlloc\\'s case: The Malloc/Free should done by ANY one thread at a time. The test had 100 Threads doing malloc/free but only one will actually malloc/free at a time and the \"Time taken\" shows the raw speed.\\nTCMalloc\\'s case: Only one thread should be malloc and doing free. (Even after this it was crashing randomly because of illegal memory access, hence i said JEMalloc hasnt crashed).\\n\\nThe test code does exactly the above.... The implementation should deal with it and avoid contending for malloc and free with multiple threads. Once we deal with it, it works well.', \"bq. The Malloc/Free should done by ANY one thread at a time\\n\\nSo should we just synchronize the JEmalloc allocator methods instead of leaving it to the caller?\\n\\nI'm fine with turning this into a real patch adding a cache allocator option, although from your test results it doesn't look like it's worth shipping TEmalloc -- more fragmentation than JE and substantially slower.\", 'Agree and will do.', 'Attached patch makes the Offheap allocation pluggable and has JEMallocAllocator.\\n\\nTo Test JEMalloc: Plz set \\n\\n#export LD_LIBRARY_PATH=/xxx/jemalloc-2.2.5/lib/\\nJVM Property: -Djava.library.path=/xxx/jemalloc-2.2.5/lib/libjemalloc.so', 'To update from IRC:\\n\\nVijay said he tried jemalloc via LD_PRELOAD and hit corruption problems related to multi-threaded use.  Which is odd, because http://www.canonware.com/jemalloc/, http://www.facebook.com/notes/facebook-engineering/scalable-memory-allocation-using-jemalloc/480222803919, and others claim jemalloc is designed for multithreaded use cases.\\n\\n(If there ARE thread safety problems w/ jemalloc, shouldn\\'t the patch synchronize somewhere?)\\n\\nI did find http://comments.gmane.org/gmane.comp.db.redis.general/7736 which corroborates \"weird problems w/ jemalloc and the jvm,\" although throwing redis into the mix definitely doesn\\'t simplify things.\\n\\nI think this is worth following up on for two reasons:\\n\\n# I\\'d much rather let people customize malloc via preload, than having to write Java + JNA stubs for each\\n# If there are corruption problems with jemalloc, it could affect our cache too\\n\\nCan you follow up on http://www.canonware.com/mailman/listinfo/jemalloc-discuss and see if they can clear anything up?', 'Have an update:\\n\\nJason Evan says: \"LD_PRELOAD\\'ing jemalloc should be okay as long as the JVM doesn\\'t statically link a different malloc implementation.  I expect that if it isn\\'t safe, you\\'ll experience crashes quite early on, so give it a try and see what happens.\"\\n\\nI have also conformed the unsafe isn\\'t statically linked to native Malloc by adding a printf in the malloc c code which basically count\\'s the number of times it is called. Looks like PRELOAD is a better option. I am running a long running test and will close this ticket once it is successful. Thanks!\\n\\n', 'Vijay, can you please also test Hoard Memory Allocator (http://www.hoard.org/) as a comparison to jemalloc?', \">>> Vijay, can you please also test Hoard Memory Allocator (http://www.hoard.org/) as a comparison to jemalloc?\\nSure, but looks like LD_PRELOAD causes crashes due to various reasons and doesn't sound like a good solution. I will check in the mailing list about the same.\\n\\n[vijay_tcasstest@vijay_tcass-i-a91ee8cd crash]$ grep -A2 Problematic *\\nhs_err_pid1309.log:# Problematic frame:\\nhs_err_pid1309.log-# C  [libjemalloc.so+0xb79a]\\nhs_err_pid1309.log-#\\n--\\nhs_err_pid1622.log:# Problematic frame:\\nhs_err_pid1622.log-# C  [libjemalloc.so+0x57f4]  free+0x54\\nhs_err_pid1622.log-#\\n--\\nhs_err_pid16902.log:# Problematic frame:\\nhs_err_pid16902.log-# C  [libjemalloc.so+0xb79a]\\nhs_err_pid16902.log-[error occurred during error reporting (printing problematic frame), id 0xb]\\n--\\nhs_err_pid16902.log:# Problematic frame:\\nhs_err_pid16902.log-# C  [libjemalloc.so+0xb79a]\\nhs_err_pid16902.log-[error occurred during error reporting (printing problematic frame), id 0xb]\\n--\\nhs_err_pid29892.log:# Problematic frame:\\nhs_err_pid29892.log-# C  [libjemalloc.so+0xb79a]\\nhs_err_pid29892.log-#\\n--\\nhs_err_pid30273.log:# Problematic frame:\\nhs_err_pid30273.log-# C  [libjemalloc.so+0xb79a]\\nhs_err_pid30273.log-#\\n--\\nhs_err_pid30645.log:# Problematic frame:\\nhs_err_pid30645.log-# C  [libjemalloc.so+0xb79a]\\nhs_err_pid30645.log-#\\n--\\nhs_err_pid4037.log:# Problematic frame:\\nhs_err_pid4037.log-# C  [libjemalloc.so+0xb79a]\\nhs_err_pid4037.log-[error occurred during error reporting (printing problematic frame), id 0xb]\\n--\\nhs_err_pid7733.log:# Problematic frame:\\nhs_err_pid7733.log-# C  [libc.so.6+0x618a2]\\nhs_err_pid7733.log-[error occurred during error reporting (printing problematic frame), id 0xb]\\n--\\nhs_err_pid7733.log:# Problematic frame:\\nhs_err_pid7733.log-# C  [libc.so.6+0x618a2]\\nhs_err_pid7733.log-[error occurred during error reporting (printing problematic frame), id 0xb]\", \"Hi Pavel, \\nLooks like howard malloc can work seamlessly with LD_PRELOAD but JEmalloc doesn't work well with LD_PRELOAD.\\n\\n{quote}\\nJason says: The crash in free() is the only one that tells me anything at all, and my only guesses are 1) mixed allocator usage or 2) application error, e.g. double free().  I really don't know anything about how the JVM is structured internally , how it interacts with malloc, how it uses/abuses dlopen(), etc., so I'm not going to be of much help without a lot more background information.\\n{quote}\\n\\nIn other hand the attached patch avoids the crashes.\\n\\nComparision of the mallocs:\\n\\n{code}\\n[vijay_tcasstest@vijay_tcass-i-08e1f16c java]$ java -cp /apps/nfcassandra_server/lib/concurrentlinkedhashmap-lru-1.2.jar:/apps/nfcassandra_server/lib/jna-3.3.0.jar:/apps/nfcassandra_server/lib/apache-cassandra-1.1.0-beta2-SNAPSHOT.jar:. com.sun.jna.MallocAllocator 50000 2000000\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   10569792   61118428          0     146360    1864972\\n-/+ buffers/cache:    8558460   63129760\\nSwap:            0          0          0\\n**** Starting Test! ****\\nTotal bytes read: 101423216640\\nTime taken: 28587\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   15950408   55737812          0     146360    1865184\\n-/+ buffers/cache:   13938864   57749356\\nSwap:            0          0          0\\n**** ending Test!**** \\n\\n[vijay_tcasstest@vijay_tcass-i-08e1f16c java]$ export LD_LIBRARY_PATH=/home/vijay_tcasstest/howard/\\n[vijay_tcasstest@vijay_tcass-i-08e1f16c java]$ java -Djava.library.path=/home/vijay_tcasstest/howard/ -cp /apps/nfcassandra_server/lib/concurrentlinkedhashmap-lru-1.2.jar:/apps/nfcassandra_server/lib/jna-3.3.0.jar:/apps/nfcassandra_server/lib/apache-cassandra-1.1.0-beta2-SNAPSHOT.jar:. com.sun.jna.HowardMallocAllocator 50000 2000000\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   10573476   61114744          0     146320    1864972\\n-/+ buffers/cache:    8562184   63126036\\nSwap:            0          0          0\\n**** Starting Test! ****\\nTotal bytes read: 101366196224\\nTime taken: 33959\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   16292664   55395556          0     146320    1865184\\n-/+ buffers/cache:   14281160   57407060\\nSwap:            0          0          0\\n**** ending Test!**** \\n\\n[vijay_tcasstest@vijay_tcass-i-08e1f16c java]$ export LD_LIBRARY_PATH=/home/vijay_tcasstest/jemalloc/lib/\\n[vijay_tcasstest@vijay_tcass-i-08e1f16c java]$ java -Djava.library.path=/home/vijay_tcasstest/jemalloc/lib/ -cp /apps/nfcassandra_server/lib/concurrentlinkedhashmap-lru-1.2.jar:/apps/nfcassandra_server/lib/jna-3.3.0.jar:/apps/nfcassandra_server/lib/apache-cassandra-1.1.0-beta2-SNAPSHOT.jar:. com.sun.jna.JEMallocAllocator 50000 2000000\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   10572896   61115324          0     146332    1864972\\n-/+ buffers/cache:    8561592   63126628\\nSwap:            0          0          0\\n**** Starting Test! ****\\nTotal bytes read: 101360272384\\nTime taken: 29310\\n             total       used       free     shared    buffers     cached\\nMem:      71688220   13243604   58444616          0     146340    1865184\\n-/+ buffers/cache:   11232080   60456140\\nSwap:            0          0          0\\n**** ending Test!**** \\n{code}\\n\\n\", 'Howard allocator uses even more memory (~300 MB more) than standard allocator but jemalloc buys as ~2.5 GB which is pretty good. The last thing here would be to investigate what causes free() segfaults with jemalloc so different memory allocators could be used without any structural changes to the code...\\n\\nWould be helpful if you could describe here the situation when that segfault happens.', 'Segfaults happen in multiple places (opening a file, accessing malloc, while calling free, and in a lot of unrelated cases)... \\nUnless we open JDK source code and figure out how it is structured it is hard to say when exactly it can fails (Let me know if you want to take a look at the hs_err*.log). \\n\\nIn the bright side at least we can isolate this by calling via JNI, and we dont see the issue by loading JEMalloc via LD_LIBRARY_PATH. In v2 I removed the synchronization, i have also attached it here (Plz note the yaml setting is not included just to hide it for now). Thanks!\\nNote: \"jemalloc 2.2.5\" release works fine and so as the git/dev branch.', \"This patch doesn't apply cleanly to trunk (anymore?). Rebase?\", 'Hi Paul, Sorry somehow missed update on the ticket... Rebased in v3\\n\\nFYI: To enable JEMalloc we need to update\\ncassandra-env.sh with\\nexport LD_LIBRARY_PATH=~/jemalloc/lib/\\nJVM_OPTS=\"-Djava.library.path=~/jemalloc/lib/\"\\n\\ncassandra.yaml with\\nmemory_allocator: org.apache.cassandra.io.util.JEMallocAllocator\\n', \"So to summarize:\\n\\n- We don't need JNI\\n- LD_PRELOAD makes things segfault but LD_LIBRARY_PATH works fine\\n\\nRight?\\n\\nIt looks to me like we can make Allocator just return a long.  Then the Memory heirarchy doesn't need to change at all.\\n\\nMessier but easy: just add a static Allocator to Memory.\\n\\nMore refactoring but cleaner? move allocation outside of Memory and replace with constructor (long reference, long bytes); add allocate(long bytes) and allocateRefCounted(long bytes) factory to Allocator.  RefCountedMemory would need to wrap Memory instead of subclassing.\\n\\nI also suggest adding a commented-out example to cassandra.yaml and cassandra-env.sh to illustrate how to enable this for those brave enough to try it.  (This will go into 1.3 so plenty of time to test.)\", \"Hi Jonathan,\\n\\n{quote}\\nWe don't need JNI\\nLD_PRELOAD makes things segfault but LD_LIBRARY_PATH works fine\\n\\nRight?\\n{quote}\\nWe dont need any additional JNI, but we do use JNA to load the library :).\\n\\n{quote}\\nMore refactoring but cleaner?\\n{quote}\\nthe problem is that free should be called by the same allocator, hence attached patch doesnt have the refactor :)\\n\\nRest Done, Thanks!\", 'Good point on free.\\n\\nNits:\\n\\n- cassandra.yaml has comments but no actual memory_allocator option\\n- should rename to IAllocator to follow convention\\n- INSTANCE should not be capitalized\\n\\nRest LGTM, ship it!', \"Committed with the Nit's fixed, Thanks!\", 'A little bit late but better ever than never ...\\n\\n\"JEMalloc but the only issue with it is that (both TCMalloc and JEMalloc) are kind of single threaded (at-least they crash in my test otherwise).\"\\n\\nJEmalloc must be configured with :\\n{code}\\n--disable-lazy-lock\\n    Disable code that wraps pthread_create() to detect when an application\\n    switches from single-threaded to multi-threaded mode, so that it can avoid\\n    mutex locking/unlocking operations while in single-threaded mode.  In\\n    practice, this feature usually has little impact on performance unless\\n    thread-specific caching is disabled.\\n{code}\\n\\njemalloc, by deafult wraps pthread API and tries to detect when when an application\\n    switches from single-threaded to multi-threaded mode. This trick does not work inside JVM of course.\\n\\n', 'Thanks!']\n",
            "my_comment: just add a static Allocator to Memory.\n",
            "\n",
            "More refactoring but cleaner? move allocation outside of Memory and replace with constructor (long reference long bytes); add allocate(long bytes) and allocateRefCounted(long bytes) factory to Allocator.  RefCountedMemory would need to wrap Memory instead of subclassing.\n",
            "\n",
            "I also suggest adding a commented-out example to cassandra.yaml and cassandra-env.sh to illustrate how to enable this for those brave enough to try it.  (This will go into 1.3 so plenty of time to test.)\" \"Hi Jonathan\n",
            "\n",
            "{quote}\n",
            "We dont need JNI\n",
            "LD_PRELOAD makes things segfault but LD_LIBRARY_PATH works fine\n",
            "\n",
            "Right?\n",
            "{quote}\n",
            "We dont need any additional JNI but we do use JNA to load the library :).\n",
            "\n",
            "{quote}\n",
            "More refactoring but cleaner?\n",
            "{quote}\n",
            "the problem is that free should be called by the same allocator hence attached patch doesnt have the refactor :)\n",
            "\n",
            "Rest Done Thanks!\" Good point on free.\n",
            "\n",
            "Nits:\n",
            "\n",
            "- cassandra.yaml has comments but no actual memory_allocator option\n",
            "- should rename to IAllocator to follow convention\n",
            "- INSTANCE should not be capitalized\n",
            "\n",
            "Rest LGTM ship it! \"Committed with the Nits fixed Thanks!\" A little bit late but better ever than never ...\n",
            "\n",
            "\"JEMalloc but the only issue with it is that (both TCMalloc and JEMalloc) are kind of single threaded (at-least they crash in my test otherwise).\"\n",
            "\n",
            "JEmalloc must be configured with :\n",
            "{code}\n",
            "--disable-lazy-lock\n",
            "    Disable code that wraps pthread_create() to detect when an application\n",
            "    switches from single-threaded to multi-threaded mode so that it can avoid\n",
            "    mutex locking/unlocking operations while in single-threaded mode.  In\n",
            "    practice this feature usually has little impact on performance unless\n",
            "    thread-specific caching is disabled.\n",
            "{code}\n",
            "\n",
            "jemalloc by deafult wraps pthread API and tries to detect when when an application\n",
            "    switches from single-threaded to multi-threaded mode. This trick does not work inside JVM of course.\n",
            "\n",
            " Thanks!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4004\n",
            "issue_type: Sub-task\n",
            "summary: Add support for ReversedType\n",
            "description: It would be nice to add a native syntax for the use of ReversedType. I'm sure there is anything in SQL that we inspired ourselves from, so I would propose something like:\n",
            "{noformat}\n",
            "CREATE TABLE timeseries (\n",
            "  key text,\n",
            "  time uuid,\n",
            "  value text,\n",
            "  PRIMARY KEY (key, time DESC)\n",
            ")\n",
            "{noformat}\n",
            "\n",
            "Alternatively, the DESC could also be put after the column name definition but one argument for putting it in the PK instead is that this only apply to keys.\n",
            "architectural impact: NO\n",
            "comments: ['Attached simple patch implementing the syntax above (a simple test has been push in the dtests).', 'I think I\\'d prefer to move this into our \"implementation extensions\" section as {{WITH CLUSTERING ORDERED BY (time DESC)}} or something similar.\\n\\nI also don\\'t see any code to account for, if we\\'ve already ordered it as DESC in the clustering declaration, that\\'s going to affect what we need to do for ORDER BY DESC (or ASC).', 'The idea of my patch is that adding DESC to a field in a table declaration changes the order of records *logically* (it also does it physically, but that is an implementation detail). The typical example (which I think is very common) is for time series, where you\\'re often interested by having record \"by default\" in reverse time order (from the more recent to the older). In that case a SELECT without any ORDER BY would return records directly in reverse time order. So at least given that definition, no, I don\\'t think there is anything to do for the code of ORDER BY in SELECT.\\n\\nI reckon there is another way to add ReversedType, which I\\'m assuming is what you are referring to, which consists in *not* changing the logical order, but to change the physical order. In which case, yes, we would have to take that into account for ORDER BY during SELECT.\\n\\nI\\'ll admit I\\'m less of a fan of that second option though. It seems that changing the logical order is 1) more natural (to me at least obviously) and 2) avoids having to use ORDER BY DESC in all your query.\\n\\nbq. I think I\\'d prefer to move this into our \"implementation extensions\" section as WITH CLUSTERING ORDERED BY (time DESC) or something similar\\n\\nWhy not, but I do prefer \"my\" notation in that it\\'s more concise and, at least if it changes the logical order, I guess it doesn\\'t feel like being just an \\'implementation extension\\'.', 'bq. The idea of my patch is that adding DESC to a field in a table declaration changes the order of records logically \\n\\nThat makes sense for the old world of ReversedType and {{reversed}} slice flag, but I don\\'t think it makes sense for CQL. When I ask for \"ORDER BY X DESC\" I expect the largest X first, period. So a table declaration like this makes sense as an optimization if DESC is your most frequent query type, but it shouldn\\'t change the semantics of the query itself.', 'bq. but I don\\'t think it makes sense for CQL.\\n\\nWhy wouldn\\'t it?  The notion of what is largest or smallest only make sense once you\\'ve defined what ordering you\\'re talking about (what I\\'m calling the logical ordering). We do still allow in CQL custom orderings (which is useful), so why giving a simple syntax to define the reverse ordering or an existing one wouldn\\'t make sense? With my first patch, \"ORDER BY X DESC\" does *always* return the largest X first, given the ordering.\\n\\nbq. but it shouldn\\'t change the semantics of the query itself\\n\\nTo be precise, it doesn\\'t change the semantic of the query, it changes the logical ordering (which happens to be the same than the physical one but that last part is an implementation detail) of records in the table.\\n\\n\\nNow, looking more closely at the alternative of keeping the logical ordering unchanged but changing the physical ordering (in order to get faster reversed queries), I think this just doesn\\'t work. And by \"doesn\\'t work\", I mean that as soon as we have composites, it would be costly to implement (making it useless). Typically, suppose you follow that idea and declare:\\n{noformat}\\nCREATE TABLE timeseries (\\n  key text,\\n  kind int,\\n  time timestamp,\\n  value text,\\n  PRIMARY KEY (key, kind, time)\\n) WITH CLUSTERING ORDER BY (kind ASC, time DESC)\\n{noformat}\\n\\nNow, if the query is:\\n{noformat}\\nSELECT kind, time FROM timeseries WHERE key = <somevalue> LIMIT 200;\\n{noformat}\\nthen, if the DESC above is \"just an optimisation for reversed queries, then the expected result is (say):\\n{noformat}\\nkind | time\\n-----------\\n   0 |    0\\n   0 |    1\\n   ...\\n   0 |   99\\n   0 |  100\\n   1 |    0\\n   1 |    1\\n   1 |    2\\n   ...\\n{noformat}\\nbut the physical layout is now in fact:\\n{noformat}\\nkind | time\\n-----------\\n   0 |  100\\n   0 |   99\\n   ...\\n   0 |    1\\n   0 |    0\\n   1 |  100\\n   1 |   99\\n   1 |   98\\n   ...\\n{noformat}\\nI don\\'t see how to implement that query efficiently (without potentially making many queries).\\n\\nLastly, while I think that changing the logical ordering is the correct way to deal with this, the question of the syntax is another matter. I do happen to like the syntax in the description of this ticket, but I don\\'t care too much either.\\n', 'bq. it doesn\\'t change the semantic of the query, it changes the logical ordering (which happens to be the same than the physical one but that last part is an implementation detail) of records in the table\\n\\nFrom the standpoint of a new user, this is sophistry.  If I ask for X DESC but I get X1 before X10, then you\\'ve most certainly changed the semantics of my query out from under me.\\n\\nWith Thrift we\\'ve been doing the equivalent of writing assembly code.  CQL lets us step up a level of abstraction. This is a Good Thing, but it will require a little getting used to for us veterans.\\n\\nbq. if the DESC above is \"just an optimisation for reversed queries\"\\n\\nRemember that {{SELECT X WHERE Y}} is NOT the reverse order from {{SELECT X WHERE Y ORDER BY X DESC}}, or put another way, {{ORDER BY}} is never implicit.  If {{ORDER BY}} is unspecified, any order is valid, so we are free to use the most efficient one.\\n\\n(The other relevant context here is that we decided not to support arbitrary orderings.  The two valid orderings a user may request correspond to physical/clustered order, and the reverse of that.  Both of which are reasonably efficient, but the former is more so.)\\n\\nSo, I think we should continue to enforce that restriction, and allow clustering order only to optimize for forwards/backwards (and by implication, to affect the default result ordering).  In your example, the permissible orderings would be KIND ASC, time DESC (most efficient, and what we should use when no explicit ORDER BY is given) and KIND DESC, time ASC.\\n\\nbq. the question of the syntax is another matter\\n\\nMy syntax preference falls out of the way I view the problem above. :)  I.e., that it\\'s there if you want to optimize for DESC but it doesn\\'t change things in a really fundamental way. So I\\'d prefer to have it in the WITH extensions section along with similar options like COMPACT STORAGE.', 'bq. From the standpoint of a new user, this is sophistry.\\n\\nHow is that sophistry, seriously? I think it\\'s very important that the clustered part of th PK induces an ordering of records (which SQL don\\'t have, but we\\'re not talking about SQL here, right). It\\'s important  because you don\\'t model things the same way in Cassandra than traditionally you do in SQL: you denormalize, you model time series etc... for which the notion that there is an ordering or records is not an implementation detail, nor something dealt with at query time (contrarily to SQL), but is an important part of the model. It would be confusing for brand new user to *not* say that ordering is part of the data model (and again yes, that\\'s a difference with SQL). I also don\\'t see how saying that is in any way related to being a veteran and whatnot. I 200% agree that CQL3 is an abstraction and that it is A Good Thing. I\\'m saying the ordering induced by the PK should be part of that abstraction.\\n\\nBut then it\\'s natural that SELECT without ORDER BY should return records in that clustering order, which will indeed not be the same than the order of with ORDER BY ASC *unless* Y is the first clustered key. But if is the first clustered key, then yes, SELECT and SELECT ORDER BY ASC should be the same (and they are). But then it\\'s not rocket science to say that if the ordering is \\'reversed alphabetical order\\', then \\'z\\' > \\'a\\' and thus a SELECT ORDER BY ASC returns \\'z\\' before \\'a\\'.\\n\\nSo I absolutely and strongly refute that this proposal is somehow sophistry and even more so that it\\'s a negation of the abstractive nature of CQL3 or influenced by the thrift API any more that the solution you\\'re pushing for.\\n\\nbq. The other relevant context here is that we decided not to support arbitrary orderings\\n\\nI\\'m either misunderstanding what you call \\'arbitrary orderings\\' or I have not been part of that discussion. Because if you talk of custom types, then CQL3 does support them (you can declare CREATE TABLE foo (k \"myCustomType\" PRIMARY KEY)). And I\\'m -1 on removing that support, unless someone has compelling reason to do so because I certainly don\\'t see any and that\\'s useful. And yes, I do see this as a good reason to go with my proposal, since it\\'s not very consistent if\\n{noformat}\\nCREATE TABLE foo (\\n    k1 uuid,\\n    k2 \"myEfficientComplexNumberType\",\\n    c text,\\n    PRIMARY KEY (k1, k2)\\n) WITH CLUSTERING ORDER BY (k2 DESC)\\n{noformat}\\nis *not* the same than\\n{noformat}\\nCREATE TABLE foo (\\n    k1 uuid,\\n    k2 \"myReversedEfficientComplexNumberType\",\\n    c text,\\n    PRIMARY KEY (k1, k2)\\n)\\n{noformat}', 'bq. I\\'m either misunderstanding what you call \\'arbitrary orderings\\' or I have not been part of that discussion\\n\\nI think you are misunderstanding.  This is what I\\'m referring to:\\n\\n{code}\\n.           if (stmt.parameters.orderBy != null)\\n            {\\n                CFDefinition.Name name = cfDef.get(stmt.parameters.orderBy);\\n                if (name == null)\\n                    throw new InvalidRequestException(String.format(\"Order by on unknown column %s\", stmt.parameters.orderBy));\\n\\n                if (name.kind != CFDefinition.Name.Kind.COLUMN_ALIAS || name.position != 0)\\n                    throw new InvalidRequestException(String.format(\"Order by is currently only supported on the second column of the PRIMARY KEY (if any), got %s\", stmt.parameters.orderBy));\\n            }\\n{code}\\n\\nbq. How is that sophistry, seriously? \\n\\n\"ORDER BY X DESC\" does not mean \"give me them in the reverse order that Xes are in on disk\", it means \"give me larger values before smaller ones.\"  This isn\\'t open for debate, it\\'s a very clear requirement.\\n\\nRemember that clustering is not new ground for databases; SQL has been there, done that.  As I mentioned when we were designing the CQL3 schema syntax, RDBMSes have had a concept of clustered indexes for a long, long time.  But clustering on an index ASC or DESC does not affect the results other than as an optimization; when you ORDER BY X, that\\'s what you get.\\n\\nSQL and CQL are declarative languages: \"Here is what I want; you figure out how to give me the results.\"  This has proved a good design.  Modifying the semantics of a query based on index or clustering or other declarations elsewhere has ZERO precedent and is bad design to boot; you don\\'t want users to have to consult their DDL when debugging, to know what results a query will give.\\n\\nThus, the only design that makes sense in the larger context of a declarative language is to treat the clustering as an optimization as I\\'ve described (or \"as an index\", if you prefer), and continue to reject ORDER BY requests that are neither forward- nor reverse-clustered.', 'bq. This is what I\\'m referring to:\\n\\nWait, what happened to \"Third (and this is the big one) I strongly suspect that we\\'re going to start supporting at least limited run-time ordering in the near future\" from CASSANDRA-3925. How can I reconcile that with \"The other relevant context here is that we decided not to support arbitrary orderings\"?\\n\\nbq. \"ORDER BY X DESC\" does not mean \"give me them in the reverse order that Xes are in on disk\"\\n\\nI *never* suggested that, not even a little. Not more than you did.\\n\\nbq. it means \"give me larger values before smaller ones.\" This isn\\'t open for debate, it\\'s a very clear requirement\\n\\nSure. But the definition of larger versus smaller depends on what ordering you are talking about. This isn\\'t open for debate either. Math have closed that debate for ages. And SQL is not excluded from that rule, but it just happens that SQL has default orderings (based on the column type) and you can\\'t define new ones. But we can do that in CQL. We can independently of this ticket because of custom types.\\n\\nAgain, once you consider custom types (which we have), you can\\'t hide behind that the fact that value X is larger than Y depends on the ordering induces by your custom types. That\\'s the ASC order, and DESC is the reverse of that. If someone define it\\'s own custom types being \"reverseIntegerType\", how can you avoid SELECT ORDER BY DESC to not return 1 before 3? You can\\'t, and returning 1 before 3 absolutely make sense because 1 is larger than 3 if the order is \\'reverseInteger\\'.\\n\\nbq. SQL and CQL are declarative languages: \"Here is what I want; you figure out how to give me the results.\" This has proved a good design.\\n\\nSure, *nothing* in what I\\'m suggesting is at odd with that.\\n\\nbq. Modifying the semantics of a query based on index or clustering\\n\\nAgain, I\\'m not suggesting any such thing at all. The semantic of a SELECT X ORDER BY Y depends on what ordering relation is defined for Y *because the ordering relation is what defines the order*. SQL has a limited and non customizable set of types *and* (implicitly) define an ordering relation for each one. If one type was \\'thing\\' it would have to define the ordering of \\'thing\\' otherwise ORDER BY queries wouldn\\'t be properly defined. CQL also has a default set of types which have associated ordering relation. I\\'m *only* suggesting we add a simple syntax so that given a type/relation (a default one or a custom one btw), we can define the type/ordering relation that validate the same value but have the reversed ordering.', 'bq. what happened to \"Third (and this is the big one) I strongly suspect that we\\'re going to start supporting at least limited run-time ordering in the near future\" from CASSANDRA-3925\\n\\nNothing, except that it\\'s a separate ticket\\'s worth of work.\\n\\nbq. I never suggested that [ORDER BY depends on disk order], not even a little. Not more than you did.\\n\\nI really don\\'t see the distinction between saying \"disk order\" and \"clustering order,\" as in \"the clustered part of th PK induces an ordering of records ... SELECT without ORDER BY should return records in that clustering order ... SELECT ORDER BY ASC returns \\'z\\' before \\'a\\'.\" \\n\\nBut disk order or clustering order, I don\\'t care which you call it; I reject both as modifiers of the semantics of ASC and DESC.  (But again, SELECT with no explicit ORDER BY is free to return in any order we like.)\\n\\nbq. the fact that value X is larger than Y depends on the ordering induces by your custom types\\n\\nAgreed.  But that\\'s not the same as reverse-clustering on a type: \"y int ... PRIMARY KEY (x, y DESC)\" (to use your syntax) is NOT the same as \"y ReversedInt ... PRIMARY KEY (x, y).\"  In the former, ORDER BY Y DESC should give larger Y before smaller (that is, 100 before 1); in the latter, the reverse.', 'bq. Nothing, except that it\\'s a separate ticket\\'s worth of work.\\n\\nOh ok. For the records I didn\\'t implied otherwise.\\n\\nbq. But that\\'s not the same as reverse-clustering on a type: \"y int ... PRIMARY KEY (x, y DESC)\" (to use your syntax) is NOT the same as \"y ReversedInt ... PRIMARY KEY (x, y).\" In the former, ORDER BY Y DESC\\n\\nWhat?! I said that I wasn\\'t sure my syntax was good. But with all I\\'ve said I expected it was clear that what I want to do with this ticket from day one is to allow to define \"y ReversedInt ... PRIMARY KEY\" but without having to write a custom java class since we don\\'t have to and that is *exactly* what my patch implements. I\\'m fine saying my syntax suck and allow to write is \"y reversed(int) .. PK\". But to be clear, I don\\'t think that option is a bad fit at all for CQL3, and that\\'s not the C* veteran talk.\\n\\nbq. In the former, ORDER BY Y DESC should give larger Y before smaller (that is, 100 before 1); in the latter, the reverse\\n\\nTo my defence, you\\'re attributing *your* semantic to *my* made up syntax (which again, may be is counter-intuitive to you with your background but is really not to me, and I made it clear that it was a suggestion. I even said in the description that \"Alternatively, the DESC could also be put after the column name definition\").\\n\\nbq. I really don\\'t see the distinction between saying \"disk order\" and \"clustering order,\" as in \"the clustered part of th PK induces an ordering of records ...\\n\\nMaybe with the reversed(int) syntax it makes it more clear, but when I talk about ordering of records, I\\'m saying that we should say that in CQL the model defines an ordering of the rows (where rows is in the sense of SQL) in tables, order that is defined as the ordering implied by the types of the \"clustered\" keys (and to be clear, I don\\'t care what clustering mean in SQL, I\\'m reusing the name because you\\'re using it, but I *only* mean by that term the fields in the PK after the first one). That doesn\\'t imply the disk order has to respect it (though it will but that\\'s an implementation detail). In other words, and somewhat unrelated to this issue, I think there would be value to say that the order of SELECT without any ORDER BY is something defined by CQL (while SQL does not do that). I think there would be value because I think it helps understanding which model are a good fit for CQL.\\n\\nNow, and to sum up, I think that having the \"y reversed(int)\" syntax has the following advantages over just allowing to change the on-disk order:\\n# I do think that in most case it\\'s more natural to define a reversed type rather than just adding an optim for reversed queries. Typically, it means that \\'y reversed(\"myCustomType\")\\' is the same than \\'y \"myReversedCustomType\"\\' which has a nice consistency to it. In the alternative, and even though I\\'m *not* saying it\\'s ill defined in any way, I do think that have a form of syntactic double negation that is not equivalent to removing both is kind of weird.\\n# Though that seems to be very clear to you, I do think that it\\'s not necessarily clear per se (i.e to anyone that may not be familiar with SQL clustering for instance) that \"WITH CLUSTERING ORDER (x DESC)\" does not change the ordering (and by that I mean \\'does not semantically mean \"x reversed(type)\"\\').\\n# With that solution, we can maintain (without doing anything) the fact that a select without ORDERING respect the ordering implied by the \"clustering\". I think it\\'s convenient for C*. Again, lots of efficient model for C* uses that ordering, so it feels like a better idea to say \\'oh, and contrarily to SQL the order of records in a table is defined (and thus the default ordering or SELECT) and lots of good modeling pattern for C* rely on this\\'.', 'bq. the model defines an ordering of the rows (where rows is in the sense of SQL) in tables, order that is defined as the ordering implied by the types of the \"clustered\" keys (and to be clear, I don\\'t care what clustering mean in SQL, I\\'m reusing the name because you\\'re using it, but I only mean by that term the fields in the PK after the first one). That doesn\\'t imply the disk order has to respect it\\n\\nI think the mental model of rows as predicates, queries returning sets of rows with no inherent order, and ORDER BY as specifying the desired order, is much simpler and easier to reason about (see prior point about having to consult DDL + QUERY to figure out what order results are supposed to appear in).\\n\\nbq. To my defence, you\\'re attributing your semantic to my made up syntax \\n\\nI was trying to say that I view ReversedType(Int32Type) as modification of Int32Type (which should not affect int ordering) and not a completely new type, the way the (hypothetical) ReversedInt (or BackwardsInt, or AlmostNotQuiteInt) type would be.  Since the latter isn\\'t really related to an int at all, even though they look a lot like ints in many respects.\\n\\nbq. I do think that in most case it\\'s more natural to define a reversed type rather than just adding an optim for reversed queries. \\n\\nI don\\'t follow.\\n\\nbq. I do think that have a form of syntactic double negation that is not equivalent to removing both is kind of weird... I do think that it\\'s not necessarily clear per se (i.e to anyone that may not be familiar with SQL clustering for instance) that \"WITH CLUSTERING ORDER (x DESC)\" does not change the ordering\\n\\nBut saying \"{{ORDER BY X DESC}} always gives you higher X first\" (and ASC always gives you lower first) is the only way to avoid the double negation!  Otherwise in your original syntax of PK (X, Y DESC), the only way to get 1 to sort before 100 is to ask for ORDER BY Y DESC so the DESC cancel out!\\n\\nI just can\\'t agree that \"ORDER BY Y DESC\" giving {1, 100} is going to be less confusing than {100, 1}, no matter how much we tell users, \"No, you see, it\\'s really just reversing the clustering order, which you already reversed...\"\\n\\nUsers may not be familiar with clustering, but they\\'re *very* familiar with ORDER BY, which as I said above, is very clear on what it does.  Clustering is the closest example of how performance hints should *not* change the semantics of the query, but indexes fall into the same category.\\n\\nIt may also be worth pointing out that it\\'s worth preserving CQL compatibility with Hive; queries that execute on both (and to the best of my knowledge CQL3 is a strict subset of Hive SQL) should not give different results.', 'bq. I think the mental model of rows as predicates, queries returning sets of rows with no inherent order, and ORDER BY as specifying the desired order, is much simpler and easier to reason about\\n\\nMuch simpler and easier than what? You\\'re pretending that what I\\'m saying is somehow a complete revolution where it\\'s not. I\\'m only suggesting that it would be a good idea for CQL to say that by default rows are returned in a defined order. Saying the order is defined (by opposition to not be) hardly breaks the mental mode. And that certainly does not change in any way that ORDER BY would still specify the desired order.\\n\\nbq. having to consult DDL + QUERY to figure out what order results are supposed to appear in\\n\\nAgain, imho that does not apply to my suggestion at all. People already have to know their DDL for QUERY.  They need to know which column names are defined and which type they have (since without knowing the type *you cannot know the ordering*). Since *all* I\\'m suggesting is a convenient syntax to define \"x ReversedInt\", and since you can *already* define \"x ReversedInt\" (provided you write such AbstractType), my suggestion doesn\\'t change one bit how much people will have to consult the DDL.\\n\\nbq. Users may not be familiar with clustering, but they\\'re very familiar with ORDER BY\\n\\nI\\'ve *never* suggested to change the semantic of ORDER BY *at all*. ORDER BY x ASC is defined as \\'return rows in the order induces by the type of x\\'. I\\'m suggesting a syntax to define new types from existing ones. How is that even close to be related to changing what ORDER BY does?\\n\\n\\nAnyway, I\\'m still convinced that adding a simple syntax to allow to define a type with reversed order given an existing type (custom or predefined) is in fact cleaner from the point of semantic than the alternative you are suggesting (and it is certainly not the semantic disrupting monster you\\'re seem to be suggesting it is for a reason that is beyond me), and at a personal level I find it more natural (as in: the syntax naturally imply the semantic).\\n\\nBut anyway, since we\\'re going nowhere, I\\'m giving up.\\n\\nI\\'m attaching the patch with the alternative you\\'ve suggested. I do not think that\\'s the best solution for CQL but from a pure technical point of point it does is a solution so so be it.\\n', \"I may be misreading this, but it looks like it treats {{ORDER BY X DESC, Y ASC}} the same as {{ORDER BY Y ASC, X DESC}} since it iterates in HashMap's entrySet order.  A simple fix might be to use a LinkedHashMap.\", \"Which hashmap are you talking about? If it's {{stmt.parameters.orderings}}, the actual order of elements in the map is not used by the code so that shouldn't matter. I'll note there is a test for this patch at https://github.com/riptano/cassandra-dtest/blob/master/cql_tests.py (the line {{require('#4004')}} needs to be commented to actually run the test) showing that ORDER BY X DESC, Y ASC is the reverse of ORDER BY Y ASC, X DESC.\\n\\nAs a side note, I've convinced myself that 4004_alternative.txt is maybe a bit more natural than creating a new, reversed type so I'm good with that.\\n\\n\", \"My concern is that that the order of the terms in the {{ORDER BY}} clause is relevant -- in the dtest, c1 is always given first (which is valid).  But I don't think the code would reject {{c2 DESC, c1 ASC}}, although it should.\\n\\nSo what I'm looking for is either a reference to the ordering from {{columnAliases}}, or an ordered map for {{definedOrderings}}, which I'm not seeing.\", \"You're right. Attached v2 fixes that (and I've added a check we do catch that in the dtest).\", '+1', 'Committed, thanks']\n",
            "my_comment: the syntax naturally imply the semantic).\n",
            "\n",
            "e going nowhere I\\m giving up.\n",
            "\n",
            "I\\m attaching the patch with the alternative you\\ve suggested. I do not think that\\s the best solution for CQL but from a pure technical point of point it does is a solution so so be it.\n",
            " \"I may be misreading this but it looks like it treats {{ORDER BY X DESC Y ASC}} the same as {{ORDER BY Y ASC X DESC}} since it iterates in HashMaps entrySet order.  A simple fix might be to use a LinkedHashMap.\" \"Which hashmap are you talking about? If its {{stmt.parameters.orderings}} the actual order of elements in the map is not used by the code so that shouldnt matter. Ill note there is a test for this patch at https://github.com/riptano/cassandra-dtest/blob/master/cql_tests.py (the line {{require(#4004)}} needs to be commented to actually run the test) showing that ORDER BY X DESC Y ASC is the reverse of ORDER BY Y ASC X DESC.\n",
            "\n",
            "As a side note Ive convinced myself that 4004_alternative.txt is maybe a bit more natural than creating a new reversed type so Im good with that.\n",
            "\n",
            "\" \"My concern is that that the order of the terms in the {{ORDER BY}} clause is relevant -- in the dtest c1 is always given first (which is valid).  But I dont think the code would reject {{c2 DESC c1 ASC}} although it should.\n",
            "\n",
            "So what Im looking for is either a reference to the ordering from {{columnAliases}} or an ordered map for {{definedOrderings}} which Im not seeing.\" \"Youre right. Attached v2 fixes that (and Ive added a check we do catch that in the dtest).\" +1 Committed thanks\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4136\n",
            "issue_type: Bug\n",
            "summary: get_paged_slices doesn't reset startColumn after first row\n",
            "description: As an example, consider the WordCount example (see CASSANDRA-3883).  WordCountSetup inserts 1000 rows, each with three columns: text3, text4, int1.  (Some other miscellaneous columns are inserted in a few rows, but we can ignore them here.)\n",
            "\n",
            "Paging through with get_paged_slice calls with a count of 99, CFRecordReader will first retrieve 33 rows, the last of which we will call K.  Then it will attempt to fetch 99 more columns, starting with row K column text4.\n",
            "\n",
            "The bug is that it will only fetch text4 for *each* subsequent row K+i, instead of returning (K, text4), (K+1, int1), (K+1, int3), (K+1, text4), etc.\n",
            "architectural impact: NO\n",
            "comments: ['Attached patch adds the ability to do paging through multiple rows. The support added by the patch is limited to what get_paged_slices requires (in particular using a SliceQueryFilter where finish != \"\" is not supported with that new option). The patch contains a unit test.', 'With this and the 3883 patches I get\\n\\n{noformat}\\n$ cat /tmp/word_count5/part-r-00000\\n0       250\\n1       250\\n2       250\\n3       250\\nword1   2002\\nword2   1\\n{noformat}\\n\\nwhich is the expected result.\\n\\n+1', 'Committed, thanks']\n",
            "my_comment: Attached patch adds the ability to do paging through multiple rows. The support added by the patch is limited to what get_paged_slices requires (in particular using a SliceQueryFilter where finish != \"\" is not supported with that new option). The patch contains a unit test. With this and the 3883 patches I get\n",
            "\n",
            "{noformat}\n",
            "$ cat /tmp/word_count5/part-r-00000\n",
            "0       250\n",
            "1       250\n",
            "2       250\n",
            "3       250\n",
            "word1   2002\n",
            "word2   1\n",
            "{noformat}\n",
            "\n",
            "which is the expected result.\n",
            "\n",
            "+1 Committed thanks\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4249\n",
            "issue_type: Bug\n",
            "summary: LOGGING: Info log is not displaying number of rows read from saved cache at startup\n",
            "description: As part of commit with revision c9270f4e info logging for number of rows read from saved cache is not working. \n",
            "This is happening because we are not incrementing the counter cachedRowsRead in ColumnFamilyStore.initRowCache().\n",
            "architectural impact: NO\n",
            "comments: ['This is a very small change.', 'committed, thanks!']\n",
            "my_comment: This is a very small change. committed thanks!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4308\n",
            "issue_type: Improvement\n",
            "summary: Promote the use of IFilter for internal commands\n",
            "description: All read commands (the IReadCommand) work on slice and names filters, but none of them uses the SliceQueryFilter and NamesQueryFilter classes (RangeSliceCommand uses SlicePrediate from thrift and {SliceFrom, SliceByNames}ReadReadCommand interns the arguments).\n",
            "\n",
            "The main problem of that is that it follows that those command don't share the serialization code for the column filters. Which isn't good for code reuse, but also makes it a pain to add new fields to the filter SliceQueryFilter (which CASSANDRA-3885 will need, but probably CASSANDRA-3647 too).\n",
            "\n",
            "architectural impact: NO\n",
            "comments: ['Attaching patch that reuse the IFilter for command serialization, which will make it easier to add new field to them if needed.\\n\\nThis also remove some uses of thrift internally as a bonus.', 'LGTM.\\n\\nnit: You can now replace QueryFilter.getFilter(SlicePredicate, AbstractType) which is only used in deprecated IndexScanCommand and ColumnFamilyStoreTest,  with newly added ThriftValidation.asIFilter.', 'Committed (with nit fixed). Thanks.']\n",
            "my_comment: You can now replace QueryFilter.getFilter(SlicePredicate AbstractType) which is only used in deprecated IndexScanCommand and ColumnFamilyStoreTest  with newly added ThriftValidation.asIFilter. Committed (with nit fixed). Thanks.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4416\n",
            "issue_type: Improvement\n",
            "summary: Include metadata for system keyspace itself in schema_* tables\n",
            "description: The `system.schema_keyspaces`, `system.schema_columnfamilies`, and `system.schema_columns` virtual tables allow clients to query schema and layout information through CQL. This will be invaluable when users start to make more use of the CQL-only protocol (CASSANDRA-2478), since there will be no other way to determine certain information about available columnfamilies, keyspaces, or show metadata about them.\n",
            "\n",
            "However, the system keyspace itself, and all the columnfamilies in it, are not represented in the schema_* tables:\n",
            "\n",
            "{noformat}\n",
            "cqlsh> select * from system.schema_keyspaces where \"keyspace\" = 'system';\n",
            "cqlsh> \n",
            "cqlsh> select * from system.schema_columnfamilies where \"keyspace\" = 'system';\n",
            "cqlsh> \n",
            "cqlsh> select * from system.schema_columns where \"keyspace\" = 'system';\n",
            "cqlsh> \n",
            "{noformat}\n",
            "\n",
            "It would be greatly helpful to clients which do more introspection than the minimum (say, for example, cqlsh) to be able to get information on the structure and availability of schema-definition tables.\n",
            "architectural impact: NO\n",
            "comments: ['Not sure what a good solution here is, since these are defined programatically (and can and do change).', \"I suppose we could artificially write them at startup. At each startup we could erase everything (relating to the system keyspace) to be sure we don't have old info and then dump the new info.\", \"Tagging version 1.2 since the metdata isn't even specified internally until CASSANDRA-4018.\", 'This is more important now, after CASSANDRA-4377, since as far as I can tell there is no way at all for a client or user to be able to see all the tables in the system and system_traces keyspaces.', \"Patch attached implementing Sylvain's suggestion to delete out and write anew the schema entries for hardcoded tables.\\n\\nAlso removes SSTableExport restriction that you can only run it if non-system tables have been defined which is kind of nonsensical.\", \"nit: in SystemTable.finishStartup, the code currently tries to delete the keyspaces, columnsfamilies and columns table from the system_trace keyspace too. It's harmless but useless so we might want to add a 'if' or move the deletion outside the 'for'.\\n\\n+1 otherwise.\", \"Hmm, I still don't see it...  we run\\n\\n{code}\\nDELETE FROM system.{schema_keyspaces, schema_columnfamilies, schema_columns} WHERE keyspace_name = '{system, system_traces}'\\n{code}\\n\\nso system_traces is the partition key of the data being removed, not the keyspace we're DELETEing from.\", \"Oh right, I shouldn't review tickets before my second coffee. +1.\", 'committed', \"This patch causes Cassandra to not start if there is at least one keyspace besides the system ones.\\n\\nRun create keyspace test with replication = {'class':'SimpleStrategy', 'replication_factor':1}; from cqlsh, stop cassandra, start cassandra, and you'll observe the following stacktrace:\\n\\njava.lang.RuntimeException: Attempting to load already loaded column family system.batchlog\\n\\tat org.apache.cassandra.config.Schema.load(Schema.java:396)\\n\\tat org.apache.cassandra.config.Schema.load(Schema.java:112)\\n\\tat org.apache.cassandra.config.Schema.load(Schema.java:97)\\n\\tat org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:564)\\n\\tat org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:214)\\n\\tat org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:393)\\n\\tat org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:436)\\nException encountered during startup: Attempting to load already loaded column family system.batchlog\\n\\nIn fact you don't even need to create a new keyspace - just start/stop cassandra a couple times.\\n\", \"The problem being, when we load the table from the system table, the system ones have already been loaded and shouldn't be loaded a second time. Attaching a patch that ignore the system keyspaces when reading schema tables (we only write those for client sake, we never need them internally, so let just do as if they weren't there).\", '+1', 'Alright, fix committed.']\n",
            "my_comment: Attempting to load already loaded column family system.batchlog\n",
            "\n",
            "In fact you dont even need to create a new keyspace - just start/stop cassandra a couple times.\n",
            "\" \"The problem being when we load the table from the system table the system ones have already been loaded and shouldnt be loaded a second time. Attaching a patch that ignore the system keyspaces when reading schema tables (we only write those for client sake we never need them internally so let just do as if they werent there).\" +1 Alright fix committed.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-447\n",
            "issue_type: New Feature\n",
            "summary: Ability to temporary set minimum and maximum compaction threshold\n",
            "description: We need the ability to temporary set minimum and maximum compaction threshold.  This is needed so that we can turn off compaction during BMT.\n",
            "\n",
            "\n",
            "architectural impact: NO\n",
            "comments: ['Added the ability set/get max/min compaction threshold in MBean interface\\nUpdated nodeprobe with these new commands.', \"let's make a CompactionManagerMBean instead of echoing calls from SS to MCM.  SS is starting to get cluttered.\\n\\nalso,\\n\\n - make the variables non-static so we don't need two versions of the getter and setter methods.  (this is ok since MCM is a singleton)\\n - follow Cassandra brace placement convention\\n\", 'Created new MinorCompactionManagerMBean, removed SS wrappings, and updated NodeProbe\\nMade static variables regular instance variables in MCM.\\n\\n', 'committed.  also renamed MCM -> CM.', 'Integrated in Cassandra #199 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/199/])\\n    rename MinorCompactionManager -> CompactionManager.  patch by jbellis for \\nadd mbean to get/set compaction thresholds.  patch by Sammy Yu; reviewed by jbellis for \\n']\n",
            "my_comment: Added the ability set/get max/min compaction threshold in MBean interface\n",
            "Updated nodeprobe with these new commands. \"lets make a CompactionManagerMBean instead of echoing calls from SS to MCM.  SS is starting to get cluttered.\n",
            "\n",
            "also\n",
            "\n",
            " - make the variables non-static so we dont need two versions of the getter and setter methods.  (this is ok since MCM is a singleton)\n",
            " - follow Cassandra brace placement convention\n",
            "\" Created new MinorCompactionManagerMBean removed SS wrappings and updated NodeProbe\n",
            "Made static variables regular instance variables in MCM.\n",
            "\n",
            " committed.  also renamed MCM -> CM. Integrated in Cassandra #199 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/199/])\n",
            "    rename MinorCompactionManager -> CompactionManager.  patch by jbellis for \n",
            "add mbean to get/set compaction thresholds.  patch by Sammy Yu; reviewed by jbellis for \n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4495\n",
            "issue_type: Improvement\n",
            "summary: Don't tie client side use of AbstractType to JDBC\n",
            "description: We currently expose the AbstractType to java clients that want to reuse them though the cql.jdbc.* classes. I think this shouldn't be tied to the JDBC standard. JDBC was make for SQL DB, which Cassandra is not (CQL is not SQL and will never be). Typically, there is a fair amount of the JDBC standard that cannot be implemented with C*, and there is a number of specificity of C* that are not in JDBC (typically the set and maps collections).\n",
            "\n",
            "So I propose to extract simple type classes with just a compose and decompose method (but without ties to jdbc, which would allow all the jdbc specific method those types have) in the purpose of exporting that in a separate jar for clients (we could put that in a org.apache.cassandra.type package for instance). We could then deprecate the jdbc classes with basically the same schedule than CQL2.\n",
            "\n",
            "Let me note that this is *not* saying there shouldn't be a JDBC driver for Cassandra.\n",
            "architectural impact: NO\n",
            "comments: ['bq. So I propose to extract simple type classes with just a compose and decompose method\\n\\nwhy not just expose the AbstractType classes at that point?', 'bq. why not just expose the AbstractType classes at that point?\\n\\nI though about that, but I think that at least currently that means pull pretty mull all of Cassandra (why the goal is to allow clients to pull just the minimum useful to them).', 'I would suggest adding {{getString(ByteBuffer bytes)}} and {{getType()}} as well. The JDBC specific stuff like {{isCurrency()}} and {{isSigned()}} are of course easily moved over to client side.', \"[~ardot] [~mfiguiere] [~urandom] I don't suppose I can interest one of you in this?\", \"First cut of this is over at:\\n\\nhttps://github.com/carlyeks/cassandra/tree/4495\\n\\nI don't like the name (composer), but using abstracttype et al caused a lot of conflicts. I'm open to a better name or, if AbstractType really is the best name, I'll just fully qualify the names.\", 'Could this solution be enhanced to address the collection types? (List, Set, and Map). This is really awkward to do on the client side. \\n\\n{{getType()}} in each class would also be handy when you are passed the AbstractType.\\n\\nIs there a reason you did not remove the {{o.a.c.cql.jdbc}} package from the build?\\n\\nGreat job BTW...', 'Just pushed up a couple of new commits.\\n\\n- Removed the o.a.c.cql.jdbc namespace\\n- Added getType\\n- Added List,Set,Map implementations\\n  - Haven\\'t figured out how to merge the {List,Set,Map}Type\\'s compose and decompose because of their usage of validate\\n- Added a \"asCompose()\" call to AbstractType which returns the AbstractComposer for each type', \"bq. Haven't figured out how to merge the {List,Set,Map}\\n\\nCan you elaborate?\", 'Just meant that the Collection-types cannot use the Collection-composers because they depend on validation, which the composers do not have. Not sure if it is worth adding, or if we are happy without it.', \"Can't you just move the {{validate()}} method in the classes in {{o.a.c.types}} from the ones on {{o.a.c.db.marshal}}? Then the collections classes in {{o.a.c.types}} will have access to them.\", \"This adds validate to the composer.\\n\\nA couple of things to be aware of:\\n- Replaced o.a.c.db.marshal.MarshalException with o.a.c.types.MarshalException as it may be thrown to a client\\n- Collection composer validate no ops, reasoning below\\n\\nI'm not sure that there is any reason to validate the Collections. It seems that the previous validation would fail on, for example, Map<Map<TimeUUID, string>, TimeUUID>, as the validation of a Map is actually validating using the Value type of the Map, rather than iterating through the values and making sure that an entry is valid. This entry-wise validation happens when we call compose.\", \"Haven't really look at the detail of the patch, but for what it's worth, I've somehow never been a fan of the compose/decompose terminology. I'd prefer say encode/decode or serialize/deserialize. And BooleanCodec or BooleanSerializer sounds better to my hear than BooleanComposer. But do feel free to discard that opinion if it's just me being french and if composer sounds perfectly fine to you guys.\", 'I like that name a lot more. Attached an updated version which renames to *Serializer, and renames the methods to (de)serialize.', 'WDYT, Rick?', 'LGTM. Should be no problem to incorporate into client side work. Thanks for the enhancements!', 'Alright, committed then.', \"For info, took the liberty to do the following renames:\\n* renamed the package from type to serializers, since that's what the classes are called.\\n* made AbstractSerializer an interface since I didn't see a good reason to have it an abstract class. Renamed into TypeSerializer too.\\n* renamed the asComposer() method in AbstractType to getSerializer() (I suspect that was a left-over of the initial patch)\"]\n",
            "my_comment: bq. So I propose to extract simple type classes with just a compose and decompose method\n",
            "\n",
            "why not just expose the AbstractType classes at that point? bq. why not just expose the AbstractType classes at that point?\n",
            "\n",
            "I though about that but I think that at least currently that means pull pretty mull all of Cassandra (why the goal is to allow clients to pull just the minimum useful to them). I would suggest adding {{getString(ByteBuffer bytes)}} and {{getType()}} as well. The JDBC specific stuff like {{isCurrency()}} and {{isSigned()}} are of course easily moved over to client side. \"[~ardot] [~mfiguiere] [~urandom] I dont suppose I can interest one of you in this?\" \"First cut of this is over at:\n",
            "\n",
            "https://github.com/carlyeks/cassandra/tree/4495\n",
            "\n",
            "I dont like the name (composer) but using abstracttype et al caused a lot of conflicts. Im open to a better name or if AbstractType really is the best name Ill just fully qualify the names.\" Could this solution be enhanced to address the collection types? (List Set and Map). This is really awkward to do on the client side. \n",
            "\n",
            "{{getType()}} in each class would also be handy when you are passed the AbstractType.\n",
            "\n",
            "Is there a reason you did not remove the {{o.a.c.cql.jdbc}} package from the build?\n",
            "\n",
            "Great job BTW... Just pushed up a couple of new commits.\n",
            "\n",
            "- Removed the o.a.c.cql.jdbc namespace\n",
            "- Added getType\n",
            "- Added ListSetMap implementations\n",
            "  - Haven\\t figured out how to merge the {ListSetMap}Type\\s compose and decompose because of their usage of validate\n",
            "- Added a \"asCompose()\" call to AbstractType which returns the AbstractComposer for each type \"bq. Havent figured out how to merge the {ListSetMap}\n",
            "\n",
            "Can you elaborate?\" Just meant that the Collection-types cannot use the Collection-composers because they depend on validation which the composers do not have. Not sure if it is worth adding or if we are happy without it. \"Cant you just move the {{validate()}} method in the classes in {{o.a.c.types}} from the ones on {{o.a.c.db.marshal}}? Then the collections classes in {{o.a.c.types}} will have access to them.\" \"This adds validate to the composer.\n",
            "\n",
            "A couple of things to be aware of:\n",
            "- Replaced o.a.c.db.marshal.MarshalException with o.a.c.types.MarshalException as it may be thrown to a client\n",
            "- Collection composer validate no ops reasoning below\n",
            "\n",
            "Im not sure that there is any reason to validate the Collections. It seems that the previous validation would fail on for example Map<Map<TimeUUID string> TimeUUID> as the validation of a Map is actually validating using the Value type of the Map rather than iterating through the values and making sure that an entry is valid. This entry-wise validation happens when we call compose.\" \"Havent really look at the detail of the patch but for what its worth Ive somehow never been a fan of the compose/decompose terminology. Id prefer say encode/decode or serialize/deserialize. And BooleanCodec or BooleanSerializer sounds better to my hear than BooleanComposer. But do feel free to discard that opinion if its just me being french and if composer sounds perfectly fine to you guys.\" I like that name a lot more. Attached an updated version which renames to *Serializer and renames the methods to (de)serialize. WDYT Rick? LGTM. Should be no problem to incorporate into client side work. Thanks for the enhancements! Alright committed then. \"For info took the liberty to do the following renames:\n",
            "* renamed the package from type to serializers since thats what the classes are called.\n",
            "* made AbstractSerializer an interface since I didnt see a good reason to have it an abstract class. Renamed into TypeSerializer too.\n",
            "* renamed the asComposer() method in AbstractType to getSerializer() (I suspect that was a left-over of the initial patch)\"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4624\n",
            "issue_type: Bug\n",
            "summary: ORDER BY validation is not restrictive enough\n",
            "description: We're not able to do order by on anything that is a key range. However, we only refuse queries that have an empty where clause, but that doesn't exclude all key ranges at all.\n",
            "architectural impact: NO\n",
            "comments: ['+1', 'Committed, thanks']\n",
            "my_comment: +1 Committed thanks\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4647\n",
            "issue_type: Improvement\n",
            "summary: Rename NodeId to CounterId\n",
            "description: NodeId is pretty close to the hostId introduced by vnodes, so let's rename NodeId to CounterId to avoid any potential confusion. It's probably a better name anyway.\n",
            "architectural impact: NO\n",
            "comments: [\"Patch attached that does the renaming pretty much everywhere. I even edited the comments (at least for those returned by a simple grep). The only thing that still has NodeId is the system table NodeIdInfo, since changing the name would require migrating it's data.\", '+1', 'Committed, thanks']\n",
            "my_comment: [\"Patch attached that does the renaming pretty much everywhere. I even edited the comments (at least for those returned by a simple grep). The only thing that still has NodeId is the system table NodeIdInfo since changing the name would require migrating its data.\" +1 Committed thanks\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4709\n",
            "issue_type: Bug\n",
            "summary: (CQL3) Missing validation for IN queries on column not part of the PK\n",
            "description: Copy-pasting from the original mail (http://mail-archives.apache.org/mod_mbox/cassandra-user/201209.mbox/%3C20120922185826.GO6205@pslp2%3E):\n",
            "{noformat}\n",
            "[cqlsh 2.2.0 | Cassandra 1.1.5 | CQL spec 3.0.0 | Thrift protocol 19.32.0]\n",
            "Use HELP for help.\n",
            "cqlsh> \n",
            "cqlsh> create keyspace xpl1 WITH strategy_class ='SimpleStrategy' and strategy_options:replication_factor=1;\n",
            "cqlsh> use xpl1;\n",
            "cqlsh:xpl1> create table t1 (pk varchar primary key, col1 varchar, col2 varchar);\n",
            "cqlsh:xpl1> create index t1_c1 on t1(col1);\n",
            "cqlsh:xpl1> create index t1_c2 on t1(col2);\n",
            "cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1','foo1','bar1');\n",
            "cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1a','foo1','bar1');\n",
            "cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1b','foo1','bar1');\n",
            "cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1c','foo1','bar1');\n",
            "cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk2','foo2','bar2');\n",
            "cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk3','foo3','bar3');\n",
            "cqlsh:xpl1> select * from t1 where col2='bar1';\n",
            " pk   | col1 | col2\n",
            "------+------+------\n",
            " pk1b | foo1 | bar1\n",
            "  pk1 | foo1 | bar1\n",
            " pk1a | foo1 | bar1\n",
            " pk1c | foo1 | bar1\n",
            "\n",
            "cqlsh:xpl1> select * from t1 where col2 in ('bar1', 'bar2') ;\n",
            "cqlsh:xpl1> \n",
            "{noformat}\n",
            "\n",
            "We should either make that last query work or refuse the query but returning nothing is wrong.\n",
            "architectural impact: NO\n",
            "comments: [\"For now I think we should just refuse the query since this would require secondary indexes to do an OR which they can't do right now. Attaching patch that simply refuse such queries.\", '+1', 'Forgot to mark that one resolved somehow, doing it now.']\n",
            "my_comment: [\"For now I think we should just refuse the query since this would require secondary indexes to do an OR which they cant do right now. Attaching patch that simply refuse such queries.\" +1 Forgot to mark that one resolved somehow doing it now.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4750\n",
            "issue_type: New Feature\n",
            "summary: Add jmx/nodetool methods to enable/disable hinted handoff\n",
            "description: Title says it all.\n",
            "architectural impact: NO\n",
            "comments: ['Methods to enable/disable using JMX have already been created.', \"Hmm, this looks good, but I'm not sure the behavior for setHintedHandoffEnabled is entirely correct.  It prevents future hints, but delivery can still occur for existing hints.  So if you encounter a situation where you have a ton of stored hints and just want to stop them, you're kind of stuck.\", 'I think that would be good to save current methods and add methods for pause/resume hints delivery processes. So we will have separate methods to disable/enable the future hints storing and for pause/resume hints delivery processes. It will be implemented in HHOM as boolean flag that could be changed through JMX and nodetool. Also that flag should be save own state in system.local CF (patch will be creaed only for 1.2 version).\\n\\nWhat do you think about it? ', \"Brandon, let's try to figure out what we want to do in this ticket. \\na) we want to get 4 methods: for enable/disable and for pause/resume hints delivery process. \\nWe will have ability for full control of hints delivery process.\\nb) we want to get 2 methods: (enable future hints storing + resume hints delivery process) and (disable future hints storing + pause hints delivery process) methods.\\nWe will have ability to stop/start the current hints delivery and future hints storing processes together.\\nc) we want to get 2 methods: for resume/pause hints delivery process.\\nWe will have ability to pause/resume the current hints delivery process only (without of any ability to control future hints storing).\\n\\nSo what do you think about it?\", \"I'm fine with a), but I don't see any reason to persist any flags in the system CF, that's what the yaml is for.\", 'Please review the patch.', \"v3 fixes the yaml indentation so it parse correctly, adds a check/break out of the inner loop over the page size, and finally logs that hints are paused at the end so it's clear that they may not all be delivered.\", 'Alexey, can you review v3?', 'This looks good to me.', 'Committed.']\n",
            "my_comment: for resume/pause hints delivery process.\n",
            "We will have ability to pause/resume the current hints delivery process only (without of any ability to control future hints storing).\n",
            "\n",
            "So what do you think about it?\" \"Im fine with a) but I dont see any reason to persist any flags in the system CF thats what the yaml is for.\" Please review the patch. \"v3 fixes the yaml indentation so it parse correctly adds a check/break out of the inner loop over the page size and finally logs that hints are paused at the end so its clear that they may not all be delivered.\" Alexey can you review v3? This looks good to me. Committed.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4802\n",
            "issue_type: Bug\n",
            "summary: Regular startup log has confusing \"Bootstrap/Replace/Move completed!\" without boostrap, replace, or move\n",
            "description: A regular startup completes successfully, but it has a confusing message the end of the startup:\n",
            "\n",
            "\"  INFO 15:19:29,137 Bootstrap/Replace/Move completed! Now serving reads.\"\n",
            "\n",
            "This happens despite no bootstrap, replace, or move.\n",
            "\n",
            "While purely cosmetic, this makes you wonder what the node just did - did it just bootstrap?!  It should simply read something like \"Startup completed! Now serving reads\" unless it actually has done one of the actions in the error message.\n",
            "\n",
            "\n",
            "\n",
            "Complete log at the end:\n",
            "\n",
            "\n",
            "INFO 15:13:30,522 Log replay complete, 6274 replayed mutations\n",
            " INFO 15:13:30,527 Cassandra version: 1.0.12\n",
            " INFO 15:13:30,527 Thrift API version: 19.20.0\n",
            " INFO 15:13:30,527 Loading persisted ring state\n",
            " INFO 15:13:30,541 Starting up server gossip\n",
            " INFO 15:13:30,542 Enqueuing flush of Memtable-LocationInfo@1828864224(29/36 serialized/live bytes, 1 ops)\n",
            " INFO 15:13:30,543 Writing Memtable-LocationInfo@1828864224(29/36 serialized/live bytes, 1 ops)\n",
            " INFO 15:13:30,550 Completed flushing /data2/data-cassandra/system/LocationInfo-hd-274-Data.db (80 bytes)\n",
            " INFO 15:13:30,563 Starting Messaging Service on port 7000\n",
            " INFO 15:13:30,571 Using saved token 31901471898837980949691369446728269823\n",
            " INFO 15:13:30,572 Enqueuing flush of Memtable-LocationInfo@294410307(53/66 serialized/live bytes, 2 ops)\n",
            " INFO 15:13:30,573 Writing Memtable-LocationInfo@294410307(53/66 serialized/live bytes, 2 ops)\n",
            " INFO 15:13:30,579 Completed flushing /data2/data-cassandra/system/LocationInfo-hd-275-Data.db (163 bytes)\n",
            " INFO 15:13:30,581 Node kaos-cass02.xxxxxxx/1.2.3.4 state jump to normal\n",
            " INFO 15:13:30,598 Bootstrap/Replace/Move completed! Now serving reads.\n",
            " INFO 15:13:30,600 Will not load MX4J, mx4j-tools.jar is not in the classpath\n",
            "\n",
            "architectural impact: NO\n",
            "comments: ['How about just saying:\\nBootstrap completed! Now serving reads.\\n\\n? Do we need any additional information?', \"I think the point is that we should not print it if we didn't actually bootstrap, and we should be able to distinguish between bootstrap/replace/move.\", 'Move doesnt use the same code anymore, replace uses this but there are other log info explaining that....\\n\\nIf Bootstrap is a wrong word then how about: Startup completed?\\n(I am still looking for an abstract word :))', 'Bootstrap means something specifically with cassandra in that you think some data has streamed in.\\n\\nI think \"Startup completed\" would be great.\\n\\nIf there IS a bootstrap/replace/move then I think the message ought to specify which has happened and that it\\'s ready now (if it\\'s easy to do) :)', 'Committed https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blobdiff;f=src/java/org/apache/cassandra/service/StorageService.java;h=8de0bd24632c89ea1b41c952ee6ec2db58808894;hp=7d92fbe0ff15c8c686a93425f4fccca49b921c0b;hb=d525cf969c042b21a9375446f5449ee82d7d1484;hpb=7e937b3d1308c0774e4b0366b6e66b14af1dd5f6\\n\\nLet me know if you need more info, i will reopen this ticket.', \"This isn't quite what I had in mind.  It's not a semantic issue, it's a logical issue.  We should clearly indicate the operation that was actually performed, which after a quick glance at the code means we need to store this state somewhere to do so.\", 'Had a discussion with Brandon offline, \\nThere is enough information in the logs to show the operation was Bootstrap vs Repair vs Startup, so closing the ticket for now.']\n",
            "my_comment: Startup completed?\n",
            "(I am still looking for an abstract word :)) Bootstrap means something specifically with cassandra in that you think some data has streamed in.\n",
            "\n",
            "I think \"Startup completed\" would be great.\n",
            "\n",
            "If there IS a bootstrap/replace/move then I think the message ought to specify which has happened and that it\\s ready now (if it\\s easy to do) :) Committed https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blobdiff;f=src/java/org/apache/cassandra/service/StorageService.java;h=8de0bd24632c89ea1b41c952ee6ec2db58808894;hp=7d92fbe0ff15c8c686a93425f4fccca49b921c0b;hb=d525cf969c042b21a9375446f5449ee82d7d1484;hpb=7e937b3d1308c0774e4b0366b6e66b14af1dd5f6\n",
            "\n",
            "Let me know if you need more info i will reopen this ticket. \"This isnt quite what I had in mind.  Its not a semantic issue its a logical issue.  We should clearly indicate the operation that was actually performed which after a quick glance at the code means we need to store this state somewhere to do so.\" Had a discussion with Brandon offline \n",
            "There is enough information in the logs to show the operation was Bootstrap vs Repair vs Startup so closing the ticket for now.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4874\n",
            "issue_type: Improvement\n",
            "summary: Possible authorizaton handling impovements\n",
            "description: I'll create another issue with my suggestions about fixing/improving IAuthority interfaces. This one lists possible improvements that aren't related to grant/revoke methods.\n",
            "\n",
            "Inconsistencies:\n",
            "- CREATE COLUMNFAMILY: P.CREATE on the KS in CQL2 vs. P.CREATE on the CF in CQL3 and Thrift\n",
            "- BATCH: P.UPDATE or P.DELETE on CF in CQL2 vs. P.UPDATE in CQL3 and Thrift (despite remove* in Thrift asking for P.DELETE)\n",
            "- DELETE: P.DELETE in CQL2 and Thrift vs. P.UPDATE in CQL3\n",
            "- DROP INDEX: no checks in CQL2 vs. P.ALTER on the CF in CQL3\n",
            "\n",
            "Other issues/suggestions\n",
            "- CQL2 DROP INDEX should require authorization\n",
            "- current permission checks are inconsistent since they are performed separately by CQL2 query processor, Thrift CassandraServer and CQL3 statement classes.\n",
            "We should move it to one place. SomeClassWithABetterName.authorize(Operation, KS, CF, User), where operation would be a enum\n",
            "(ALTER_KEYSPACE, ALTER_TABLE, CREATE_TABLE, CREATE, USE, UPDATE etc.), CF should be nullable.\n",
            "- we don't respect the hierarchy when checking for permissions, or, to be more specific, we are doing it wrong. take  CQL3 INSERT as an example:\n",
            "we require P.UPDATE on the CF or FULL_ACCESS on either KS or CF. However, having P.UPDATE on the KS won't allow you to perform the statement, only FULL_ACCESS will do.\n",
            "I doubt this was intentional, and if it was, I say it's wrong. P.UPDATE on the KS should allow you to do updates on KS's cfs.\n",
            "Examples in http://www.datastax.com/dev/blog/dynamic-permission-allocation-in-cassandra-1-1 point to it being a bug, since REVOKE UPDATE ON ks FROM omega is there.\n",
            "- currently we lack a way to set permission on cassandra/keyspaces resource. I think we should be able to do it. See the following point on why.\n",
            "- currently to create a keyspace you must have a P.CREATE permission on that keyspace THAT DOESN'T EVEN EXIST YET. So only a superuser can create a keyspace,\n",
            "or a superuser must first grant you a permission to create it. Which doesn't look right to me. P.CREATE on cassandra/keyspaces should allow you to create new\n",
            "keyspaces without an explicit permission for each of them.\n",
            "- same goes for CREATE TABLE. you need P.CREATE on that not-yet-existing CF of FULL_ACCESS on the whole KS. P.CREATE on the KS won't do. this is wrong.\n",
            "- since permissions don't map directly to statements, we should describe clearly in the documentation what permissions are required by what cql statement/thrift method.\n",
            "\n",
            "Full list of current permission requirements: https://gist.github.com/3978182\n",
            "\n",
            "architectural impact: NO\n",
            "comments: ['bq. CREATE COLUMNFAMILY: P.CREATE on the KS in CQL2 vs. P.CREATE on the CF in CQL3 and Thrift\\n\\nCQL2 sounds correct to me, how can you have permissions on an object that doesn\\'t exist yet?\\n\\nBut that would imply that KS create permissioning is also broken, which you mention.\\n\\nMaybe we should have a \"cluster\" or \"all\" top-level permission: having create on all, allows creating keyspaces.  This would fit with the heirarchy design you describe too (GRANT UPDATE ON ALL TO foo), and gives a nice shorthand for granting system-wide permissions (or a subset of them) w/o making someone a superuser.\\n\\nbq. BATCH: P.UPDATE or P.DELETE on CF in CQL2 vs. P.UPDATE in CQL3 and Thrift (despite remove* in Thrift asking for P.DELETE)\\n\\nISTM that the correct behavior is to permission-check each statement in the batch separately.\\n\\nbq. DELETE: P.DELETE in CQL2 and Thrift vs. P.UPDATE in CQL3\\n\\nSeems obvious.\\n\\nbq. DROP INDEX: no checks in CQL2 vs. P.ALTER on the CF in CQL3\\n\\nALTER sounds reasonable.\\n\\nbq. We should move it to one place. SomeClassWithABetterName.authorize\\n\\nI\\'m not sure this really improves things, you\\'ve just created an abstraction layer with different names but fundamentally you still have to insert the correct auth call in each query processing path.\\n\\nbq. P.UPDATE on the KS should allow you to do updates on KS\\'s cfs\\n\\n+1\\n\\n', \"{quote}\\nbq.DELETE: P.DELETE in CQL2 and Thrift vs. P.UPDATE in CQL3\\n\\nSeems obvious.\\n{quote}\\n\\nNot to me. Should be P.DELETE in Thrift and CQL3? If so, then what do you think about requiring both P.UPDATE and P.DELETE for inserts/updates with TTL set?\\n\\n{quote}\\nbq. We should move it to one place. SomeClassWithABetterName.authorize\\n\\nI'm not sure this really improves things, you've just created an abstraction layer with different names but fundamentally you still have to insert the correct auth call in each query processing path.\\n{quote}\\n\\nYou are right. This won't be needed once we fix permission inheritance. Then one method in ClientState (modified current hasAccees) will be sufficient. No need for another enum.\\n\\n[~jbellis] Can you look at CASSANDRA-4875 as well please?\", 'bq. Should be P.DELETE in Thrift and CQL3? \\n\\nYes.\\n\\nbq. If so, then what do you think about requiring both P.UPDATE and P.DELETE for inserts/updates with TTL set?\\n\\nYes, we should.  (This is why I\\'m not 100% sure it makes sense to distinguish between UPDATE and DELETE at all, but \"require both for ttl\" is probably the best compromise.)', \"bq. I'm not 100% sure it makes sense to distinguish between UPDATE and DELETE at all\\n\\nI'd agree with that. From a security perspective I don't see the difference between updating a value with crap or deleting it, so imo both permission will always be set together and so it seems to me that having both only help people at making the mistake of revoking one permission without the other. Just my 2 cents though.\", \"bq. I don't see the difference between updating a value with crap or deleting it\\n\\nMost time there is no difference, unless you have a null-column in the first place and only care about the column name.\", 'bq. unless you have a null-column in the first place and only care about the column name\\n\\nNot sure I follow.', \"Imagine a wide row representing a time series, where every column's name is a timestamp and every column's value is already null - you don't care about the value. In this case there is a difference between overwriting the value with crap (doesn't matter) and removing the column entirely (matters).\", \"Btw I'm not advocating for keeping P.DELETE, just saying that there is sometimes a difference from security standpoint.\", \"Ok, I understand that example. It's a fairly specific use case imo (in term of security I mean), and there will always been cases where whatever permissions we allow won't be precise enough for someone. I'm still of the opinion that it's not worth the potential foot shooting of setting P.DELETE without P.UPDATE by mistake, but that's just an opinion.\", \"Also, if we are getting rid of P.DELETE in favor of one unified permission for insert/update/delete, then I'd rather add some new name that won't match any of the operations, to avoid confusion. Say, P.MODIFY.\", 'Single patch for CASSANDRA-4874 and CASSANDRA-4875.', 'v2 is almost identical to v1 (differs by 2 lines, one of which is a comment).\\nThis way it will work with SimpleAuthenticator.', 'v3: Pulled out NativeAuthority. Still, please have a look at it while reviewing the issue - it might explain some implementation decisions.', \"Looks pretty good to me.  Comments:\\n\\n- Should document what happened to IAuth/LegacyAuth in News\\n- grantOption feels like premature complexity to me -- why not just a single GRANT permission?\\n\\nNits: \\n\\n- unrelated cqlsh cleanup should be separate.\\n- Not 100% sure that IResource is going to be useful in its present form (for e.g. functions) so I'd be inclined to just use DataResource everywhere for now, but it's probably okay the way it is for now.\\n\", \"Thanks.\\n\\nbq. Should document what happened to IAuth/LegacyAuth in News\\nWill do.\\n\\nbq. grantOption feels like premature complexity to me – why not just a single GRANT permission?\\nI assume having P.GRANT would give a user ability to grant and revoke any permission on the resource (including P.GRANT itself). This would also simplify grant and revoke implementations (check for P.GRANT and that's all). If so then I agree and I like it.\\n\\nbq. unrelated cqlsh cleanup should be separate.\\nSorry. Couldn't resist.\\n\\nbq. Not 100% sure that IResource is going to be useful in its present form (for e.g. functions) so I'd be inclined to just use DataResource everywhere for now, but it's probably okay the way it is for now.\\nThe idea was to avoid breaking IAuthority interface in the future. Hence the only assumptions were that any new resource type will be hierarchical, and every level will have a printable name. TBH I don't have a strong opinion on this point. We can just see what happens (if?) to functions/triggers/what not and if necessary slightly alter IAuthority in 1.3/1.4. It's YAGNI vs. not breaking stuff. Just using DataResource everywhere for now is fine by me. A second opinion maybe?\", 'BTW unfortunately just replacing \"implements IAuthority\" with \"extends LegacyAuthority\" won\\'t be enough if a particular implementation used Permission.ALL and/or Permission.NONE. They were returning (mutable) EnumSet-s and I had to replace them with ImmutableSet-s. I wish there was an immutable subclass of EnumSet, but there isn\\'t, because Java. That\\'s the only issue though.', 'bq. I assume having P.GRANT would give a user ability to grant and revoke any permission on the resource (including P.GRANT itself)\\n\\nRight.\\n\\nbq. The idea was to avoid breaking IAuthority interface in the future\\n\\nThat makes sense.', \"Actually, that wouldn't work (a single GRANT permission).\\nI looked at two options, but both don't work:\\n1. Allow GRANT owners GRANT and REVOKE any permission on the resource (even those they don't have) - now we get the old FULL_ACCESS and all the associated problems (you have to check for GRANT and not just the requested permission and, secondly, REVOKE <perm> won't work as expected if a user still has GRANT permission on the resource).\\n2. Allow GRANT owners GRANT and REVOKE only the permissions they already have on the resource. This one is trickier:\\n2.1 Make P.GRANT non-recursive (disallow granting P.GRANT). Now you need to involve a superuser every time you want to allow someone grant permissions. This is bad since it involves superusers unnecessarily.\\n2.2 Make P.GRANT recursive (allow granting P.GRANT). Now we've got ourselves easy permission escalation. User A has every permission but P.GRANT. User B has P.SELECT and P.GRANT (or just P.GRANT). User B grants P.GRANT to user A. A grants B every permission he has. Now the two of them together have more permissions than the sum of their old ones. \", \"So WITH GRANT OPTION is here to stay. It's simple to understand and doesn't leak permissions.\", \"As for IResource and functions - my idea was that each resource type will have its own root (no common global root). DataResources start with 'data', FunctionResources will be represented as 'functions[/..]. So there will usually be at least two levels independent of the resource type. This might even allow user-specified resource types as long as they follow the convention and as long as we have a map root name -> resource class.\\nSo I vote for leaving IResource alone.\", \"bq. REVOKE <perm> won't work as expected if a user still has GRANT permission on the resource).\\n\\nOf course you'd need to revoke GRANT as well.  Works as designed!\\n\\nbq. I vote for leaving IResource alone\\n\\nWFM.\", \"bq. Of course you'd need to revoke GRANT as well. Works as designed!\\nI don't like this design :( P.GRANT (P.AUTHORIZE) would be an equivalent of the old FULL_ACCESS, but on steroids. A super-permission that includes other permissions.\\nNo, permissions should not intersect.\", \"It's a meta-permission really (permission to grant permissions). It just doesn't belong there.\", 'Also queries like \"LIST ALTER PERMISSIONS ON data/ks\" now won\\'t show everybody capable of altering data/ks. Will have to list grant permissions as well (since a user with P.GRANT will be able to just give P.ALTER to himself at any moment).', 'You\\'re overthinking it.  You still have that problem with fine-grained grant, it\\'s just more complex.  Nor does \"include user with GRANT as a user with LIST\" make sense, because if you want to include users who could POTENTIALLY have LIST, well, a user with GRANT can give LIST to anyone, not just himself.', \"I probably am. I'll think about it.\\nBut, to clarify - by LIST you meant ALTER, right?\", 'To match your example, yes.', '[~jbellis]\\nNow I do think you are more right than I am.\\nWhat do you think about this option: introduce Permission.AUTHORIZE. Let users with AUTHORIZE on a resource GRANT and REVOKE permissions on that resource to/from others, but only the permissions they already have (on that resource or its parent, recursively), including AUTHORIZE itself? I considered this before, but I did overthink it back then.', \"This will allow to get rid of grantOption and the permission checks in grant() and authorize() in IAuthority implementations. I'll just add two checks to grant statement and revokestatement: hasAccess(AUTHORIZE, resource) and hasAccees(permissiionToBeGratnedOrRevoked, resource).\", \"Limiting it to the permissions you already have at least means that you won't be able to re-grant the revoked permission back to yourself on your own.\", \"I previously did overthink 2.2 in https://issues.apache.org/jira/browse/CASSANDRA-4874?focusedCommentId=13500289&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13500289\\nIt's not really an escalation - these two users will still be limited to the permissions they collectively own. So 2.2 should actually work.\", \"bq. Limiting it to the permissions you already have at least means that you won't be able to re-grant the revoked permission back to yourself on your own.\\n\\nMakes sense to me.\", 'v3 vs. v4 changes:\\n- moved cqlsh and Cql.g cleanup to a separate patch (remove-consistency-vestigest-cqlsh-and-cqlg.txt) - apply it first\\n- renamed IAuthority to IAuthorizer to be consistent with IAuthenticator\\n- renamed {AllowAll,Simple,Legacy}Authority to *Authorizer\\n- Permission.AUTHORIZE replaced WITH GRANT OPTION\\n- CREATE KEYSPACE now requires CREATE on ALL KEYSPACES (used to require CREATE on the not-yet-existing keyspace)\\n- CREATE TABLE now requires CREATE on the parent keyspace (used to require CREATE on the not-yet-existing table)\\n- new IAuthorizer.revokeAll(IResource droppedResource) method is called for cleanup when a keyspace/table gets dropped\\n- IAuthorizer.protectedResources now returns a set of IResource, not DataResource (future-proofing the interface)\\n- QueryProcessor.processStatement() now calls validate() first and then checkAccess() (used to be the other way around)\\n- GRANT, REVOKE and LIST all check the existence of the resource in question (boolean exists() method has been added to IResource)\\n- updated NEWS.txt\\n- modified CQL3 syntax (cqlsh autocompletion has been updated as well)\\n\\nThe new CQL3 statements:\\n- LIST { ALL [PERMISSIONS] | <perm> [PERMISSION] } [OF <user>]\\n- LIST { ALL [PERMISSIONS] | <perm> [PERMISSION] } ON ALL KEYSPACES [OF <user>] [NORECURSIVE]\\n- LIST { ALL [PERMISSIONS] | <perm> [PERMISSION] } ON KEYSPACE <keyspace> [of <user>] [NORECURSIVE]\\n- LIST { ALL [PERMISSIONS] | <perm> [PERMISSION] } ON [TABLE] [<keyspace>.]<table> [of <user>] [NORECURSIVE]\\n- GRANT { ALL [PERMISSIONS] | <perm> [PERMISSION] } ON ALL KEYSPACES TO <user>\\n- GRANT { ALL [PERMISSIONS] | <perm> [PERMISSION] } ON KEYSPACE <keyspace> TO <user>\\n- GRANT { ALL [PERMISSIONS] | <perm> [PERMISSION] } ON [TABLE] [<keyspace>.]<table> TO <user>\\n- REVOKE { ALL [PERMISSIONS] | <perm> [PERMISSION] } ON ALL KEYSPACES FROM <user>\\n- REVOKE { ALL [PERMISSIONS] | <perm> [PERMISSION] } ON KEYSPACE <keyspace> FROM <user>\\n- REVOKE { ALL [PERMISSIONS] | <perm> [PERMISSION] } ON [TABLE] [<keyspace>.]<table> FROM <user>', 'Not to self: one authentication rewrite is complete, make sure to:\\n1) Validate user existence in GRANT/REVOKE/LIST PERMISSIONS\\n2) Call IAuthority#revokeAll(String droppedUser) when a user is dropped', \"Attached 'warn-authority.txt' patch that\\n1. Prints a warning on startup if it detects 'authority' param\\n2. Throws ConfigurationException if 'authority' isn't set to 'AllowAllAuthority'\\n\\nThis way upgrading users who didn't care about authorization in the first place won't have to update anything.\\nThis is friendlier than just exiting, but I'm not 100% sure if we should be friendly. Not 100% but pretty sure still.\", 'Looks good overall.  A couple comments:\\n\\n- Prefer just returning Permission.NONE|ALL instead of copyOf; copying makes me think that the original (or possibly the copy) should be mutable, which is not the case here\\n- Why the rewrite of DropIndexStatement?\\n', 'Do we want to support \"show me all the permissions that have been granted to object X?\"', \"attached 'move-resource-on-to-iauthorizer.txt' that moves filtering on resource to IAuthorizer itself away from ListPermissionsStatement#execute.\", 'bq. Prefer just returning Permission.NONE|ALL instead of copyOf; copying makes me think that the original (or possibly the copy) should be mutable, which is not the case here\\nThis is only the case with SimpleAuthorizer. The old IAuthority had authorize() return a EnumSet and Permission.ALL|NONE don\\'t return a EnumSet anymore (EnumsSet is mutable). copyOf is there to convert Set<Permission> to EnumSet<Permission>.\\n\\nbq. Why the rewrite of DropIndexStatement?\\nThere used to be no way to get the CF from DropIndexStatement and I needed one to do a permission check for DROP INDEX (alter on parent cf). Now there is a way, but that required a rewrite.\\n\\nbq. Do we want to support \"show me all the permissions that have been granted to object X?\"\\nWe do, of cource. The latest (last?) patch makes this more explicit.', '+1', 'Committed, thanks.']\n",
            "my_comment: one authentication rewrite is complete make sure to:\n",
            "1) Validate user existence in GRANT/REVOKE/LIST PERMISSIONS\n",
            "2) Call IAuthority#revokeAll(String droppedUser) when a user is dropped \"Attached warn-authority.txt patch that\n",
            "1. Prints a warning on startup if it detects authority param\n",
            "2. Throws ConfigurationException if authority isnt set to AllowAllAuthority\n",
            "\n",
            "This way upgrading users who didnt care about authorization in the first place wont have to update anything.\n",
            "This is friendlier than just exiting but Im not 100% sure if we should be friendly. Not 100% but pretty sure still.\" Looks good overall.  A couple comments:\n",
            "\n",
            "- Prefer just returning Permission.NONE|ALL instead of copyOf; copying makes me think that the original (or possibly the copy) should be mutable which is not the case here\n",
            "- Why the rewrite of DropIndexStatement?\n",
            " Do we want to support \"show me all the permissions that have been granted to object X?\" \"attached move-resource-on-to-iauthorizer.txt that moves filtering on resource to IAuthorizer itself away from ListPermissionsStatement#execute.\" bq. Prefer just returning Permission.NONE|ALL instead of copyOf; copying makes me think that the original (or possibly the copy) should be mutable which is not the case here\n",
            "This is only the case with SimpleAuthorizer. The old IAuthority had authorize() return a EnumSet and Permission.ALL|NONE don\\t return a EnumSet anymore (EnumsSet is mutable). copyOf is there to convert Set<Permission> to EnumSet<Permission>.\n",
            "\n",
            "bq. Why the rewrite of DropIndexStatement?\n",
            "There used to be no way to get the CF from DropIndexStatement and I needed one to do a permission check for DROP INDEX (alter on parent cf). Now there is a way but that required a rewrite.\n",
            "\n",
            "bq. Do we want to support \"show me all the permissions that have been granted to object X?\"\n",
            "We do of cource. The latest (last?) patch makes this more explicit. +1 Committed thanks.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4919\n",
            "issue_type: Bug\n",
            "summary: StorageProxy.getRangeSlice sometimes returns incorrect number of columns\n",
            "description: When deployed on a single node, number of columns is correct.\n",
            "When deployed on a cluster, total number of returned columns is slightly lower than desired. \n",
            "architectural impact: NO\n",
            "comments: ['Attaching a patch fixing paged column iteration.', 'Is there a dtest for this?', \"There's a wide row test and a range slice test, but not a combination of the two.\", 'committed, and create https://github.com/riptano/cassandra-dtest/issues/5 to follow up w/ dtest.', 'I note for the testing part that this only concern getRangeSlice with the isPaging option (i.e. for thrift, get_paged_slice) and that currently CQL never calls that so this is not reproducible with CQL. We may end up using the isPaging thing in CQL with CASSANDRA-4851 however.', 'Just added wide_slice_test to putget_test.py in the dtests.']\n",
            "my_comment: Attaching a patch fixing paged column iteration. Is there a dtest for this? \"Theres a wide row test and a range slice test but not a combination of the two.\" committed and create https://github.com/riptano/cassandra-dtest/issues/5 to follow up w/ dtest. I note for the testing part that this only concern getRangeSlice with the isPaging option (i.e. for thrift get_paged_slice) and that currently CQL never calls that so this is not reproducible with CQL. We may end up using the isPaging thing in CQL with CASSANDRA-4851 however. Just added wide_slice_test to putget_test.py in the dtests.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-4992\n",
            "issue_type: Bug\n",
            "summary: TTL/WRITETIME function against collection column returns invalid value\n",
            "description: Since we cannot query individual content of collection in 1.2, TTL/WRITETIME function on collection column does not make sense. But currently we can perform those function on collection and get deserialization error like:\n",
            "\n",
            "{code}\n",
            "value '\\x00\\x03\\x00\\x01c\\x00\\x01b\\x00\\x01a' (in col 'writetime(l)') can't be deserialized as bigint: unpack requires a string argument of length 8\n",
            "{code}\n",
            "\n",
            "Looks like it tries to deserialize whole list/set/map content as bigint for WRITETIME and int for TTL.\n",
            "architectural impact: NO\n",
            "comments: ['Simple patch attached to just refuse said function on collection columns.', '+1', 'Commited, thanks']\n",
            "my_comment: Simple patch attached to just refuse said function on collection columns. +1 Commited thanks\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5015\n",
            "issue_type: Improvement\n",
            "summary: move bloom_filter_fp_chance to compaction options\n",
            "description: This setting doesn't take affect until data is recompacted, so should be moved into compaction options.\n",
            "\n",
            "Alternatively, we could do what index_interval does, and rebuild it on startup if changed.\n",
            "architectural impact: NO\n",
            "comments: ['bq. Alternatively, we could do what index_interval does, and rebuild it on startup if changed.\\n\\nCan you elaborate?', \"I believe Jonathan's referring to how in SSTableReader.load we recompute the index summary if we can't read it from disk. In fact, SSTableReader.load already takes a boolean to decide if it should rebuild the bloom filter, so it seems we only need to pass true if we detect the bloom_filter_fp_chance has changed. Though that later detection may require that we save the bffc in the SSTable metadata. \", 'https://github.com/iamaleksey/cassandra/compare/5015', \"Should probably assume that if bffpc is not in metadata, then it's correct (unless it's an ancient strings-in-bloom-filter file).\\n\\nOtherwise LGTM.\", \"bq. Should probably assume that if bffpc is not in metadata, then it's correct (unless it's an ancient strings-in-bloom-filter file).\\n\\nCommitted with this change. Thanks.\"]\n",
            "my_comment: bq. Alternatively we could do what index_interval does and rebuild it on startup if changed.\n",
            "\n",
            "Can you elaborate? \"I believe Jonathans referring to how in SSTableReader.load we recompute the index summary if we cant read it from disk. In fact SSTableReader.load already takes a boolean to decide if it should rebuild the bloom filter so it seems we only need to pass true if we detect the bloom_filter_fp_chance has changed. Though that later detection may require that we save the bffc in the SSTable metadata. \" https://github.com/iamaleksey/cassandra/compare/5015 \"Should probably assume that if bffpc is not in metadata then its correct (unless its an ancient strings-in-bloom-filter file).\n",
            "\n",
            "Otherwise LGTM.\" \"bq. Should probably assume that if bffpc is not in metadata then its correct (unless its an ancient strings-in-bloom-filter file).\n",
            "\n",
            "Committed with this change. Thanks.\"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5050\n",
            "issue_type: Bug\n",
            "summary: Cql3 token queries broken\n",
            "description: Currently any select statement that uses a token() predicate breaks with \"Bad Input\"\n",
            "\n",
            "After tracing the logic this error is caused in getTokenBounds because it assumes the token term is an actual token string what will pass the tokenizer\n",
            "\n",
            "\n",
            "architectural impact: NO\n",
            "comments: ['+1, committed, thanks']\n",
            "my_comment: +1 committed thanks\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5121\n",
            "issue_type: Bug\n",
            "summary: system.peers.tokens is empty after node restart\n",
            "description: Using a 2 nodes fresh cluster (127.0.0.1 & 127.0.0.2) running latest 1.2, I’m querying system.peers to get the nodes of the cluster and their respective token. But it seems there is a problem after either node restart.\n",
            "\n",
            "When both node starts up, querying system.peers seems ok:\n",
            "\n",
            "{code}\n",
            "127.0.0.1> select * from system.peers;\n",
            "+-----------------+------------------------------------------+---------------+-----------+---------------------+-----------------+------------------------------------------+-------------------------------------------+\n",
            "| data_center     | host_id                                  | peer          | rack      | release_version     | rpc_address     | schema_version                           | tokens                                    |\n",
            "+=================+==========================================+===============+===========+=====================+=================+==========================================+===========================================+\n",
            "| datacenter1     | 4819cbb0-9741-4fe0-8d7d-95941b0247bf     | 127.0.0.2     | rack1     | 1.2.0               | 127.0.0.2       | 59adb24e-f3cd-3e02-97f0-5b395827453f     | 56713727820156410577229101238628035242    |\n",
            "+-----------------+------------------------------------------+---------------+-----------+---------------------+-----------------+------------------------------------------+-------------------------------------------+\n",
            "{code}\n",
            "\n",
            "But as soon as one node is restarted (let’s say 127.0.0.2), tokens column is then empty:\n",
            "\n",
            "{code}\n",
            "127.0.0.1> select * from system.peers;\n",
            "+-----------------+------------------------------------------+---------------+-----------+---------------------+-----------------+------------------------------------------+-------------+\n",
            "| data_center     | host_id                                  | peer          | rack      | release_version     | rpc_address     | schema_version                           | tokens      |\n",
            "+=================+==========================================+===============+===========+=====================+=================+==========================================+=============+\n",
            "| datacenter1     | 4819cbb0-9741-4fe0-8d7d-95941b0247bf     | 127.0.0.2     | rack1     | 1.2.0               | 127.0.0.2       | 59adb24e-f3cd-3e02-97f0-5b395827453f     |             |\n",
            "+-----------------+------------------------------------------+---------------+-----------+---------------------+-----------------+------------------------------------------+-------------+\n",
            "{code}\n",
            "\n",
            "{code}\n",
            "Log server side:\n",
            "DEBUG 22:08:01,608 Responding: ROWS [peer(system, peers), org.apache.cassandra.db.marshal.InetAddressType][data_center(system, peers), org.apache.cassandra.db.marshal.UTF8Type][host_id(system, peers), org.apache.cassandra.db.marshal.UUIDType][rack(system, peers), org.apache.cassandra.db.marshal.UTF8Type][release_version(system, peers), org.apache.cassandra.db.marshal.UTF8Type][rpc_address(system, peers), org.apache.cassandra.db.marshal.InetAddressType][schema_version(system,\n",
            "peers), org.apache.cassandra.db.marshal.UUIDType][tokens(system, peers), org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type)]\n",
            " | 127.0.0.2 | datacenter1 | 4819cbb0-9741-4fe0-8d7d-95941b0247bf | rack1 | 1.2.0 | 127.0.0.2 | 59adb24e-f3cd-3e02-97f0-5b395827453f | null\n",
            "{code}\n",
            "\n",
            "Restarting the other node (127.0.0.1) restore back the tokens column.\n",
            "\n",
            "architectural impact: NO\n",
            "comments: ['In StorageService.handleStateNormal, when we see an endpoint come up which we already knew about: \\n\\n{noformat}\\nelse if (endpoint.equals(currentOwner))\\n{\\n    // set state back to normal, since the node may have tried to leave, but failed and is now back up\\n    // no need to persist, token/ip did not change\\n{noformat}\\n\\nI think the bug is that then we call \\n{noformat}\\nSystemTable.updateTokens(endpoint, tokensToUpdateInSystemTable);\\n{noformat}\\nwith an empty collection and SystemTable.updateTokens overwrites the current entry rather than adding to it.\\n\\nFix would be\\n{noformat}\\n- // no need to persist, token/ip did not change\\n+ if (!isClientMode)\\n+    tokensToUpdateInSystemTable.add(token);\\n{noformat}', \"I agree on Sam's analysis but I would suggest the slightly different patch attached, because I think the intend was that if tokensToUpdateInSystemTable is empty, we don't update anything. In particular, in the case where we are relocating, I don't think we want remove the tokens from the system table either (and in the case where there is a token conflict I think the initial intent was also to leave things as they are, even though in that case maybe actually removing the token is not a bad idea?).\\n\", '+1', 'Committed, thanks', 'Hi, commit ec35427fdfbc46a8adeafc042651f552b9bcc1a0 breaks RelocateTest:\\n\\n{noformat}\\n$ ant clean build test -Dtest.name=RelocateTest\\n...\\n    [junit] Testsuite: org.apache.cassandra.service.RelocateTest\\n    [junit] Tests run: 2, Failures: 2, Errors: 0, Time elapsed: 6.215 sec\\n    [junit] \\n    [junit] Testcase: testWriteEndpointsDuringRelocate(org.apache.cassandra.service.RelocateTest):\\tFAILED\\n    [junit] removeTokens should be used instead\\n    [junit] junit.framework.AssertionFailedError: removeTokens should be used instead\\n    [junit] \\tat org.apache.cassandra.db.SystemTable.updateTokens(SystemTable.java:324)\\n    [junit] \\tat org.apache.cassandra.db.SystemTable.updateLocalTokens(SystemTable.java:342)\\n    [junit] \\tat org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1393)\\n    [junit] \\tat org.apache.cassandra.service.StorageService.onChange(StorageService.java:1166)\\n    [junit] \\tat org.apache.cassandra.service.RelocateTest.createInitialRing(RelocateTest.java:106)\\n    [junit] \\tat org.apache.cassandra.service.RelocateTest.testWriteEndpointsDuringRelocate(RelocateTest.java:128)\\n    [junit] \\n    [junit] \\n    [junit] Testcase: testRelocationSuccess(org.apache.cassandra.service.RelocateTest):\\tFAILED\\n    [junit] removeTokens should be used instead\\n    [junit] junit.framework.AssertionFailedError: removeTokens should be used instead\\n    [junit] \\tat org.apache.cassandra.db.SystemTable.updateTokens(SystemTable.java:324)\\n    [junit] \\tat org.apache.cassandra.db.SystemTable.updateLocalTokens(SystemTable.java:342)\\n    [junit] \\tat org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1393)\\n    [junit] \\tat org.apache.cassandra.service.StorageService.onChange(StorageService.java:1166)\\n    [junit] \\tat org.apache.cassandra.service.RelocateTest.createInitialRing(RelocateTest.java:106)\\n    [junit] \\tat org.apache.cassandra.service.RelocateTest.testRelocationSuccess(RelocateTest.java:177)\\n    [junit] \\n    [junit] \\n    [junit] Test org.apache.cassandra.service.RelocateTest FAILED\\n\\nBUILD FAILED\\n...\\n{noformat}\\n\\n\\nAfter commit e6b6eaa583e8fc15f03c3e27664bf7fc06b3af0a, testWriteEndpointsDuringRelocate passes but testRelocationSuccess still fails:\\n{noformat}\\n$ ant clean build test -Dtest.name=RelocateTest\\n...\\n    [junit] Testcase: testRelocationSuccess(org.apache.cassandra.service.RelocateTest):\\tFAILED\\n    [junit] removeEndpoint should be used instead\\n    [junit] junit.framework.AssertionFailedError: removeEndpoint should be used instead\\n    [junit] \\tat org.apache.cassandra.db.SystemTable.updateTokens(SystemTable.java:316)\\n    [junit] \\tat org.apache.cassandra.db.SystemTable.updateLocalTokens(SystemTable.java:334)\\n    [junit] \\tat org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1394)\\n    [junit] \\tat org.apache.cassandra.service.StorageService.onChange(StorageService.java:1166)\\n    [junit] \\tat org.apache.cassandra.service.RelocateTest.testRelocationSuccess(RelocateTest.java:193)\\n    [junit] \\n    [junit] \\n    [junit] Test org.apache.cassandra.service.RelocateTest FAILED\\n...\\n{noformat}\\n', 'You should update, this was fixed in 17adf8e4f72114d336140fac5157a35e63d1f53a', 'Updated; test now passes, thanks!']\n",
            "my_comment: removeEndpoint should be used instead\n",
            "    [junit] \\tat org.apache.cassandra.db.SystemTable.updateTokens(SystemTable.java:316)\n",
            "    [junit] \\tat org.apache.cassandra.db.SystemTable.updateLocalTokens(SystemTable.java:334)\n",
            "    [junit] \\tat org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1394)\n",
            "    [junit] \\tat org.apache.cassandra.service.StorageService.onChange(StorageService.java:1166)\n",
            "    [junit] \\tat org.apache.cassandra.service.RelocateTest.testRelocationSuccess(RelocateTest.java:193)\n",
            "    [junit] \n",
            "    [junit] \n",
            "    [junit] Test org.apache.cassandra.service.RelocateTest FAILED\n",
            "...\n",
            "{noformat}\n",
            " You should update this was fixed in 17adf8e4f72114d336140fac5157a35e63d1f53a Updated; test now passes thanks!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5137\n",
            "issue_type: Bug\n",
            "summary: Make sure SSTables left over from compaction get deleted and logged\n",
            "description: When opening ColumnFamily, cassandra checks SSTable files' ancestors and skips loading already compacted ones. Those files are expected to be deleted, but currently that never happens.\n",
            "Also, there is no indication of skipping loading file in the log, so it is confusing especially doing upgradesstables.\n",
            "architectural impact: NO\n",
            "comments: ['We need to mark skipping SSTable as compacted to be removed.', 'Hmm.\\n\\nThis patch is correct as far as it goes but I think the existing assumption is broken: that if we have any sstable with ancestor X, then X is safe to delete.\\n\\nSpecifically, LCS will create multiple sstables from a given set of ancestors, so unless we know that we finished the compaction (and finished writing all the resulting descendant sstables), we could lose data if we delete the ancestors themselves.\\n\\nOne possible fix:\\n\\n# Add a flag to SSTM for \"this was the final sstable in the compaction\"\\n# When we scan sstables, we can delete ancestors if we find that marker in any of the descendants\\n# Otherwise, we should delete the *descendants* and leave the ancestors alone (so we don\\'t doublecount data for counters)', \"I think you're right.\\n\\nA fourth pseudo-solution could be to wait the end of the compaction to rename all the newly created writers (i.e remove the tmp markers). It's not bulletproof though as I don't think we can rename multiple files atomically but just wanted to mention it.\\n\\nMaybe at least for 1.1, 3. is the best/simplest option. On the longer term, maybe 1. is better.\", 'You\\'re right, you don\\'t actually need a marker since if compaction completes the next step is to remove the ancestors.  I think \"if ancestors are still alive, assume compaction didn\\'t finish and delete the descendants\" is good enough.', 'bq. I think \"if ancestors are still alive, assume compaction didn\\'t finish and delete the descendants\" is good enough.\\n\\nyeah agreed.', \"Hum wait, it only works if we have all ancestors though. What if just one ancestor don't get deleted for some reason (or only some of the SSTableDeletingTask have executed before a crash)? It's easy enough to check that we have *all* ancestors, but what if we don't? We're back to square one :(\", \"You're right.  Guess we need a compaction-finished flag after all.\\n\\nInstead of storing it in sstable metadata, maybe we could store it in system.local the way we do with truncation information.  Unfortunately 1.1 doesn't support Maps so we'd be doing two separate implementations for 1.1 and 1.2.\\n\\nShould we just say that for 1.1 we'll retain all sstables (counter users will get overcounts, everyone else just gets extra compaction work) and fix it better in 1.2?\", 'Something like this...\\n\\n{code}\\ncreate table compaction_log (\\n  id uuid primary key,\\n  inputs set<int>,\\n  outputs set<int>\\n);\\n{code}\\n\\nWhen we start a compaction, we add it to the log.  When we finish, we remove it.  If we restart and compaction_log is not empty, we remove any sstables from the outputs set.', \"bq. Should we just say that for 1.1 we'll retain all sstables\\n\\nFor 1.1, I'd suggest doing my fourth pseudo-solution above, i.e. moving the renaming of newly created writes at the end of the compaction (it's trivial). Then at startup, we could indeed retain all sstables for normal CF, but for counter we would keep removing the predecessors as we do now. It wouldn't totally fix the risk of losing counters, but it would make it very unlikely (you'd need to fail exactly in the middle of the bulk renaming a newly create sstable writers), while just retaining all sstables would make it very easy to have overcounts.\\n\\nFor 1.2, you compaction_log solution does seem reasonable.\", \"V2 implements Sylvain's idea that renames written SSTables at the end of compaction.\\n\\nFor 1.2, let's open different issue for Jonathan's suggestion.\", \"The code of v2 looks alright, but let's also disable the filtering in ColumnFamilyStore.ctor for non-counter CFs so we take zero chance of losing data (and since reusing an already compacted sstable is a bit inefficient but harmless).\\n\\nbq. For 1.2, let's open different issue for Jonathan's suggestion\\n\\nAgreed.\", 'Attached v3 that also changes the filtering part only for counter CF.', \"+1 (though do commit your v1 along the way, no way in keeping sstable we're not going to use, even if it's only for counters).\", 'Committed v1 + v3, and opened CASSANDRA-5151 for better solution.']\n",
            "my_comment: that if we have any sstable with ancestor X then X is safe to delete.\n",
            "\n",
            "Specifically LCS will create multiple sstables from a given set of ancestors so unless we know that we finished the compaction (and finished writing all the resulting descendant sstables) we could lose data if we delete the ancestors themselves.\n",
            "\n",
            "One possible fix:\n",
            "\n",
            "# Add a flag to SSTM for \"this was the final sstable in the compaction\"\n",
            "# When we scan sstables we can delete ancestors if we find that marker in any of the descendants\n",
            "# Otherwise we should delete the *descendants* and leave the ancestors alone (so we don\\t doublecount data for counters) \"I think youre right.\n",
            "\n",
            "A fourth pseudo-solution could be to wait the end of the compaction to rename all the newly created writers (i.e remove the tmp markers). Its not bulletproof though as I dont think we can rename multiple files atomically but just wanted to mention it.\n",
            "\n",
            "e right you don\\t actually need a marker since if compaction completes the next step is to remove the ancestors.  I think \"if ancestors are still alive assume compaction didn\\t finish and delete the descendants\" is good enough. bq. I think \"if ancestors are still alive assume compaction didn\\t finish and delete the descendants\" is good enough.\n",
            "\n",
            "yeah agreed. \"Hum wait it only works if we have all ancestors though. What if just one ancestor dont get deleted for some reason (or only some of the SSTableDeletingTask have executed before a crash)? Its easy enough to check that we have *all* ancestors but what if we dont? Were back to square one :(\" \"Youre right.  Guess we need a compaction-finished flag after all.\n",
            "\n",
            "Instead of storing it in sstable metadata maybe we could store it in system.local the way we do with truncation information.  Unfortunately 1.1 doesnt support Maps so wed be doing two separate implementations for 1.1 and 1.2.\n",
            "\n",
            "Should we just say that for 1.1 well retain all sstables (counter users will get overcounts everyone else just gets extra compaction work) and fix it better in 1.2?\" Something like this...\n",
            "\n",
            "{code}\n",
            "create table compaction_log (\n",
            "  id uuid primary key\n",
            "  inputs set<int>\n",
            "  outputs set<int>\n",
            ");\n",
            "{code}\n",
            "\n",
            "When we start a compaction we add it to the log.  When we finish we remove it.  If we restart and compaction_log is not empty we remove any sstables from the outputs set. \"bq. Should we just say that for 1.1 well retain all sstables\n",
            "\n",
            "For 1.1 Id suggest doing my fourth pseudo-solution above i.e. moving the renaming of newly created writes at the end of the compaction (its trivial). Then at startup we could indeed retain all sstables for normal CF but for counter we would keep removing the predecessors as we do now. It wouldnt totally fix the risk of losing counters but it would make it very unlikely (youd need to fail exactly in the middle of the bulk renaming a newly create sstable writers) while just retaining all sstables would make it very easy to have overcounts.\n",
            "\n",
            "For 1.2 you compaction_log solution does seem reasonable.\" \"V2 implements Sylvains idea that renames written SSTables at the end of compaction.\n",
            "\n",
            "For 1.2 lets open different issue for Jonathans suggestion.\" \"The code of v2 looks alright but lets also disable the filtering in ColumnFamilyStore.ctor for non-counter CFs so we take zero chance of losing data (and since reusing an already compacted sstable is a bit inefficient but harmless).\n",
            "\n",
            "bq. For 1.2 lets open different issue for Jonathans suggestion\n",
            "\n",
            "Agreed.\" Attached v3 that also changes the filtering part only for counter CF. \"+1 (though do commit your v1 along the way no way in keeping sstable were not going to use even if its only for counters).\" Committed v1 + v3 and opened CASSANDRA-5151 for better solution.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5244\n",
            "issue_type: Bug\n",
            "summary: Compactions don't work while node is bootstrapping\n",
            "description: It seems that there is a race condition in StorageService that prevents compactions from completing while node is in a bootstrap state.\n",
            "\n",
            "I have been able to reproduce this multiple times by throttling streaming throughput to extend the bootstrap time while simultaneously inserting data to the cluster.\n",
            "\n",
            "The problems lies in the synchronization of initServer(int delay) and reportSeverity(double incr) methods as they both try to acquire the instance lock of StorageService through the use of synchronized keyword. As initServer does not return until the bootstrap has completed, all calls to reportSeverity will block until that. However, reportSeverity is called when starting compactions in CompactionInfo and thus all compactions block until bootstrap completes. \n",
            "\n",
            "This might severely degrade node's performance after bootstrap as it might have lots of compactions pending while simultaneously starting to serve reads.\n",
            "\n",
            "I have been able to solve the issue by adding a separate lock for reportSeverity and removing its class level synchronization. This of course is not a valid approach if we must assume that any of Gossiper's IEndpointStateChangeSubscribers could potentially end up calling back to StorageService's synchronized methods. However, at least at the moment, that does not seem to be the case.\n",
            "\n",
            "Maybe somebody with more experience about the codebase comes up with a better solution?\n",
            "\n",
            "(This might affect DynamicEndpointSnitch as well, as it also calls to reportSeverity in its setSeverity method)\n",
            "architectural impact: NO\n",
            "comments: [\"Thanks for the detective work, Jouni.  I'll let Brandon comment on solutions; in the meantime, marking Minor since while inconvenient this does not compromise correctness.\", 'This is more severe than we originally thought, and causes CASSANDRA-5129 when there is a secondary index:\\n\\n{noformat}\\n\"CompactionExecutor:1\" daemon prio=10 tid=0x00007effbc03c800 nid=0x7abf waiting for monitor entry [0x00007effc843a000]\\n   java.lang.Thread.State: BLOCKED (on object monitor)\\n    at org.apache.cassandra.service.StorageService.reportSeverity(StorageService.java:905)\\n    - waiting to lock <0x00000000ca576ac8> (a org.apache.cassandra.service.StorageService)\\n    at org.apache.cassandra.db.compaction.CompactionInfo$Holder.started(CompactionInfo.java:141)\\n    at org.apache.cassandra.metrics.CompactionMetrics.beginCompaction(CompactionMetrics.java:90)\\n    at org.apache.cassandra.db.compaction.CompactionManager$9.run(CompactionManager.java:813)\\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\\n    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\\n    at java.util.concurrent.FutureTask.run(FutureTask.java:138)\\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\\n    at java.lang.Thread.run(Thread.java:662)\\n{noformat}', \"It seems to me the only reason we're synchronizing here is for the increment, and we don't need to get our own severity out of gossip, so we can just track a local AtomicDouble instead.\", '+1', 'Committed.', 'this looks good.']\n",
            "my_comment: BLOCKED (on object monitor)\n",
            "    at org.apache.cassandra.service.StorageService.reportSeverity(StorageService.java:905)\n",
            "    - waiting to lock <0x00000000ca576ac8> (a org.apache.cassandra.service.StorageService)\n",
            "    at org.apache.cassandra.db.compaction.CompactionInfo$Holder.started(CompactionInfo.java:141)\n",
            "    at org.apache.cassandra.metrics.CompactionMetrics.beginCompaction(CompactionMetrics.java:90)\n",
            "    at org.apache.cassandra.db.compaction.CompactionManager$9.run(CompactionManager.java:813)\n",
            "    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n",
            "    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n",
            "    at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n",
            "    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n",
            "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n",
            "    at java.lang.Thread.run(Thread.java:662)\n",
            "{noformat} \"It seems to me the only reason were synchronizing here is for the increment and we dont need to get our own severity out of gossip so we can just track a local AtomicDouble instead.\" +1 Committed. this looks good.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5250\n",
            "issue_type: Improvement\n",
            "summary: Improve LeveledScanner work estimation\n",
            "description: See https://issues.apache.org/jira/browse/CASSANDRA-5222?focusedCommentId=13577420&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13577420\n",
            "architectural impact: NO\n",
            "comments: ['filters out sstables not intersecting the range given\\n\\nI might have totally misunderstood the comment in CASSANDRA-5222 though. \\n\\nIt also \"solves\" CASSANDRA-5249 in another way, i left that code in there though since the same amount of work needs to be done.', \"I would say, go ahead and rip out EmptyCompactionScanner.\\n\\nI was also hoping that we could get a better estimate for sstables where the range we're scanning is only part of the sstable.  One way would be to do lookups against our in-memory IndexSummary for the start and end of the range, and assume that 1/10 of the rows in the summary correspond to 1/10 of the size on disk.  Obviously not 100% accurate but a lot better than throwing the entire sstable size in each time.\", 'does a better job estimating work\\n\\nalso removes EmptyCompactionScanner and changes SSTableBoundedScanner constructor back to taking a Range (felt a bit cleaner)', 'My OCD wanted to get rid of the == null test. :)', 'looks like you missed a negation:\\nif (intersecting.isEmpty())\\n\\nv4 attached with the fix', 'Thanks, committed.']\n",
            "my_comment: filters out sstables not intersecting the range given\n",
            "\n",
            "I might have totally misunderstood the comment in CASSANDRA-5222 though. \n",
            "\n",
            "It also \"solves\" CASSANDRA-5249 in another way i left that code in there though since the same amount of work needs to be done. \"I would say go ahead and rip out EmptyCompactionScanner.\n",
            "\n",
            "I was also hoping that we could get a better estimate for sstables where the range were scanning is only part of the sstable.  One way would be to do lookups against our in-memory IndexSummary for the start and end of the range and assume that 1/10 of the rows in the summary correspond to 1/10 of the size on disk.  Obviously not 100% accurate but a lot better than throwing the entire sstable size in each time.\" does a better job estimating work\n",
            "\n",
            "also removes EmptyCompactionScanner and changes SSTableBoundedScanner constructor back to taking a Range (felt a bit cleaner) My OCD wanted to get rid of the == null test. :) looks like you missed a negation:\n",
            "if (intersecting.isEmpty())\n",
            "\n",
            "v4 attached with the fix Thanks committed.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-527\n",
            "issue_type: Improvement\n",
            "summary: clean up gossip notifications to the rest of the system\n",
            "description: None\n",
            "architectural impact: NO\n",
            "comments: [\"    add onAlive, onDead methods to gossiper listener interface so listeners don't have to try to infer that by less accurate circumstantial evidence\\n\", 'update convict as well as suspect', 'Looks OK to me. +1', 'It looks like Gossiper.instance().register(_) is only called from a single thread, but since isAlive is synchronized, register/unregister should probably be as well.', 'I will commit that change but I think it could use a more intensive thread safety analysis.  (For another ticket.)', \"Integrated in Cassandra #249 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/249/])\\n    add synchronized to register/unregister methods and remove unused code.  patch by Stu Hood and jbellis for \\nadd onAlive, onDead methods to gossiper listener interface so listeners don't have to try to infer that by less accurate circumstantial evidence\\npatch by jbellis; reviewed by Jaakko Laine for \\n\", 'committed']\n",
            "my_comment: [\"    add onAlive onDead methods to gossiper listener interface so listeners dont have to try to infer that by less accurate circumstantial evidence\n",
            "\" update convict as well as suspect Looks OK to me. +1 It looks like Gossiper.instance().register(_) is only called from a single thread but since isAlive is synchronized register/unregister should probably be as well. I will commit that change but I think it could use a more intensive thread safety analysis.  (For another ticket.) \"Integrated in Cassandra #249 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/249/])\n",
            "    add synchronized to register/unregister methods and remove unused code.  patch by Stu Hood and jbellis for \n",
            "add onAlive onDead methods to gossiper listener interface so listeners dont have to try to infer that by less accurate circumstantial evidence\n",
            "patch by jbellis; reviewed by Jaakko Laine for \n",
            "\" committed\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5407\n",
            "issue_type: Bug\n",
            "summary: Repair exception when getPositionsForRanges returns empty iterator\n",
            "description: CASSANDRA-5250 broke repair, this re-adds the code from CASSANDRA-5249\n",
            "architectural impact: NO\n",
            "comments: ['What is causing the breakage?  Is it possible to add a test that exposes the problem?', \"my fix in CASSANDRA-5250 just fixed it for LCS (tests if intersecting sstables is empty in LeveledScanner), and this re-broke it for STCS\\n\\ni'll try to write a unit test for this\", 'adds a unit test that would have found the bug', 'LGTM, committed']\n",
            "my_comment: What is causing the breakage?  Is it possible to add a test that exposes the problem? \"my fix in CASSANDRA-5250 just fixed it for LCS (tests if intersecting sstables is empty in LeveledScanner) and this re-broke it for STCS\n",
            "\n",
            "ill try to write a unit test for this\" adds a unit test that would have found the bug LGTM committed\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5605\n",
            "issue_type: Bug\n",
            "summary: Crash caused by insufficient disk space to flush\n",
            "description: A few times now I have seen our Cassandra nodes crash by running themselves out of memory. It starts with the following exception:\n",
            "\n",
            "{noformat}\n",
            "ERROR [FlushWriter:13000] 2013-05-31 11:32:02,350 CassandraDaemon.java (line 164) Exception in thread Thread[FlushWriter:13000,5,main]\n",
            "java.lang.RuntimeException: Insufficient disk space to write 8042730 bytes\n",
            "        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:42)\n",
            "        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)\n",
            "        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
            "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
            "        at java.lang.Thread.run(Thread.java:722)\n",
            "{noformat} \n",
            "\n",
            "After which, it seems the MemtablePostFlusher stage gets stuck and no further memtables get flushed: \n",
            "\n",
            "{noformat} \n",
            "INFO [ScheduledTasks:1] 2013-05-31 11:59:12,467 StatusLogger.java (line 68) MemtablePostFlusher               1        32         0\n",
            "INFO [ScheduledTasks:1] 2013-05-31 11:59:12,469 StatusLogger.java (line 73) CompactionManager                 1         2\n",
            "{noformat} \n",
            "\n",
            "What makes this ridiculous is that, at the time, the data directory on this node had 981GB free disk space (as reported by du). We primarily use STCS and at the time the aforementioned exception occurred, at least one compaction task was executing which could have easily involved 981GB (or more) worth of input SSTables. Correct me if I am wrong but but Cassandra counts data currently being compacted against available disk space. In our case, this is a significant overestimation of the space required by compaction since a large portion of the data being compacted has expired or is an overwrite.\n",
            "\n",
            "More to the point though, Cassandra should not crash because its out of disk space unless its really actually out of disk space (ie, dont consider 'phantom' compaction disk usage when flushing). I have seen one of our nodes die in this way before our alerts for disk space even went off.\n",
            "architectural impact: NO\n",
            "comments: ['Do you have multiple data directories?  If you only have one, it may have been blacklisted and marked read-only by a previous issue, can you check the logs for anything like that?', 'We have only one data directory. There is nothing in the log about it being blacklisted.', 'Am not sure if the following information helps but we too hit this issue in production today. We were running with cassandra 1.2.4 and two patches CASSANDRA-5554 & CASSANDRA-5418. \\n\\nWe were running with RF=3 and LCS. \\n\\nWe ran into this issue while using sstablelaoder to push data from  remote 1.2.4 cluster nodes to another cluster\\n\\nWe cross checked using JMX if blacklisting is the cause of this bug and it looks like it is definitely not the case. \\n\\nWe however saw a pile up of pending compactions ~ 1800 pending compactions per node when node crashed. Surprising thing is that the \"Insufficient disk space to write xxxx bytes\" appears much before the node crashes. For us it started appearing aprrox 3 hours before the node crashed. \\n\\nThe cluster which showed this behavior was having loads of writes occurring ( We were using multiple SSTableLoaders to stream data into this cluster. ). We pushed in almost 15 TB worth data ( including the RF =3 ) in a matter of 16 hours. We were not serving any reads from this cluster as we were still migrating data to it. \\n\\nAnother interesting behavior observed that nodes were neighbors in most of the time. \\n\\nAm not sure if the above information helps but wanted to add it to the context of the ticket.  ', \"Apologies if this isn't directly relevant, but I seem to be experiencing the same issue using 1.2.8 launched via ccm for integration testing. One differentiating feature here is that this happened the first time the node was ever brought up (0 data). The integration tests attempting to use the ccm cluster never completed due to this hang, but all they do is test creation of a fairly simple schema and then attempt to write and then read back a single row. There is plenty of free disk space available...\\n\\nHere's what was in the log:\\n INFO [main] 2013-08-07 14:56:46,763 CassandraDaemon.java (line 118) Logging initialized\\n INFO [main] 2013-08-07 14:56:46,807 CassandraDaemon.java (line 145) JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.7.0_25\\n INFO [main] 2013-08-07 14:56:46,808 CassandraDaemon.java (line 183) Heap size: 8248098816/8248098816\\n INFO [main] 2013-08-07 14:56:46,808 CassandraDaemon.java (line 184) Classpath: /opt/ccmlib_cassandra/ccm/lwcdbng_test_cluster/node1/conf:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/build/classes/main:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/build/classes/thrift:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/antlr-3.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/avro-1.4.0-fixes.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/avro-1.4.0-sources-fixes.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/commons-cli-1.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/commons-codec-1.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/commons-lang-2.6.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/compress-lzf-0.8.4.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/concurrentlinkedhashmap-lru-1.3.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/guava-13.0.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/high-scale-lib-1.1.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jackson-core-asl-1.9.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jackson-mapper-asl-1.9.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jamm-0.2.5.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jbcrypt-0.3m.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jline-1.0.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/json-simple-1.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/libthrift-0.7.0.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/log4j-1.2.16.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/lz4-1.1.0.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/metrics-core-2.0.3.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/netty-3.5.9.Final.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/servlet-api-2.5-20081211.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/slf4j-api-1.7.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/slf4j-log4j12-1.7.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/snakeyaml-1.6.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/snappy-java-1.0.5.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/snaptree-0.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jamm-0.2.5.jar\\n INFO [main] 2013-08-07 14:56:46,822 CLibrary.java (line 65) JNA not found. Native methods will be disabled.\\n INFO [main] 2013-08-07 14:56:46,891 DatabaseDescriptor.java (line 132) Loading settings from file:/opt/ccmlib_cassandra/ccm/lwcdbng_test_cluster/node1/conf/cassandra.yaml\\n INFO [main] 2013-08-07 14:56:47,821 DatabaseDescriptor.java (line 150) Data files directories: [/opt/ccmlib_cassandra/ccm/lwcdbng_test_cluster/node1/data]\\n INFO [main] 2013-08-07 14:56:47,822 DatabaseDescriptor.java (line 151) Commit log directory: /opt/ccmlib_cassandra/ccm/lwcdbng_test_cluster/node1/commitlogs\\n INFO [main] 2013-08-07 14:56:47,822 DatabaseDescriptor.java (line 191) DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap\\n INFO [main] 2013-08-07 14:56:47,822 DatabaseDescriptor.java (line 205) disk_failure_policy is stop\\n INFO [main] 2013-08-07 14:56:48,000 DatabaseDescriptor.java (line 273) Global memtable threshold is enabled at 2622MB\\n INFO [main] 2013-08-07 14:56:49,142 DatabaseDescriptor.java (line 401) Not using multi-threaded compaction\\n INFO [main] 2013-08-07 14:56:52,610 CacheService.java (line 111) Initializing key cache with capacity of 100 MBs.\\n INFO [main] 2013-08-07 14:56:52,675 CacheService.java (line 140) Scheduling key cache save to each 14400 seconds (going to save all keys).\\n INFO [main] 2013-08-07 14:56:52,678 CacheService.java (line 154) Initializing row cache with capacity of 0 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider\\n INFO [main] 2013-08-07 14:56:52,698 CacheService.java (line 166) Scheduling row cache save to each 0 seconds (going to save all keys).\\n INFO [main] 2013-08-07 14:56:57,163 DatabaseDescriptor.java (line 535) Couldn't detect any schema definitions in local storage.\\n INFO [main] 2013-08-07 14:56:57,164 DatabaseDescriptor.java (line 540) To create keyspaces and column families, see 'help create' in cqlsh.\\n INFO [main] 2013-08-07 14:56:57,258 CommitLog.java (line 120) No commitlog files found; skipping replay\\n INFO [main] 2013-08-07 14:56:57,715 StorageService.java (line 456) Cassandra version: 1.2.8-SNAPSHOT\\n INFO [main] 2013-08-07 14:56:57,715 StorageService.java (line 457) Thrift API version: 19.36.0\\n INFO [main] 2013-08-07 14:56:57,716 StorageService.java (line 458) CQL supported versions: 2.0.0,3.0.5 (default: 3.0.5)\\n INFO [main] 2013-08-07 14:56:57,795 StorageService.java (line 483) Loading persisted ring state\\n INFO [main] 2013-08-07 14:56:57,805 StorageService.java (line 564) Starting up server gossip\\n WARN [main] 2013-08-07 14:56:57,823 SystemTable.java (line 573) No host ID found, created 6abf69c6-8472-4607-96b7-4a532ed8e2a8 (Note: This should happen exactly once per node).\\n INFO [main] 2013-08-07 14:56:57,852 ColumnFamilyStore.java (line 630) Enqueuing flush of Memtable-local@2038679449(367/367 serialized/live bytes, 15 ops)\\nERROR [FlushWriter:1] 2013-08-07 14:56:57,860 CassandraDaemon.java (line 192) Exception in thread Thread[FlushWriter:1,5,main]\\njava.lang.RuntimeException: Insufficient disk space to write 452 bytes\\n        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:42)\\n        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)\\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\\n        at java.lang.Thread.run(Thread.java:724)\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,243 GCInspector.java (line 119) GC for ParNew: 2786 ms for 1 collections, 230275096 used; max is 8248098816\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,250 StatusLogger.java (line 53) Pool Name                    Active   Pending   Blocked\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,291 StatusLogger.java (line 68) MemtablePostFlusher               1         1         0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,292 StatusLogger.java (line 68) FlushWriter                       0         0         0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,293 StatusLogger.java (line 68) commitlog_archiver                0         0         0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,294 StatusLogger.java (line 73) CompactionManager                 0         0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,522 StatusLogger.java (line 85) MessagingService                n/a       0,0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,523 StatusLogger.java (line 95) Cache Type                     Size                 Capacity               KeysToSave                                                         Provider\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,524 StatusLogger.java (line 96) KeyCache                          0                104857600                      all                                                                 \\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,525 StatusLogger.java (line 102) RowCache                          0                        0                      all              org.apache.cassandra.cache.SerializingCacheProvider\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,525 StatusLogger.java (line 109) ColumnFamily                Memtable ops,data\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,526 StatusLogger.java (line 112) system.local                              0,0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,526 StatusLogger.java (line 112) system.peers                              0,0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,527 StatusLogger.java (line 112) system.batchlog                           0,0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,527 StatusLogger.java (line 112) system.NodeIdInfo                         0,0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,528 StatusLogger.java (line 112) system.LocationInfo                       0,0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,528 StatusLogger.java (line 112) system.Schema                             0,0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,529 StatusLogger.java (line 112) system.Migrations                         0,0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,529 StatusLogger.java (line 112) system.schema_keyspaces                 8,251\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,530 StatusLogger.java (line 112) system.schema_columns               398,24717\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,530 StatusLogger.java (line 112) system.schema_columnfamilies           369,22187\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,530 StatusLogger.java (line 112) system.IndexInfo                          0,0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,531 StatusLogger.java (line 112) system.range_xfers                        0,0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,531 StatusLogger.java (line 112) system.peer_events                        0,0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,532 StatusLogger.java (line 112) system.hints                              0,0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,532 StatusLogger.java (line 112) system.HintsColumnFamily                  0,0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,532 StatusLogger.java (line 112) system_traces.sessions                    0,0\\n INFO [ScheduledTasks:1] 2013-08-07 15:14:08,533 StatusLogger.java (line 112) system_traces.events                      0,0\\n\", 'After CASSANDRA-4292 flushing checks the space reserved by compactions.  So check if you have a bunch of pending compactions with a huge size.', 'Have seen a lot of instances of people hitting this reported recently.  I think we probably shouldn\\'t fail stuff if C* \"thinks\" it might not have enough free space, because stuff is reserved.  Probably a good idea to use that reserving information as a hint to which drive to pick for the JBOD case, but if no drive will fit the data when looking at reserved, pick the one with the most free space (like we used to).', 'Patchset to avoid prematurely declaring ourselves out of space at https://github.com/jbellis/cassandra/commits/5605', '+1 to the patch.\\n\\nThough (maybe in separate ticket?) we still need to add error handler to FlushRunnable so postExecutor does not get blocked.', \"bq. we still need to add error handler to FlushRunnable so postExecutor does not get blocked\\n\\nI'm not sure we want to unblock it -- if the flush errors out, then we definitely don't want commitlog segments getting cleaned up.  What did you have in mind?\", 'Ah, right.\\nJust wondered if we can do something about preventing filling up postExecutor queue.', \"Committed.  If you come up with a good idea there, let's open a new ticket.\"]\n",
            "my_comment: 2786 ms for 1 collections 230275096 used; max is 8248098816\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08250 StatusLogger.java (line 53) Pool Name                    Active   Pending   Blocked\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08291 StatusLogger.java (line 68) MemtablePostFlusher               1         1         0\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08292 StatusLogger.java (line 68) FlushWriter                       0         0         0\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08293 StatusLogger.java (line 68) commitlog_archiver                0         0         0\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08294 StatusLogger.java (line 73) CompactionManager                 0         0\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08522 StatusLogger.java (line 85) MessagingService                n/a       00\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08523 StatusLogger.java (line 95) Cache Type                     Size                 Capacity               KeysToSave                                                         Provider\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08524 StatusLogger.java (line 96) KeyCache                          0                104857600                      all                                                                 \n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08525 StatusLogger.java (line 102) RowCache                          0                        0                      all              org.apache.cassandra.cache.SerializingCacheProvider\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08525 StatusLogger.java (line 109) ColumnFamily                Memtable opsdata\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08526 StatusLogger.java (line 112) system.local                              00\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08526 StatusLogger.java (line 112) system.peers                              00\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08527 StatusLogger.java (line 112) system.batchlog                           00\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08527 StatusLogger.java (line 112) system.NodeIdInfo                         00\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08528 StatusLogger.java (line 112) system.LocationInfo                       00\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08528 StatusLogger.java (line 112) system.Schema                             00\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08529 StatusLogger.java (line 112) system.Migrations                         00\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08529 StatusLogger.java (line 112) system.schema_keyspaces                 8251\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08530 StatusLogger.java (line 112) system.schema_columns               39824717\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08530 StatusLogger.java (line 112) system.schema_columnfamilies           36922187\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08530 StatusLogger.java (line 112) system.IndexInfo                          00\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08531 StatusLogger.java (line 112) system.range_xfers                        00\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08531 StatusLogger.java (line 112) system.peer_events                        00\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08532 StatusLogger.java (line 112) system.hints                              00\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08532 StatusLogger.java (line 112) system.HintsColumnFamily                  00\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08532 StatusLogger.java (line 112) system_traces.sessions                    00\n",
            " INFO [ScheduledTasks:1] 2013-08-07 15:14:08533 StatusLogger.java (line 112) system_traces.events                      00\n",
            "\" After CASSANDRA-4292 flushing checks the space reserved by compactions.  So check if you have a bunch of pending compactions with a huge size. Have seen a lot of instances of people hitting this reported recently.  I think we probably shouldn\\t fail stuff if C* \"thinks\" it might not have enough free space because stuff is reserved.  Probably a good idea to use that reserving information as a hint to which drive to pick for the JBOD case but if no drive will fit the data when looking at reserved pick the one with the most free space (like we used to). Patchset to avoid prematurely declaring ourselves out of space at https://github.com/jbellis/cassandra/commits/5605 +1 to the patch.\n",
            "\n",
            "Though (maybe in separate ticket?) we still need to add error handler to FlushRunnable so postExecutor does not get blocked. \"bq. we still need to add error handler to FlushRunnable so postExecutor does not get blocked\n",
            "\n",
            "Im not sure we want to unblock it -- if the flush errors out then we definitely dont want commitlog segments getting cleaned up.  What did you have in mind?\" Ah right.\n",
            "Just wondered if we can do something about preventing filling up postExecutor queue. \"Committed.  If you come up with a good idea there lets open a new ticket.\"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-578\n",
            "issue_type: Bug\n",
            "summary: get_range_slice NPE\n",
            "description: If I call get_range_slice with arguments in the SliceRange structure, then it seems to NPE.  I think it only does it when there is nothing in the range specified in the column slice start and end.\n",
            "\n",
            "\n",
            "ERROR - Error in ThreadPoolExecutor\n",
            "java.lang.RuntimeException: java.lang.NullPointerException\n",
            "        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:55)\n",
            "        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)\n",
            "        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n",
            "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n",
            "        at java.lang.Thread.run(Thread.java:619)\n",
            "Caused by: java.lang.NullPointerException\n",
            "        at org.apache.cassandra.db.Row.addColumnFamily(Row.java:96)\n",
            "        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1469)\n",
            "        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:41)\n",
            "        ... 4 more\n",
            "ERROR - Fatal exception in thread Thread[ROW-READ-STAGE:8,5,main]\n",
            "java.lang.RuntimeException: java.lang.NullPointerException\n",
            "        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:55)\n",
            "        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)\n",
            "        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n",
            "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n",
            "        at java.lang.Thread.run(Thread.java:619)\n",
            "Caused by: java.lang.NullPointerException\n",
            "        at org.apache.cassandra.db.Row.addColumnFamily(Row.java:96)\n",
            "        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1469)\n",
            "        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:41)\n",
            "        ... 4 more\n",
            "\n",
            "architectural impact: NO\n",
            "comments: ['02\\n    make Row contain a single, final CF reference\\n\\n01\\n    r/m unused Row code, and move table variable into callers rather than serializing it redundantly\\n', 'Still get this error with the patches applied.\\n\\nDEBUG - range_slice\\nDEBUG - reading org.apache.cassandra.db.RangeSliceCommand@4ef18d37 from 31@/127.0.0.1\\nERROR - Error in ThreadPoolExecutor\\njava.lang.RuntimeException: java.lang.NullPointerException\\n        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:53)\\n        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\\n        at java.lang.Thread.run(Thread.java:619)\\nCaused by: java.lang.NullPointerException\\n        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:58)\\n        at org.apache.cassandra.db.RowSerializer.serialize(Row.java:92)\\n        at org.apache.cassandra.db.RangeSliceReply.getReply(RangeSliceReply.java:53)\\n        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:46)\\n        ... 4 more\\nERROR - Fatal exception in thread Thread[ROW-READ-STAGE:11,5,main]\\njava.lang.RuntimeException: java.lang.NullPointerException\\n        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:53)\\n        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\\n        at java.lang.Thread.run(Thread.java:619)\\nCaused by: java.lang.NullPointerException\\n        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:58)\\n        at org.apache.cassandra.db.RowSerializer.serialize(Row.java:92)\\n        at org.apache.cassandra.db.RangeSliceReply.getReply(RangeSliceReply.java:53)\\n        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:46)\\n        ... 4 more\\n', 'can you reproduce against the default set of columnfamilies?', 'Let me try to do that.\\n\\njust fyi in org.apache.cassandra.db.row.RowSerializer row.cf is null which is causing the NPE.', \"I can reproduce it here, I'm good.\", '03\\n    allow serializing null CF; add get_range_slice test exercising this', \"I am getting a couple errors that I wasn't getting before and I think its related.\\n\\n\\n2009-11-24_05:06:54.65928 java.lang.NullPointerException\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.db.Row.digest(Row.java:75)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:103)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:46)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:88)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:453)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:381)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:170)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:245)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:208)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:758)\\n2009-11-24_05:06:54.65928       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:712)\\n2009-11-24_05:06:54.65928       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)\\n2009-11-24_05:06:54.65928       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\\n2009-11-24_05:06:54.65928       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\\n2009-11-24_05:06:54.65928       at java.lang.Thread.run(Thread.java:636)\\n2009-11-24_05:06:56.82921 ERROR - Internal error processing get_slice\\n2009-11-24_05:06:56.82921 java.lang.NullPointerException\\n2009-11-24_05:06:56.82921       at org.apache.cassandra.db.Row.digest(Row.java:75)\\n2009-11-24_05:06:56.82921       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:103)\\n2009-11-24_05:06:56.82921       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:46)\\n2009-11-24_05:06:56.82921       at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:88)\\n2009-11-24_05:06:56.82921       at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:453)\\n2009-11-24_05:06:56.82921       at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:381)\\n2009-11-24_05:06:56.82921       at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)\\n2009-11-24_05:06:56.82921       at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:170)\\n2009-11-24_05:06:56.82921       at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:245)\\n2009-11-24_05:06:56.82921       at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:208)\\n2009-11-24_05:06:56.82921       at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:758)\\n2009-11-24_05:06:56.82921       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:712)\\n2009-11-24_05:06:56.82921       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)\\n2009-11-24_05:06:56.82921       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\\n2009-11-24_05:06:56.82921       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\\n2009-11-24_05:06:56.82921       at java.lang.Thread.run(Thread.java:636)\\n\\n\\n\\n2009-11-24_05:06:57.54647 java.lang.NullPointerException\\n2009-11-24_05:06:57.54647       at org.apache.cassandra.db.Row.digest(Row.java:75)\\n2009-11-24_05:06:57.54647       at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:85)\\n2009-11-24_05:06:57.54647       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)\\n2009-11-24_05:06:57.54647       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\\n2009-11-24_05:06:57.54647       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\\n2009-11-24_05:06:57.54647       at java.lang.Thread.run(Thread.java:636)\\n2009-11-24_05:06:57.62647 ERROR - Error in ThreadPoolExecutor\\n2009-11-24_05:06:57.62647 java.lang.NullPointerException\\n2009-11-24_05:06:57.62647       at org.apache.cassandra.db.Row.digest(Row.java:75)\\n2009-11-24_05:06:57.62647       at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:85)\\n2009-11-24_05:06:57.62647       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)\\n2009-11-24_05:06:57.62647       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\\n2009-11-24_05:06:57.62647       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\\n2009-11-24_05:06:57.62647       at java.lang.Thread.run(Thread.java:636)\\n2009-11-24_05:06:57.62647 ERROR - Fatal exception in thread Thread[ROW-READ-STAGE:3,5,main]\\n2009-11-24_05:06:57.62647 java.lang.NullPointerException\\n2009-11-24_05:06:57.62647       at org.apache.cassandra.db.Row.digest(Row.java:75)\\n2009-11-24_05:06:57.62647       at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:85)\\n2009-11-24_05:06:57.62647       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)\\n2009-11-24_05:06:57.62647       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\\n2009-11-24_05:06:57.62647       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\\n2009-11-24_05:06:57.62647       at java.lang.Thread.run(Thread.java:636)\\n\", 'updated patch 03', '\\n\\n2009-11-24_06:00:42.63108 ERROR - Internal error processing get_slice\\n2009-11-24_06:00:42.63108 java.lang.NullPointerException\\n2009-11-24_06:00:42.63108       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:130)\\n2009-11-24_06:00:42.63108       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:46)\\n2009-11-24_06:00:42.63108       at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:88)\\n2009-11-24_06:00:42.63108       at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:453)\\n2009-11-24_06:00:42.63108       at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:381)\\n2009-11-24_06:00:42.63108       at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)\\n2009-11-24_06:00:42.63108       at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:170)\\n2009-11-24_06:00:42.63108       at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:245)\\n2009-11-24_06:00:42.63108       at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:208)\\n2009-11-24_06:00:42.63108       at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:758)\\n2009-11-24_06:00:42.63108       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:712)\\n2009-11-24_06:00:42.63108       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)\\n2009-11-24_06:00:42.63108       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\\n2009-11-24_06:00:42.63108       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\\n2009-11-24_06:00:42.63108       at java.lang.Thread.run(Thread.java:636)\\n', 'updated 03 again', \"I now seem to be getting tihs.\\n\\n\\n2009-11-25_06:42:26.20349 java.lang.NullPointerException                                                                                                                                \\n2009-11-25_06:42:26.20349       at org.apache.cassandra.db.ColumnFamily.delete(ColumnFamily.java:252)                                                                                   \\n2009-11-25_06:42:26.20349       at org.apache.cassandra.db.ColumnFamily.resolve(ColumnFamily.java:402)                                                                                  \\n2009-11-25_06:42:26.20349       at org.apache.cassandra.db.Row.resolve(Row.java:62)                                                                                                     \\n2009-11-25_06:42:26.20349       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:122)                                                             \\n2009-11-25_06:42:26.20349       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:46)                                                              \\n2009-11-25_06:42:26.20349       at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:88)                                                                \\n2009-11-25_06:42:26.20349       at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:477)                                                                          \\n2009-11-25_06:42:26.20349       at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:381)                                                                        \\n2009-11-25_06:42:26.20349       at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)                                                               \\n2009-11-25_06:42:26.20349       at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:170)                                                                      \\n2009-11-25_06:42:26.20349       at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:245)                                                         \\n2009-11-25_06:42:26.20349       at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:208)                                                                     \\n2009-11-25_06:42:26.20349       at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:758)                                                               \\n2009-11-25_06:42:26.20349       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:712)                                                                         \\n2009-11-25_06:42:26.20349       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)                                                             \\n2009-11-25_06:42:26.20349       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)                                                                      \\n2009-11-25_06:42:26.20349       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)                                                                      \\n2009-11-25_06:42:26.20349       at java.lang.Thread.run(Thread.java:636)\\n\\n\\nAnd I am not sure if this is related but it seems new too:\\n\\njava.lang.RuntimeException: Unable to load comparator class ''.  probably this means you have obsolete sstables lying around                                  \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:136)                                                       \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:107)                                                          \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.db.RowSerializer.deserialize(Row.java:110)                                                                                      \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:122)                                                                    \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:98)                                                                     \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.service.ReadResponseResolver.isDataPresent(ReadResponseResolver.java:154)                                                       \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.service.QuorumResponseHandler.response(QuorumResponseHandler.java:97)                                                           \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:37)                                                                     \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)                                                                        \\n2009-11-25_06:55:56.50945       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)                                                                      \\n2009-11-25_06:55:56.50945       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)                                                                      \\n2009-11-25_06:55:56.50945       at java.lang.Thread.run(Thread.java:636)                                                                                                                \\n2009-11-25_06:55:56.50945 Caused by: java.lang.ClassNotFoundException:                                                                                                                  \\n2009-11-25_06:55:56.50945       at java.lang.Class.forName0(Native Method)                                                                                                              \\n2009-11-25_06:55:56.50945       at java.lang.Class.forName(Class.java:186)                                                                                                              \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:132)                                                       \\n2009-11-25_06:55:56.50945       ... 11 more                                                                                                                                             \\n2009-11-25_06:55:56.50945 ERROR - Fatal exception in thread Thread[RESPONSE-STAGE:2,5,main]                                                                                             \\n2009-11-25_06:55:56.50945 java.lang.RuntimeException: Unable to load comparator class ''.  probably this means you have obsolete sstables lying around                                  \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:136)                                                       \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:107)                                                          \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.db.RowSerializer.deserialize(Row.java:110)                                                                                      \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:122)                                                                    \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:98)                                                                     \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.service.ReadResponseResolver.isDataPresent(ReadResponseResolver.java:154)                                                       \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.service.QuorumResponseHandler.response(QuorumResponseHandler.java:97)                                                           \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:37)                                                                     \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)                                                                        \\n2009-11-25_06:55:56.50945       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)                                                                      \\n2009-11-25_06:55:56.50945       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)                                                                      \\n2009-11-25_06:55:56.50945       at java.lang.Thread.run(Thread.java:636)                                                                                                                \\n2009-11-25_06:55:56.50945 Caused by: java.lang.ClassNotFoundException:                                                                                                                  \\n2009-11-25_06:55:56.50945       at java.lang.Class.forName0(Native Method)                                                                                                              \\n2009-11-25_06:55:56.50945       at java.lang.Class.forName(Class.java:186)                                                                                                              \\n2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:132)                                                       \\n2009-11-25_06:55:56.50945       ... 11 more                \", 'It seems for the comparator problem the \"\" is getting read as \\\\u0000.    But then if I make that change I start getting:\\n\\n2009-11-25_17:00:45.31534 INFO - Exception was generated at : 11/25/2009 17:00:45 on thread RESPONSE-STAGE:2                     \\n2009-11-25_17:00:45.31534                                                                                                        \\n2009-11-25_17:00:45.31534 java.io.EOFException                                                                                   \\n2009-11-25_17:00:45.31534       at java.io.DataInputStream.readFully(DataInputStream.java:197)                                   \\n2009-11-25_17:00:45.31534       at java.io.DataInputStream.readUTF(DataInputStream.java:609)                                     \\n2009-11-25_17:00:45.31534       at java.io.DataInputStream.readUTF(DataInputStream.java:564)                                     \\n2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:124)\\n2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:107)   \\n2009-11-25_17:00:45.31534       at org.apache.cassandra.db.RowSerializer.deserialize(Row.java:110)                               \\n2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:122)             \\n2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:98)              \\n2009-11-25_17:00:45.31534       at org.apache.cassandra.service.ReadResponseResolver.isDataPresent(ReadResponseResolver.java:154)\\n2009-11-25_17:00:45.31534       at org.apache.cassandra.service.QuorumResponseHandler.response(QuorumResponseHandler.java:97)    \\n2009-11-25_17:00:45.31534       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:37)              \\n2009-11-25_17:00:45.31534       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)                 \\n2009-11-25_17:00:45.31534       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)               \\n2009-11-25_17:00:45.31534       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)               \\n2009-11-25_17:00:45.31534       at java.lang.Thread.run(Thread.java:636)', 'the comparator problem is because we are changing Row internals which affects the commit log.  the simplest is to just wipe out the commitlogs, otherwise, go back to the old version and do a nodeprobe flush to clear them out before patching\\n\\nworking on a fix for the other NPE now', 'updated patch 03 with fix for latest NPE', 'I think that did it!  Thank you so much!', 'committed', 'Integrated in Cassandra #269 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/269/])\\n    allow serializing null CF; add get_range_slice test exercising this\\npatch by jbellis; tested by Dan Di Spaltro for \\nmake Row contain a single, final CF reference\\npatch by jbellis; tested by Dan Di Spaltro for \\nr/m unused Row code, and move table variable into callers rather than serializing it redundantly\\npatch by jbellis; tested by Dan Di Spaltro for \\n']\n",
            "my_comment: 11/25/2009 17:00:45 on thread RESPONSE-STAGE:2                     \n",
            "2009-11-25_17:00:45.31534                                                                                                        \n",
            "2009-11-25_17:00:45.31534 java.io.EOFException                                                                                   \n",
            "2009-11-25_17:00:45.31534       at java.io.DataInputStream.readFully(DataInputStream.java:197)                                   \n",
            "2009-11-25_17:00:45.31534       at java.io.DataInputStream.readUTF(DataInputStream.java:609)                                     \n",
            "2009-11-25_17:00:45.31534       at java.io.DataInputStream.readUTF(DataInputStream.java:564)                                     \n",
            "2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:124)\n",
            "2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:107)   \n",
            "2009-11-25_17:00:45.31534       at org.apache.cassandra.db.RowSerializer.deserialize(Row.java:110)                               \n",
            "2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:122)             \n",
            "2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:98)              \n",
            "2009-11-25_17:00:45.31534       at org.apache.cassandra.service.ReadResponseResolver.isDataPresent(ReadResponseResolver.java:154)\n",
            "2009-11-25_17:00:45.31534       at org.apache.cassandra.service.QuorumResponseHandler.response(QuorumResponseHandler.java:97)    \n",
            "2009-11-25_17:00:45.31534       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:37)              \n",
            "2009-11-25_17:00:45.31534       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)                 \n",
            "2009-11-25_17:00:45.31534       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)               \n",
            "2009-11-25_17:00:45.31534       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)               \n",
            "2009-11-25_17:00:45.31534       at java.lang.Thread.run(Thread.java:636) the comparator problem is because we are changing Row internals which affects the commit log.  the simplest is to just wipe out the commitlogs otherwise go back to the old version and do a nodeprobe flush to clear them out before patching\n",
            "\n",
            "working on a fix for the other NPE now updated patch 03 with fix for latest NPE I think that did it!  Thank you so much! committed Integrated in Cassandra #269 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/269/])\n",
            "    allow serializing null CF; add get_range_slice test exercising this\n",
            "patch by jbellis; tested by Dan Di Spaltro for \n",
            "make Row contain a single final CF reference\n",
            "patch by jbellis; tested by Dan Di Spaltro for \n",
            "r/m unused Row code and move table variable into callers rather than serializing it redundantly\n",
            "patch by jbellis; tested by Dan Di Spaltro for \n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5856\n",
            "issue_type: Bug\n",
            "summary: AE in ArrayBackedSortedColumns\n",
            "description: {noformat}\n",
            "ERROR [ReadStage:3] 2013-08-07 06:58:21,485 CassandraDaemon.java (line 192) Exception in thread Thread[ReadStage:3,5,main]\n",
            "java.lang.AssertionError: Added column does not sort as the last column\n",
            "    at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:131)\n",
            "    at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:119)\n",
            "    at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:114)\n",
            "    at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:171)\n",
            "    at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:136)\n",
            "    at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:84)\n",
            "    at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:291)\n",
            "    at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)\n",
            "    at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1390)\n",
            "    at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1213)\n",
            "    at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1125)\n",
            "    at org.apache.cassandra.db.Table.getRow(Table.java:347)\n",
            "    at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:70)\n",
            "    at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1047)\n",
            "    at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1593)\n",
            "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
            "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
            "    at java.lang.Thread.run(Thread.java:722)\n",
            "{noformat}\n",
            "\n",
            "test_column_index_stress in wide_rows_test will reproduce this within ~20 runs and bisect strongly points to a regression in CASSANDRA-5762\n",
            "architectural impact: NO\n",
            "comments: [\"First guess: something's getting confused about reversed-ness.  Is that part of the test in question?\", 'CASSANDRA-5762 is probably causing this indirectly by forcing more code to go through the slice path, rather than introducing a bug in collation directly.', \"bq. First guess: something's getting confused about reversed-ness\\n\\nSecond guess: there's a bug in ISR's code for reversed fetches (CASSANDRA-5712).\\n\\nMight need to make it print out the cells in collectReducedColumns to see...\", \"If there's a bug in ISR it's probably older than 5712.  {{prefetched}} makes my head hurt.\", 'Patch to get more information from the assert.\\n\\n(NB: the existing error message indicates that this is NOT a reversed slice.  So, beats the hell out of me how this could be erroring out.  Hence, the need for more information.)', \"I'll get to it today/tomorrow. Yeah. 5762 is very unlikely to be the cause of it.\", 'Two of the patched assertion failures:\\n\\n{noformat}\\njava.lang.AssertionError: Added cell val31254: does not sort as the last; contents are val2960::false:0@1376407099059000,val2960:value:false:4@1376407099059000,val31254::false:0@1376407110826001,val31254:value:false:4@1376407110826001, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)\\n{noformat}\\n\\n{noformat}\\njava.lang.AssertionError: Added cell val54806: does not sort as the last; contents are val22917::false:0@1376408295872003,val22917:value:false:4@1376408295872003,val54806::false:0@1376408305568001,val54806:value:false:4@1376408305568001, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type\\n{noformat}\\n', \"Here's the most concise one I've seen:\\n\\n{noformat}\\njava.lang.AssertionError: Added cell val11599: does not sort as the last; contents are val11599::false:0@1376409359730001,val11599:value:false:4@1376409359730001, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)\\n{noformat}\", \"So, this is a lot simpler than the test makes it look.  It's caused simple by asking for the same column by name twice:\\n\\n{noformat}\\ncqlsh> create keyspace foo WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\\ncqlsh> use foo;\\ncqlsh:foo> create table bar (row varchar, name varchar, value int, PRIMARY KEY (row, name));\\ncqlsh:foo> update bar set value = 1 WHERE row = 'baz' AND name = 'qux';\\ncqlsh:foo> select value from bar where row='baz' AND name in ('qux', 'qux');\\nRequest did not complete within rpc_timeout.\\n{noformat}\\n\\nResults in:\\n{noformat}\\njava.lang.AssertionError: Added cell qux: does not sort as the last; contents are qux::false:0@1376590034567000,qux:value:false:4@1376590034567000, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)\\n{noformat}\", '+1\\n\\nNit: can we make buildBound return Collection to avoid the extra copy?', \"Committed, thanks.\\n\\nbq. Nit: can we make buildBound return Collection to avoid the extra copy?\\n\\nNo, unfortunately we can't. Well, we kinda can, but we'd then have to create an extra iterator for getKeyBound (won't be able to just .get(0)) and two extra iterators in makeFilter (again, won't be able to just .get(i) - and we need to iterate over startBounds and endBounds simultaneously to build the ColumnSlices).\"]\n",
            "my_comment: can we make buildBound return Collection to avoid the extra copy?\n",
            "\n",
            "No unfortunately we cant. Well we kinda can but wed then have to create an extra iterator for getKeyBound (wont be able to just .get(0)) and two extra iterators in makeFilter (again wont be able to just .get(i) - and we need to iterate over startBounds and endBounds simultaneously to build the ColumnSlices).\"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-5935\n",
            "issue_type: New Feature\n",
            "summary: Allow disabling slab allocation\n",
            "description: Arena allocation (CASSANDRA-2252) dramatically improves garbage collection performance, but it can limit the number of tables that can be practically defined in the schema since you have a minimum of 1MB of heap per table.\n",
            "\n",
            "Although having more than dozens or hundreds of tables defined is almost certainly a Bad Idea (just as it is a design smell in a relational database), it's relatively straightforward to allow disabling the SlabAllocator.\n",
            "architectural impact: NO\n",
            "comments: ['Note that I\\'ve deliberately omitted this setting from cassandra.yaml; see above on \"almost certainly a Bad Idea.\"', 'v2 fixes constructor visibility', 'lgtm. ', 'Committed to 1.2.10 and 2.0.1 (but not 2.0.0).']\n",
            "my_comment: Note that I\\ve deliberately omitted this setting from cassandra.yaml; see above on \"almost certainly a Bad Idea.\" v2 fixes constructor visibility lgtm.  Committed to 1.2.10 and 2.0.1 (but not 2.0.0).\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6012\n",
            "issue_type: Bug\n",
            "summary: CAS does not always correctly replay inProgress rounds\n",
            "description: Paxos says that on receiving the result of a prepare from a quorum of acceptors, the proposer should propose the value of the higher-number proposal accepted amongst the ones returned by the acceptors, and only propose his own value if no acceptor has send us back a previously accepted value.\n",
            "\n",
            "But in PrepareCallback we only keep the more recent inProgress commit regardless of whether is has an update. Which means we could ignore a value already accepted by some acceptors if any of the acceptor send us a more recent ballot than the other acceptor but with no values. The net effect is that we can mistakenly accept two different values for the same round.\n",
            "\n",
            "architectural impact: NO\n",
            "comments: ['Attaching fix: as far as checking if we should finish an inProgress round, we only need to keep the most recent inProgress commit that has a value. But so as to not break the optimization of CASSANDRA-5667, the patch also keep the most recent inProgress, regardless of whether it has a value or not.\\n', '+1', 'Committed, thanks']\n",
            "my_comment: as far as checking if we should finish an inProgress round we only need to keep the most recent inProgress commit that has a value. But so as to not break the optimization of CASSANDRA-5667 the patch also keep the most recent inProgress regardless of whether it has a value or not.\n",
            " +1 Committed thanks\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6047\n",
            "issue_type: Bug\n",
            "summary: Memory leak when using snapshot repairs\n",
            "description: Running nodetool repair repeatedly with the -snapshot parameter results in a native memory leak. The JVM process will take up more and more physical memory until it is killed by the Linux OOM killer.\n",
            "\n",
            "The command used was as follows:\n",
            "\n",
            "nodetool repair keyspace -local -snapshot -pr -st start_token -et end_token\n",
            "\n",
            "Removing the -snapshot flag prevented the memory leak.  The subrange repair necessitated multiple repairs, so it made the problem noticeable, but I believe the problem would be reproducible even if you ran repair repeatedly without specifying a start and end token.\n",
            "\n",
            "Notes from [~yukim]:\n",
            "\n",
            "Probably the cause is too many snapshots. Snapshot sstables are opened during validation, but memories used are freed when releaseReferences called. But since snapshots never get marked compacted, memories never freed.\n",
            "\n",
            "We only cleanup mmap'd memories when sstable is mark compacted. https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/io/sstable/SSTableReader.java#L974\n",
            "\n",
            "Validation compaction never marks snapshots compacted.\n",
            "\n",
            "architectural impact: NO\n",
            "comments: ['Patch attached to let SSTableReader implement Closeable and do clean up at #close. Validation compaction against snapshot calls #close at the end of validation.', '+1', 'Committed.']\n",
            "my_comment: Patch attached to let SSTableReader implement Closeable and do clean up at #close. Validation compaction against snapshot calls #close at the end of validation. +1 Committed.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6170\n",
            "issue_type: New Feature\n",
            "summary: Modify AbstractCassandraDaemon.initLog4j() to allow for hotfixing log level on on a cassandra class\n",
            "description: When customer wants to bump up log level of a cassandra class, here is the procedure they follow:\n",
            "\n",
            "# Add the class name and log level to log4j-server.properties into a revision identified directory.\n",
            "# The new directory is then symbolically linked to the location where cassandra looks for that directory.\n",
            "\n",
            "However cassandra and it's log4j continue to watch the old location \n",
            "\n",
            "The reason for this AbstractCassandraDaemon.initLog4j() uses this \n",
            "\n",
            "configFileName = new File(configLocation.toURI()).getCanonicalPath();\n",
            "\n",
            "The customer believes if we change that to the following, this will allow the symlink to work properly.\n",
            "\n",
            "configFileName = new File(configLocation.toURI()).getPath();\n",
            "\n",
            "If it were possible to add a configuration that would invoke this change on \"True\" that would be ideal.  Or find another method to allow hotfixing the log4j changes.\n",
            "architectural impact: NO\n",
            "comments: [\"Don't see any reason not to allow a symlink here, trivial patch attached.\", '+1', 'Committed.  For trunk, I assumed logback would just work.']\n",
            "my_comment: [\"Dont see any reason not to allow a symlink here trivial patch attached.\" +1 Committed.  For trunk I assumed logback would just work.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6217\n",
            "issue_type: Bug\n",
            "summary: replace doesn't clean up system.peers if you have a new IP\n",
            "description: When you use replace_token (or replace_node or replace_address) if the new node has a different IP, the old node will still be in system.peers\n",
            "architectural impact: NO\n",
            "comments: ['Existing nodes will already have the correct peers state due to the token conflict(s) from the replacer, but the replacer will still have the dead node in its own peers table.  The simplest thing to do is finish replacing by removing the replace_address from the table, since either it will be our own (which should not appear there) or it will be the old node.  Trivial patch to do so.', '+1 LGTM', 'Committed.']\n",
            "my_comment: Existing nodes will already have the correct peers state due to the token conflict(s) from the replacer but the replacer will still have the dead node in its own peers table.  The simplest thing to do is finish replacing by removing the replace_address from the table since either it will be our own (which should not appear there) or it will be the old node.  Trivial patch to do so. +1 LGTM Committed.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6349\n",
            "issue_type: Bug\n",
            "summary: IOException in MessagingService.run() causes orphaned storage server socket\n",
            "description: The refactoring of reading the message header in MessagingService.run() vs IncomingTcpConnection seems to mishandle IOException as the loop is broken and MessagingService.SocketThread never seems to get reinitialized.\n",
            "\n",
            "To reproduce: telnet to port 7000 and send random data. This then prevents any new or restarting node in the cluster from handshaking with this defunct storage port.\n",
            "architectural impact: NO\n",
            "comments: ['One of the options is to close the socket on IOException and continue', 'LGTM, committed', \"I suspect these changes introduced an infinite loop if the ServerSocket gets closed (not sure how that is happening though). We've been seeing some major problems with Cassandra 2.0.3 when a new cluster is coming up for the first time, and it seems to be a result of this. With logging set to debug, system.log is getting pummelled with these exception messages:\\n\\n{noformat}\\nDEBUG [ACCEPT-localhost-grid/10.96.99.178] 2013-12-06 22:55:39,759 MessagingService.java (line 905) Error reading the socket null\\njava.net.SocketException: Socket closed\\n        at java.net.PlainSocketImpl.socketAccept(Native Method)\\n        at java.net.AbstractPlainSocketImpl.accept(Unknown Source)\\n        at java.net.ServerSocket.implAccept(Unknown Source)\\n        at sun.security.ssl.SSLServerSocketImpl.accept(Unknown Source)\\n        at org.apache.cassandra.net.MessagingService$SocketThread.run(MessagingService.java:865)\\n{noformat}\\n\\nIt looks like once in this state, nothing will break it out; prior to this change the IOException catch block was throwing another exception, now it just keeps looping, using the (seemingly closed) ServerSocket. Restarting Cassandra seems to be the only way to resolve this. I'll probably be recommending we drop back to 2.0.2 until this problem is fixed (or we can understand why the ServerSocket is closed...)\\n\", \"The current SocketThread's code detects whether the ServerSocket is closing by catching AsynchronousCloseException/ClosedChannelException and breaking the endless loop.\\nAnd it looks like SSLServerSocketImpl throws a different exception (SocketException) which the thread doesn't handle\\n\\nDo we really need {{while(true)}} there? Why can't we use {{while (!server.isClosed())}} instead?\\n\", 'CASSANDRA-6468', 'Please also note that handling the protocol magic and version handshake in the while loop allows an attacker to open a connection and not send any data, preventing any further connections. Prior revisions handled all the handshaking in the resulting thread where it might be more appropriate.']\n",
            "my_comment: Socket closed\n",
            "        at java.net.PlainSocketImpl.socketAccept(Native Method)\n",
            "        at java.net.AbstractPlainSocketImpl.accept(Unknown Source)\n",
            "        at java.net.ServerSocket.implAccept(Unknown Source)\n",
            "        at sun.security.ssl.SSLServerSocketImpl.accept(Unknown Source)\n",
            "        at org.apache.cassandra.net.MessagingService$SocketThread.run(MessagingService.java:865)\n",
            "{noformat}\n",
            "\n",
            "It looks like once in this state nothing will break it out; prior to this change the IOException catch block was throwing another exception now it just keeps looping using the (seemingly closed) ServerSocket. Restarting Cassandra seems to be the only way to resolve this. Ill probably be recommending we drop back to 2.0.2 until this problem is fixed (or we can understand why the ServerSocket is closed...)\n",
            "\" \"The current SocketThreads code detects whether the ServerSocket is closing by catching AsynchronousCloseException/ClosedChannelException and breaking the endless loop.\n",
            "And it looks like SSLServerSocketImpl throws a different exception (SocketException) which the thread doesnt handle\n",
            "\n",
            "Do we really need {{while(true)}} there? Why cant we use {{while (!server.isClosed())}} instead?\n",
            "\" CASSANDRA-6468 Please also note that handling the protocol magic and version handshake in the while loop allows an attacker to open a connection and not send any data preventing any further connections. Prior revisions handled all the handshaking in the resulting thread where it might be more appropriate.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6380\n",
            "issue_type: Bug\n",
            "summary: SSTableReader.loadSummary may leave an open file\n",
            "description: When {{SSTableReader.loadSummary}} catches _IOException_ it tries to delete {{summariesFile}}, but the {{iStream}} is still open and the file is locked, so {{FileUtils.deleteWithConfirm}} fails, at least on Windows\n",
            "architectural impact: NO\n",
            "comments: ['Attaching the patch to close the stream to unlock the file', \"'finally' block does close iStream, so I think there isn't a problem in current code.\", \"LGTM; committed\\n\\n(Didn't see Yuki's comment.  The problem is that the catch block runs before the finally, so it tries to delete before the close, which works on linux but not windows.)\"]\n",
            "my_comment: Attaching the patch to close the stream to unlock the file \"finally block does close iStream so I think there isnt a problem in current code.\" \"LGTM; committed\n",
            "\n",
            "(Didnt see Yukis comment.  The problem is that the catch block runs before the finally so it tries to delete before the close which works on linux but not windows.)\"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6487\n",
            "issue_type: Improvement\n",
            "summary: Log WARN on large batch sizes\n",
            "description: Large batches on a coordinator can cause a lot of node stress. I propose adding a WARN log entry if batch sizes go beyond a configurable size. This will give more visibility to operators on something that can happen on the developer side. \n",
            "\n",
            "New yaml setting with 5k default.\n",
            "\n",
            "{{# Log WARN on any batch size exceeding this value. 5k by default.}}\n",
            "{{# Caution should be taken on increasing the size of this threshold as it can lead to node instability.}}\n",
            "\n",
            "{{batch_size_warn_threshold: 5k}}\n",
            "\n",
            "\n",
            "architectural impact: NO\n",
            "comments: [\"If it's not out of the way, it would help to include the keyspace and column family and maybe the session ID/info.\", \"Sure. Can't see any reason not to add more info if it's easy to add. \", \"Added the batch_size_warn_threshold setting to cassandra.yaml and altered QueryProcessor#processBatch to log a WARN is the batch's size is more than the added setting.\\n\\nSome example output of what the warning looks like (keep in mind that multiple kss/cfs can be updated with one batch)\\n{code}\\nWARN  14:29:12 Batch of statements for ks.cf pairs to be updated [test.cf, test.cf2, test2.cf] is of size 11024 and exceeds specified threshold of 7168.\\nWARN  14:31:59 Batch of statements for ks.cf pairs to be updated [test.cf] is of size 1448 and exceeds specified threshold of 1024.\\n{code}\", 'Can you make it configure by 1s instead of 1000s?\\n\\nBikeshed: would prefer format of\\n\\n{noformat}\\nBatch of statements for [test.cf, test.cf2, test2.cf] is of size 11024, exceeding specified threshold of 7168\\n{noformat}', 'Sure thing, changed from kb to bytes and updated the warning message in v2.', 'Oops, I skimmed too fast and thought we were counting statements not bytes.  Is that what you were thinking when you estimated 5k [~pmcfadin]?', \"Yes that was in bytes. Just in my own experience, I don't recommend more than ~100 mutations per batch. Doing some quick math I came up with 5k as 100 x 50 byte mutations. \\n\\nTotally up for debate.\", \"Okay, then the first patch is actually what we want for that.\\n\\nProblem is, we can't compute size-in-bytes without MemoryMeter, which is reflection-based so I wouldn't want to put it in the fast path.\\n\\nIf you're okay with counting statements instead I think that will be more lightweight.\", 'We can count statements but the problem there is that if a batch has identical statements grouped together, then counting statements means we are just guessing whether the batch is large or not. Example:\\n\\n{noformat}\\n// 10 statements in a batch when measured with memory meter creates a batch of size 81912 bytes.\\n// for a table with schema \"CREATE TABLE tbl (col1 text PRIMARY KEY);\"\\nPreparedStatement prepStatement = session.prepare(\"INSERT INTO db.tbl (col1) VALUES (?)\");\\nBatchStatement batch = new BatchStatement();\\nbatch.add(prepStatement.bind(\"val1\"));\\n...\\nbatch.add(prepStatement.bind(\"val10\"));\\n{noformat}\\n\\nIncreasing it to 100 produces a batch of size 82456, _544_ bytes more for 10x statements. That being said, this is for batches where queries are identical, and we are only displaying a warning, not actually restricting such batches, so I\\'ll attach the patch with the updated message style suggest by [~jbellis] where the default is set to 50 statements per batch (based on twissandra\\'s model below it allows for 16 posts in a batch where the batch of 48 queries is of size 89kb which seems reasonable.\\n\\n{noformat}\\nINSERT INTO tweets (tweet_id, username, body) VALUES (8ffb88b3-ae60-48c2-bb96-c4f2d08c4ceb, \\'lyubent\\', \\'epic msg\\');\\nINSERT INTO userline (username, tweet_id, time) VALUES (\\'lyubent\\', 8ffb88b3-ae60-48c2-bb96-c4f2d08c4ceb, 66bd94a0-c17d-11e3-9c7a-4366e868fc79);\\nINSERT INTO timeline (username, tweet_id, time) VALUES (\\'follower\\', 8ffb88b3-ae60-48c2-bb96-c4f2d08c4ceb, 66bd94a0-c17d-11e3-9c7a-4366e868fc79);\\n{noformat}\\n\\np.s. Example warn output: \\n\\n{noformat}\\nWARN 17:28:19,652 Batch of statements for [db.timeline, db.userline, db.tweets] is of size 51, exceeding specified threshold of 50 by 1.\\n{noformat}', 'Not saying that we should, but we can calculate the size of the resulting processed collection of Mutation-s w/out using reflection, and warn based on that.', 'Is this something important enough that an Ops team might want to monitor in an automated manner, like with an mbean, for OpsCenter and other monitoring tools? Maybe count of batch size warnings, largest batch size seen, most recent batch size over the limit.', \"[~iamaleksey] I assume you mean calling {{ByteBuffer#limit}} in {{BatchStatement#executeWithPerStatementVariables}}, I like the idea, it will be much more accurate than just counting queries and it's just a loop with a counter, and shouldn't hurt the fast path, right? /cc [~benedict]. \\n\\nbq. Maybe count of batch size warnings, largest batch size seen, most recent batch size over the limit.\\n[~jkrupan] +1, maybe also something like total statement count over the limit (e.g. if a batch exceeds the limit by 10, and this occurs 4 times, that metric will end up with 40). \", \"No, that's not what I meant. I meant the size of the resulting Mutation-s (RowMutation-s pre 2.1), as a sum of ColumnFamily#dataSize()-s for each of the Mutation#getColumnFamilies(). Of course it would affect the path - any extra stuff you do would.\", \"Anyway, I'm not saying that this is the way to go - merely listing options.\", \"Just noticed that we're actually already using the memory meter for checking batch size when it might get placed into the prepared statement cache, so why not log based on that value (calculated in {{BatchStatement#measureForPreparedCache}}). As for non-prepared batch statements, there we can enforce a limit based on count of statements.\", \"I suggest using the ColumnFamily.dataSize() method as Aleksey suggested: in the BatchStatement.executeWithConditions() and executeWithoutConditions() methods we have access to the fully constructed ColumnFamily objects we will apply. In the former we construct a single CF _updates_, and in the latter we can iterate over each of the IMutations and call _getColumnFamilies()_.\\n\\nWarning on the prepared size is probably not meaningful, because it does not say anything about how big the data we're applying is.\", 'Patch is back to using the 5kb limit and checks batch size via the method suggested by Aleksey (Checking column size based on the mutations that are about to be applied) ', \"Basic approach looks good. I would:\\n\\n# use Iterables.concat(Iterables.transform()) to create set of items to process (this requires changing signature to accept an Iterable instead of Collection)\\n# Only construct the set of affected KS/CFs in the case that the size warn limit is breached. You can just use an ArrayList as well, as uniqueness is guaranteed (see getMutations(), which merges mutations for the same CF into one IMutation. Feel free to add a comment to this method explaining this so the next person to read this doesn't have to figure it out)\", 'v2 uses Iterables.concat(Iterables.transfrom(...) to build an Iterable of CFs from IMutations, but I kept the hashset, using an ArrayList does yield duplicate values.', 'Good point (can have multiple partitions per CF). LGTM.\\n\\n+1\\n\\n', 'committed', \"Does this make sense to log even in the case where the batch is Type.UNLOGGED?  When writing as fast as I can, it sounds like a BatchStatement with a single execution is the fastest thing to do.  However, I'm now getting these warnings.  My options are to jack up the batch size threshold, or stop using batch statements..\", \"There's essentially no performance gain from batching across multiple partitions.\", \"I'm batching on a single partition only.  \\nI have a table defined as:\\n                                          CREATE TABLE store.blobs (\\n                                                account_name text, \\n                                                m_guid     text, \\n                                                m_blob     text, \\n                                                PRIMARY KEY (account_name, m_guid))\\n\\nI am using a prepared statement with an unlogged batch to insert many blobs into the same account at all once:\\n    INSERT INTO blobs (account_name, m_guid, m_blob) VALUES (?, ?, ?)\\n\\nMy understanding is that this is a pretty decent way of doing it:  http://www.datastax.com/dev/blog/client-side-improvements-in-cassandra-2-0\\n(re Batching Prepared Statements).\\n\\nI could do these all individually, but there would clearly be some overhead.  \\n\\nSo, the options are to not use the prepared statement / batch, jack up the threshold, or change the Cassandra code to avoid logging on unlogged batches..  \", \"We experienced some problems when using (ridiculously) big batches here, Cassandra was throwing odd exceptions:\\n{code}\\njava.lang.IndexOutOfBoundsException: Invalid combined index of 1565817280, maximum is 482109\\n        at org.jboss.netty.buffer.SlicedChannelBuffer.<init>(SlicedChannelBuffer.java:46)\\n        at org.jboss.netty.buffer.HeapChannelBuffer.slice(HeapChannelBuffer.java:201)\\n        at org.jboss.netty.buffer.AbstractChannelBuffer.readSlice(AbstractChannelBuffer.java:323)\\n        at org.apache.cassandra.transport.CBUtil.readValue(CBUtil.java:295)\\n        at org.apache.cassandra.transport.CBUtil.readValueList(CBUtil.java:340)\\n        at org.apache.cassandra.transport.messages.BatchMessage$1.decode(BatchMessage.java:62)\\n        at org.apache.cassandra.transport.messages.BatchMessage$1.decode(BatchMessage.java:43)\\n        at org.apache.cassandra.transport.Message$ProtocolDecoder.decode(Message.java:212)\\n        at org.jboss.netty.handler.codec.oneone.OneToOneDecoder.handleUpstream(OneToOneDecoder.java:66)\\n        at org.jboss.netty.handler.codec.oneone.OneToOneDecoder.handleUpstream(OneToOneDecoder.java:68)\\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)\\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\\n        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)\\n        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)\\n        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n        at java.lang.Thread.run(Unknown Source)\\n{code}\\n\\nWe fixed our code so it behaved and stopped sending huge batches, but since it appears that Cassandra can't handle too big batches, wouldn't it make more sense to simply refuse them instead of warning / crashing ?\", 'What happens if a single-statement BATCH exceeds the limit?\\n\\nI ask this because the batch size limit will impact setting the timestamp on a statement. If we have a collection of updates, the decision to batch or not batch them happens further downstream, when a collection of statements are analyzed. \\n\\nHOWEVER, the UPDATE statement only supports the USING timestamp in the middle of the statement.\\n\\nThe BATCH statement allows you to make the timestamp decision later on.\\n\\nIf a BATCH is encountered with a SINGLE STATEMENT, can the limit be ignored and have it be treated as a normal update?\\n\\nI ask because there is discussion of making this a hard limit.', \"Is the batch size (5kB) also relevant, if\\n1) a batch writes only a single partition\\n2) or it contains only a single statement/insert?\\n\\nBackground: We're using [akka-persistence-cassandra|https://github.com/krasserm/akka-persistence-cassandra/] (which writes single events as batch as well AFAICS) and get warnings for ap_messages like {{WARN \\\\[SharedPool-Worker-78] 2015-11-11 17:30:07,489 BatchStatement.java:252 - Batch of prepared statements for \\\\[search.ap_messages] is of size 10243, exceeding specified threshold of 5120 by 5123.}}.\\nTherefore I'd like to better understand this issue to know better how we should proceed.\", \"[~martin.grotzke] Yes the batch size is relevant in both scenario 1 and 2. Remember it's a warning, you can tweek the settings to see increase the batch size using the setting {{batch_size_warn_threshold_in_kb}}. Also notice that the fail threshold is 10x the warn threshold by default ({{batch_size_fail_threshold_in_kb: 50}}).\", '[~lyubent] Can you please explain, *why* the batch size is relevant in both szenarios 1) and 2)?\\n\\nWhat are the extra costs of a single-partition batch (with multiple statements/inserts), so that this warning should be logged?\\nHow\\'s a single-statement batch (obviously going to a single-partition) differently handled than a single-statement not sent as BATCH?\\n\\nRegarding single-partition batches, my understanding is that they don\\'t cause any extra costs. This understanding is based e.g. on CASSANDRA-6737 (\"A batch statements on a single partition should not create a new CF object for each update\") and on http://christopher-batey.blogspot.de/2015/02/cassandra-anti-pattern-misuse-of.html, which says (in the paragraph \"So when should you use unlogged batches?\") {quote}Well customer id is the partition key, so this will be no more coordination work than a single insert and it can be done with a single operation at the storage layer.{quote}\\nWhat\\'s wrong with this understanding, in which way are single-partition batches more expensive?', \"Any feedback here? I'd like to understand the issues that are highlighted by the log warn in combination with 1) single-partition and/or 2) single-statement batches.\", 'Hi, I wanted to echo the question from [~martin.grotzke], we\\'re doing a similar use in which we\\'re saving an Aggregate as a list of \"events\" under the aggregate key. What this means is that the batch is always limited to 1 key, and therefore should be executed exclusively on one node and no intra-node communication/dependency should be needed.\\nThis is why I also think that this use should be safe. \\n\\nI understand that the warning is just a warning, but better understanding this can help shape better use of the Cassandra model.', \"[~martin.grotzke] and [~pragone] Sorry! I just caught these comments. \\n\\nValid points on a single partition and I think it warrants a change in the way we log WARN and ERROR for batches. The original intent was to prevent horrible anti-patterns on multi-partition batches. In the case of a single partition update, the impact is only in network payload size. Since there is no need for the coordinator to track all of the mutations across the batch partitions, the load is much less. \\n\\nI'll make an updated ticket to reflect that difference. \\n\\nThanks for the comments and raising this issue. \", '\\n    \\nThere is also a DOC ticket. I\\'ll have to look it up for you tomorrow.\\xa0\\nLorina\\nSent from my Verizon Wireless 4G LTE smartphone\\n\\n-------- Original message --------\\nFrom: \"Patrick McFadin (JIRA)\" <jira@apache.org> \\nDate: 12/15/2015  21:21  (GMT-08:00) \\nTo: lorina@datastax.com \\nSubject: [jira] [Commented] (CASSANDRA-6487) Log WARN on large batch sizes \\n\\n\\n\\xa0\\xa0\\xa0 [ https://issues.apache.org/jira/browse/CASSANDRA-6487?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15059511#comment-15059511 ] \\n\\nPatrick McFadin commented on CASSANDRA-6487:\\n--------------------------------------------\\n\\n[~martin.grotzke] and [~pragone] Sorry! I just caught these comments. \\n\\nValid points on a single partition and I think it warrants a change in the way we log WARN and ERROR for batches. The original intent was to prevent horrible anti-patterns on multi-partition batches. In the case of a single partition update, the impact is only in network payload size. Since there is no need for the coordinator to track all of the mutations across the batch partitions, the load is much less. \\n\\nI\\'ll make an updated ticket to reflect that difference. \\n\\nThanks for the comments and raising this issue. \\n\\n\\n\\n\\n--\\nThis message was sent by Atlassian JIRA\\n(v6.3.4#6332)\\n', 'Thanks [~pmcfadin] for the clarification! I saw you created CASSANDRA-10876 and also the related CASSANDRA-8825 - I wasn\\'t aware of this ticket, good to see.\\n\\nWould you say the cost of a single partition single statement batch is exactly the same as a \"normal\" single statement?\\nAnd how would you compare a single partition batch with multiple insert statements to just multiple insert statements in terms of server load / throughput - is executing multiple single partition statements as a batch a valid approach to increase throughput?']\n",
            "my_comment: [jira] [Commented] (CASSANDRA-6487) Log WARN on large batch sizes \n",
            "\n",
            "\n",
            "\\xa0\\xa0\\xa0 [ https://issues.apache.org/jira/browse/CASSANDRA-6487?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15059511#comment-15059511 ] \n",
            "\n",
            "Patrick McFadin commented on CASSANDRA-6487:\n",
            "--------------------------------------------\n",
            "\n",
            "[~martin.grotzke] and [~pragone] Sorry! I just caught these comments. \n",
            "\n",
            "Valid points on a single partition and I think it warrants a change in the way we log WARN and ERROR for batches. The original intent was to prevent horrible anti-patterns on multi-partition batches. In the case of a single partition update the impact is only in network payload size. Since there is no need for the coordinator to track all of the mutations across the batch partitions the load is much less. \n",
            "\n",
            "I\\ll make an updated ticket to reflect that difference. \n",
            "\n",
            "Thanks for the comments and raising this issue. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "--\n",
            "This message was sent by Atlassian JIRA\n",
            "(v6.3.4#6332)\n",
            " Thanks [~pmcfadin] for the clarification! I saw you created CASSANDRA-10876 and also the related CASSANDRA-8825 - I wasn\\t aware of this ticket good to see.\n",
            "\n",
            "Would you say the cost of a single partition single statement batch is exactly the same as a \"normal\" single statement?\n",
            "And how would you compare a single partition batch with multiple insert statements to just multiple insert statements in terms of server load / throughput - is executing multiple single partition statements as a batch a valid approach to increase throughput?\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6495\n",
            "issue_type: Bug\n",
            "summary: LOCAL_SERIAL  use QUORUM consistency level to validate expected columns\n",
            "description: If CAS is done at LOCAL_SERIAL consistency level, only the nodes from the local data center should be involved. \n",
            "Here we are using QUORAM to validate the expected columns. This will require nodes from more than one DC. \n",
            "We should use LOCAL_QUORAM here when CAS is done at LOCAL_SERIAL. \n",
            "\n",
            "Also if we have 2 DCs with DC1:3,DC2:3, a single DC down will cause CAS to not work even for LOCAL_SERIAL. \n",
            "\n",
            "\n",
            "architectural impact: NO\n",
            "comments: ['[~jbellis]\\nFor this should we use LOCAL_QUORAM for LOCAL_SERIAL or use the consistency level of commit. I think adding a third CL for this will be too confusing. So I think we can use CL of commit for validating columns.  ', 'Pretty sure we *must* use LOCAL_QUORUM. LOCAL_SERIAL should still provided serializability within the local data-center and this require doing a LOCAL_QUORUM. Using the CL of the commit would be incorrect in most case.', 'I think that \"cas\" CL of SERIAL -> read at Q, LOCAL_SERIAL -> LQ.', 'OK. Let me do that', 'committed the StorageProxy change.\\n\\nLeaving the AbstractPaxosCallback timeout alone; the intention is that CasContentionTimeout is for \"the replicas are responding normally, but I couldn\\'t get a ballot accepted within X seconds because I\\'m competing with other transactions.\"  So using WriteTimeout for a response to a single prepare or propose is correct.']\n",
            "my_comment: [~jbellis]\n",
            "For this should we use LOCAL_QUORAM for LOCAL_SERIAL or use the consistency level of commit. I think adding a third CL for this will be too confusing. So I think we can use CL of commit for validating columns.   Pretty sure we *must* use LOCAL_QUORUM. LOCAL_SERIAL should still provided serializability within the local data-center and this require doing a LOCAL_QUORUM. Using the CL of the commit would be incorrect in most case. I think that \"cas\" CL of SERIAL -> read at Q LOCAL_SERIAL -> LQ. OK. Let me do that committed the StorageProxy change.\n",
            "\n",
            "Leaving the AbstractPaxosCallback timeout alone; the intention is that CasContentionTimeout is for \"the replicas are responding normally but I couldn\\t get a ballot accepted within X seconds because I\\m competing with other transactions.\"  So using WriteTimeout for a response to a single prepare or propose is correct.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6706\n",
            "issue_type: Bug\n",
            "summary: Duplicate rows returned when in clause has repeated values\n",
            "description: If a value is repeated within an IN clause then repeated rows are returned for  the repeats:\n",
            "cqlsh> create table t1(c1 text primary key);\n",
            "cqlsh> insert into t1(c1) values ('A');\n",
            "cqlsh> select * from t1;\n",
            "\n",
            " c1\n",
            "----\n",
            "  A\n",
            "\n",
            "cqlsh> select * from t1 where c1 = 'A';\n",
            "\n",
            " c1\n",
            "----\n",
            "  A\n",
            "\n",
            "cqlsh> select * from t1 where c1 in( 'A');\n",
            "\n",
            " c1\n",
            "----\n",
            "  A\n",
            "\n",
            "cqlsh:dslog> select * from t1 where c1 in( 'A','A');\n",
            "\n",
            " c1\n",
            "----\n",
            "  A\n",
            "  A\n",
            "\n",
            "architectural impact: NO\n",
            "comments: [\"That is kind of the intended behavior. Is it the best behavior? I don't know, though I'm not sure it matters much in practice tbh. But when there is an IN, we do order the resulting rows following the order of the values in the IN (unless there is an explicit ordering that takes precedence of course) which kind of suggest we consider the IN values as a list rather than a set, and from that perspective, it's probably not entirely crazy to return duplicate results in that case. In particular, if you use a prepared marker for an IN, the server will expect a list, not a set for the values (and changing now would really break users). It's easy enough to avoid the duplication client side if you don't want duplicates.\\n\\nDon't get me wrong, I'm not saying not returning duplicate in that case would be inferior, but rather that I don't see a big problem with the current behavior and so that I'd rather not introduce a breaking change, even a small one, for no good reason.\", 'If I were performing a calculation summing the results then the answer would be wrong. \\nI suppose it is arguable that I should know better than to put duplicates in an in clause but this bit me when I was generating a query by aggregating selection parameters from separate sources.\\nMy background is much more extensive in relational databases and in Postgres (to take my preferred example) I would only get back one row for this query which in terms of correctness is absolutely what I would expect.', \"[~slebresne] Should this be closed as Won't Fix or Not a Problem, or left open?\", 'IRC discussion shows a desire to fix this in 3.0, but only to warn in 2.1.', 'No DBA or developer familiar with SQL will expect this behavior. 100% of them will find it unexpected, and many will conclude it is a bug.\\n\\nI understand CQL is not SQL, but people expect similar semantics.', 'This behavior has apparently already been changed in trunk during the {{SelectStatement}} refactoring CASSANDRA-7981.', 'The patch for 2.1 make sure that Cassandra will log a warning the first time a user execute a query containing an IN restriction with duplicate values on the primary key.\\n\\nThe patch for trunk add unit tests to check the behavior and fix the {{CHANGES.txt}} files.', '[~snazy] can you review?', 'Sure :)', '+1 on the 2.1 patch. It triggers the warning once for the example in the ticket description.\\n\\nThe trunk patch should have tests for duplicate IN values on the partition key.\\nIt checks on duplicate IN values on the clustering key (even 2.1 does not return duplicates on CK duplicates).\\nYou can simply add other assertions like {{SELECT * FROM %s WHERE k1 IN (?, ?) AND k2 = ?}} or {{SELECT * FROM %s WHERE k1 IN (?, ?) AND k2 IN (?, ?)}}.', 'Adds the missing tests.', '+1\\n\\ncommitted as 0c2eaa9 (2.1) + 732986b (trunk, merge-commit)']\n",
            "my_comment: [\"That is kind of the intended behavior. Is it the best behavior? I dont know though Im not sure it matters much in practice tbh. But when there is an IN we do order the resulting rows following the order of the values in the IN (unless there is an explicit ordering that takes precedence of course) which kind of suggest we consider the IN values as a list rather than a set and from that perspective its probably not entirely crazy to return duplicate results in that case. In particular if you use a prepared marker for an IN the server will expect a list not a set for the values (and changing now would really break users). Its easy enough to avoid the duplication client side if you dont want duplicates.\n",
            "\n",
            "Dont get me wrong Im not saying not returning duplicate in that case would be inferior but rather that I dont see a big problem with the current behavior and so that Id rather not introduce a breaking change even a small one for no good reason.\" If I were performing a calculation summing the results then the answer would be wrong. \n",
            "I suppose it is arguable that I should know better than to put duplicates in an in clause but this bit me when I was generating a query by aggregating selection parameters from separate sources.\n",
            "My background is much more extensive in relational databases and in Postgres (to take my preferred example) I would only get back one row for this query which in terms of correctness is absolutely what I would expect. \"[~slebresne] Should this be closed as Wont Fix or Not a Problem or left open?\" IRC discussion shows a desire to fix this in 3.0 but only to warn in 2.1. No DBA or developer familiar with SQL will expect this behavior. 100% of them will find it unexpected and many will conclude it is a bug.\n",
            "\n",
            "I understand CQL is not SQL but people expect similar semantics. This behavior has apparently already been changed in trunk during the {{SelectStatement}} refactoring CASSANDRA-7981. The patch for 2.1 make sure that Cassandra will log a warning the first time a user execute a query containing an IN restriction with duplicate values on the primary key.\n",
            "\n",
            "The patch for trunk add unit tests to check the behavior and fix the {{CHANGES.txt}} files. [~snazy] can you review? Sure :) +1 on the 2.1 patch. It triggers the warning once for the example in the ticket description.\n",
            "\n",
            "The trunk patch should have tests for duplicate IN values on the partition key.\n",
            "It checks on duplicate IN values on the clustering key (even 2.1 does not return duplicates on CK duplicates).\n",
            "You can simply add other assertions like {{SELECT * FROM %s WHERE k1 IN (? ?) AND k2 = ?}} or {{SELECT * FROM %s WHERE k1 IN (? ?) AND k2 IN (? ?)}}. Adds the missing tests. +1\n",
            "\n",
            "committed as 0c2eaa9 (2.1) + 732986b (trunk merge-commit)\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6747\n",
            "issue_type: Improvement\n",
            "summary: MessagingService should handle failures on remote nodes.\n",
            "description: While going through the code of MessagingService, I discovered that we don't handle callbacks on failure very well. If a Verb Handler on the remote machine throws an exception, it goes right through uncaught exception handler. The machine which triggered the message will keep waiting and will timeout. On timeout, it will so some stuff hard coded in the MS like hints and add to Latency. There is no way in IAsyncCallback to specify that to do on timeouts and also on failures. \n",
            "\n",
            "Here are some examples which I found will help if we enhance this system to also propagate failures back.  So IAsyncCallback will have methods like onFailure.\n",
            "\n",
            "1) From ActiveRepairService.prepareForRepair\n",
            "\n",
            "   IAsyncCallback callback = new IAsyncCallback()\n",
            "       {\n",
            "           @Override\n",
            "           public void response(MessageIn msg)\n",
            "           {\n",
            "               prepareLatch.countDown();\n",
            "           }\n",
            "\n",
            "           @Override\n",
            "           public boolean isLatencyForSnitch()\n",
            "           {\n",
            "               return false;\n",
            "           }\n",
            "       };\n",
            "\n",
            "       List<UUID> cfIds = new ArrayList<>(columnFamilyStores.size());\n",
            "       for (ColumnFamilyStore cfs : columnFamilyStores)\n",
            "           cfIds.add(cfs.metadata.cfId);\n",
            "\n",
            "       for(InetAddress neighbour : endpoints)\n",
            "       {\n",
            "           PrepareMessage message = new PrepareMessage(parentRepairSession, cfIds, ranges);\n",
            "           MessageOut<RepairMessage> msg = message.createMessage();\n",
            "           MessagingService.instance().sendRR(msg, neighbour, callback);\n",
            "       }\n",
            "       try\n",
            "       {\n",
            "           prepareLatch.await(1, TimeUnit.HOURS);\n",
            "       }\n",
            "       catch (InterruptedException e)\n",
            "       {\n",
            "           parentRepairSessions.remove(parentRepairSession);\n",
            "           throw new RuntimeException(\"Did not get replies from all endpoints.\", e);\n",
            "       }\n",
            "\n",
            "2) During snapshot phase in repair, if SnapshotVerbHandler throws an exception, we will wait forever. \n",
            "architectural impact: NO\n",
            "comments: ['MS is primarily designed around the needs of mutations and reads, where it\\'s probably not worth distinguishing between failure and timeout since (a) they should both be rare and (b) when the replica does fail completely it turns into a timeout anyway.\\n\\nBut for repair specifically where Prepare can take arbitrarily long (so it\\'s difficult to just pick a timeout and assume, \"if we haven\\'t heard back it must have failed\") then I agree we should make a bigger effort to notify peers of failures.', 'While going through the code of MS, I have found another problem. \\nWhen we send a message through sendRR in MS, we add its callback to an expiring map with a timeout of RPC timeout in most cases. \\nWhen the timeout is hit if the response did not come, the map will remove the callback from the map. \\nSo all late arriving messages will not be able to run the callback.  \\nThe problem with this is that in  ActiveRepairService.prepareForRepair(as given in JIRA description), waiting for 1 hour does not make sense since it will never get the response back once RPC time has passed. \\nAlso snapshot phase in repair will hang forever. \\nThere might be more places where this is a problem. ', 'I am adding a new interface which could be used if we want negative ack back from the remote node or need a callback on timeout. \\n\\nThis new interface is used in these two places. We can use this at other places but I have only updated two places.I can create new JIRA for these if that make more sense. \\nSnapshotTask.java and ActiveRepairService.java\\n\\nThis also fixes the problem in the previous comment for these use cases. ', \"[~kohlisankalp] I like your approach. \\nOne thing you need to change is in SnapshotTask's callback#onFailure, you can't just throw RuntimeException, you have to call task.setException so repair knows there's exception during snapshotting.\", 'Please review v2 with your suggestions. ', 'Thanks [~kohlisankalp], I updated your patch with following:\\n\\n* MessageOut object is immutable and MessageOut#withParameter returns new object, so we have to use that instead of original.\\n* RTE throwed from ActiveRepairService#prepareForRepair has to be catched and notified to client so repair command not to hang.\\n\\nFor remote snapshot fail, the patch certainly catches the error on coordinator side, but it still hangs(marked as TODO in RepairJob#sendTreeRequest). This is handled in CASSANDRA-6455.', 'Looks good. ', 'Committed, thanks!']\n",
            "my_comment: MS is primarily designed around the needs of mutations and reads where it\\s probably not worth distinguishing between failure and timeout since (a) they should both be rare and (b) when the replica does fail completely it turns into a timeout anyway.\n",
            "\n",
            "But for repair specifically where Prepare can take arbitrarily long (so it\\s difficult to just pick a timeout and assume \"if we haven\\t heard back it must have failed\") then I agree we should make a bigger effort to notify peers of failures. While going through the code of MS I have found another problem. \n",
            "When we send a message through sendRR in MS we add its callback to an expiring map with a timeout of RPC timeout in most cases. \n",
            "When the timeout is hit if the response did not come the map will remove the callback from the map. \n",
            "So all late arriving messages will not be able to run the callback.  \n",
            "The problem with this is that in  ActiveRepairService.prepareForRepair(as given in JIRA description) waiting for 1 hour does not make sense since it will never get the response back once RPC time has passed. \n",
            "Also snapshot phase in repair will hang forever. \n",
            "There might be more places where this is a problem.  I am adding a new interface which could be used if we want negative ack back from the remote node or need a callback on timeout. \n",
            "\n",
            "This new interface is used in these two places. We can use this at other places but I have only updated two places.I can create new JIRA for these if that make more sense. \n",
            "SnapshotTask.java and ActiveRepairService.java\n",
            "\n",
            "This also fixes the problem in the previous comment for these use cases.  \"[~kohlisankalp] I like your approach. \n",
            "One thing you need to change is in SnapshotTasks callback#onFailure you cant just throw RuntimeException you have to call task.setException so repair knows theres exception during snapshotting.\" Please review v2 with your suggestions.  Thanks [~kohlisankalp] I updated your patch with following:\n",
            "\n",
            "* MessageOut object is immutable and MessageOut#withParameter returns new object so we have to use that instead of original.\n",
            "* RTE throwed from ActiveRepairService#prepareForRepair has to be catched and notified to client so repair command not to hang.\n",
            "\n",
            "For remote snapshot fail the patch certainly catches the error on coordinator side but it still hangs(marked as TODO in RepairJob#sendTreeRequest). This is handled in CASSANDRA-6455. Looks good.  Committed thanks!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6753\n",
            "issue_type: Bug\n",
            "summary: Cassandra2.1~beta1 Stall at Boot\n",
            "description: I was trying out the new release for several perf. improvements that I am very interested in. After upgrading my cassandra from 2.0.5 to the beta version, cassandra is stalled while init the column families.\n",
            "\n",
            "I might misconfigure something, but it seems it is suck in a loop. I added a couple debug statements, but, on second thought, I think I should just leave it to the experts...\n",
            "\n",
            "It's looping in the following over and over:\n",
            "\n",
            "{code:title=src/java/org/apache/cassandra/utils/memory/Pool.java#needsCleaning}\n",
            "0 >= -858993472 && true && true\n",
            "{code}\n",
            "\n",
            "{code:title=Log}\n",
            "INFO  [HeapSlabPoolCleaner] 2014-02-21 22:28:40,073 Keyspace.java:77 - java.lang.Thread.getStackTrace(Unknown Source),\n",
            "org.apache.cassandra.db.Keyspace$1.apply(Keyspace.java:77),\n",
            "org.apache.cassandra.db.Keyspace$1.apply(Keyspace.java:74),\n",
            "com.google.common.collect.Iterators$8.transform(Iterators.java:794),\n",
            "com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48),\n",
            "org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:2278),\n",
            "org.apache.cassandra.db.ColumnFamilyStore$FlushLargestColumnFamily.run(ColumnFamilyStore.java:1043),\n",
            "org.apache.cassandra.utils.memory.PoolCleanerThread.run(PoolCleanerThread.java:70)\n",
            "{code}\n",
            "\n",
            "They may be totally unrelated or a normal behavior. Let me know if there is any other info I should provide.\n",
            "architectural impact: NO\n",
            "comments: [\"Some obvious questions:\\n- Do you see any other errors?\\n- The 0 and -858993472 correspond to the used() and nextClean part of that method, respectively, correct? What about the limit and the cleanThreshold? What do they say?\\n- This is consistent, every time you start?\\n\\nThis is definitely not normal, and is almost certainly a bug, but it shouldn't ever stop Cassandra from starting. So, I wonder if there is a strange interaction going on with some other problem, which may be easier to track down if we can figure out if there is another such problem.\\n\\nCould you attach the output from jstacking the process?\\n\\nThe easiest possibility to explain this is that somehow the memtable_cleanup_threshold is negative. We don't actually check this on startup, which is an oversight. The fact that the value for nextClean is exactly \\\\-0.4 * 2Gb has me suspicious - with an 8Gb heap, we would default to a 2Gb limit, and default cleanup_threshold is 0.4. Is it possible you accidentally added a '\\\\-' prefix to the line in the config file? Unlikely, I know, but it would explain it instantly :-)\\n\\n\", 'aha! good call!\\n\\n{code:title=src/java/org/apache/cassandra/utils/memory/Pool.java#needsCleaning}\\nused(0) >= nextClean(-858993472) && updateNextClean(true) && cleanerThread(true) -- limit(-2147483648), cleanThreshold(0.400000)\\n{code}\\n\\nIt seems to be working after:\\n{code:title=src/java/org/apache/cassandra/config/DatabaseDescriptor.java:1385}\\n(long) conf.memtable_total_space_in_mb << 20\\n{code}\\n\\nAnd failed with an exception (doh!)... but it is unrelated to this ticket.', 'Uploaded a simple patch to both correct the overflow and prevent provision of bad cleanup thresholds', 'committed']\n",
            "my_comment: [\"Some obvious questions:\n",
            "- Do you see any other errors?\n",
            "- The 0 and -858993472 correspond to the used() and nextClean part of that method respectively correct? What about the limit and the cleanThreshold? What do they say?\n",
            "- This is consistent every time you start?\n",
            "\n",
            "This is definitely not normal and is almost certainly a bug but it shouldnt ever stop Cassandra from starting. So I wonder if there is a strange interaction going on with some other problem which may be easier to track down if we can figure out if there is another such problem.\n",
            "\n",
            "Could you attach the output from jstacking the process?\n",
            "\n",
            "The easiest possibility to explain this is that somehow the memtable_cleanup_threshold is negative. We dont actually check this on startup which is an oversight. The fact that the value for nextClean is exactly \\\\-0.4 * 2Gb has me suspicious - with an 8Gb heap we would default to a 2Gb limit and default cleanup_threshold is 0.4. Is it possible you accidentally added a \\\\- prefix to the line in the config file? Unlikely I know but it would explain it instantly :-)\n",
            "\n",
            "\" aha! good call!\n",
            "\n",
            "{code:title=src/java/org/apache/cassandra/utils/memory/Pool.java#needsCleaning}\n",
            "used(0) >= nextClean(-858993472) && updateNextClean(true) && cleanerThread(true) -- limit(-2147483648) cleanThreshold(0.400000)\n",
            "{code}\n",
            "\n",
            "It seems to be working after:\n",
            "{code:title=src/java/org/apache/cassandra/config/DatabaseDescriptor.java:1385}\n",
            "(long) conf.memtable_total_space_in_mb << 20\n",
            "{code}\n",
            "\n",
            "And failed with an exception (doh!)... but it is unrelated to this ticket. Uploaded a simple patch to both correct the overflow and prevent provision of bad cleanup thresholds committed\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6962\n",
            "issue_type: Improvement\n",
            "summary: examine shortening path length post-5202\n",
            "description: From CASSANDRA-5202 discussion:\n",
            "\n",
            "{quote}\n",
            "Did we give up on this?\n",
            "Could we clean up the redundancy a little by moving the ID into the directory name? e.g., ks/cf-uuid/version-generation-component.db\n",
            "I'm worried about path length, which is limited on Windows.\n",
            "Edit: to give a specific example, for KS foo Table bar we now have\n",
            "/var/lib/cassandra/flush/foo/bar-2fbb89709a6911e3b7dc4d7d4e3ca4b4/foo-bar-ka-1-Data.db\n",
            "I'm proposing\n",
            "/var/lib/cassandra/flush/foo/bar-2fbb89709a6911e3b7dc4d7d4e3ca4b4/ka-1-Data.db\n",
            "{quote}\n",
            "architectural impact: NO\n",
            "comments: [\"It's actually not clear to me that the benefit of a shorter path length is worth giving up the redundancy that makes it easy to manually toss sstables around.  But manually pushing sstables is a lot less necessary in 2014 than it was 4 years ago...\", \"I'm also not sure I'm a fan of completely removing any indication of the table name in the file name: feels pretty error prone. What about keeping the keyspace/table name as now (for the sake of making it easy to not mix sstables by mistake), but limit them to say 10 characters each (just for the file name), truncating the name if necessary?\", 'bq. limit them to say 10 characters each (just for the file name), truncating the name if necessary?\\n\\nWe can truncate name to fit within os path limit adaptively with some calculation.\\n\\nHow about completely omit Keyspace name but keep ColumnFamily name and adaptively adjust(truncate) its name?', 'This turns out to be a bit complex than I first thought because secondary index CFs are flushing to the same directory. :(\\nAny ideas?', \"I'm inclined to say we should leave it alone for 2.1.  If Windows users start yelling for it in 3.0 then we can address then.  Lots of other stuff to work on in the meantime.\", \"Patch attached with unit test.\\nI ended up the following naming convention/directory structure:\\n\\n{code}\\n/var/lib/cassandra/data/ks/cf-a85fc210cb1011e3a15f9d25721dbb44\\n    /.idx\\n        ka-1-Data.db\\n        ...\\n    /snapshots\\n        /my_snapshot\\n            /.idx\\n                ka-1-Data.db\\n                ...\\n            ka-1-Data.db\\n            ...\\n    /backups\\n        /.idx\\n            ka-1-Data.db\\n            ...\\n        ka-1-Data.db\\n        ...\\n    ka-1-Data.db\\n    ...\\n    ks-cf-jb-123-Data.db    (Older version can co exist)\\n    ks-cf.idx-jb-2-Data.db\\n{code}\\n\\nHighlight:\\n\\n* Keyspace/ColumnFamily name is omitted from filename.\\n* If it is 'temporary' file, then file name would be 'tmp-ka-1-Data.db'.\\n* Each secondary index has its own SSTable directory whose name starts with '.'. This is to distinguish from snapshots/backup directories.\\n* Older version of SSTable file can co-exist along with new directory structure.\", \"Still not a fan of removing the keyspace/table name from the filename. Imo we should go with Jonathan's suggestion above, leave this alone until people actually start yelling cause I don't remember a single user reporting that path lengths was a blocker, so I'd rather not change the directory layout once again (which might break user scripts) until we have practical evidence it's a problem..\", 'WDYT [~JoshuaMcKenzie]?  How likely is this to be a problem for Windows users?', 'CASSANDRA-4110 and the limitations in Schema.java provide us some protection but there\\'s really nothing to stop users nesting their cassandra data 250 characters deep in a path and having things blow up on them regardless of what length we limit ourselves to.\\n\\nOn snapshots we\\'ll be using 204 chars worst-case (48 KS, 48 CF, *2 each, 9 for \"snapshots\", 3 for slashes) so that doesn\\'t leave us a lot of breathing room on path for data_file_directories.  Maybe lowering the NAME_LENGTH in Schema.java would be appropriate given CASSANDRA-7136?  Do we have a lot of users rolling out 40+ char KS and CF names in general, much less on Windows?', 'We had a lot of people hit issues the first time we lowered the max name lengths in schemas', \"Looks like it was lowered across the board and not on a per-platform basis.  I can see a file-path limitation on linux being a surprise but it's part of the ecosystem people are used to with windows.\", 'bq. Looks like it was lowered across the board and not on a per-platform basis.\\n\\nYeah, nobody *really* needs keyspace names that long; not worth explaining why your linux snapshot broke when you moved to windows if we can avoid it.\\n\\nbq. On snapshots we\\'ll be using 204 chars worst-case (48 KS, 48 CF, *2 each, 9 for \"snapshots\", 3 for slashes) so that doesn\\'t leave us a lot of breathing room on path for data_file_directories.\\n\\nSo...  review this for 3.0?  It\\'s not a ton of code.', \"3.0 seems like the right time frame to me with the other changes we have going in.  This shouldn't be a huge undertaking.\", 'Officially tagging [~joshuamckenzie] as reviewer', \"[~yukim] - could I get a rebase to trunk instead of 2.1-beta2?  There's a variety of file access issues in beta2 (on Windows) that are making testing this a bit of a headache.  I'll continue testing on linux but want to verify on both platforms, though I don't expect any platform-specific differences.\", '[~JoshuaMcKenzie] Attaching patch against trunk.', 'w/len 48 CF and KS and a long username (30 chars), on Windows path length still allows a bit over 50 chars for a secondary index name - should be plenty.\\nFile names look good, tmp names, snapshots, secondary indexes all check out on both linux and windows.\\n\\n+1', 'Committed. Thanks for review.', \"So, we did end up removing the ks and cf name from the sstable filename here right?\\n\\nBecause that breaks at least CQLSSTableWriter (or more precisely, AbstractSSTableSimpleWriter) because it doesn't force the sstable to be in a directory that is name after the KS and CF and breaks the code [here|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/AbstractSSTableSimpleWriter.java#L66-L88] (more precisely the {{desc.cfname.equals(columnFamily)}} test fails because {{desc.cfname}} is wrong).\\n\\nI suppose we could fix that code by removing the test in question as it wouldn't rally break that code, but I have to say that I'm rather uncomfortable with the fact that the filename has nothing identifying the table anymore. It feels way too easy to mistakenly copy sstable for different tables in the same directory and ending up overwriting stuffs we shouldn't. It also feels like we should have a way to find out the ks and table name without relying on where the sstable file is, as that's rather fragile imo. Maybe we should record this in the metadata or something.\"]\n",
            "my_comment: feels pretty error prone. What about keeping the keyspace/table name as now (for the sake of making it easy to not mix sstables by mistake) but limit them to say 10 characters each (just for the file name) truncating the name if necessary?\" bq. limit them to say 10 characters each (just for the file name) truncating the name if necessary?\n",
            "\n",
            "We can truncate name to fit within os path limit adaptively with some calculation.\n",
            "\n",
            "How about completely omit Keyspace name but keep ColumnFamily name and adaptively adjust(truncate) its name? This turns out to be a bit complex than I first thought because secondary index CFs are flushing to the same directory. :(\n",
            "Any ideas? \"Im inclined to say we should leave it alone for 2.1.  If Windows users start yelling for it in 3.0 then we can address then.  Lots of other stuff to work on in the meantime.\" \"Patch attached with unit test.\n",
            "I ended up the following naming convention/directory structure:\n",
            "\n",
            "{code}\n",
            "/var/lib/cassandra/data/ks/cf-a85fc210cb1011e3a15f9d25721dbb44\n",
            "    /.idx\n",
            "        ka-1-Data.db\n",
            "        ...\n",
            "    /snapshots\n",
            "        /my_snapshot\n",
            "            /.idx\n",
            "                ka-1-Data.db\n",
            "                ...\n",
            "            ka-1-Data.db\n",
            "            ...\n",
            "    /backups\n",
            "        /.idx\n",
            "            ka-1-Data.db\n",
            "            ...\n",
            "        ka-1-Data.db\n",
            "        ...\n",
            "    ka-1-Data.db\n",
            "    ...\n",
            "    ks-cf-jb-123-Data.db    (Older version can co exist)\n",
            "    ks-cf.idx-jb-2-Data.db\n",
            "{code}\n",
            "\n",
            "Highlight:\n",
            "\n",
            "* Keyspace/ColumnFamily name is omitted from filename.\n",
            "* If it is temporary file then file name would be tmp-ka-1-Data.db.\n",
            "* Each secondary index has its own SSTable directory whose name starts with .. This is to distinguish from snapshots/backup directories.\n",
            "* Older version of SSTable file can co-exist along with new directory structure.\" \"Still not a fan of removing the keyspace/table name from the filename. Imo we should go with Jonathans suggestion above leave this alone until people actually start yelling cause I dont remember a single user reporting that path lengths was a blocker so Id rather not change the directory layout once again (which might break user scripts) until we have practical evidence its a problem..\" WDYT [~JoshuaMcKenzie]?  How likely is this to be a problem for Windows users? CASSANDRA-4110 and the limitations in Schema.java provide us some protection but there\\s really nothing to stop users nesting their cassandra data 250 characters deep in a path and having things blow up on them regardless of what length we limit ourselves to.\n",
            "\n",
            "On snapshots we\\ll be using 204 chars worst-case (48 KS 48 CF *2 each 9 for \"snapshots\" 3 for slashes) so that doesn\\t leave us a lot of breathing room on path for data_file_directories.  Maybe lowering the NAME_LENGTH in Schema.java would be appropriate given CASSANDRA-7136?  Do we have a lot of users rolling out 40+ char KS and CF names in general much less on Windows? We had a lot of people hit issues the first time we lowered the max name lengths in schemas \"Looks like it was lowered across the board and not on a per-platform basis.  I can see a file-path limitation on linux being a surprise but its part of the ecosystem people are used to with windows.\" bq. Looks like it was lowered across the board and not on a per-platform basis.\n",
            "\n",
            "Yeah nobody *really* needs keyspace names that long; not worth explaining why your linux snapshot broke when you moved to windows if we can avoid it.\n",
            "\n",
            "bq. On snapshots we\\ll be using 204 chars worst-case (48 KS 48 CF *2 each 9 for \"snapshots\" 3 for slashes) so that doesn\\t leave us a lot of breathing room on path for data_file_directories.\n",
            "\n",
            "So...  review this for 3.0?  It\\s not a ton of code. \"3.0 seems like the right time frame to me with the other changes we have going in.  This shouldnt be a huge undertaking.\" Officially tagging [~joshuamckenzie] as reviewer \"[~yukim] - could I get a rebase to trunk instead of 2.1-beta2?  Theres a variety of file access issues in beta2 (on Windows) that are making testing this a bit of a headache.  Ill continue testing on linux but want to verify on both platforms though I dont expect any platform-specific differences.\" [~JoshuaMcKenzie] Attaching patch against trunk. w/len 48 CF and KS and a long username (30 chars) on Windows path length still allows a bit over 50 chars for a secondary index name - should be plenty.\n",
            "File names look good tmp names snapshots secondary indexes all check out on both linux and windows.\n",
            "\n",
            "+1 Committed. Thanks for review. \"So we did end up removing the ks and cf name from the sstable filename here right?\n",
            "\n",
            "Because that breaks at least CQLSSTableWriter (or more precisely AbstractSSTableSimpleWriter) because it doesnt force the sstable to be in a directory that is name after the KS and CF and breaks the code [here|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/AbstractSSTableSimpleWriter.java#L66-L88] (more precisely the {{desc.cfname.equals(columnFamily)}} test fails because {{desc.cfname}} is wrong).\n",
            "\n",
            "I suppose we could fix that code by removing the test in question as it wouldnt rally break that code but I have to say that Im rather uncomfortable with the fact that the filename has nothing identifying the table anymore. It feels way too easy to mistakenly copy sstable for different tables in the same directory and ending up overwriting stuffs we shouldnt. It also feels like we should have a way to find out the ks and table name without relying on where the sstable file is as thats rather fragile imo. Maybe we should record this in the metadata or something.\"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-6972\n",
            "issue_type: Improvement\n",
            "summary: Throw an ERROR when auto_bootstrap: true and bootstrapping node is listed in seeds\n",
            "description: Obviously when this condition exists the node will not bootstrap.  But it is not obvious from the logs why it is not bootstrapping.  Throwing an error would make it obvious and therefore faster to correct.\n",
            "architectural impact: NO\n",
            "comments: ['The catch with doing this is, now everyone has to go around and put autobootstrap: false in their seed configs.', 'Yes, the right fix is to just log it explicitly at info.  We added that back in CASSANDRA-746 but it got undone at some point.', 'Committed basically the same thing from that ticket.  The circle of life is complete.']\n",
            "my_comment: false in their seed configs. Yes the right fix is to just log it explicitly at info.  We added that back in CASSANDRA-746 but it got undone at some point. Committed basically the same thing from that ticket.  The circle of life is complete.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7078\n",
            "issue_type: Improvement\n",
            "summary: Make sstable2json output more readable\n",
            "description: sstable2json writes the entire file as a single JSON line.  also, local timestamp for delete is given as hex bytes instead of an int.\n",
            "architectural impact: NO\n",
            "comments: ['fix attached', 'also renames \"columns\" field in output to \"cells\"', 'I feel like you are missing matching changes to SSTableImport. Can you make SSTableExportTest pass first before I have a look?', 'v2', 'Now SSTableImportTest. v3?', 'v3 also removes \"old\" format support from import (pre-1.0 stuff) and supercolumn support.', 'LGTM, +1.', 'committed']\n",
            "my_comment: fix attached also renames \"columns\" field in output to \"cells\" I feel like you are missing matching changes to SSTableImport. Can you make SSTableExportTest pass first before I have a look? v2 Now SSTableImportTest. v3? v3 also removes \"old\" format support from import (pre-1.0 stuff) and supercolumn support. LGTM +1. committed\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-708\n",
            "issue_type: New Feature\n",
            "summary: Set Cache Capacity via nodeprobe\n",
            "description: CASSANDRA-688 added the ability to set capacity via JMX. Adding this to nodeprobe will be useful for changing a node's capacity on fly.\n",
            "architectural impact: NO\n",
            "comments: [\"03\\n    better division of aggregated key cache capacity among sstable caches\\n\\n02\\n    add cache info to cfstats; add setcachecapacity\\n\\n01\\n    use 0-capacity cache instead of null to indicate no caching; this means we don't need to worry about creating & destroying caches from JMX (and synchronizing on that)\\n\", '+1', 'rebased & committed', \"Integrated in Cassandra #335 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/335/])\\n    better division of aggregated capacity among sstable caches\\npatch by jbellis; reviewed by goffinet for \\nadd cache info to cfstats; add setcachecapacity\\npatch by jbellis; reviewed by goffinet for \\nuse 0-capacity cache instead of null to indicate no caching; this means we don't need to worry about creating & destroying cache objects from JMX\\npatch by jbellis; reviewed by goffinet for \\n\"]\n",
            "my_comment: [\"03\n",
            "    better division of aggregated key cache capacity among sstable caches\n",
            "\n",
            "02\n",
            "    add cache info to cfstats; add setcachecapacity\n",
            "\n",
            "01\n",
            "    use 0-capacity cache instead of null to indicate no caching; this means we dont need to worry about creating & destroying caches from JMX (and synchronizing on that)\n",
            "\" +1 rebased & committed \"Integrated in Cassandra #335 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/335/])\n",
            "    better division of aggregated capacity among sstable caches\n",
            "patch by jbellis; reviewed by goffinet for \n",
            "add cache info to cfstats; add setcachecapacity\n",
            "patch by jbellis; reviewed by goffinet for \n",
            "use 0-capacity cache instead of null to indicate no caching; this means we dont need to worry about creating & destroying cache objects from JMX\n",
            "patch by jbellis; reviewed by goffinet for \n",
            "\"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7107\n",
            "issue_type: Improvement\n",
            "summary: General minor tidying of CollationController path\n",
            "description: There is a lot of unnecessary boiler plate when grabbing an iterator from an in-memory column family. This patch:\n",
            "\n",
            "* Removes FakeCellName\n",
            "* Avoids wrapping a non-OnDiskAtomIterator as an OnDiskAtomIterator except when the wrapping is useful\n",
            "* Removes ColumnSlice.NavigableSetIterator and creates a simpler more direct equivalent in ABTC\n",
            "* Does not construct a SliceIterator in either ABSC or ABTC if only one slice is requested (just returns that slice as an Iterator)\n",
            "* Does not construct multiple list indirections in ABSC when constructing a slice\n",
            "* Shares forward/reverse iterators in ABSC between slices and full-iteration\n",
            "* Avoids O(N) comparisons during collation of results into an ABSC, by using the knowledge that all columns are provided in insertion order from a merge iterator\n",
            "\n",
            "\n",
            "architectural impact: NO\n",
            "comments: [\"Whilst the main point of this is to tidy up, I also find a roughly 15-20% performance bump for the following stress command:\\n\\nbq. cassandra-stress read n=10000000 -key populate=1..100 -col slice n=fixed\\\\(1000\\\\) size=fixed\\\\(1\\\\) -rate threads=50 -mode thrift\\n\\nI've pushed the changes [here|https://github.com/belliottsmith/cassandra/tree/7107]\\n\\n\", 'Have only skimmed it so far. Will have a deep look once the issues behind these two unit tests are fixed (caused by the patch):\\n- org.apache.cassandra.cli.CliTest\\n- org.apache.cassandra.db.ColumnFamilyStoreTest\\n\\nThere are other tests failing, but those fail w/ and w/out the patch, both.', \"Weird. I was sure I'd run all of the unit tests before uploading.\\n\\nTwo line patch uploaded that fixes those two tests.\", \"Overall LGTM, some very welcome cleanup. Nice catch with the empty check in CollationController#collectTimeOrderedData(), too.\\n\\nSquashed commit #1 and #3 together and added to extra commits with suggested changes, pushed to https://github.com/iamaleksey/cassandra/commits/7107\\n\\nThe second original commit is not included b/c I'm very uncomfortable with leaving a ColumnFamily#append() method around - it's going to be abused by some innocent soul, and there will be hard to debug suffering as a result. Inlined it into a new CF#maybeAppendColumn() method instead, that's less likely to be called by mistake (see the last commit).\\n\\nThe second commit has a bugfix, some improvements, nits fixed, some extra cleanup and prettiness:\\n- discovered only one bug - ABSC.CellCollection#size() was not calling maybeSortCells() - fixed in the commit #2\\n- refactored ABSC#slice() and ABTC#slice() for obviousness\\n- made the second binary search in ABSC#slice() use the updated lowerBound, reducing the range to search\\n- in CollationController#collectAllData(), in the memtable loop, using Iterables#transform() instead of copying the cells into a temporary ArrayList, to potentially reduce the amount of localCopy()-ing (esp. for low query LIMITs)\\n- killed off MergeIterator#getSimple(), instead simply using toCollate.get(0) in QF#collateColumns(), avoiding creating a Reducer instance in the trivial case w/ a single source\", \"Slightly worried about a potentiall off by one at https://github.com/iamaleksey/cassandra/blob/7107/src/java/org/apache/cassandra/db/ArrayBackedSortedColumns.java#L526, but the more I stare at it, the more benign it looks to me. Wouldn't mind a second look by a third person at this particular code just to help me sleep better.\", \"LGTM. I don't think there's any off-by-1 error, unless I'm missing something: do you have a specific case you think may be off?\", \"Only the 'invert' case, although both do still seem fine to me. Don't trust myself much ¯\\\\_(ツ)_/¯\", 'Committed, thanks.', \"Well, just to be sure:\\n\\n* We translate a slice to the range \\\\[lb..ub\\\\) - i.e. lb is inclusive; \\n* We search in the range [0..lb) to translate the next slice - lb is exclusive here\\n\\nSo I'm pretty sure it's fine - lb always gets returned in the first iterator, and is never searched for building the following iterator, but is the exact boundary at which we start our search.\"]\n",
            "my_comment: do you have a specific case you think may be off?\" \"Only the invert case although both do still seem fine to me. Dont trust myself much ¯\\\\_(ツ)_/¯\" Committed thanks. \"Well just to be sure:\n",
            "\n",
            "* We translate a slice to the range \\\\[lb..ub\\\\) - i.e. lb is inclusive; \n",
            "* We search in the range [0..lb) to translate the next slice - lb is exclusive here\n",
            "\n",
            "So Im pretty sure its fine - lb always gets returned in the first iterator and is never searched for building the following iterator but is the exact boundary at which we start our search.\"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7279\n",
            "issue_type: Bug\n",
            "summary: MultiSliceTest.test_with_overlap* unit tests failing in trunk\n",
            "description: Example:\n",
            "https://cassci.datastax.com/job/trunk_utest/623/testReport/org.apache.cassandra.thrift/MultiSliceTest/\n",
            "architectural impact: NO\n",
            "comments: ['MultiSliceTest was added in commit 60fb923 where it passes.\\n{{git bisect start trunk 60fb923018a6fd2dabf04a1d4500f7b29a23a6f1}} gets me f31f689 as the first bad commit, which is the merge commit for CASSANDRA-6689', 'So this is only failing in trunk not 2.1 branch?\\n', 'the test only exists in trunk', \"This is probably my mess in that case, so I'll take a look\", \"I'm already debugging... pretty sure this is CASSANDRA-5086\", 'OK, knock yourself out :)', 'So the issue is the ABSC is blindly adding the overlapping slices so it ends up with dups.  \\n\\nWe can  fix this by:\\n  - cropping the slices so none of them intersect.\\n  - changing ABSC.maybeAppendColumn to use addColumn vs addInternal (so it checks the previous value)\\n\\nI prefer the first but there would be a number of places to change if we did it at the iterator level.  Since this is only exposed in thrift the simplest place would be there.', 'Actually it can be done in SliceQueryFilter constructor', \"Do you mean ABTC? Pretty sure ABSC treats it correctly; probably when slicing we should use the max of (previous slice end) and (new slice start) for the bound of the new iterator. I'd kind of like to fix this in a similar way to how we deal with it in ABSC though, although it would be more work (so probably best left until later); in trunk we could use the new SearchIterator (with some small tweaks) to solve the problem by always using the current position as a lower bound for the search for the next lower bound.\", \"Digging into the history of this more.  Origionally Multiple slice ranges didn't allow overlapping ranges CASSANDRA-3855 but that restriction was removed in CASSANDRA-5573\\n\\n\\n\", \"No It's in ABSC, I think it was broken in CASSANDRA-7107 since that stopped using addCell and instead just appends the cell to the array.  I think fixing it in slices so they don't overlap is the right approach.\", \"Well, there's lots of places you could consider the bug to be; I meant that ABTC listens to overlapping slices and returns the overlap, whereas ABSC doesn't, and iirc the on-disk indexed slice iterator also doesn't, so if ABTC didn't also we'd be safe again. But fixing it in the Slice def is as good a solution and more permanent in the face of more slice iterator sources. We could even simplify ABSC's slice iterators in that case as well.\", 'Attached patch to sort and crop overlapping slices.  I also had to change AtomicBtreeColumns to deal with excluding a point when the start of the finish of the previous slice == the start of the next slice ', \"Applied the patch to trunk and tested out the modified tests (haven't run a full 'ant test') - MultiSliceTest, QueryPagerTest, RangeTombstoneTest pass, but ColumnFamilyStoreTest failed with:\\n{noformat}\\ntest:\\n     [echo] running unit tests\\n    [junit] WARNING: multiple versions of ant detected in path for junit \\n    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class\\n    [junit]      and jar:file:/home/mshuler/git/cassandra/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class\\n    [junit] Testsuite: org.apache.cassandra.db.ColumnFamilyStoreTest\\n    [junit] Tests run: 35, Failures: 8, Errors: 0, Skipped: 0, Time elapsed: 17.085 sec\\n    [junit] \\n    [junit] ------------- Standard Output ---------------\\n    [junit] ERROR 20:07:21 Unable to delete build/test/cassandra/data/Keyspace1/Indexed2-a7e63420e1ec11e3a5c09b2001e5c823/Keyspace1-Indexed2.birthdate_index-ka-1-Data.db (it will be removed on server restart; we'll also retry after GC)\\n    [junit] ERROR 20:07:21 Unable to delete build/test/cassandra/data/Keyspace1/Indexed2-a7e63420e1ec11e3a5c09b2001e5c823/Keyspace1-Indexed2.birthdate_index-ka-1-Data.db (it will be removed on server restart; we'll also retry after GC)\\n    [junit] ERROR 20:07:23 Missing component: build/test/cassandra/data/Keyspace1/Standard3-a7e60d12e1ec11e3a5c09b2001e5c823/Keyspace1-Standard3-ka-1-Summary.db\\n    [junit] ERROR 20:07:23 Missing component: build/test/cassandra/data/Keyspace1/Standard3-a7e60d12e1ec11e3a5c09b2001e5c823/Keyspace1-Standard3-ka-1-Summary.db\\n    [junit] ERROR 20:07:23 Missing component: build/test/cassandra/data/Keyspace1/Standard4-a7e60d13e1ec11e3a5c09b2001e5c823/Keyspace1-Standard4-ka-3-Summary.db\\n    [junit] ERROR 20:07:23 Missing component: build/test/cassandra/data/Keyspace1/Standard4-a7e60d13e1ec11e3a5c09b2001e5c823/Keyspace1-Standard4-ka-3-Summary.db\\n    [junit] ------------- ---------------- ---------------\\n    [junit] Testcase: testMultiRangeSomeEmptyNoIndex(org.apache.cassandra.db.ColumnFamilyStoreTest):    FAILED\\n    [junit] Columns did not match. Expected: [colI, colD, colC, colA] but got:[]\\n    [junit] junit.framework.AssertionFailedError: Columns did not match. Expected: [colI, colD, colC, colA] but got:[]\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.findRowGetSlicesAndAssertColsFound(ColumnFamilyStoreTest.java:2101)\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testMultiRangeSomeEmptyNoIndex(ColumnFamilyStoreTest.java:1419)\\n    [junit] \\n    [junit] \\n    [junit] Testcase: testMultiRangeSomeEmptyIndexed(org.apache.cassandra.db.ColumnFamilyStoreTest):    FAILED\\n    [junit] Columns did not match. Expected: [colI, colD, colC, colA] but got:[colD, colC]\\n    [junit] junit.framework.AssertionFailedError: Columns did not match. Expected: [colI, colD, colC, colA] but got:[colD, colC]\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.findRowGetSlicesAndAssertColsFound(ColumnFamilyStoreTest.java:2101)\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testMultiRangeSomeEmptyIndexed(ColumnFamilyStoreTest.java:1468)\\n    [junit] \\n    [junit] \\n    [junit] Testcase: testMultiRangeContiguousNoIndex(org.apache.cassandra.db.ColumnFamilyStoreTest):   FAILED\\n    [junit] Columns did not match. Expected: [colI, colG, colF, colE, colD, colC, colA] but got:[]\\n    [junit] junit.framework.AssertionFailedError: Columns did not match. Expected: [colI, colG, colF, colE, colD, colC, colA] but got:[]\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.findRowGetSlicesAndAssertColsFound(ColumnFamilyStoreTest.java:2101)\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testMultiRangeContiguousNoIndex(ColumnFamilyStoreTest.java:1517)\\n    [junit] \\n    [junit] \\n    [junit] Testcase: testMultiRangeContiguousIndexed(org.apache.cassandra.db.ColumnFamilyStoreTest):   FAILED\\n    [junit] Columns did not match. Expected: [colI, colG, colF, colE, colD, colC, colA] but got:[colG, colF, colE, colD, colC]\\n    [junit] junit.framework.AssertionFailedError: Columns did not match. Expected: [colI, colG, colF, colE, colD, colC, colA] but got:[colG, colF, colE, colD, colC]\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.findRowGetSlicesAndAssertColsFound(ColumnFamilyStoreTest.java:2101)\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testMultiRangeContiguousIndexed(ColumnFamilyStoreTest.java:1567)\\n    [junit] \\n    [junit] \\n    [junit] Testcase: testMultiRangeIndexed(org.apache.cassandra.db.ColumnFamilyStoreTest):     FAILED\\n    [junit] Columns did not match. Expected: [colI, colG, colE, colD, colC, colA] but got:[colG, colE, colD, colC]\\n    [junit] junit.framework.AssertionFailedError: Columns did not match. Expected: [colI, colG, colE, colD, colC, colA] but got:[colG, colE, colD, colC]\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.findRowGetSlicesAndAssertColsFound(ColumnFamilyStoreTest.java:2101)\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testMultiRangeIndexed(ColumnFamilyStoreTest.java:1628)\\n    [junit] \\n    [junit] \\n    [junit] Testcase: testMultipleRangesSlicesNoIndexedColumns(org.apache.cassandra.db.ColumnFamilyStoreTest):  FAILED\\n    [junit] Columns did not match. Expected: [cola] but got:[]\\n    [junit] junit.framework.AssertionFailedError: Columns did not match. Expected: [cola] but got:[]\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.findRowGetSlicesAndAssertColsFound(ColumnFamilyStoreTest.java:2101)\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testMultiRangeSlicesBehavior(ColumnFamilyStoreTest.java:1965)\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testMultipleRangesSlicesNoIndexedColumns(ColumnFamilyStoreTest.java:1637)\\n    [junit] \\n    [junit] \\n    [junit] Testcase: testMultipleRangesSlicesWithIndexedColumns(org.apache.cassandra.db.ColumnFamilyStoreTest):        FAILED\\n    [junit] Columns did not match. Expected: [cola] but got:[]\\n    [junit] junit.framework.AssertionFailedError: Columns did not match. Expected: [cola] but got:[]\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.findRowGetSlicesAndAssertColsFound(ColumnFamilyStoreTest.java:2101)\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testMultiRangeSlicesBehavior(ColumnFamilyStoreTest.java:1965)\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testMultipleRangesSlicesWithIndexedColumns(ColumnFamilyStoreTest.java:1644)\\n    [junit] \\n    [junit] \\n    [junit] Testcase: testMultipleRangesSlicesInMemory(org.apache.cassandra.db.ColumnFamilyStoreTest):  FAILED\\n    [junit] Columns did not match. Expected: [cola] but got:[]\\n    [junit] junit.framework.AssertionFailedError: Columns did not match. Expected: [cola] but got:[]\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.findRowGetSlicesAndAssertColsFound(ColumnFamilyStoreTest.java:2101)\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testMultiRangeSlicesBehavior(ColumnFamilyStoreTest.java:1965)\\n    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testMultipleRangesSlicesInMemory(ColumnFamilyStoreTest.java:1651)\\n    [junit] \\n    [junit] \\n    [junit] Test org.apache.cassandra.db.ColumnFamilyStoreTest FAILED\\n{noformat}\\n\\nI backed the patch out, so HEAD is now at e5ab470, and ColumnFamilyStoreTest passes ok\", 'Were you just testing this? All the Slices are in one direction but the order should not matter. \\n{code}\\n-        req.setColumn_slices(Arrays.asList(columnSliceFrom(\"e\", \"a\"), columnSliceFrom(\"g\", \"d\")));\\n+        req.setColumn_slices(Arrays.asList( columnSliceFrom(\"g\", \"d\"), columnSliceFrom(\"e\", \"a\")));\\n{code}\\n\\nGood call. This assert was backwards\\n{code}\\n     private static void assertColumnNameMatches(List<String> expected , List<ColumnOrSuperColumn> actual)\\n     {\\n-        Assert.assertEquals(actual+\" \"+expected +\" did not have same number of elements\", actual.size(), expected.size());\\n+        Assert.assertEquals(actual+\" \"+expected +\" did not have same number of elements\", expected.size(), actual.size());\\n         for (int i = 0 ; i< expected.size() ; i++)\\n         {\\n-            Assert.assertEquals(actual.get(i) +\" did not equal \"+ expected.get(i), \\n-                    new String(actual.get(i).getColumn().getName()), expected.get(i));\\n+            Assert.assertEquals(actual.get(i) +\" did not equal \"+ expected.get(i), expected.get(i),\\n+                    new String(actual.get(i).getColumn().getName()));\\n         }\\n     }\\n{code}', \"I linked two issues I opened up months ago which are also this issue. \\n{quote}\\nchanging ABSC.maybeAppendColumn to use addColumn vs addInternal (so it checks the previous value)\\n{quote}\\nNot to be critical, but I think ABSC is hard to groc. As an nube without context it is very hard to me understand what it's purpose is and how it is supposed to behave without more Java doc and direct testing. For a speciality data structure the intent is lost without more documentation. Suggestion: maybe we can beef up the javadoc in the patch?\", 'v2 fixes the ColumnFamilyStoreTests', 'v2 looks good to me for the unit tests', 'Two questions:\\n\\nIn removeOverlappingSlices, what is the rational for not doing anything when we have no comparator? Is there ever a time when we do not have a comparator? I would assume no comparator BytesType. \\n{code}\\n if (comparator == null || slices.length <= 1)\\n+            return slices;\\n{code}\\n\\nThis is a test with bad input:\\n{code}\\n    @Test\\n    public void testBadStuff() throws CharacterCodingException\\n    {\\n      ColumnSlice aToE = new ColumnSlice(cellname(\"a\"), cellname(\"e\"));\\n      ColumnSlice dToG = new ColumnSlice(cellname(\"g\"), cellname(\"d\"));\\n      CellNameType c = new SimpleDenseCellNameType(UTF8Type.instance);\\n      ColumnSlice [] result =   SliceQueryFilter.removeOverlappingSlices(new ColumnSlice[]{ dToG, aToE }, true, c );\\n      Assert.assertEquals(1, result.length);\\n      Assert.assertEquals(\"g\", ByteBufferUtil.string(result[0].start.get(0)));\\n      Assert.assertEquals(\"a\", ByteBufferUtil.string(result[0].finish.get(0)));\\n    }\\n{code}\\n\\njunit.framework.ComparisonFailure: null expected:<[a]> but was:<[d]>\\n\\nMaybe it is redundant checking, but should we explode more violently in these cases? Like the case above is it better to fail hard then return something undefined?', 'Patch that adds some direct testing, and a test that returns bad output but maybe should explode?', 'I think I found an edge case. Shouldn\\'t the all slice from \"\" to \"\" absorb \"d\" to \"g\" ?\\n\\n{code}\\n  @Test\\n    public void testWithEmpty() throws CharacterCodingException\\n    {\\n      ColumnSlice aToE = new ColumnSlice(cellname(\"\"), cellname(\"\"));\\n      ColumnSlice dToG = new ColumnSlice(cellname(\"d\"), cellname(\"g\"));\\n      CellNameType c = new SimpleDenseCellNameType(UTF8Type.instance);\\n      \\n      ColumnSlice [] result =   SliceQueryFilter.removeOverlappingSlices(new ColumnSlice[]{ aToE, dToG }, false, c );\\n      Assert.assertEquals(1, result.length);\\n      Assert.assertEquals(\"\", ByteBufferUtil.string(result[0].start.get(0)));\\n      Assert.assertEquals(\"\", ByteBufferUtil.string(result[0].finish.get(0)));\\n    }\\n{code}\\n\\njunit.framework.AssertionFailedError: expected:<1> but was:<2>\\n\\n', 'bq. what is the rational for not doing anything when we have no comparator?\\n\\nThis is for the case when a replica deserializes the slice over the wire internally (from the coordinators request), It\\'s redundant to check in this case since the coordinator has already done this check.\\n\\nbq. Maybe it is redundant checking, but should we explode more violently in these cases?\\n\\nIf you run with -ea you get an assertion error.   \\n\\nbq. I think I found an edge case. Shouldn\\'t the all slice from \"\" to \"\" absorb \"d\" to \"g\" ?\\n\\ncellname(\"\") get\\'s you another AssertionError.  You should be using Composites.EMPTY with that it does the right thing.  These things are checked on input', 'Composite.empty who knew? It does work as expected. Tx.', 'Added some light documentation that explains some of the nuances of how the overlapping thing works.', \"We've always assumed it was the job of whomever built the slices (so mainly the thrift code and the CQL3 code) to de-overlap (and make sure things were in the right order). Wouldn't it be cleaner to keep it that way, i.e. to push the deoverlapping in places of code that might generate overlapping slices (which part of the code is doing that btw?), rather that pass the comparator every time but having it null half of the time?\", \"Given the fact that it used to be validated but was subsequently removed in CASSANDRA-5573 I think it needs to be centralized here to keep things simple.  We can move the pre-processing to a different location vs the constructor but I don't see an issue with keeping it as is.\", \"bq. Given the fact that it used to be validated but was subsequently removed in CASSANDRA-5573\\n\\nCould you expand on what exactly in CASSANDRA-5573 removed a previous validation of this? I'm not totally sure I follow.\", \"\\nfrom that commit:\\n\\nbq. you can kill both ColumnSlice.validate() overloads (only used by QP.validateSLiceFilter) altogether if you ditch QueryProcessor.validateSliceFilter() (unused).\\n\\nWe used to not allow overlapping ranges but, 5573 removed that restriction.  Ed's tests exposed that it wasn't properly handled.  \\n\", \"But wait. What I'm saying is that the CQL3 code never generate overlapping ranges. If it does, it's a bug that needs to be fixed but that I'm not aware of.\\n\\nSo that this call to removeOverlappingSlices is doing work that, at least for CQL3, is wasting good cpu cycles. If we're freaked out that the CQL3 might create overlapping slices by mistake in the future, I guess I'm not opposed to re-adding validation of the slices (preferably in an assert), but validating the slices is much simpler than what removeOverlappingSlices does (it doesn't have to allocate a Comparator object or sort an array that is already sorted).\\n\\nAnd if thrift is the only place that accepts overlapping slices and does de-ovelapping silently, then it's only there that we should deoverlap.\", \"{quote}\\nBut wait. What I'm saying is that the CQL3 code never generate overlapping ranges. If it does, it's a bug that needs to be fixed but that I'm not aware of.\\n{quote}\\n\\nDoes any CQL code generate multiple ranges at all? It could be a bug not yet hit by CQL because it is unable to leverage this feature at the current moment but in the near future might?\\n\", \"CASSANDRA-4762 would add this to cql. Is the CPU cost really that high here? It's only run once per query\", 'Solve the problem in one place. For CQL the code path is skipped in all cases currently. When 4762 is added we talking about sorting a list of a few slices when the feature is active.  \\n\\nI like where Jake has it. Put the logic closer to the storage engine, do not count multiple components thrift/cql to have to do this separately. As this ticket shows it is easy to miss the bug under the assumption that something understands some implicit contract.', 'bq. Does any CQL code generate multiple ranges at all?\\n\\nYes it does.\\n\\nbq. CASSANDRA-4762 would add this to cql\\n\\nI\\'m pretty sure CASSANDRA-4762 can be done so that slices are generated non-overlapping by construction. And if it turns out that it\\'s really much more convenient to generate overlapping slices and de-overlap after the fact, then fine, we can call the deoverlapping code then. But that\\'s still not a reason to call it everytime when there is many cases where we know it\\'s useless.\\n\\nbq. Is the CPU cost really that high here? It\\'s only run once per query\\n\\nBut if it\\'s useless, it\\'s kind of too costly, isn\\'t it ? What is the downsides of doing the de-overlapping only when we need to and just adding a check in an assertion otherwise? I doubt it\\'ll be much more code if any. I\\'m fine providing a patch doing that if it\\'s just a matter of \"you\\'d rather not spend more time on this\" (which I respect).', 'I do not want to bikeshed over performance of implementation. I only care that it works, produces the same result across versions, and coverage is in place so that it continues working. ', 'bq. What is the downsides of doing the de-overlapping only when we need to and just adding a check in an assertion otherwise?\\n\\nHow do you know \"when you need to\"?  You  only need to check this when there are more than one slice is passed in.  This de-overlapping only happens if > 1 slice is passed in.  If you have something clever in mind then feel free to code it up.  In the end I thing the amount of cpu time will be the same.', 'Also take into account that the normal case is a user would likely send 2-3 slices, in order, without overlaps. Not sure of the sorting algo but in that case it might not be many passes through the list.', 'One other thing. cassandra.server has a similiar notion of a ColumnSlice so we should be able to transport this logic to thrift nd let cql handle this differently. \\n\\nThough it feels lile in the future cql could benefit from storage layer de duping like selecting parts of collections that may have overlaps or something.\\n\\n', \"The problem as I see it, is the bulk of the testing was in ColumnFamilyStoreTest, so the other collaborating components are not clear with pre/post conditions. Since none of the collaborators are defined as who handles the de duplication it is a very open ended discussion. Review this table:\\n\\n||Component||Accepts overlaping ranges?||Action||\\n|Cassandra.Server multiSliceRequest|Yes|Blindly pass Along|\\n|SliceQueryFilter (with Jake's Patch)|Yes|Deduplicate and de-overlap|\\n|ColumnFamilyStore|Yes| Never receives an overlapped slice |\\n\\nSuggestion. Construct a similar table for the new current and/or new CQL paths. Use that determine where the overlapping check should be. Go back and define in each piece of the API what they do and do not accept.\\n\\n\\n\\n\\n\", 'Attaching a patch for how I would do this. It only deoverlap/sort slices for thrift, but does validate (in an assertion) the slices for CQL \"to avoid whatever future bug everyone seems to assume we\\'ll introduce\". I\\'ll note that said validation is more thorough than what the de-overlapping method would protect against since it validates that each slices in the proper order (while the de-overlapping method would have silently do the wrong thing in that case, potentially hiding a bug deeper rather than protecting against one).\\n\\nThe patch also fix 2 bugs in the handling of the empty finish bounds by the de-overlapping function: 1) in the initial sorting, it is the finish bound that should be special cased, the start bound is automatically handled by the comparator, and 2) later the finish bound also needs special casing when testing for inclusion. Both are covered by additional unit tests. ', \"I'd be comfortable with just asserting (always, regardless of if assertions are enabled) on the thrift path to keep the patch simple. Multi slices are a new thing to thrift world, so constraining them sensibly (to inputs we don't have to massage to make sense) seems reasonable to me. The only possible point of contention would be two ranges with equal end/starts, which we would reject but which are easy to understand what should be meant. I don't think they're a severe casualty though.\\n\\nIt'd be nice to take the opportunity to simultaneously clean up the ABSC code to no longer enforce this assumption while we're imposing it elsewhere.\\n\\nAlso, there's at least one spot where the constructor can be called that isn't covered by [~slebresne]'s patch, so I'd suggest either moving the assert into the constructor, or creating a static method for construction that requires stipulating if the assert is always enforced (thrift), or only if assertions are enabled. I'm a little concerned that we can easily introduce new code paths that use them incorrectly but that won't be covered by any assertions as it stands.\", \"bq. there's at least one spot where the constructor can be called that isn't covered by Sylvain Lebresne's patch\\n\\nThere is more than one, but the spots I didn't add the assertion to where the ones where it was easy to verify that things were ok by construction. But I'm fine putting the assertion everywhere is we're really all that freaked out by bug there though (it just happen I don't share that fear). \", 'Looks good.\\n\\nThe actual result is still different from the original result, but this a result of merging the slices pre-count.\\n{code}\\nsetCount(6)\\n....\\n       req.setColumn_slices(Arrays.asList(columnSliceFrom(\"e\", \"a\"), columnSliceFrom(\"g\", \"d\")));\\n-        assertColumnNameMatches(Arrays.asList(\"g\", \"e\", \"d\", \"c\", \"b\", \"a\"), server.get_multi_slice(req)); \\n+        assertColumnNameMatches(Arrays.asList(\"g\", \"f\", \"e\", \"d\", \"c\", \"b\"), server.get_multi_slice(req));\\n     }\\n{code}\\n\\nEven though in some cases we just changed the test to match the results. The code written does make more sense in the long run.', \"I'm neutral on the asserts, but if we're adding them I think we should be consistent, especially as they should be computationally cheap even with them enabled (and free when disabled). The CQL3CasConditions didn't look (at-a-glance-)trivial to me that it would definitely produce safe slices. \", \"Ok +1 I'll backport this to 2.1 then commit. Thanks.\", \"bq. The CQL3CasConditions didn't look (at-a-glance-)trivial to me that it would definitely produce safe slices.\\n\\nIt's creating the slices by using the {{slice()}} method on composites coming from a map sorted by the comparator. But I'm happy with adding the assertion there (I suppose Jake can just chime that in his backport).\", 'Looks like some tests in org.apache.cassandra.cql3.MultiColumnRelationTest started failing due to assertion check', 'The problem (with MultiColumnRelationTest) is that the check in ColumnSlice is assuming that an empty value will still sort as a lower bound even for a reverse comparator as this is what has always been done by {{AbstractType.reverseComparator}}. However, somehow, CASSANDRA-5417 didn\\'t seem to have carried that over to {{AbstractCType.reverseComparator()}}. So anyway, attaching a v2 that simply add \"proper\" handling of empty to {{AbstractCType.reverseComparator()}}. As far as I can tell that breaks no other tests. Note that this part won\\'t be needed in 2.0 backport since {{AbstracType}} does the right thing there.\\n\\nv2 also adds the assert in CQL3CasConditions while at it.\\n', 'Committed thanks']\n",
            "my_comment: 1) in the initial sorting it is the finish bound that should be special cased the start bound is automatically handled by the comparator and 2) later the finish bound also needs special casing when testing for inclusion. Both are covered by additional unit tests.  \"Id be comfortable with just asserting (always regardless of if assertions are enabled) on the thrift path to keep the patch simple. Multi slices are a new thing to thrift world so constraining them sensibly (to inputs we dont have to massage to make sense) seems reasonable to me. The only possible point of contention would be two ranges with equal end/starts which we would reject but which are easy to understand what should be meant. I dont think theyre a severe casualty though.\n",
            "\n",
            "Itd be nice to take the opportunity to simultaneously clean up the ABSC code to no longer enforce this assumption while were imposing it elsewhere.\n",
            "\n",
            "Also theres at least one spot where the constructor can be called that isnt covered by [~slebresne]s patch so Id suggest either moving the assert into the constructor or creating a static method for construction that requires stipulating if the assert is always enforced (thrift) or only if assertions are enabled. Im a little concerned that we can easily introduce new code paths that use them incorrectly but that wont be covered by any assertions as it stands.\" \"bq. theres at least one spot where the constructor can be called that isnt covered by Sylvain Lebresnes patch\n",
            "\n",
            "There is more than one but the spots I didnt add the assertion to where the ones where it was easy to verify that things were ok by construction. But Im fine putting the assertion everywhere is were really all that freaked out by bug there though (it just happen I dont share that fear). \" Looks good.\n",
            "\n",
            "The actual result is still different from the original result but this a result of merging the slices pre-count.\n",
            "{code}\n",
            "setCount(6)\n",
            "....\n",
            "       req.setColumn_slices(Arrays.asList(columnSliceFrom(\"e\" \"a\") columnSliceFrom(\"g\" \"d\")));\n",
            "-        assertColumnNameMatches(Arrays.asList(\"g\" \"e\" \"d\" \"c\" \"b\" \"a\") server.get_multi_slice(req)); \n",
            "+        assertColumnNameMatches(Arrays.asList(\"g\" \"f\" \"e\" \"d\" \"c\" \"b\") server.get_multi_slice(req));\n",
            "     }\n",
            "{code}\n",
            "\n",
            "Even though in some cases we just changed the test to match the results. The code written does make more sense in the long run. \"Im neutral on the asserts but if were adding them I think we should be consistent especially as they should be computationally cheap even with them enabled (and free when disabled). The CQL3CasConditions didnt look (at-a-glance-)trivial to me that it would definitely produce safe slices. \" \"Ok +1 Ill backport this to 2.1 then commit. Thanks.\" \"bq. The CQL3CasConditions didnt look (at-a-glance-)trivial to me that it would definitely produce safe slices.\n",
            "\n",
            "Its creating the slices by using the {{slice()}} method on composites coming from a map sorted by the comparator. But Im happy with adding the assertion there (I suppose Jake can just chime that in his backport).\" Looks like some tests in org.apache.cassandra.cql3.MultiColumnRelationTest started failing due to assertion check The problem (with MultiColumnRelationTest) is that the check in ColumnSlice is assuming that an empty value will still sort as a lower bound even for a reverse comparator as this is what has always been done by {{AbstractType.reverseComparator}}. However somehow CASSANDRA-5417 didn\\t seem to have carried that over to {{AbstractCType.reverseComparator()}}. So anyway attaching a v2 that simply add \"proper\" handling of empty to {{AbstractCType.reverseComparator()}}. As far as I can tell that breaks no other tests. Note that this part won\\t be needed in 2.0 backport since {{AbstracType}} does the right thing there.\n",
            "\n",
            "v2 also adds the assert in CQL3CasConditions while at it.\n",
            " Committed thanks\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7364\n",
            "issue_type: Bug\n",
            "summary: assert error in StorageProxy.submitHint\n",
            "description: in 2.1-rc1. assert error and hector based client ends with all nodes down message (its single node cluster). I assume that client connection got closed.\n",
            "\n",
            "INFO  18:28:33 Compacting [SSTableReader(path='c:\\cassandra-2.1\\data\\test\\sipdb-\n",
            "58f51090ee6511e3815625991ef2b954\\test-sipdb-ka-3-Data.db'), SSTableReader(path='\n",
            "c:\\cassandra-2.1\\data\\test\\sipdb-58f51090ee6511e3815625991ef2b954\\test-sipdb-ka-\n",
            "1-Data.db'), SSTableReader(path='c:\\cassandra-2.1\\data\\test\\sipdb-58f51090ee6511\n",
            "e3815625991ef2b954\\test-sipdb-ka-4-Data.db'), SSTableReader(path='c:\\cassandra-2\n",
            ".1\\data\\test\\sipdb-58f51090ee6511e3815625991ef2b954\\test-sipdb-ka-2-Data.db'), S\n",
            "STableReader(path='c:\\cassandra-2.1\\data\\test\\sipdb-58f51090ee6511e3815625991ef2\n",
            "b954\\test-sipdb-ka-6-Data.db')]\n",
            "ERROR 18:29:50 Exception in thread Thread[Thrift:16,5,main]\n",
            "java.lang.AssertionError: localhost/127.0.0.1\n",
            "        at org.apache.cassandra.service.StorageProxy.submitHint(StorageProxy.jav\n",
            "a:870) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]\n",
            "        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:49\n",
            "3) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]\n",
            "        at org.apache.cassandra.service.StorageProxy.mutateWithTriggers(StorageP\n",
            "roxy.java:537) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]\n",
            "        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.\n",
            "java:1095) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]\n",
            "        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.\n",
            "java:1077) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]\n",
            "        at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraSer\n",
            "ver.java:970) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]\n",
            "        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.getResul\n",
            "t(Cassandra.java:3996) ~[apache-cassandra-thrift-2.1.0-rc1.jar:2.1.0-rc1]\n",
            "        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.getResul\n",
            "t(Cassandra.java:3980) ~[apache-cassandra-thrift-2.1.0-rc1.jar:2.1.0-rc1]\n",
            "        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[\n",
            "libthrift-0.9.1.jar:0.9.1]\n",
            "        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[li\n",
            "bthrift-0.9.1.jar:0.9.1]\n",
            "        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run\n",
            "(CustomTThreadPoolServer.java:201) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]\n",
            "        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.\n",
            "java:1145) ~[na:1.7.0_60]\n",
            "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor\n",
            ".java:615) ~[na:1.7.0_60]\n",
            "        at java.lang.Thread.run(Thread.java:745) ~[na:1.7.0_60]\n",
            "INFO  18:29:55 1 MUTATION messages dropped in last 5000ms\n",
            "architectural impact: NO\n",
            "comments: ['The bug only affects WTE handling w/ CL.ANY.', \"bikeshedding to keep the equality check out of submitHint -- worried that it could hide bugs silently.  Prefer to force callers to think about what they're doing.\", \"Not sure if I agree, but, it's trivial, so committed anyway. Thanks.\"]\n",
            "my_comment: The bug only affects WTE handling w/ CL.ANY. \"bikeshedding to keep the equality check out of submitHint -- worried that it could hide bugs silently.  Prefer to force callers to think about what theyre doing.\" \"Not sure if I agree but its trivial so committed anyway. Thanks.\"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7496\n",
            "issue_type: Bug\n",
            "summary: ClassCastException in MessagingService\n",
            "description: Got the following exception when running repair on a 3 node ccm cluster\n",
            "\n",
            "{code}\n",
            "ERROR [EXPIRING-MAP-REAPER:1] 2014-07-03 21:24:33,063 CassandraDaemon.java:166 - Exception in thread Thread[EXPIRING-MAP-REAPER:1,5,main]\n",
            "java.lang.ClassCastException: org.apache.cassandra.net.CallbackInfo cannot be cast to org.apache.cassandra.net.WriteCallbackInfo\n",
            "\tat org.apache.cassandra.net.MessagingService$5.apply(MessagingService.java:352) ~[main/:na]\n",
            "\tat org.apache.cassandra.net.MessagingService$5.apply(MessagingService.java:335) ~[main/:na]\n",
            "\tat org.apache.cassandra.utils.ExpiringMap$1.run(ExpiringMap.java:98) ~[main/:na]\n",
            "\tat org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:75) ~[main/:na]\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_60]\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304) [na:1.7.0_60]\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178) [na:1.7.0_60]\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [na:1.7.0_60]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_60]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_60]\n",
            "\tat java.lang.Thread.run(Thread.java:745) [na:1.7.0_60]\n",
            "{code}\n",
            "\n",
            "Looks like that block (MessagingService, li. 352), was changed with CASSANDRA-7245.\n",
            "\n",
            "While I produced this on trunk, I compared the MS code on trunk with 2.1.0 and it is  the same. This is the change that 7245 introduced:\n",
            "\n",
            "pre-7245\n",
            "{code}\n",
            "                if (expiredCallbackInfo.shouldHint())\n",
            "                {\n",
            "                    Mutation mutation = (Mutation) ((WriteCallbackInfo) expiredCallbackInfo).sentMessage.payload;\n",
            "                    return StorageProxy.submitHint(mutation, expiredCallbackInfo.target, null);\n",
            "                }\n",
            "{code}\n",
            "\n",
            "7245:\n",
            "{code}\n",
            "                Mutation mutation = (Mutation) ((WriteCallbackInfo) expiredCallbackInfo).sentMessage.payload;\n",
            "\n",
            "                try\n",
            "                {\n",
            "                    if (expiredCallbackInfo.shouldHint())\n",
            "                    {\n",
            "                        return StorageProxy.submitHint(mutation, expiredCallbackInfo.target, null);\n",
            "                    }\n",
            "                }\n",
            "                finally\n",
            "                {\n",
            "                    //We serialized a hint so we don't need this mutation anymore\n",
            "                    mutation.release();\n",
            "                }\n",
            "{code}\n",
            "\n",
            "architectural impact: NO\n",
            "comments: [\"Looks like the cast to WCI should be moved back after the shouldHint() check as CallbackInfo.shouldHint() is hardcoded to return false, thus it could never downcast anyway.\\n\\nSo probably this will fix it (move the cast after the shouldHint() check):\\n{code}\\n        if (expiredCallbackInfo.shouldHint())\\n        {\\n            try\\n            {\\n                Mutation mutation = (Mutation) ((WriteCallbackInfo) expiredCallbackInfo).sentMessage.payload;\\n                return StorageProxy.submitHint(mutation, expiredCallbackInfo.target, null);\\n            }\\n            finally\\n            {\\n                //We serialized a hint so we don't need this mutation anymore\\n                mutation.release();\\n            }\\n        }\\n{code} \\n\", \"Patch with your fix attached.  Can you describe your ccm scenario that caused this?  I'd like to add a dtest for this.\", \"Committed so we can roll rc3.\\n\\nIf you want to follow up with a test, it looks to me like this will happen 100% of the time when we have a callback on a non-write, expired message.  (Not actually sure what this includes -- we have lots of non-write messages with callbacks, but most aren't allowed to expire.  Clearly there must be some.)\", \"[~tjake] I produced this by building a 3 node cluster in ccm. Then I ran 'stress write' to get some data into the cluster (with RF=2), calling flush every tens seconds so I could monitor the amount of data per node (didn't need much data, just something to play with). Then I ran \\n{code}ccm node1 nodetool repair Keyspace1 Standard1{code}\\n, node1 would get the exception in under 30 seconds (usually occurring a few times during the span over the repair). \\n\\nAs I was testing out some changes I'm working on, I don't believe the cause of the messages timing out during repair is a problem in 2.1 (just my own bugs to work out on my branch).\"]\n",
            "my_comment: [\"Looks like the cast to WCI should be moved back after the shouldHint() check as CallbackInfo.shouldHint() is hardcoded to return false thus it could never downcast anyway.\n",
            "\n",
            "So probably this will fix it (move the cast after the shouldHint() check):\n",
            "{code}\n",
            "        if (expiredCallbackInfo.shouldHint())\n",
            "        {\n",
            "            try\n",
            "            {\n",
            "                Mutation mutation = (Mutation) ((WriteCallbackInfo) expiredCallbackInfo).sentMessage.payload;\n",
            "                return StorageProxy.submitHint(mutation expiredCallbackInfo.target null);\n",
            "            }\n",
            "            finally\n",
            "            {\n",
            "                //We serialized a hint so we dont need this mutation anymore\n",
            "                mutation.release();\n",
            "            }\n",
            "        }\n",
            "{code} \n",
            "\" \"Patch with your fix attached.  Can you describe your ccm scenario that caused this?  Id like to add a dtest for this.\" \"Committed so we can roll rc3.\n",
            "\n",
            "If you want to follow up with a test it looks to me like this will happen 100% of the time when we have a callback on a non-write expired message.  (Not actually sure what this includes -- we have lots of non-write messages with callbacks but most arent allowed to expire.  Clearly there must be some.)\" \"[~tjake] I produced this by building a 3 node cluster in ccm. Then I ran stress write to get some data into the cluster (with RF=2) calling flush every tens seconds so I could monitor the amount of data per node (didnt need much data just something to play with). Then I ran \n",
            "{code}ccm node1 nodetool repair Keyspace1 Standard1{code}\n",
            " node1 would get the exception in under 30 seconds (usually occurring a few times during the span over the repair). \n",
            "\n",
            "As I was testing out some changes Im working on I dont believe the cause of the messages timing out during repair is a problem in 2.1 (just my own bugs to work out on my branch).\"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-758\n",
            "issue_type: New Feature\n",
            "summary: support wrapped range queries\n",
            "description: we want to support scanning from KeyX to KeyA where X > A.  (Thus over the alphabet this would include X Y Z A.)  this is important to allow hadoop to scan each key in the ring exactly once.\n",
            "architectural impact: NO\n",
            "comments: ['add wrapped range support + test', '+1 Looks good to me.', 'committed', 'Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])\\n    ']\n",
            "my_comment: add wrapped range support + test +1 Looks good to me. committed Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])\n",
            "    \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7745\n",
            "issue_type: Bug\n",
            "summary: Background LCS compactions stall with pending compactions remaining\n",
            "description: We've hit a scenario where background LCS compactions will stall. compactionstats output shows hundreds of pending compactions but none active. The thread dumps show no CompactionExecutor threads running, and no compaction activity is being logged to system.log.  This seems to happen when there are no writes to the node. There are no flushes logged either, and when writes resume, compactions seem to resume as well, but still don't ever get to 0.\n",
            "architectural impact: NO\n",
            "comments: ['I think we have concurrency bug in CompactionManager that compactingCF count remains 1 when there is no active compaction.\\n{{submitBackground}} adds compacting CF to compactingCF and {{BackgroundCompactionTask}} removes the same CF after compaction.\\nCurrently this can happen in either order, and when remove happens first, compacting count remains 1.\\n\\nAttaching patch to fix this.\\n\\nedit: Though it should continue to compact if further flushing happens and there is compaction thread available.', '+1', 'Committed, thanks!']\n",
            "my_comment: Though it should continue to compact if further flushing happens and there is compaction thread available. +1 Committed thanks!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-78\n",
            "issue_type: Bug\n",
            "summary: Interrupted recovery requires manual intervention to fix\n",
            "description: Originally reported by Alexander Staubo: \"If you kill the server while it is going through its initial \"row recovery\" phase, you risk ending up with a database that's corrupt and will fail with \"negative seek\" exceptions and similar.\"\n",
            "\n",
            "Prashant replied:\n",
            "\n",
            "\"The commit logs are only deleted after a successful recovery. You should still have teh commit log if u killed the server while recovering ? When u restart the server it should generate a new file , for compactions we name intermediate files with a .tmp and only on successful dump do we place them as usable files , this same logic is required at recovery and there is a fix coming up which will do it .\n",
            "\n",
            "\"So with the state that u have today there will  be no data loss ass commit logs still exist but its a round about process to recover it since now u haave to delete the intermediate file and then do teh recovery again.\"\n",
            "architectural impact: NO\n",
            "comments: [\"Prashant's memory seems to be wrong here -- the only place getTempFileName is in the anticompaction (bootstrap) code.  Everything else uses getNextFileName directly.\", \"while writing these patches I checked and closeRename does fsync (via FileChannel.force).  Have not audited commitlog similarly.\\n\\nAdditionally, the bootstrap code uses the potentially unsafe CFS.getFileName instead of getTempFileName.  Not sure if there's actually a problem there.  (Just a note to self to come back to this after 0.3)\", '+1', 'Integrated in Cassandra #59 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/59/])\\n    use getTempFileName / closeRename to avoid problems w/ half-written sstables.\\npatch by jbellis; reviewed by Eric Evans for \\nclean up anticompaction code a little.\\npatch by jbellis; reveiewed by Eric Evans for \\n', 'committed']\n",
            "my_comment: [\"Prashants memory seems to be wrong here -- the only place getTempFileName is in the anticompaction (bootstrap) code.  Everything else uses getNextFileName directly.\" \"while writing these patches I checked and closeRename does fsync (via FileChannel.force).  Have not audited commitlog similarly.\n",
            "\n",
            "Additionally the bootstrap code uses the potentially unsafe CFS.getFileName instead of getTempFileName.  Not sure if theres actually a problem there.  (Just a note to self to come back to this after 0.3)\" +1 Integrated in Cassandra #59 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/59/])\n",
            "    use getTempFileName / closeRename to avoid problems w/ half-written sstables.\n",
            "patch by jbellis; reviewed by Eric Evans for \n",
            "clean up anticompaction code a little.\n",
            "patch by jbellis; reveiewed by Eric Evans for \n",
            " committed\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-7983\n",
            "issue_type: Bug\n",
            "summary: nodetool repair triggers OOM\n",
            "description: Customer has a 3 node cluster with 500Mb data on each node\n",
            "\n",
            "{noformat}\n",
            "[cassandra@nbcqa-chc-a02 ~]$ nodetool status\n",
            "Note: Ownership information does not include topology; for complete information, specify a keyspace\n",
            "Datacenter: CH2\n",
            "===============\n",
            "Status=Up/Down\n",
            "|/ State=Normal/Leaving/Joining/Moving\n",
            "--  Address        Load       Tokens  Owns   Host ID                               Rack\n",
            "UN  162.150.4.234  255.26 MB  256     33.2%  4ad1b6a8-8759-4920-b54a-f059126900df  RAC1\n",
            "UN  162.150.4.235  318.37 MB  256     32.6%  3eb0ec58-4b81-442e-bee5-4c91da447f38  RAC1\n",
            "UN  162.150.4.167  243.7 MB   256     34.2%  5b2c1900-bf03-41c1-bb4e-82df1655b8d8  RAC1\n",
            "[cassandra@nbcqa-chc-a02 ~]$\n",
            "{noformat}\n",
            "\n",
            "After we run repair command, system runs into OOM after some 45 minutes\n",
            "Nothing else is running\n",
            "\n",
            "{noformat}\n",
            "[cassandra@nbcqa-chc-a02 ~]$ date\n",
            "Fri Sep 19 15:55:33 UTC 2014\n",
            "[cassandra@nbcqa-chc-a02 ~]$ nodetool repair -st -9220354588320251877 -et -9220354588320251873\n",
            "Sep 19, 2014 4:06:08 PM ClientCommunicatorAdmin Checker-run\n",
            "WARNING: Failed to check the connection: java.net.SocketTimeoutException: Read timed out\n",
            "{noformat}\n",
            "\n",
            "Here is when we run OOM\n",
            "\n",
            "{noformat}\n",
            "ERROR [ReadStage:28914] 2014-09-19 16:34:50,381 CassandraDaemon.java (line 199) Exception in thread Thread[ReadStage:28914,5,main]\n",
            "java.lang.OutOfMemoryError: Java heap space\n",
            "        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:69)\n",
            "        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)\n",
            "        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:43)\n",
            "        at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile.createReader(CompressedPoolingSegmentedFile.java:48)\n",
            "        at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)\n",
            "        at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1195)\n",
            "        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:57)\n",
            "        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)\n",
            "        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)\n",
            "        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)\n",
            "        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)\n",
            "        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)\n",
            "        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)\n",
            "        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1547)\n",
            "        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1376)\n",
            "        at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:333)\n",
            "        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)\n",
            "        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1363)\n",
            "        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1927)\n",
            "        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
            "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
            "        at java.lang.Thread.run(Unknown Source)\n",
            "{noformat}\n",
            "\n",
            "Cassandra process pegs 1 of the 8 CPU's 100% \n",
            "\n",
            "{noformat}\n",
            "top - 16:50:07 up 11 days,  2:01,  2 users,  load average: 0.54, 0.60, 0.65\n",
            "Tasks: 175 total,   1 running, 174 sleeping,   0 stopped,   0 zombie\n",
            "Cpu0  :  0.0%us,  0.0%sy,  0.0%ni,100.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st\n",
            "Cpu1  :100.0%us,  0.0%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st\n",
            "Cpu2  :  0.0%us,  0.0%sy,  0.0%ni,100.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st\n",
            "Cpu3  :  0.7%us,  0.3%sy,  0.0%ni, 99.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st\n",
            "Cpu4  :  0.3%us,  0.3%sy,  0.0%ni, 99.3%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st\n",
            "Cpu5  :  0.3%us,  0.3%sy,  0.0%ni, 99.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.3%st\n",
            "Cpu6  :  0.0%us,  0.3%sy,  0.0%ni, 99.7%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st\n",
            "Cpu7  :  0.3%us,  0.3%sy,  0.0%ni, 99.3%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st\n",
            "Mem:  16332056k total, 16167212k used,   164844k free,   149956k buffers\n",
            "Swap:        0k total,        0k used,        0k free,  8360056k cached\n",
            "\n",
            "  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND\n",
            " 2161 cassandr  20   0 11.5g 6.9g 227m S 107.8 44.0 281:59.49 java\n",
            " 9942 root      20   0  106m 2320 1344 S  1.0  0.0   0:00.03 dhclient-script\n",
            "28969 opscente  20   0 4479m 188m  12m S  0.7  1.2  56:24.24 java\n",
            " 5123 cassandr  20   0 1788m 107m  28m S  0.3  0.7   0:08.09 java\n",
            "    1 root      20   0 19228 1352 1072 S  0.0  0.0   0:00.82 init\n",
            "    2 root      20   0     0    0    0 S  0.0  0.0   0:00.02 kthreadd\n",
            "    3 root      RT   0     0    0    0 S  0.0  0.0   0:05.52 migration/0\n",
            "    4 root      20   0     0    0    0 S  0.0  0.0   0:13.15 ksoftirqd/0\n",
            "    5 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 migration/0\n",
            "    6 root      RT   0     0    0    0 S  0.0  0.0   0:03.33 watchdog/0\n",
            "    7 root      RT   0     0    0    0 S  0.0  0.0   0:04.88 migration/1\n",
            "    8 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 migration/1\n",
            "    9 root      20   0     0    0    0 S  0.0  0.0   0:19.21 ksoftirqd/1\n",
            "   10 root      RT   0     0    0    0 S  0.0  0.0   0:03.24 watchdog/1\n",
            "   11 root      RT   0     0    0    0 S  0.0  0.0   0:05.46 migration/2\n",
            "   12 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 migration/2\n",
            "   13 root      20   0     0    0    0 S  0.0  0.0   0:16.87 ksoftirqd/2\n",
            "   14 root      RT   0     0    0    0 S  0.0  0.0   0:03.49 watchdog/2\n",
            "   15 root      RT   0     0    0    0 S  0.0  0.0   0:05.31 migration/3\n",
            "   16 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 migration/3\n",
            "   17 root      20   0     0    0    0 S  0.0  0.0   0:19.33 ksoftirqd/3\n",
            "   18 root      RT   0     0    0    0 S  0.0  0.0   0:03.43 watchdog/3\n",
            "   19 root      RT   0     0    0    0 S  0.0  0.0   0:05.36 migration/4\n",
            "   20 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 migration/4\n",
            "   21 root      20   0     0    0    0 S  0.0  0.0   0:17.64 ksoftirqd/4\n",
            "   22 root      RT   0     0    0    0 S  0.0  0.0   0:03.18 watchdog/4\n",
            "   23 root      RT   0     0    0    0 S  0.0  0.0   0:05.31 migration/5\n",
            "{noformat}\n",
            "\n",
            "There is a 12Gb HeapDump, the memory leak suspects show the following trace\n",
            "\n",
            "{noformat}\n",
            " Thread Stack\n",
            "\n",
            "RMI TCP Connection(1621)-162.150.4.235\n",
            "  at org.apache.cassandra.service.StorageService.createRepairRangeFrom(Ljava/lang/String;Ljava/lang/String;)Ljava/util/Collection; (StorageService.java:2612)\n",
            "  at org.apache.cassandra.service.StorageService.forceRepairRangeAsync(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;ZLjava/util/Collection;Ljava/util/Collection;[Ljava/lang/String;)I (StorageService.java:2541)\n",
            "  at sun.reflect.NativeMethodAccessorImpl.invoke0(Ljava/lang/reflect/Method;Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Native Method)\n",
            "  at sun.reflect.NativeMethodAccessorImpl.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at java.lang.reflect.Method.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at sun.reflect.misc.Trampoline.invoke(Ljava/lang/reflect/Method;Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at sun.reflect.GeneratedMethodAccessor8.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at java.lang.reflect.Method.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at sun.reflect.misc.MethodUtil.invoke(Ljava/lang/reflect/Method;Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Ljava/lang/reflect/Method;Ljava/lang/Object;[Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Ljava/lang/Object;Ljava/lang/Object;[Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Ljava/lang/Object;Ljava/lang/Object;[Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at com.sun.jmx.mbeanserver.PerInterface.invoke(Ljava/lang/Object;Ljava/lang/String;[Ljava/lang/Object;[Ljava/lang/String;Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at com.sun.jmx.mbeanserver.MBeanSupport.invoke(Ljava/lang/String;[Ljava/lang/Object;[Ljava/lang/String;)Ljava/lang/Object; (Unknown Source)\n",
            "  at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Ljavax/management/ObjectName;Ljava/lang/String;[Ljava/lang/Object;[Ljava/lang/String;)Ljava/lang/Object; (Unknown Source)\n",
            "  at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Ljavax/management/ObjectName;Ljava/lang/String;[Ljava/lang/Object;[Ljava/lang/String;)Ljava/lang/Object; (Unknown Source)\n",
            "  at javax.management.remote.rmi.RMIConnectionImpl.doOperation(I[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at javax.management.remote.rmi.RMIConnectionImpl.access$300(Ljavax/management/remote/rmi/RMIConnectionImpl;I[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run()Ljava/lang/Object; (Unknown Source)\n",
            "  at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(I[Ljava/lang/Object;Ljavax/security/auth/Subject;)Ljava/lang/Object; (Unknown Source)\n",
            "  at javax.management.remote.rmi.RMIConnectionImpl.invoke(Ljavax/management/ObjectName;Ljava/lang/String;Ljava/rmi/MarshalledObject;[Ljava/lang/String;Ljavax/security/auth/Subject;)Ljava/lang/Object; (Unknown Source)\n",
            "  at sun.reflect.GeneratedMethodAccessor37.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at java.lang.reflect.Method.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)\n",
            "  at sun.rmi.server.UnicastServerRef.dispatch(Ljava/rmi/Remote;Ljava/rmi/server/RemoteCall;)V (Unknown Source)\n",
            "  at sun.rmi.transport.Transport$1.run()Ljava/lang/Void; (Unknown Source)\n",
            "  at sun.rmi.transport.Transport$1.run()Ljava/lang/Object; (Unknown Source)\n",
            "  at java.security.AccessController.doPrivileged(Ljava/security/PrivilegedExceptionAction;Ljava/security/AccessControlContext;)Ljava/lang/Object; (Native Method)\n",
            "  at sun.rmi.transport.Transport.serviceCall(Ljava/rmi/server/RemoteCall;)Z (Unknown Source)\n",
            "  at sun.rmi.transport.tcp.TCPTransport.handleMessages(Lsun/rmi/transport/Connection;Z)V (Unknown Source)\n",
            "  at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0()V (Unknown Source)\n",
            "  at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run()V (Unknown Source)\n",
            "  at java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V (Unknown Source)\n",
            "  at java.util.concurrent.ThreadPoolExecutor$Worker.run()V (Unknown Source)\n",
            "  at java.lang.Thread.run()V (Unknown Source)\n",
            "{noformat}\n",
            "\n",
            "The file is way too large to be attached here.  \n",
            "It's currently held at my SCP server\n",
            "Let us know if there is any other info you may need\n",
            "I will be posting the other 2 system logs as soon as I get them \n",
            "architectural impact: NO\n",
            "comments: ['system logs from other 2 nodes', 'We got the same problem on Cassandra 2.0.10. I\\'ve traced it to a bug in StorageService#createRepairRangeFrom which gets stuck in an infinite loop allocating memory. This happens when you try to repair (using -st and -et) the very \"first\" range in the ring and the lowest token in the ring is not the minimum token for that partitioner. The problem is the following lines:\\n\\n{code}\\nToken previous = tokenMetadata.getPredecessor(TokenMetadata.firstToken(tokenMetadata.sortedTokens(), parsedEndToken));\\nwhile (parsedBeginToken.compareTo(previous) < 0)\\n  ...\\n  previous = tokenMetadata.getPredecessor(previous);\\n}\\n{code}\\n\\nprevious will never become less than parsedBeginToken.\\n\\nThis bug was introduced with CASSANDRA-7317.\\n', 'Good detective work, [~yarin].', \"I've attached a patch. I ended up rewriting createRepairRangeFrom to make it simpler and added some unit tests.\", 'Committed, thanks!']\n",
            "my_comment: system logs from other 2 nodes We got the same problem on Cassandra 2.0.10. I\\ve traced it to a bug in StorageService#createRepairRangeFrom which gets stuck in an infinite loop allocating memory. This happens when you try to repair (using -st and -et) the very \"first\" range in the ring and the lowest token in the ring is not the minimum token for that partitioner. The problem is the following lines:\n",
            "\n",
            "{code}\n",
            "Token previous = tokenMetadata.getPredecessor(TokenMetadata.firstToken(tokenMetadata.sortedTokens() parsedEndToken));\n",
            "while (parsedBeginToken.compareTo(previous) < 0)\n",
            "  ...\n",
            "  previous = tokenMetadata.getPredecessor(previous);\n",
            "}\n",
            "{code}\n",
            "\n",
            "previous will never become less than parsedBeginToken.\n",
            "\n",
            "This bug was introduced with CASSANDRA-7317.\n",
            " Good detective work [~yarin]. \"Ive attached a patch. I ended up rewriting createRepairRangeFrom to make it simpler and added some unit tests.\" Committed thanks!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-810\n",
            "issue_type: Improvement\n",
            "summary: Do away with the streaming directory\n",
            "description: It muddies the approaches we use when deducing keyspace, cf, etc. from paths.\n",
            "architectural impact: NO\n",
            "comments: [\"didn't have any adverse affects when streaming.\", 'Files to be streamed should probably stay marked as temporary so that they get cleaned up automatically if the process dies.', '+1', 'r912620.', 'Integrated in Cassandra #364 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/364/])\\n    No longer use a streaming directory. Patch by Gary Dusbabek and Stu Hood, reviewed by same.  .\\n']\n",
            "my_comment: [\"didnt have any adverse affects when streaming.\" Files to be streamed should probably stay marked as temporary so that they get cleaned up automatically if the process dies. +1 r912620. Integrated in Cassandra #364 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/364/])\n",
            "    No longer use a streaming directory. Patch by Gary Dusbabek and Stu Hood reviewed by same.  .\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8147\n",
            "issue_type: Bug\n",
            "summary: Secondary indexing of map keys does not work properly when mixing contains and contains_key\n",
            "description: If you have a table with a map column and an index on the map key selecting data using a contains key and a contains will not return the expected data.\n",
            "\n",
            "The problem can be reproduced using the following unit test:\n",
            "{code}\n",
            "    @Test\n",
            "    public void testMapKeyContainsAndValueContains() throws Throwable\n",
            "    {\n",
            "        createTable(\"CREATE TABLE %s (account text, id int, categories map<text,text>, PRIMARY KEY (account, id))\");\n",
            "        createIndex(\"CREATE INDEX ON %s(keys(categories))\");\n",
            "\n",
            "        execute(\"INSERT INTO %s (account, id , categories) VALUES (?, ?, ?)\", \"test\", 5, map(\"lmn\", \"foo\"));\n",
            "\n",
            "        assertRows(execute(\"SELECT * FROM %s WHERE account = ? AND id = ? AND categories CONTAINS KEY ? AND categories CONTAINS ? ALLOW FILTERING\", \"test\", 5, \"lmn\", \"foo\"), row(\"test\", 5, map(\"lmn\", \"foo\")));  \n",
            "    }\n",
            "{code}\n",
            "architectural impact: NO\n",
            "comments: ['I think there is not currently support for indexing both values and keys (ref: http://www.datastax.com/dev/blog/cql-in-2-1)\\n\\nIn the example above the index is on keys only, but the select is attempting to query a values index too. So the empty result is sort of correct, but there should probably be a warning message to prevent confusion about this (\"index does not exist\" or something like that).\\n\\n/cc [~slebresne]', 'The empty result is not correct, no matter how you look at it. There are only two things that can happen in response to the query:\\nyou either get the good result back or you get an error with a message telling you what you did wrong.\\nI agree that querying a key and a value and on map does not make a lot of sense, so an error message will be perfectly acceptable.  ', \"I don't totally agree with all this. It is correct that we currently only allow one index per CQL column, and so one cannot index both the keys and values of a given map, but that's not what the test does. The test only has an index on the keys.\\n\\nRegarding the {{SELECT}}, provided you do have an indexed clause (which that example has), it's allowed to have other non-indexed clause (it will require {{ALLOW FILTERING}} but it's used in the example too).  So I'm not sure why this doesn't work, but it should (it's worth testing on current 2.1 branch though, maybe this has been fixed since 2.1.0).\\n\\nbq. I agree that querying a key and a value and on map does not make a lot of sense\\n\\nOut of curiosity, why wouldn't that make sense?\", \"{quote}(it's worth testing on current 2.1 branch though, maybe this has been fixed since 2.1.0).{quote}\\nI tested it on the latest 2.1\\n\\n{quote}Out of curiosity, why wouldn't that make sense?{quote}\\nAs a map can only have one value associated to a given key, using such a query means that you want to check that the key exists and that the value is the one you think it should be. If you select using a contains key only you will be able to have the same information but you will also know if it is the key which is missing or the value which is not what you expect.\\nThat is why I think that it does not make a lot of sense and that an error message will be fine for me if I was a user.\\nNow as a user it is also true that it will also give me a better sense of robustness if the query was handled properly ;-)   \\n\", \"bq. As a map can only have one value associated to a given key, using such a query means that you want to check that the key exists and that the value is the one you think it should be.\\n\\nThat's not what the query means, no. Asking for maps that contains a given key and a given value does not imply that said given value must be associated to said given key.\\nBesides, even if that was what the query means, the query still make sense. It might not be terribly useful, but it make sense, so I'd still not think throwing an error would not be very user friendly.\", 'The problem was cause by the fact that the IndexSearcher was trying to use the key index to search for the contains value.\\nThis patch fix this problem and also fix the SelectStatement check for the indexed column (if we do a select contains on a map were only the key is indexed we should reject the query).\\nThe patch also replace the index option Strings with constants to avoid typos issues.', 'Hmm, so this makes some of the same fixes as CASSANDRA-8155, but in a different way.  Personally, I prefer putting the logic for \"does this index support this operator\" in the index code instead of the operator code (as the 8155 patch does).  However, it would be good to include your test cases and the constants instead of String options.\\n\\nDo you want to take a look at that patch and see if you agree?', 'I like the {{supportsOperator}} approach of CASSANDRA-8155 but I think that we should do the validation during the preparation phase of the select statement and not at execution time. It seems more user friendly to me.\\n\\nSo personally I would kind of merge the two patches as follow:\\n* Use the constants instead of String options and tests from this patch\\n* Use the  {{supportsOperator}} approach of CASSANDRA-8155 instead of {{isValidIndexFor}} in the {{SecondaryIndex}} as I found it much nicer\\n* Keep some validation in the SelectStatement during the preparation phase (if you do not like the approach of this patch we can use an other one).', \"bq. So personally I would kind of merge the two patches as follow\\n\\nThat sounds good to me.  Can you take care of making that patch and I'll resolve 8155 as a duplicate?\", 'No problem, I will do it.', 'Patch resulting from the merge of the V1 patch and of the one of CASSANDRA-8155', 'Overall this looks good.\\n\\nThe only remaining use of {{Relation.allowsIndexQueryOn()}} is in {{SelectStatement.processRelationEntity()}}.  We could remove {{allowsIndexQueryOn()}} (and {{CollectionType.isMap()}}) by doing something like this in {{processRelationEntity()}}:\\n\\n{code}\\nSecondaryIndex index = indexManager.getIndexForColumn(def.name.bytes);\\nif (index != null && index.supportsOperator(relation.operator()))\\n    return new boolean[]{true, def.kind == ColumnDefinition.Kind.CLUSTERING_COLUMN};\\n{code}\\n\\n(It looks like we need to be able to convert {{Relation.Type}} enum values to {{IndexExpression.Operator}} enum values, though.)', '{{Relation.Type}} and {{IndexExpression.Operator}} are really similar. Should we merge them? If yes, in which package do you think that we should put the resulting class? ', \"bq. Relation.Type and IndexExpression.Operator are really similar. Should we merge them?\\n\\nI think that's a good idea, if you're willing to do it (perhaps as a second patch, for clarity).\\n\\nbq.  in which package do you think that we should put the resulting class?\\n\\nI'm tempted to just use Relation.Type, but if you think that's a bad idea for some reason, I would make an o.a.c.cql3.Operator class.\", 'Additional patch that replaces {{Relation.Type}} and {{IndexExpression.Operator}} by {{Operator}} and fix the last remaining issue.', 'Thanks! Committed.']\n",
            "my_comment: http://www.datastax.com/dev/blog/cql-in-2-1)\n",
            "\n",
            "In the example above the index is on keys only but the select is attempting to query a values index too. So the empty result is sort of correct but there should probably be a warning message to prevent confusion about this (\"index does not exist\" or something like that).\n",
            "\n",
            "/cc [~slebresne] The empty result is not correct no matter how you look at it. There are only two things that can happen in response to the query:\n",
            "you either get the good result back or you get an error with a message telling you what you did wrong.\n",
            "I agree that querying a key and a value and on map does not make a lot of sense so an error message will be perfectly acceptable.   \"I dont totally agree with all this. It is correct that we currently only allow one index per CQL column and so one cannot index both the keys and values of a given map but thats not what the test does. The test only has an index on the keys.\n",
            "\n",
            "Regarding the {{SELECT}} provided you do have an indexed clause (which that example has) its allowed to have other non-indexed clause (it will require {{ALLOW FILTERING}} but its used in the example too).  So Im not sure why this doesnt work but it should (its worth testing on current 2.1 branch though maybe this has been fixed since 2.1.0).\n",
            "\n",
            "bq. I agree that querying a key and a value and on map does not make a lot of sense\n",
            "\n",
            "Out of curiosity why wouldnt that make sense?\" \"{quote}(its worth testing on current 2.1 branch though maybe this has been fixed since 2.1.0).{quote}\n",
            "I tested it on the latest 2.1\n",
            "\n",
            "{quote}Out of curiosity why wouldnt that make sense?{quote}\n",
            "As a map can only have one value associated to a given key using such a query means that you want to check that the key exists and that the value is the one you think it should be. If you select using a contains key only you will be able to have the same information but you will also know if it is the key which is missing or the value which is not what you expect.\n",
            "That is why I think that it does not make a lot of sense and that an error message will be fine for me if I was a user.\n",
            "Now as a user it is also true that it will also give me a better sense of robustness if the query was handled properly ;-)   \n",
            "\" \"bq. As a map can only have one value associated to a given key using such a query means that you want to check that the key exists and that the value is the one you think it should be.\n",
            "\n",
            "Thats not what the query means no. Asking for maps that contains a given key and a given value does not imply that said given value must be associated to said given key.\n",
            "Besides even if that was what the query means the query still make sense. It might not be terribly useful but it make sense so Id still not think throwing an error would not be very user friendly.\" The problem was cause by the fact that the IndexSearcher was trying to use the key index to search for the contains value.\n",
            "This patch fix this problem and also fix the SelectStatement check for the indexed column (if we do a select contains on a map were only the key is indexed we should reject the query).\n",
            "The patch also replace the index option Strings with constants to avoid typos issues. Hmm so this makes some of the same fixes as CASSANDRA-8155 but in a different way.  Personally I prefer putting the logic for \"does this index support this operator\" in the index code instead of the operator code (as the 8155 patch does).  However it would be good to include your test cases and the constants instead of String options.\n",
            "\n",
            "Do you want to take a look at that patch and see if you agree? I like the {{supportsOperator}} approach of CASSANDRA-8155 but I think that we should do the validation during the preparation phase of the select statement and not at execution time. It seems more user friendly to me.\n",
            "\n",
            "So personally I would kind of merge the two patches as follow:\n",
            "* Use the constants instead of String options and tests from this patch\n",
            "* Use the  {{supportsOperator}} approach of CASSANDRA-8155 instead of {{isValidIndexFor}} in the {{SecondaryIndex}} as I found it much nicer\n",
            "* Keep some validation in the SelectStatement during the preparation phase (if you do not like the approach of this patch we can use an other one). \"bq. So personally I would kind of merge the two patches as follow\n",
            "\n",
            "That sounds good to me.  Can you take care of making that patch and Ill resolve 8155 as a duplicate?\" No problem I will do it. Patch resulting from the merge of the V1 patch and of the one of CASSANDRA-8155 Overall this looks good.\n",
            "\n",
            "The only remaining use of {{Relation.allowsIndexQueryOn()}} is in {{SelectStatement.processRelationEntity()}}.  We could remove {{allowsIndexQueryOn()}} (and {{CollectionType.isMap()}}) by doing something like this in {{processRelationEntity()}}:\n",
            "\n",
            "{code}\n",
            "SecondaryIndex index = indexManager.getIndexForColumn(def.name.bytes);\n",
            "if (index != null && index.supportsOperator(relation.operator()))\n",
            "    return new boolean[]{true def.kind == ColumnDefinition.Kind.CLUSTERING_COLUMN};\n",
            "{code}\n",
            "\n",
            "(It looks like we need to be able to convert {{Relation.Type}} enum values to {{IndexExpression.Operator}} enum values though.) {{Relation.Type}} and {{IndexExpression.Operator}} are really similar. Should we merge them? If yes in which package do you think that we should put the resulting class?  \"bq. Relation.Type and IndexExpression.Operator are really similar. Should we merge them?\n",
            "\n",
            "I think thats a good idea if youre willing to do it (perhaps as a second patch for clarity).\n",
            "\n",
            "bq.  in which package do you think that we should put the resulting class?\n",
            "\n",
            "Im tempted to just use Relation.Type but if you think thats a bad idea for some reason I would make an o.a.c.cql3.Operator class.\" Additional patch that replaces {{Relation.Type}} and {{IndexExpression.Operator}} by {{Operator}} and fix the last remaining issue. Thanks! Committed.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8180\n",
            "issue_type: Improvement\n",
            "summary: Optimize disk seek using min/max column name meta data when the LIMIT clause is used\n",
            "description: I was working on an example of sensor data table (timeseries) and face a use case where C* does not optimize read on disk.\n",
            "\n",
            "{code}\n",
            "cqlsh:test> CREATE TABLE test(id int, col int, val text, PRIMARY KEY(id,col)) WITH CLUSTERING ORDER BY (col DESC);\n",
            "cqlsh:test> INSERT INTO test(id, col , val ) VALUES ( 1, 10, '10');\n",
            "...\n",
            ">nodetool flush test test\n",
            "...\n",
            "cqlsh:test> INSERT INTO test(id, col , val ) VALUES ( 1, 20, '20');\n",
            "...\n",
            ">nodetool flush test test\n",
            "...\n",
            "cqlsh:test> INSERT INTO test(id, col , val ) VALUES ( 1, 30, '30');\n",
            "...\n",
            ">nodetool flush test test\n",
            "{code}\n",
            "\n",
            "After that, I activate request tracing:\n",
            "\n",
            "{code}\n",
            "cqlsh:test> SELECT * FROM test WHERE id=1 LIMIT 1;\n",
            " activity                                                                  | timestamp    | source    | source_elapsed\n",
            "---------------------------------------------------------------------------+--------------+-----------+----------------\n",
            "                                                        execute_cql3_query | 23:48:46,498 | 127.0.0.1 |              0\n",
            "                            Parsing SELECT * FROM test WHERE id=1 LIMIT 1; | 23:48:46,498 | 127.0.0.1 |             74\n",
            "                                                       Preparing statement | 23:48:46,499 | 127.0.0.1 |            253\n",
            "                                  Executing single-partition query on test | 23:48:46,499 | 127.0.0.1 |            930\n",
            "                                              Acquiring sstable references | 23:48:46,499 | 127.0.0.1 |            943\n",
            "                                               Merging memtable tombstones | 23:48:46,499 | 127.0.0.1 |           1032\n",
            "                                               Key cache hit for sstable 3 | 23:48:46,500 | 127.0.0.1 |           1160\n",
            "                               Seeking to partition beginning in data file | 23:48:46,500 | 127.0.0.1 |           1173\n",
            "                                               Key cache hit for sstable 2 | 23:48:46,500 | 127.0.0.1 |           1889\n",
            "                               Seeking to partition beginning in data file | 23:48:46,500 | 127.0.0.1 |           1901\n",
            "                                               Key cache hit for sstable 1 | 23:48:46,501 | 127.0.0.1 |           2373\n",
            "                               Seeking to partition beginning in data file | 23:48:46,501 | 127.0.0.1 |           2384\n",
            " Skipped 0/3 non-slice-intersecting sstables, included 0 due to tombstones | 23:48:46,501 | 127.0.0.1 |           2768\n",
            "                                Merging data from memtables and 3 sstables | 23:48:46,501 | 127.0.0.1 |           2784\n",
            "                                        Read 2 live and 0 tombstoned cells | 23:48:46,501 | 127.0.0.1 |           2976\n",
            "                                                          Request complete | 23:48:46,501 | 127.0.0.1 |           3551\n",
            "{code}\n",
            "\n",
            "We can clearly see that C* hits 3 SSTables on disk instead of just one, although it has the min/max column meta data to decide which SSTable contains the most recent data.\n",
            "\n",
            "Funny enough, if we add a clause on the clustering column to the select, this time C* optimizes the read path:\n",
            "\n",
            "{code}\n",
            "cqlsh:test> SELECT * FROM test WHERE id=1 AND col > 25 LIMIT 1;\n",
            " activity                                                                  | timestamp    | source    | source_elapsed\n",
            "---------------------------------------------------------------------------+--------------+-----------+----------------\n",
            "                                                        execute_cql3_query | 23:52:31,888 | 127.0.0.1 |              0\n",
            "               Parsing SELECT * FROM test WHERE id=1 AND col > 25 LIMIT 1; | 23:52:31,888 | 127.0.0.1 |             60\n",
            "                                                       Preparing statement | 23:52:31,888 | 127.0.0.1 |            277\n",
            "                                  Executing single-partition query on test | 23:52:31,889 | 127.0.0.1 |            961\n",
            "                                              Acquiring sstable references | 23:52:31,889 | 127.0.0.1 |            971\n",
            "                                               Merging memtable tombstones | 23:52:31,889 | 127.0.0.1 |           1020\n",
            "                                               Key cache hit for sstable 3 | 23:52:31,889 | 127.0.0.1 |           1108\n",
            "                               Seeking to partition beginning in data file | 23:52:31,889 | 127.0.0.1 |           1117\n",
            " Skipped 2/3 non-slice-intersecting sstables, included 0 due to tombstones | 23:52:31,889 | 127.0.0.1 |           1611\n",
            "                                Merging data from memtables and 1 sstables | 23:52:31,890 | 127.0.0.1 |           1624\n",
            "                                        Read 1 live and 0 tombstoned cells | 23:52:31,890 | 127.0.0.1 |           1700\n",
            "                                                          Request complete | 23:52:31,890 | 127.0.0.1 |           2140\n",
            "{code}\n",
            "architectural impact: NO\n",
            "comments: [\"Querying for all the rows with the primary key {{WHERE id=1}} indeed requires scanning all the rows. By limiting your query only to those clustering column rows that also have {{col > 25}} is precisely the kind of query optimization that users should be doing with well thought out schema and queries that answer efficiently. This is by design. If I'm misunderstanding something here, please re-open and explain further :)\", \"I'm also really curious as to how you expect this optimization to work, because I don't see how this could work.\", \"Oh, unless you mean to order the sstables by mean min column names and to query them one at a time (like we do for names filter) in the specific case where their column names interval don't intersect. In which case, yes, that could be indeed useful with the compaction strategy from CASSANDRA-6602.\", 'The idea is:\\n\\nIf there is no restriction on clustering columns but only on LIMIT:\\n\\n1) Order SSTables by min/max column depending on the first clustering order\\n2) Hit the first SSTable and start sequential read until reaching LIMIT\\n3) If LIMIT is large enough, switch to another SSTable and so on\\n\\n', 'Another test showing that it could be optimized further:\\n\\n{code}\\ncqlsh:test> SELECT * FROM test WHERE id=1 AND col<40 LIMIT 1;\\n\\n id | col | val\\n----+-----+-----\\n  1 |  30 |  30\\n\\n(1 rows)\\n\\n\\nTracing session: 2725c710-5b86-11e4-aeed-814585a29e7b\\n\\n activity                                                                  | timestamp    | source    | source_elapsed\\n---------------------------------------------------------------------------+--------------+-----------+----------------\\n                                                        execute_cql3_query | 16:00:46,850 | 127.0.0.1 |              0\\n                 Parsing SELECT * FROM test WHERE id=1 AND col<40 LIMIT 1; | 16:00:46,850 | 127.0.0.1 |             77\\n                                                       Preparing statement | 16:00:46,850 | 127.0.0.1 |            244\\n                                  Executing single-partition query on test | 16:00:46,851 | 127.0.0.1 |           1485\\n                                              Acquiring sstable references | 16:00:46,851 | 127.0.0.1 |           1500\\n                                               Merging memtable tombstones | 16:00:46,851 | 127.0.0.1 |           1547\\n                                               Key cache hit for sstable 3 | 16:00:46,852 | 127.0.0.1 |           1641\\n                         Seeking to partition indexed section in data file | 16:00:46,852 | 127.0.0.1 |           1651\\n                                               Key cache hit for sstable 2 | 16:00:46,854 | 127.0.0.1 |           4054\\n                         Seeking to partition indexed section in data file | 16:00:46,854 | 127.0.0.1 |           4068\\n                                               Key cache hit for sstable 1 | 16:00:46,855 | 127.0.0.1 |           5232\\n                         Seeking to partition indexed section in data file | 16:00:46,855 | 127.0.0.1 |           5249\\n Skipped 0/3 non-slice-intersecting sstables, included 0 due to tombstones | 16:00:46,855 | 127.0.0.1 |           5499\\n                                Merging data from memtables and 3 sstables | 16:00:46,855 | 127.0.0.1 |           5515\\n                                        Read 2 live and 0 tombstoned cells | 16:00:46,855 | 127.0.0.1 |           5598\\n                                                          Request complete | 16:00:46,855 | 127.0.0.1 |           5997\\n{code}\\n\\n Now reversing the inequality on the clustering column, C* does scan 3 SSTables instead of just one (since LIMIT = 1)', \"[~slebresne], [~thobbs], [~iamaleksey] : \\n\\nI think it might make sense if I implement this change directly on a branch based on {{8099_engine_refactor}}? First of all I found it *much easier* to understand and secondly I don't particularly want to rebase or merge later on once 8099 is merged into trunk. Any concerns?\\n\\nI've been looking at the code on 8099 today, and I cannot find a way to implement this unless we iterate twice, the first time to count until the limit has been reached in {{SinglePartitionSliceCommand}} and the second time to return the data. Or have I missed something? If not, I think we need to store the data in memory via an {{ArrayBackedPartition}}, is this correct?\\n\\nHere is a very inefficient and ugly way to do this, may I have some pointers on to improve on it?\\n\\nhttps://github.com/stef1927/cassandra/commits/8180-8099\\n\\nSpecifically in {{querySSTablesByClustering()}} at line 254 of {{SinglePartitionSliceCommand.java}}.\", 'bq. l I found it much easier to understand\\n\\nGlad that it\\'s the case.\\n\\nbq. I think it might make sense if I implement this change directly on a branch based on {{8099_engine_refactor}}\\n\\nI wouldn\\'t be the one to blame you for that.\\n\\nbq. I cannot find a way to implement this unless we iterate twice, the first time to count until the limit has been reached in {{SinglePartitionSliceCommand}} and the second time to return the data\\n\\nYou actually don\\'t have to care about the limit (in SinglePartitionSliceCommand at least). The way to do this would be to return an iterator that first query and return the results of the first sstable and once it has returned all results, it transparently query the 2nd sstable and start returning those results, etc...\\n\\nThat being said, I do suspect doing this at the merging level (in MergeIterator) would be better. The idea would be to special the merge iterator to take specific iterators that expose some {{lowerBound()}} method. That method would be allowed to return a value that is not returned by the iterator but is lower than anything it will return. The merge iterator would use those lower bound as initial {{Candidate}} for the iterators but know that when it consumes those canditates it should just discard them (and get the actual next value of the iterator). Basically, we\\'d add a way for the iterator to say \"don\\'t bother using me until you\\'ve at least reached value X\".  The sstable iterators would typically implement that {{lowerBound}} method by returning the sstable \"min column name\". Provided we make sure the sstable iterators don\\'t do any work unless their {{hasNext/next}} methods are called, we wouldn\\'t actually use a sstable until we\\'ve reached it\\'s \"min column name\".\\n\\nDoing it that way would 2 advantages over doing it at the \"collation\" level:\\n# this is more general as it would work even if the sstables min/max column name intersects (it\\'s harder/uglier to do the same at the collation level imo).\\n# this would work for range queries too.\\n\\nWe may want to build that on top of CASSANDRA-8915 however.\\n', \"Using {{MergeIterator}} is a great idea, I still have some details to iron out but it's already looking much better.\\n\\nI have one question : the iterator is over atoms but the sstable min and max column names are lists of ByteBuffer, which I can compare with atoms using the ClusteringComparator but it would be nice to convert the lower bound to an atom, so we can  have only one generic type (the {{In}} type) in the MergeIterator specialization, which must feed atoms upstream. Is there a way to do this or do I just have to settle for having two different (comparable) types in MergeIterator?\\n\", 'bq. it would be nice to convert the lower bound to an atom\\n\\nI suspect you don\\'t need to have an \"Atom\", only a \"Clusterable\", in which case you can convert the lower bound to a \"Clustering\" with something like {{new SimpleClustering(sstable.minClusteringValues.toArray(new ByteBuffer\\\\[metadata.clusteringColumns().size()\\\\]))}}.', \"You are correct a Clusterable was sufficient.\\n\\nThere is one disk access I am not sure if we can remove, when we read the partition level deletion:\\n\\n{code}\\nstatus.mostRecentPartitionTombstone = Math.max(status.mostRecentPartitionTombstone, iter.partitionLevelDeletion().markedForDeleteAt());\\n{code}\\n\\nWe need to read the partition level deletion in the initial for loop of {{SinglePartitionReadCommand}} with the sstables ordered by max timestamp in order to skip older sstables:\\n\\n{code}\\nif (sstable.getMaxTimestamp() < status.mostRecentPartitionTombstone)\\n    break;\\n{code}\\n\\nI don't see how we could skip older sstables if they are not ordered by max timestamp and are instead picked lazily by the merge iterator when they become eligible according to their lower bound. As a consequence I don't know how to postpone reading the partition level deletion.\\n\\nSo far I have some code that still calls {{iter.partitionLevelDeletion()}} in that initial for loop, but other iterator methods should not be called until the table is picked by the merge iterator.\\n\\nCan we do better?\\n\\nCode is here: https://github.com/stef1927/cassandra/tree/8180-8099\", 'bq. There is one disk access I am not sure if we can remove, when we read the partition level deletion\\n\\nIt\\'s a good point. A simple optimization however could be to collect whether a sstable has any partition level deletion in the first place. I suspect that it\\'s not that uncommon to not use partition level deletion at all and so we could avoid that seeks at least in that case.\\n\\nNow I\\'ll also note that when we do have to fetch the partition deletion, if the partition is indexed, we get it from the index file. Which means we at least avoid seeking into the data file, but also mean that with the index entry comes the partition row-index, and that\\'s actually more precise than the global sstable min/max clustering values. In other words, an optimization we could do is that when we do have the index entry when the sstable iterator \"lower bound\" is queried, we should return the min clustering from the row index rather than the one from the sstable min clustering values.', \"Thanks for your input. With these two optimizations in place, I think it's starting to be in a decent shape and I would be happy for a first round of review.\\n\\nHowever, we can delay this until CASSANDRA-8099 has been completed since people are busy with it and this change in based on the same branch.\\n\\nAlso, once CASSANDRA-8915 is available, we will have to integrate it, so more work and another review will be required then.\\n\\nFinally, although I did run all unit tests and the cql dtests, and verified that the only failures also apply to the 8099 branch, it could use more testing once 8099 is stable and 8915 is integrated.\\n\\nThis is what the trace for the basic example above looks like now (with source and timestamp removed to improve formatting):\\n\\n{code}\\ncqlsh:test> SELECT * FROM test WHERE id=1 LIMIT 1;\\n\\n[...]\\n\\n  activity                                                                                        | source_elapsed\\n-------------------------------------------------------------------------------------------------+----------------\\n                                                                              Execute CQL3 query |              0\\n                            Parsing SELECT * FROM test WHERE id=1 LIMIT 1; [SharedPool-Worker-1] |            211\\n                                                       Preparing statement [SharedPool-Worker-1] |            430\\n                                  Executing single-partition query on test [SharedPool-Worker-3] |            849\\n                                              Acquiring sstable references [SharedPool-Worker-3] |            915\\n Skipped 0/3 non-slice-intersecting sstables, included 0 due to tombstones [SharedPool-Worker-3] |           1075\\n                                  Merging data from memtables and sstables [SharedPool-Worker-3] |           1133\\n                                               Key cache hit for sstable 3 [SharedPool-Worker-1] |           1390\\n                                                       Accessed 1 sstables [SharedPool-Worker-1] |           1901\\n                                        Read 1 live and 0 tombstoned cells [SharedPool-Worker-1] |           1986\\n                                                                                Request complete |           2035\\n{code}\", 'Rebased to latest 8099 branch. \\n\\nTest results available here:\\n\\nhttp://cassci.datastax.com/view/Dev/view/Stefania/job/stef1927-8180-8099-testall/\\nhttp://cassci.datastax.com/view/Dev/view/Stefania/job/stef1927-8180-8099-dtest/\\n\\nMany tests fail but so does the plain 8099 branch:\\n\\nhttp://cassci.datastax.com/view/Dev/view/Stefania/job/stef1927-8099_engine_refactor-testall/\\nhttp://cassci.datastax.com/view/Dev/view/Stefania/job/stef1927-8099_engine_refactor-dtest/', \"I'm going to rebase this and bring it back to life after the merge of CASSANDRA-8099 and CASSANDRA-8915.\", \"I have attached the latest patch based on trunk, which is ready for review.\\n\\nThe main things to note are:\\n\\n* In MergeIterator we need to discard fake lower bound values and we must advance the same iterator to check that there isn't another real value with the exact same value, in which case we must pass it to the reducer together with the other equal values from other iterators. However, we cannot just discard fake values too soon, i.e. in ManyToOne.advance() we cannot just advance as long as we have fake values, we need to wait for the heap to be sorted first. I implemented a [peek method|https://github.com/stef1927/cassandra/commit/9528e672ea65c7a71c0004adfc27e5f4d9ee0acb#diff-a9e2c345aa605d1b8d360b4c44ade32f] in the candidate, this is called for fake values when reducing values. This should not affect the correctness of the algorithm but there may be a more efficient way to do this, cc [~benedict] and [~blambov]. \\n\\n* We need to decide if we are happy with a wrapping iterator, [LowerBoundUnfilteredRowIterator|https://github.com/stef1927/cassandra/commit/9528e672ea65c7a71c0004adfc27e5f4d9ee0acb#diff-1cdf42ebc69336015e04f287e8450e51], in which case we may need a better name. I understand that wrapping too many iterators may hurt performance so we may want to look into modifying AbstractSSTableIterator directly, even though this might be a bit more work. I also wrap the merged iterator to make sure we update the metrics of iterated tables in the close method, when we know how many tables were iterated. It would be nice to at least save this wrapped iterator.\\n\\n* We need a reliable way to signal a fake Unfiltered object (the lower bound). I used an [empty row|https://github.com/stef1927/cassandra/commit/9528e672ea65c7a71c0004adfc27e5f4d9ee0acb#diff-3e7088b7213c9faaf80e18dadaaa6929] but perhaps we should use a new specialization for Unfiltered or something else.\\n\\n[~slebresne] are you happy to still be the reviewer or do you want to suggest someone else?\\n\", \"bq. Sylvain Lebresne are you happy to still be the reviewer or do you want to suggest someone else?\\n\\nI'm leaving on vacation at the end of the week and have enough on my plate until then that it's safe to assume I won't have time to look at this one. I'm happy to have a look when I'm back and have a tad more time, but it's probably a good idea to find another reviewer in the meantime in case that new reviewer has time to get to it sooner. I would suggest Branimir if he has some cycles since that ticket has a lot to do with {{MergeIterator}} and he has been dealing with that quite a bit lately.\", \"Thanks. [~blambov] would you have time to review this or shall we wait for Sylvain's return?\", \"Side note: would be nice to have some benchmarks validating the benefits of this before we commit, since that's a performance improvement.\", \"Noted. I'll work on a suitable stress profile and then attach some flight recorder or cstar_perf comparisons.\", \"Do we have a place where we're archiving these stress profiles? Seems like it would be helpful to have a profile paired with a small comment on what specific setup/portions of the system is stresses for future use. Also can socket those into the perf harness when that's ready.\", 'Yes, I have started looking at it now.', \"We definitely need a place to archive these stress profiles, attaching them to the tickets like I've done so far is not very efficient. CASSANDRA-8503 aims to collect important stress profiles for regression testing, perhaps we can continue the discussion there?\\n\\nIn terms of this specific performance optimization, so far things aren't going too well. The problem is that I don't know how to create a 'timeseries' profile with cassandra-stress, is this possible [~benedict]?  We basically need to have many clustering rows per partition and they must be ordered.\\n\\nWith the profile attached, at best I have been able to show that we are not worse:\\n\\nhttp://cstar.datastax.com/tests/id/ac8c686c-31de-11e5-95c3-42010af0688f\\nhttp://cstar.datastax.com/tests/id/11dc0080-31dd-11e5-80f3-42010af0688f\\n\\nHowever, when I changed the primary key population distribution from 1..100K to 1B, I ended up with something worse:\\n\\nhttp://cstar.datastax.com/tests/id/ac8de5ca-31de-11e5-a5b9-42010af0688f\\n\\nSo I think there is still some work to do. I have also some weirdness in the flight recorder profiles, which I have not attached due to their size but I can do if anyone is interested. I fixed two hot-spots today but there must be at least one more problem. The whole approach of a lazy wrapping iterator is a bit fragile IMO, all it takes is to call a method too soon and the entire optimization is lost, except the overhead of the wrapper iterator and the increased complexity in the merge iterator remains.\", 'Thanks, make sure you have the latest code as I have rebased and fixed a couple of things today.', 'Support for that isn\\'t fantastic, but instead of providing a select chance of 0.001, you can use the {{-insert revisit visits contents}}, and {{-pop read-lookback}} options.\\n\\n* {{visits}} tells stress to split inserts for a single partition up into multiple visits, performing a roughly equal number of inserts per visit, but ultimately inserting the entirety of the partition\\'s contents\\n* {{revisits}} tells stress _how many_ of these split-up inserts to maintain at once, and in what distribution they should be revisited (i.e. exp(1..1M) would mean the most recent to be inserted would be exponentially more likely to be visited than the oldest; uniform(1..1M) would make them all uniformly likely; both would limit the number of \"in flight\" inserts to 1M)\\n* {{read-lookback}} tells reads to operate only over those partitions that have been inserted, and only their portion that has been inserted\\n* {{contents=SORTED}} sorts the partition data before insert. This is expensive if there are many items, so you should ensure no single clustering column generates more than a couple of dozen values. If necessary, introduce more clustering columns.\\n\\nThe main problem is that these features are not well (if at all) tested, so you may encounter problems.', \"I have some worries that relate to the use of a row as the lower bound: for one there is a small risk that having two equal values produced by an iterators could be mishandled by some existing or future code, and as a second I do not know what would happen if an sstable's content starts with a range tombstone, which sorts before rows with equal clustering. Additionally, I see no changes in {{MergeIterator.OneToOne}} or {{TrivialOneToOne}}, do they not need updates?\\n\\nI believe the solution could be simpler and cleaner if the bound is given as a distinct special value that sorts before any content (similar to how range tombstone bounds sort). The existence of these special values needs only concern the arguments given to {{MergeIterator}}: the comparator, which should know to sort them before anything else with the same key (including tombstone bounds), as well as the reducer, which can then be certain that a bound is not going to be attempted to be combined with anything else, and can safely ignore it by perhaps returning null at {{getReduced}}; if necessary bounds can be easily made unequal to each other as well. {{MergeIterator}} will then pop bounds before content rather than with it, and will not need not be modified at all. \\n\", \"bq. I have some worries that relate to the use of a row as the lower bound: for one there is a small risk that having two equal values produced by an iterators could be mishandled by some existing or future code, and as a second I do not know what would happen if an sstable's content starts with a range tombstone, which sorts before rows with equal clustering. \\n\\nThe idea is that these fake Clusterable instances never leave the merge iterators. I can create a sub-class of Clusterable for clarity but it's safe to use an empty row as it is only returned as a lower bound and not as an iterated value. Have you looked at the version of the code I pushed today, where the candidate takes the lower bound from the iterator? At the moment the reducer decides if it should ignore it by checking if the row is empty. This is wrong it should be the candidate that decides that it should be ignored and the next one returned instead, except this requires a new comparison. \\n\\nbq. Additionally, I see no changes in MergeIterator.OneToOne or TrivialOneToOne, do they not need updates?\\n\\nNo they don't need updating, they never use the lower bound.\\n\\nbq. I believe the solution could be simpler and cleaner if the bound is given as a distinct special value that sorts before any content...\\n\\nYes this would solve the problem just mentioned above where we need to check if there is an equal real value immediately afterwards. I'll work on this idea, thank you very much!\\n\\n\", \"[~blambov] I've pushed a new commit where I ensure that lower bounds always compare to less than real values, when the clustering is the same. This means the existing merge iterator algorithm is now almost unchanged, the only difference is that I moved consume() into the candidate, where we make sure the lower bounds are never consumed by the reducer. We still use empty rows as lower bounds, but they are never used outside of the merge iterator candidate. We could use a specialized {{Unfiltered}} if it really bothers you however. Can you take another look?\\n\\nAs for performance, with this test the 8180 branch is ahead:\\n\\n{code}\\nuser profile=https://dl.dropboxusercontent.com/u/15683245/8180.yaml ops\\\\(insert=1,\\\\) n=5M -rate threads=300 -insert revisit=uniform\\\\(1..100\\\\) visits=fixed\\\\(25\\\\) -pop seq=1..1K read-lookback=uniform\\\\(1..1K\\\\) contents=SORTED\\n{code}\\n\\nhttp://cstar.datastax.com/graph?stats=094f57cc-3409-11e5-bd2b-42010af0688f&metric=op_rate&operation=1_user&smoothing=1&show_aggregates=true&xmin=0&xmax=1609.08&ymin=0&ymax=4637.6\\n\\nThe read command is unchanged and performance is similar or maybe still better on trunk:\\n\\n{code}\\nuser profile=https://dl.dropboxusercontent.com/u/15683245/8180.yaml ops\\\\(singleval=1,\\\\) n=5M -rate threads=300\\n{code}\\n\\nhttp://cstar.datastax.com/graph?stats=094f57cc-3409-11e5-bd2b-42010af0688f&metric=op_rate&operation=2_user&smoothing=1&show_aggregates=true&xmin=0&xmax=35.53&ymin=0&ymax=188763.3\\n\\nI think we are still visiting all sstables on the second command because the global bounds are probably the same.  Profile is attached as 8180_002.yaml.\\n\\nI've also noticed with flight recorder that {{BigTableReader.getPosition()}} and {{Tracing.trace()}} are hotspots, both on trunk and on 8180, we should probably optimize them.\", \"[~Stefania] Could you have a shot at rebasing this at some point, and [~blambov] to have another look at it once that's done. I'd hate for that code to get so outdated that its too much effort to rebase.\", 'Will rebase and also try to see if we can leverage CASSANDRA-9975 to avoid wrapping iterators.', \"Rebased and squashed [~blambov]:\\n\\n||trunk||\\n|[patch|https://github.com/stef1927/cassandra/commits/8180]|\\n|[testall|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-8180-testall/]|\\n|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-8180-dtest/]|\\n\\nThere is one issue, {{RangeTombstone.Bound.TOP}} and {{BOTTOM}} are not currently updating the min and max clustering values of {{StatsMetadata}} since their clustering values are empty arrays, see {{MetadataCollector.updateClusteringValues()}} and {{BTW.applyToMarker()}}. As a result, [some range tombstone tests|http://cassci.datastax.com/job/stef1927-8180-testall/1/testReport/junit/junit/] are failing. It's not clear to me how to capture this information in the metadata without changing its format given that the smallest / biggest values would depend on the individual clustering types, cc [~slebresne] as well.\\n\", \"{{updateClusteringValues()}} doesn't produce what you need, as it is not just {{TOP}} and {{BOTTOM}} that are mishandled, any incomplete prefix will also be. What you need is an actual {{min/maxClusteringPrefix}} which is different from the min/max of each component separately.\\n\\nI don't know if it is possible to append the data you need at the end of the stats component and have earlier versions happily ignore that data.\\n\\n\", 'I personally would prefer to not modify the behaviour of {{MergeIterator}} and keep it doing one simple thing, but this approach does have its charm.\\n\\nAn empty row will not work correctly as a lower bound. It does not sort as needed with respect to tombstone bounds, which should also be included in the test (more specifically, one that adds a row, flushes, deletes same row, flushes again, then checks if it resurfaces-- I believe this would break with the current code). Use a {{RangeTombstoneBound}} with {{DeletionTime.LIVE}} as the deletion time and a bound obtained by {{RangeTombstone.Bound.inclusiveOpen}}, which should do the right thing in both directions.\\n\\n{{IMergeIterator.LowerBound}} is cryptic, rename it to {{IteratorWithLowerBound}} to be explicit about its purpose.\\n\\nThe choice to set {{rowIndexLowerBound}} in {{partitionLevelDeletion()}} appears very arbitrary and fragile. What is the reason to do it separately from {{globalLowerBound}}? In fact, why have two separate bounds instead of one, set from the most precise information that is available at construction time?', \"Thank you for the review.\\n\\nbq. updateClusteringValues() doesn't produce what you need, as it is not just TOP and BOTTOM that are mishandled, any incomplete prefix will also be. What you need is an actual min/maxClusteringPrefix which is different from the min/max of each component separately.\\n\\nI agree that we need a full clustering prefix. What I don't understand is how things like {{shouldInclude()}} in {{ClusteringIndexNamesFilter}} or {{ClusteringIndexSliceFilter}} work. What is an example of any other incomplete prefix and do we have a gap in the tests then?\\n\\nbq. I don't know if it is possible to append the data you need at the end of the stats component and have earlier versions happily ignore that data.\\n\\nThere are 4 components that we write in the same file: VALIDATION, STATS, COMPACTION and HEADER so we can't simply keep on reading till the end of the file. However we write a TOC with the position of each component. So it should be possible but it would require changes to {{MetadataSerializer.deserialize()}} and the signature of {{IMetadataComponentSerializer.deserialize()}}, which should receive the total size to work out if there is more stuff to read at the end. I guess we can go for it.\\n\\nbq. I personally would prefer to not modify the behaviour of MergeIterator and keep it doing one simple thing, but this approach does have its charm.\\n\\nThe  changes to MergeIterator are mostly in the candidate and really minimal, the actual algorithm is unchanged. However if you do have a less invasive approach in mind I'm eager to hear it.\\n\\nbq. An empty row will not work correctly as a lower bound. It does not sort as needed with respect to tombstone bounds, which should also be included in the test (more specifically, one that adds a row, flushes, deletes same row, flushes again, then checks if it resurfaces-- I believe this would break with the current code). \\n\\nThanks, I'll add this test.\\n\\nbq. Use a RangeTombstoneBound with DeletionTime.LIVE as the deletion time and a bound obtained by RangeTombstone.Bound.inclusiveOpen, which should do the right thing in both directions.\\n\\nI'm not sure what you mean, do this for the test or the fix? I'm sure I'll work it out when I write the test though.\\n\\nbq. IMergeIterator.LowerBound is cryptic, rename it to IteratorWithLowerBound to be explicit about its purpose.\\n\\nOK\\n\\nbq. The choice to set rowIndexLowerBound in partitionLevelDeletion() appears very arbitrary and fragile. What is the reason to do it separately from globalLowerBound? In fact, why have two separate bounds instead of one, set from the most precise information that is available at construction time?\\n\\nThe global lower bound is free, since it is available in the metadata. The index lower bound is more accurate but it requires seeking the index file. Calling {{super.partitionLevelDeletion()}} also involves initializing the iterator and accessing the data file (AbstractSSTableIterator constructor). So we decided to use this more accurate bound only when we really have to access the index anyway, that is when partitionLevelDeletion() is called and there are tombstones. See [this comment|https://issues.apache.org/jira/browse/CASSANDRA-8180?focusedCommentId=14388301&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14388301] above.\\n\\nI hope to resume this in the next few days.\", \"bq. What is an example of any other incomplete prefix and do we have a gap in the tests then?\\n\\nTombstones. A {{DELETE WHERE pk = ? AND ck1 = ?}} in a table with key {{(pk, ck1, ck2)}} will generate one.\\n\\nbq. What I don't understand is how things like shouldInclude() in ClusteringIndexNamesFilter or ClusteringIndexSliceFilter work.\\n\\nIf you look at the callsites for the method, you will see that they do more work in the presence of tombstones. So one solution is not to use the {{min/maxClusteringValues}} in that case.\\n\\nbq. \\\\[MetadataSerializer.deserialize()\\\\] should receive the total size to work out if there is more stuff to read at the end.\\n\\nNo need for that, you can set a flag in {{Version}} to tell you whether or not the information is present.\\n\\nbq. I'm not sure what you mean, \\\\[use a RangeTombstoneBound\\\\] for the test or the fix?\\n\\nThis is the fix. Instead of an empty row, the lower bound should be a {{RangeTombstoneBound}} as described.\\n\\nbq. The global lower bound is free, since it is available in the metadata. The index lower bound is more accurate but it requires seeking the index file.\\n\\nIn the way you use this class, by the time {{lowerBound()}} is called, all of this is already done (by {{UnfilteredRowMergeIterator.create}}), possibly unnecessarily (if {{MergeIterator.OneToOne}} is to be used). I would just move finding the bound to {{lowerBound()}}, and I don't think it's even necessary to save the bound-- just retrieve it there, the method won't be called more than once.\\n\", 'bq. Tombstones. A {{DELETE WHERE pk = ? AND ck1 = ?}} in a table with key {{(pk, ck1, ck2)}} will generate one.\\n\\nI will add this test, thanks.\\n\\nbq. So one solution is not to use the min/maxClusteringValues in that case.\\n\\nSo maybe we could simply return a null lower bound in the presence of tombstones or is this too much of a compromise?\\n\\nbq. No need for that, you can set a flag in Version to tell you whether or not the information is present.\\n\\nDoesn\\'t this only indicate a different sstable version (\"la\", \"ma\", etc)?\\n\\nbq. This is the fix. Instead of an empty row, the lower bound should be a RangeTombstoneBound as described.\\n\\nThanks.\\n\\nbq.  I would just move finding the bound to lowerBound(), and I don\\'t think it\\'s even necessary to save the bound\\n\\nOK', \"Pushed a [commit|https://github.com/stef1927/cassandra/commit/d5cfc6fd56d50eda5d9c510591bae1d66e17ec59] where we don't use the lower bound in the presence of tombstones and {{DeleteTest}} is now passing. CI is still pending however. I would like to point out that {{iter.partitionLevelDeletion()}} is currently called for all sstables by {{queryMemtableAndDiskInternal()}} and therefore, in the presence of tombstones, we have to access the sstable anyway.\\n\\nbq. Tombstones. A DELETE WHERE pk = ? AND ck1 = ? in a table with key (pk, ck1, ck2) will generate one.\\n\\nThis case existed already in {{DeleteTest.testDeleteWithRangeAndTwoClusteringColumns()}}. It does not fail though, because the clustering comparator compares prefix values from first to last and so it works fine with incomplete prefixes.\\n\\nbq. An empty row will n\n",
            "my_comment: VALIDATION STATS COMPACTION and HEADER so we cant simply keep on reading till the end of the file. However we write a TOC with the position of each component. So it should be possible but it would require changes to {{MetadataSerializer.deserialize()}} and the signature of {{IMetadataComponentSerializer.deserialize()}} which should receive the total size to work out if there is more stuff to read at the end. I guess we can go for it.\n",
            "\n",
            "bq. I personally would prefer to not modify the behaviour of MergeIterator and keep it doing one simple thing but this approach does have its charm.\n",
            "\n",
            "The  changes to MergeIterator are mostly in the candidate and really minimal the actual algorithm is unchanged. However if you do have a less invasive approach in mind Im eager to hear it.\n",
            "\n",
            "bq. An empty row will not work correctly as a lower bound. It does not sort as needed with respect to tombstone bounds which should also be included in the test (more specifically one that adds a row flushes deletes same row flushes again then checks if it resurfaces-- I believe this would break with the current code). \n",
            "\n",
            "Thanks Ill add this test.\n",
            "\n",
            "bq. Use a RangeTombstoneBound with DeletionTime.LIVE as the deletion time and a bound obtained by RangeTombstone.Bound.inclusiveOpen which should do the right thing in both directions.\n",
            "\n",
            "Im not sure what you mean do this for the test or the fix? Im sure Ill work it out when I write the test though.\n",
            "\n",
            "bq. IMergeIterator.LowerBound is cryptic rename it to IteratorWithLowerBound to be explicit about its purpose.\n",
            "\n",
            "OK\n",
            "\n",
            "bq. The choice to set rowIndexLowerBound in partitionLevelDeletion() appears very arbitrary and fragile. What is the reason to do it separately from globalLowerBound? In fact why have two separate bounds instead of one set from the most precise information that is available at construction time?\n",
            "\n",
            "The global lower bound is free since it is available in the metadata. The index lower bound is more accurate but it requires seeking the index file. Calling {{super.partitionLevelDeletion()}} also involves initializing the iterator and accessing the data file (AbstractSSTableIterator constructor). So we decided to use this more accurate bound only when we really have to access the index anyway that is when partitionLevelDeletion() is called and there are tombstones. See [this comment|https://issues.apache.org/jira/browse/CASSANDRA-8180?focusedCommentId=14388301&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14388301] above.\n",
            "\n",
            "I hope to resume this in the next few days.\" \"bq. What is an example of any other incomplete prefix and do we have a gap in the tests then?\n",
            "\n",
            "Tombstones. A {{DELETE WHERE pk = ? AND ck1 = ?}} in a table with key {{(pk ck1 ck2)}} will generate one.\n",
            "\n",
            "bq. What I dont understand is how things like shouldInclude() in ClusteringIndexNamesFilter or ClusteringIndexSliceFilter work.\n",
            "\n",
            "If you look at the callsites for the method you will see that they do more work in the presence of tombstones. So one solution is not to use the {{min/maxClusteringValues}} in that case.\n",
            "\n",
            "bq. \\\\[MetadataSerializer.deserialize()\\\\] should receive the total size to work out if there is more stuff to read at the end.\n",
            "\n",
            "No need for that you can set a flag in {{Version}} to tell you whether or not the information is present.\n",
            "\n",
            "bq. Im not sure what you mean \\\\[use a RangeTombstoneBound\\\\] for the test or the fix?\n",
            "\n",
            "This is the fix. Instead of an empty row the lower bound should be a {{RangeTombstoneBound}} as described.\n",
            "\n",
            "bq. The global lower bound is free since it is available in the metadata. The index lower bound is more accurate but it requires seeking the index file.\n",
            "\n",
            "In the way you use this class by the time {{lowerBound()}} is called all of this is already done (by {{UnfilteredRowMergeIterator.create}}) possibly unnecessarily (if {{MergeIterator.OneToOne}} is to be used). I would just move finding the bound to {{lowerBound()}} and I dont think its even necessary to save the bound-- just retrieve it there the method wont be called more than once.\n",
            "\" bq. Tombstones. A {{DELETE WHERE pk = ? AND ck1 = ?}} in a table with key {{(pk ck1 ck2)}} will generate one.\n",
            "\n",
            "I will add this test thanks.\n",
            "\n",
            "bq. So one solution is not to use the min/maxClusteringValues in that case.\n",
            "\n",
            "So maybe we could simply return a null lower bound in the presence of tombstones or is this too much of a compromise?\n",
            "\n",
            "bq. No need for that you can set a flag in Version to tell you whether or not the information is present.\n",
            "\n",
            "Doesn\\t this only indicate a different sstable version (\"la\" \"ma\" etc)?\n",
            "\n",
            "bq. This is the fix. Instead of an empty row the lower bound should be a RangeTombstoneBound as described.\n",
            "\n",
            "Thanks.\n",
            "\n",
            "bq.  I would just move finding the bound to lowerBound() and I don\\t think it\\s even necessary to save the bound\n",
            "\n",
            "OK \"Pushed a [commit|https://github.com/stef1927/cassandra/commit/d5cfc6fd56d50eda5d9c510591bae1d66e17ec59] where we dont use the lower bound in the presence of tombstones and {{DeleteTest}} is now passing. CI is still pending however. I would like to point out that {{iter.partitionLevelDeletion()}} is currently called for all sstables by {{queryMemtableAndDiskInternal()}} and therefore in the presence of tombstones we have to access the sstable anyway.\n",
            "\n",
            "bq. Tombstones. A DELETE WHERE pk = ? AND ck1 = ? in a table with key (pk ck1 ck2) will generate one.\n",
            "\n",
            "This case existed already in {{DeleteTest.testDeleteWithRangeAndTwoClusteringColumns()}}. It does not fail though because the clustering comparator compares prefix values from first to last and so it works fine with incomplete prefixes.\n",
            "\n",
            "bq. An empty row will n\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8414\n",
            "issue_type: Improvement\n",
            "summary: Avoid loops over array backed iterators that call iter.remove()\n",
            "description: I noticed from sampling that sometimes compaction spends almost all of its time in iter.remove() in ColumnFamilyStore.removeDeletedStandard. It turns out that the cf object is using ArrayBackedSortedColumns, so deletes are from an ArrayList. If the majority of your columns are GCable tombstones then this is O(n^2). The data structure should be changed or a copy made to avoid this.\n",
            "architectural impact: NO\n",
            "comments: [\"I've edited the title because it's not quite that compaction is O(n^2), but that certain operations within a partition are. It's also not limited to just that specific method. The best solution is probably to introduce a special deletion iterator on which a call to remove() simply sets a corresponding bit to 1; once we exhaust the iterator we commit the deletes in one pass.\", '/cc [~slebresne]', \"Actually Richard's issue is with 1.2 and 2.0.\\n\\nI'm not sure how much of an issue in practice it really is for compaction in 2.1, w/ only LazilyCompactedRow there, and PreCompactedRow gone.\", \"An attempt to implement a BatchRemovalIterator on the 2.0 branch. I've also tested that it indeed is much faster. For non-removals, it should be the same thing.\", 'Do you have time to review, [~rlow]?', 'Yes, I can review this week.', 'We should integrate this for 2.1 also, since this behaviour is exhibited still, just not in compaction. In 2.1 we should use System.arraycopy and removed.nextSetBit though, as the performance will be improved, particularly for sparse removes.', \"Good point. I've attached a new patch containing code using removed.nextSetBit and Collections.copy. Should be easy now to change for 2.1.\", \"Nice backporting of the better approach.\\n\\nI've uploaded a tweaked version, the goal of which was just to clean up the variable names (and switch to a while loop) so it's more obvious what's happening. But while at it I also added use of nextClearBit in tandem with nextSetBit, as it's a minor tweak but gives better behaviour with runs of adjacent removes.\\n\\nI haven't properly reviewed otherwise, but it might be worth introducing this to CFS.removeDroppedColumns() and SliceQueryFilter.trim(), \", \"Thanks for writing the patch! A few comments:\\n\\n- v3 patch has BatchIterator interface missing.\\n- Some unnecessary formatting changes and import order switching.\\n- remove method should throw IllegalStateException if called twice on same element to adhere to Iterator spec.\\n- Calling commit twice will remove incorrect elements. Should throw IllegalStateException if commit is called more than once or make it idempotent.\\n- Could add 'assert test <= src;' to copy method to enforce comment.\", 'Hi,\\n\\nThanks for your comments!\\n\\nv4 uploaded which fixes your comments. I also added the batch iterator to CFS.removeDroppedColumns() as that was easy enough, but skipped SliceQueryFilter for now.\\n', 'v5 uploaded since I noticed there were still some unnecessary formatting changes in the unit test. Working on fixing my IntelliJ settings as I speak...', '+1 on 2.0 v5. Do you have a 2.1 version?', 'Patch for 2.1 added.\\n', 'Only minor nit is that the BitSet can be initialized with size rather than cells.length, but otherwise +1.', 'Right, of course. v6 attached.', 'I tested this on some real workload SSTables and got a 2x speedup on force compaction! Also the output was the same as before.\\n\\nCan someone commit the patch?', \"I'm hesitant to add it to 2.0 at all, but, will commit once 2.0.12 is out (which is soon), so that this gets some testing time before it gets into a 2.0-line release (2.0.13 if everything is fine).\", \"All right, committed to both 2.0 and 2.1. I fixed some nits in both - added a missing license header, made everything conform our code style, and made some renames for consistency with the other ABSC iterators we have (esp. in 2.1 impl)\\n\\nAlso, in 2.1, made AtomicBTreeColumns#getBatchRemoveIterator() throw UOE instead of adding a dummy implementation to CF.\\n\\nThe 2.1 version had three bugs in it that I fixed on commit:\\n1. maybeSortCells() wasn't being called before returning the iterator\\n2. at the end of commit, only 'size' was updated, but 'sortedSize' was not\\n3. unlike the 2.0 version, the 2.1 version was not nullifying the trimmed cells at the end of the array\", 'Marking as Resolved. Thanks everyone.', 'Thank you!']\n",
            "my_comment: [\"Ive edited the title because its not quite that compaction is O(n^2) but that certain operations within a partition are. Its also not limited to just that specific method. The best solution is probably to introduce a special deletion iterator on which a call to remove() simply sets a corresponding bit to 1; once we exhaust the iterator we commit the deletes in one pass.\" /cc [~slebresne] \"Actually Richards issue is with 1.2 and 2.0.\n",
            "\n",
            "Im not sure how much of an issue in practice it really is for compaction in 2.1 w/ only LazilyCompactedRow there and PreCompactedRow gone.\" \"An attempt to implement a BatchRemovalIterator on the 2.0 branch. Ive also tested that it indeed is much faster. For non-removals it should be the same thing.\" Do you have time to review [~rlow]? Yes I can review this week. We should integrate this for 2.1 also since this behaviour is exhibited still just not in compaction. In 2.1 we should use System.arraycopy and removed.nextSetBit though as the performance will be improved particularly for sparse removes. \"Good point. Ive attached a new patch containing code using removed.nextSetBit and Collections.copy. Should be easy now to change for 2.1.\" \"Nice backporting of the better approach.\n",
            "\n",
            "Ive uploaded a tweaked version the goal of which was just to clean up the variable names (and switch to a while loop) so its more obvious whats happening. But while at it I also added use of nextClearBit in tandem with nextSetBit as its a minor tweak but gives better behaviour with runs of adjacent removes.\n",
            "\n",
            "I havent properly reviewed otherwise but it might be worth introducing this to CFS.removeDroppedColumns() and SliceQueryFilter.trim() \" \"Thanks for writing the patch! A few comments:\n",
            "\n",
            "- v3 patch has BatchIterator interface missing.\n",
            "- Some unnecessary formatting changes and import order switching.\n",
            "- remove method should throw IllegalStateException if called twice on same element to adhere to Iterator spec.\n",
            "- Calling commit twice will remove incorrect elements. Should throw IllegalStateException if commit is called more than once or make it idempotent.\n",
            "- Could add assert test <= src; to copy method to enforce comment.\" Hi\n",
            "\n",
            "Thanks for your comments!\n",
            "\n",
            "v4 uploaded which fixes your comments. I also added the batch iterator to CFS.removeDroppedColumns() as that was easy enough but skipped SliceQueryFilter for now.\n",
            " v5 uploaded since I noticed there were still some unnecessary formatting changes in the unit test. Working on fixing my IntelliJ settings as I speak... +1 on 2.0 v5. Do you have a 2.1 version? Patch for 2.1 added.\n",
            " Only minor nit is that the BitSet can be initialized with size rather than cells.length but otherwise +1. Right of course. v6 attached. I tested this on some real workload SSTables and got a 2x speedup on force compaction! Also the output was the same as before.\n",
            "\n",
            "Can someone commit the patch? \"Im hesitant to add it to 2.0 at all but will commit once 2.0.12 is out (which is soon) so that this gets some testing time before it gets into a 2.0-line release (2.0.13 if everything is fine).\" \"All right committed to both 2.0 and 2.1. I fixed some nits in both - added a missing license header made everything conform our code style and made some renames for consistency with the other ABSC iterators we have (esp. in 2.1 impl)\n",
            "\n",
            "Also in 2.1 made AtomicBTreeColumns#getBatchRemoveIterator() throw UOE instead of adding a dummy implementation to CF.\n",
            "\n",
            "The 2.1 version had three bugs in it that I fixed on commit:\n",
            "1. maybeSortCells() wasnt being called before returning the iterator\n",
            "2. at the end of commit only size was updated but sortedSize was not\n",
            "3. unlike the 2.0 version the 2.1 version was not nullifying the trimmed cells at the end of the array\" Marking as Resolved. Thanks everyone. Thank you!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-8627\n",
            "issue_type: Improvement\n",
            "summary: Support Total/Recent latency histogram metrics for range slices\n",
            "description: The Metrics histogram is pretty bad at non-normal data like latencies and (empirically tested and theoretically) is untrustworthy at 99th percentile. For applications that care about the percentiles having the more statistically accurate version is beneficial.  Adding the deprecated methods like other latency histograms for CASSANDRA-7338 temporarily would help.\n",
            "\n",
            "This is just for 2.1 branch. CASSANDRA-5657 solves everything in 3.0.\n",
            "architectural impact: NO\n",
            "comments: ['Committed, thanks.']\n",
            "my_comment: Committed thanks.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9134\n",
            "issue_type: Bug\n",
            "summary: Fix leak detected errors in unit tests\n",
            "description: There are several of these errors when running unit tests on trunk:\n",
            "\n",
            "{code}\n",
            "    [junit] ERROR 01:09:36 LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@317c884a) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@943674927:Memory@[7f1bcc0078e0..7f1bcc007908) was not released before the reference was garbage collected\n",
            "    [junit] ERROR 01:09:36 LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@317c884a) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@943674927:Memory@[7f1bcc0078e0..7f1bcc007908) was not released before the reference was garbage collected\n",
            "    [junit] ERROR 01:09:36 Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@317c884a:\n",
            "    [junit] Thread[CompactionExecutor:1,1,main]\n",
            "    [junit] \tat java.lang.Thread.getStackTrace(Thread.java:1589)\n",
            "    [junit] \tat org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:200)\n",
            "    [junit] \tat org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:133)\n",
            "    [junit] \tat org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:60)\n",
            "    [junit] \tat org.apache.cassandra.io.util.SafeMemory.<init>(SafeMemory.java:33)\n",
            "    [junit] \tat org.apache.cassandra.io.util.SafeMemoryWriter.<init>(SafeMemoryWriter.java:31)\n",
            "    [junit] \tat org.apache.cassandra.io.sstable.IndexSummaryBuilder.<init>(IndexSummaryBuilder.java:112)\n",
            "    [junit] \tat org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter.<init>(BigTableWriter.java:491)\n",
            "    [junit] \tat org.apache.cassandra.io.sstable.format.big.BigTableWriter.<init>(BigTableWriter.java:83)\n",
            "    [junit] \tat org.apache.cassandra.io.sstable.format.big.BigFormat$WriterFactory.open(BigFormat.java:107)\n",
            "    [junit] \tat org.apache.cassandra.io.sstable.format.SSTableWriter.create(SSTableWriter.java:89)\n",
            "    [junit] \tat org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.<init>(DefaultCompactionWriter.java:53)\n",
            "    [junit] \tat org.apache.cassandra.db.compaction.CompactionTask.getCompactionAwareWriter(CompactionTask.java:253)\n",
            "    [junit] \tat org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:153)\n",
            "    [junit] \tat org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)\n",
            "    [junit] \tat org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:73)\n",
            "    [junit] \tat org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:58)\n",
            "    [junit] \tat org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:239)\n",
            "    [junit] \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n",
            "    [junit] \tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n",
            "    [junit] \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
            "    [junit] \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
            "    [junit] \tat java.lang.Thread.run(Thread.java:745)\n",
            "    [junit] \n",
            "    [junit] ERROR 01:09:36 Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@317c884a:\n",
            "    [junit] Thread[CompactionExecutor:1,1,main]\n",
            "    [junit] \tat java.lang.Thread.getStackTrace(Thread.java:1589)\n",
            "    [junit] \tat org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:200)\n",
            "    [junit] \tat org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:133)\n",
            "    [junit] \tat org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:60)\n",
            "    [junit] \tat org.apache.cassandra.io.util.SafeMemory.<init>(SafeMemory.java:33)\n",
            "    [junit] \tat org.apache.cassandra.io.util.SafeMemoryWriter.<init>(SafeMemoryWriter.java:31)\n",
            "    [junit] \tat org.apache.cassandra.io.sstable.IndexSummaryBuilder.<init>(IndexSummaryBuilder.java:112)\n",
            "    [junit] \tat org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter.<init>(BigTableWriter.java:491)\n",
            "    [junit] \tat org.apache.cassandra.io.sstable.format.big.BigTableWriter.<init>(BigTableWriter.java:83)\n",
            "    [junit] \tat org.apache.cassandra.io.sstable.format.big.BigFormat$WriterFactory.open(BigFormat.java:107)\n",
            "    [junit] \tat org.apache.cassandra.io.sstable.format.SSTableWriter.create(SSTableWriter.java:89)\n",
            "    [junit] \tat org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.<init>(DefaultCompactionWriter.java:53)\n",
            "    [junit] \tat org.apache.cassandra.db.compaction.CompactionTask.getCompactionAwareWriter(CompactionTask.java:253)\n",
            "    [junit] \tat org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:153)\n",
            "    [junit] \tat org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)\n",
            "    [junit] \tat org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:73)\n",
            "    [junit] \tat org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:58)\n",
            "    [junit] \tat org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:239)\n",
            "    [junit] \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n",
            "    [junit] \tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n",
            "    [junit] \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
            "    [junit] \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
            "    [junit] \tat java.lang.Thread.run(Thread.java:745)\n",
            "    [junit] \n",
            "{code}\n",
            "architectural impact: NO\n",
            "comments: ['Most of the leaks were fixed by CASSANDRA-9117, the patch attached covers 2 remaining unit tests (other than the RefTests).', 'Hi [~thobbs]], would you mind reviewing?', '+1, committed as {{1f65a12}}.  Thanks!']\n",
            "my_comment: Most of the leaks were fixed by CASSANDRA-9117 the patch attached covers 2 remaining unit tests (other than the RefTests). Hi [~thobbs]] would you mind reviewing? +1 committed as {{1f65a12}}.  Thanks!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-915\n",
            "issue_type: Bug\n",
            "summary: disallow column family names containing hyphens\n",
            "description: You cannot use use hyphens in column family names because hyphens are used as delimiters in sstable filenames (which are derived from the CF name). \n",
            "\n",
            "It should be an error to configure such a column family name.\n",
            "architectural impact: NO\n",
            "comments: ['The attached patch should take care of 0.6 (and trunk for the time being), but this ticket will need to be added as a dependency to CASSANDRA-44 in order to make sure that the new-shiny handles this case as well.', '+1', 'committed to 0.6 and trunk', \"Integrated in Cassandra #389 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/389/])\\n    don't allow hyphens in column family names\\n\\nPatch by eevans; reviewed by jbellis for \\n\"]\n",
            "my_comment: The attached patch should take care of 0.6 (and trunk for the time being) but this ticket will need to be added as a dependency to CASSANDRA-44 in order to make sure that the new-shiny handles this case as well. +1 committed to 0.6 and trunk \"Integrated in Cassandra #389 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/389/])\n",
            "    dont allow hyphens in column family names\n",
            "\n",
            "Patch by eevans; reviewed by jbellis for \n",
            "\"]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9271\n",
            "issue_type: Improvement\n",
            "summary: IndexSummaryManagerTest.testCompactionRace times out periodically\n",
            "description: The issue is that the amount of time the test takes is highly variable to it being biased towards creating a condition where the test has to retry the compaction it is attempting.\n",
            "\n",
            "Solution is to decrease the bias by having https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L2522 check every millisecond instead of every 100 milliseconds.\n",
            "architectural impact: NO\n",
            "comments: ['Change implemented here https://github.com/apache/cassandra/commit/40687bd44bf21b1e5caf4c42b85035e66e51e6bd', 'http://cassci.datastax.com/job/trunk_testall/63/testReport/org.apache.cassandra.io.sstable/IndexSummaryManagerTest/testCompactionRace/', 'This test is consistently timing out on the Windows CI env:\\nhttp://cassci.datastax.com/job/cassandra-2.2_utest_win32/5/testReport/junit/org.apache.cassandra.io.sstable/IndexSummaryManagerTest/testCompactionRace/', \"A couple of observations: 1) there's a non-deterministic time required for this test to pass, 2) major compactions on CI take an order of magnitude longer to complete than they do on my SSD-enabled laptop, and 3) upping the timeout on all unit tests to 120 makes this test pass on the CI box *sometimes*.\\n\\nI think we should move testCompactionRace out to a long test. [~benedict]: you're the author of this test and it's been flaky w/regards to passing off and on since introduction. Thoughts on long-testing it?\", \"Actually [~tjake] is the author; I just committed it along with the fix(es). From my POV we should *both* 1) move it to a long test; 2) make it more robust\\n\\nI'm pretty sure performing an actual major compaction is unnecessary, and this is largely where the problem comes from (with the indeterminate sleeping as we wait for cessation of other active tasks). What we need is for the instances of reader to change as we try marking the files for redistribution, and we should probably just do this in a direct tight loop so that both code sections are spinning in direct contention. This both increases the likelihood of the failure scenario, but also ensures that if we don't encounter it we can stop promptly (by just spinning for a fixed interval, and calling it a day if we haven't failed).\", \"bq. Actually T Jake Luciani is the author; I just committed it along with the fix(es)\\nNow that you mention that, I'm pretty sure we've already gone over this in JIRA comments on another ticket.\", 're-committed [~aweisberg] fix in 4362e71', 'This is still happening and pretty much hard failing.', \"I ran six builds of this in cassci and it it didn't fail in any of them.\\nhttp://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-C-9271-2-testall/\\n\\nhttps://github.com/apache/cassandra/compare/trunk...aweisberg:C-9271-2\\nhttps://github.com/apache/cassandra/compare/trunk...aweisberg:C-9271-2.diff\", 'I stopped at 106 loop runs over this test - lgtm :)', \"It may fix this behaviour, but unfortunately I am -1 on using Thread.yield(). It depends on the priority of the thread, and that is generally not good practice. It could lead to spinning burning a full CPU flat out until compactions acquiesce. I don't like making application behaviour even slightly less good in order to fix a problematic test.\\n\\nAs I stated above, it should be possible to modify this test to simply mark and unmark compacting directly, rather than actually perform a major compaction.\", \"OK I'll back out Thread.yield().\", 'I propose we remove the test and replace it with this single line. The goal of the test was to ensure we could not mark compacting the incorrect instance of an sstable. This test checks that.', 'OK. I am +1 on replacing it with the simpler assertion on the invariant maintained by Tracker.']\n",
            "my_comment: youre the author of this test and its been flaky w/regards to passing off and on since introduction. Thoughts on long-testing it?\" \"Actually [~tjake] is the author; I just committed it along with the fix(es). From my POV we should *both* 1) move it to a long test; 2) make it more robust\n",
            "\n",
            "Im pretty sure performing an actual major compaction is unnecessary and this is largely where the problem comes from (with the indeterminate sleeping as we wait for cessation of other active tasks). What we need is for the instances of reader to change as we try marking the files for redistribution and we should probably just do this in a direct tight loop so that both code sections are spinning in direct contention. This both increases the likelihood of the failure scenario but also ensures that if we dont encounter it we can stop promptly (by just spinning for a fixed interval and calling it a day if we havent failed).\" \"bq. Actually T Jake Luciani is the author; I just committed it along with the fix(es)\n",
            "Now that you mention that Im pretty sure weve already gone over this in JIRA comments on another ticket.\" re-committed [~aweisberg] fix in 4362e71 This is still happening and pretty much hard failing. \"I ran six builds of this in cassci and it it didnt fail in any of them.\n",
            "http://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-C-9271-2-testall/\n",
            "\n",
            "https://github.com/apache/cassandra/compare/trunk...aweisberg:C-9271-2\n",
            "https://github.com/apache/cassandra/compare/trunk...aweisberg:C-9271-2.diff\" I stopped at 106 loop runs over this test - lgtm :) \"It may fix this behaviour but unfortunately I am -1 on using Thread.yield(). It depends on the priority of the thread and that is generally not good practice. It could lead to spinning burning a full CPU flat out until compactions acquiesce. I dont like making application behaviour even slightly less good in order to fix a problematic test.\n",
            "\n",
            "As I stated above it should be possible to modify this test to simply mark and unmark compacting directly rather than actually perform a major compaction.\" \"OK Ill back out Thread.yield().\" I propose we remove the test and replace it with this single line. The goal of the test was to ensure we could not mark compacting the incorrect instance of an sstable. This test checks that. OK. I am +1 on replacing it with the simpler assertion on the invariant maintained by Tracker.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9411\n",
            "issue_type: Bug\n",
            "summary: Bound statement executions fail after adding a collection-type column\n",
            "description: After adding a collection-type column to an existing table, executions of statements that are already prepared result in server error (error code 0), with the error message {{java.lang.ArrayIndexOutOfBoundsException}}.\n",
            "\n",
            "To reproduce it.\n",
            "{code:java}\n",
            "session.execute(\"CREATE TABLE tbl1 (a text, b text, c text, PRIMARY KEY (a, b))\");\n",
            "//prepare initially\n",
            "PreparedStatement ps = session.prepare(\"SELECT a, b, c FROM tbl1\");\n",
            "//insert some data\n",
            "session.execute(\"INSERT INTO tbl1 (a, b, c) VALUES ('a1', 'b1', 'c1')\");\n",
            "//Executes successfully as expected\n",
            "session.execute(ps.bind());\n",
            "//Add a column of a collection type\n",
            "session.execute(\"ALTER TABLE tbl1 ADD d set<text>\");\n",
            "//All following executions fail\n",
            "session.execute(ps.bind());\n",
            "{code}\n",
            "\n",
            "Some notes:\n",
            "- This only occurs for SELECT with fields (not with SELECT *)\n",
            "- This only occurs with C* 2.0. Probably because CASSANDRA-7910 was applied for 2.1+\n",
            "- This only occurs if the column added is a collection type (list / set / map)\n",
            "- This occurs with all SELECT statements using that column family, that were already prepared.\n",
            "\n",
            "Repreparing it on all hosts fixes the issue, but for that, the user should normally restart existing application (even if the existing apps/apps versions don't handle this new field).\n",
            "architectural impact: NO\n",
            "comments: ['Server Stack trace, C* 2.0.11: \\n{code}\\nERROR [Native-Transport-Requests:53] 2015-05-21 16:49:24,657 ErrorMessage.java (line 230) Unexpected exception during request\\njava.lang.ArrayIndexOutOfBoundsException: 2\\n\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap.add(ColumnGroupMap.java:48)\\n\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap.access$200(ColumnGroupMap.java:32)\\n\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap$Builder.add(ColumnGroupMap.java:140)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.processColumnFamily(SelectStatement.java:1176)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:1079)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:285)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:241)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:65)\\n\\tat org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:158)\\n\\tat org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:309)\\n\\tat org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:132)\\n\\tat org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:321)\\n\\tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\\n\\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\\n\\tat org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\\n\\tat org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)\\n\\tat org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\\n\\tat java.lang.Thread.run(Thread.java:745)\\n{code}', 'Server stack trace using C* 2.0.13:\\n{code}\\nERROR [Native-Transport-Requests:53] 2015-05-21 17:04:10,840 ErrorMessage.java (line 231) Unexpected exception during request\\njava.lang.ArrayIndexOutOfBoundsException: 2\\n\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap.add(ColumnGroupMap.java:51)\\n\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap.access$200(ColumnGroupMap.java:35)\\n\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap$Builder.add(ColumnGroupMap.java:167)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.processColumnFamily(SelectStatement.java:1237)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:1123)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:286)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:242)\\n\\tat org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:64)\\n\\tat org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:158)\\n\\tat org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:309)\\n\\tat org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:132)\\n\\tat org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:321)\\n\\tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\\n\\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\\n\\tat org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\\n\\tat org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)\\n\\tat org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\\n\\tat java.lang.Thread.run(Thread.java:745)\\n {code}', \"The problem is that on C* 2.0, {{SelectStatement}} aliases {{CFDefinition}} instead of {{CFMetaData}}, the former being some post-processed version of {{CFMetaData}} more suited to CQL. Except that when altering the table, while {{CFMetadaData}} rebuilds a new updated {{CFDefinition}}, prepared statements will still alias the old version and have some updated information. As a result, we compute a broken index in {{ColumnGroupMap.Builder}} (the comparator passed is the proper one, with collections infos, but the {{hasCollections}} flag is outdated). The good fix is probably just to have {{SelectStatement}} alias {{CFMetaData}} directly and call {{CFMetaData.getCfDef}} when it needs the {{CFDefinition}} object. [~blerer] can you have a look at that soonish?\\n\\nI'll note that this won't affect 2.1+, CASSANDRA-7910 or not.\", 'I was planing to have a look at it right now :-)', 'The patch changes {{SelectStatement}} to store the {{CFMetadata}} instead of the {{CFDefinition}}. The {{CFDefinition}} is created in the {{execute}} methods and passed to all the methods that need it. ', '+1, committed (only to 2.0 and merged with {{--strategy=ours}} to 2.1 since nothing of that applies there).']\n",
            "my_comment: 2\n",
            "\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap.add(ColumnGroupMap.java:51)\n",
            "\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap.access$200(ColumnGroupMap.java:35)\n",
            "\\tat org.apache.cassandra.cql3.statements.ColumnGroupMap$Builder.add(ColumnGroupMap.java:167)\n",
            "\\tat org.apache.cassandra.cql3.statements.SelectStatement.processColumnFamily(SelectStatement.java:1237)\n",
            "\\tat org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:1123)\n",
            "\\tat org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:286)\n",
            "\\tat org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:242)\n",
            "\\tat org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:64)\n",
            "\\tat org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:158)\n",
            "\\tat org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:309)\n",
            "\\tat org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:132)\n",
            "\\tat org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:321)\n",
            "\\tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n",
            "\\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n",
            "\\tat org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n",
            "\\tat org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)\n",
            "\\tat org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)\n",
            "\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
            "\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
            "\\tat java.lang.Thread.run(Thread.java:745)\n",
            " {code} \"The problem is that on C* 2.0 {{SelectStatement}} aliases {{CFDefinition}} instead of {{CFMetaData}} the former being some post-processed version of {{CFMetaData}} more suited to CQL. Except that when altering the table while {{CFMetadaData}} rebuilds a new updated {{CFDefinition}} prepared statements will still alias the old version and have some updated information. As a result we compute a broken index in {{ColumnGroupMap.Builder}} (the comparator passed is the proper one with collections infos but the {{hasCollections}} flag is outdated). The good fix is probably just to have {{SelectStatement}} alias {{CFMetaData}} directly and call {{CFMetaData.getCfDef}} when it needs the {{CFDefinition}} object. [~blerer] can you have a look at that soonish?\n",
            "\n",
            "Ill note that this wont affect 2.1+ CASSANDRA-7910 or not.\" I was planing to have a look at it right now :-) The patch changes {{SelectStatement}} to store the {{CFMetadata}} instead of the {{CFDefinition}}. The {{CFDefinition}} is created in the {{execute}} methods and passed to all the methods that need it.  +1 committed (only to 2.0 and merged with {{--strategy=ours}} to 2.1 since nothing of that applies there).\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9631\n",
            "issue_type: Bug\n",
            "summary: Unnecessary required filtering for query on indexed clustering key\n",
            "description: Let's create and populate a simple table composed of one partition key {{a}}, two clustering keys {{b}} & {{c}}, and one secondary index on a standard column {{e}}:\n",
            "\n",
            "{code:sql}\n",
            "$ cqlsh 127.0.0.1\n",
            "Connected to test21 at 127.0.0.1:9160.\n",
            "[cqlsh 4.1.1 | Cassandra 2.1.6-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]\n",
            "Use HELP for help.\n",
            "cqlsh> CREATE KEYSPACE test WITH REPLICATION={'class': 'SimpleStrategy', 'replication_factor': 3};\n",
            "cqlsh> CREATE TABLE test.table1 (\n",
            "   ...     a int,\n",
            "   ...     b int,\n",
            "   ...     c int,\n",
            "   ...     d int,\n",
            "   ...     e int,\n",
            "   ...     PRIMARY KEY (a, b, c)\n",
            "   ... );\n",
            "cqlsh> CREATE INDEX table1_e ON test.table1 (e);\n",
            "cqlsh> INSERT INTO test.table1 (a, b, c, d, e) VALUES (1, 1, 1, 1, 1);\n",
            "(...)\n",
            "cqlsh> SELECT * FROM test.table1;\n",
            "\n",
            " a | b | c | d | e\n",
            "---+---+---+---+---\n",
            " 1 | 1 | 1 | 1 | 1\n",
            " 1 | 1 | 2 | 2 | 2\n",
            " 1 | 1 | 3 | 3 | 3\n",
            " 1 | 2 | 1 | 1 | 3\n",
            " 1 | 3 | 1 | 1 | 1\n",
            " 2 | 4 | 1 | 1 | 1\n",
            "\n",
            "(6 rows)\n",
            "{code}\n",
            "\n",
            "With such a schema, I am allowed to query on the indexed column without filtering by providing the first two elements of the primary key:\n",
            "{code:sql}\n",
            "cqlsh> SELECT * FROM test.table1 WHERE a=1 AND b=1 AND e=3;\n",
            "\n",
            " a | b | c | d | e\n",
            "---+---+---+---+---\n",
            " 1 | 1 | 3 | 3 | 3\n",
            "\n",
            "(1 rows)\n",
            "{code}\n",
            "\n",
            "Let's now introduce an index on the first clustering key:\n",
            "{code:sql}\n",
            "cqlsh> CREATE INDEX table1_b ON test.table1 (b);\n",
            "{code}\n",
            "\n",
            "Now, I expect the same query as above to work without filtering, but it's not:\n",
            "{code:sql}\n",
            "cqlsh> SELECT * FROM test.table1 WHERE a=1 AND b=1 AND e=3;\n",
            "Bad Request: Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING\n",
            "{code}\n",
            "\n",
            "I think this is a bug on the way secondary indexes are accounted for when checking for unfiltered queries.\n",
            "architectural impact: NO\n",
            "comments: ['Is there a chance that CASSANDRA-8418 introduced this bug ?', '/cc [~blerer]', \"It's actually not a bug. The new index you've introduced might be used by Cassandra (whether it is or not depends on some internal metrics on your data set), and filtering will be involved if that index is used as the primary index. And so requiring {{ALLOW FILTERING}} is correct.\\n\\nNow, using the index on {{b}} for that query is not smart and C* should always stick to the index on {{e}} for that query as it will be faster and won't require filtering. It's just not what happens in 2.1. So we can definitively improve this. That said, a (very) quick glance at 2.2/trunk seems to suggest that this improvement is actually made there. So [~blerer], can you double check if that is the case? If so, we can close this (as duplicate of CASSANDRA-7981?). Otherwise, let's look at improving it.\", \"{quote}Now, using the index on b for that query is not smart and C* should always stick to the index on e for that query as it will be faster and won't require filtering. It's just not what happens in 2.1.{quote}\\n\\nI had a look at the code of 2.1 and it seems that it used to ignore the index on {{b}} for this specific case. I broke that behaviour when I implemented CASSANDRA-8275. The 2.2/trunk behaviour was not affected by my patch.\\n\\nI will provide some patches to fix the problem in 2.0 and 2.1 and add some extra unit tests to all the versions.\", 'Thanks [~blerer] for this quick update ! :)', 'The patches for 2.0 and 2.1 fix the problem and add extra unit tests to validate the behaviour.\\n\\nThe patch for 2.2 just add extra unit tests.\\n\\nThe results of the tests are bellow:\\n* [unit tests for 2.0|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.0-testall/lastCompletedBuild/testReport/]\\n* [dtests for 2.0|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.0-dtest/lastCompletedBuild/testReport/]\\n* [unit tests for 2.1|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.1-testall/lastCompletedBuild/testReport/]\\n* [dtests for 2.1|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.1-dtest/lastCompletedBuild/testReport/]\\n* [unit tests for 2.2|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.2-testall/lastCompletedBuild/testReport/]\\n* [dtests for 2.2|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.2-dtest/lastCompletedBuild/testReport/]', '[~thobbs] could you review?', '+1, committed to 2.0 as {{f2db756abd135cc6ca4cf657d29fb2601764d50f}} and merged to 2.1, 2.2, and trunk.  Thanks!']\n",
            "my_comment: Is there a chance that CASSANDRA-8418 introduced this bug ? /cc [~blerer] \"Its actually not a bug. The new index youve introduced might be used by Cassandra (whether it is or not depends on some internal metrics on your data set) and filtering will be involved if that index is used as the primary index. And so requiring {{ALLOW FILTERING}} is correct.\n",
            "\n",
            "Now using the index on {{b}} for that query is not smart and C* should always stick to the index on {{e}} for that query as it will be faster and wont require filtering. Its just not what happens in 2.1. So we can definitively improve this. That said a (very) quick glance at 2.2/trunk seems to suggest that this improvement is actually made there. So [~blerer] can you double check if that is the case? If so we can close this (as duplicate of CASSANDRA-7981?). Otherwise lets look at improving it.\" \"{quote}Now using the index on b for that query is not smart and C* should always stick to the index on e for that query as it will be faster and wont require filtering. Its just not what happens in 2.1.{quote}\n",
            "\n",
            "I had a look at the code of 2.1 and it seems that it used to ignore the index on {{b}} for this specific case. I broke that behaviour when I implemented CASSANDRA-8275. The 2.2/trunk behaviour was not affected by my patch.\n",
            "\n",
            "I will provide some patches to fix the problem in 2.0 and 2.1 and add some extra unit tests to all the versions.\" Thanks [~blerer] for this quick update ! :) The patches for 2.0 and 2.1 fix the problem and add extra unit tests to validate the behaviour.\n",
            "\n",
            "The patch for 2.2 just add extra unit tests.\n",
            "\n",
            "The results of the tests are bellow:\n",
            "* [unit tests for 2.0|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.0-testall/lastCompletedBuild/testReport/]\n",
            "* [dtests for 2.0|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.0-dtest/lastCompletedBuild/testReport/]\n",
            "* [unit tests for 2.1|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.1-testall/lastCompletedBuild/testReport/]\n",
            "* [dtests for 2.1|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.1-dtest/lastCompletedBuild/testReport/]\n",
            "* [unit tests for 2.2|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.2-testall/lastCompletedBuild/testReport/]\n",
            "* [dtests for 2.2|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-9631-2.2-dtest/lastCompletedBuild/testReport/] [~thobbs] could you review? +1 committed to 2.0 as {{f2db756abd135cc6ca4cf657d29fb2601764d50f}} and merged to 2.1 2.2 and trunk.  Thanks!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9636\n",
            "issue_type: Bug\n",
            "summary: Duplicate columns in selection causes AssertionError\n",
            "description: Prior to CASSANDRA-9532, unaliased duplicate fields in a selection would be silently ignored. Now, they trigger a server side exception and an unfriendly error response, which we should clean up. Duplicate columns *with* aliases are not affected.\n",
            "\n",
            "{code}\n",
            "CREATE KEYSPACE ks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
            "CREATE TABLE ks.t1 (k int PRIMARY KEY, v int);\n",
            "INSERT INTO ks.t2 (k, v) VALUES (0, 0);\n",
            "SELECT k, v FROM ks.t2;\n",
            "SELECT k, v, v AS other_v FROM ks.t2;\n",
            "SELECT k, v, v FROM ks.t2;\n",
            "{code}\n",
            "\n",
            "The final statement results in this error response & server side stacktrace:\n",
            "{code}\n",
            "ServerError: <ErrorMessage code=0000 [Server error] message=\"java.lang.AssertionError\">\n",
            "\n",
            "\n",
            "ERROR 13:01:30 Unexpected exception during request; channel = [id: 0x44d22e61, /127.0.0.1:39463 => /127.0.0.1:9042]\n",
            "java.lang.AssertionError: null\n",
            "        at org.apache.cassandra.cql3.ResultSet.addRow(ResultSet.java:63) ~[main/:na]\n",
            "        at org.apache.cassandra.cql3.statements.Selection$ResultSetBuilder.build(Selection.java:355) ~[main/:na]\n",
            "        at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:1226) ~[main/:na]\n",
            "        at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:299) ~[main/:na]\n",
            "        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:238) ~[main/:na]\n",
            "        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:67) ~[main/:na]\n",
            "        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:238) ~[main/:na]\n",
            "        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:260) ~[main/:na]\n",
            "        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:119) ~[main/:na]\n",
            "        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:439) [main/:na]\n",
            "        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:335) [main/:na]\n",
            "        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]\n",
            "        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]\n",
            "        at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]\n",
            "        at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]\n",
            "        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_45]\n",
            "        at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [main/:na]\n",
            "        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]\n",
            "        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]\n",
            "{code}\n",
            "\n",
            "\n",
            "This issue also presents on the head of the 2.2 branch and on 2.0.16. However, the prior behaviour is different on both of those branches.\n",
            "In the 2.0 line prior to CASSANDRA-9532, duplicate columns would actually be included in the results, as opposed to being silently dropped as per 2.1.x\n",
            "In 2.2, the assertion error seen above precedes CASSANDRA-9532 and is also triggered for both aliased and unaliased duplicate columns.\n",
            "architectural impact: NO\n",
            "comments: [\"I'm seeing the same error als on a query not containing duplicates in 2.0.16:\\nSELECT writetime(valid) as wtv, unixtimestampof(now()) as ts, ip, valid, usr, db, impersonate, test FROM session WHERE id = 'FVAIFWJVUWXUPNJVGDGSUDXKVTWKCI'\\n\\nThe table definition being:\\nCREATE TABLE session (\\n  id text,\\n  db text,\\n  impersonate int,\\n  ip set<inet>,\\n  login_token text,\\n  oauth text,\\n  test boolean,\\n  usr text,\\n  valid boolean,\\n  PRIMARY KEY ((id))\\n) WITH\\n  bloom_filter_fp_chance=0.010000 AND\\n  caching='ALL' AND\\n  comment='' AND\\n  dclocal_read_repair_chance=0.000000 AND\\n  gc_grace_seconds=864000 AND\\n  index_interval=128 AND\\n  read_repair_chance=0.100000 AND\\n  replicate_on_write='true' AND\\n  populate_io_cache_on_flush='false' AND\\n  default_time_to_live=0 AND\\n  speculative_retry='99.0PERCENTILE' AND\\n  memtable_flush_period_in_ms=0 AND\\n  compaction={'class': 'LeveledCompactionStrategy'} AND\\n  compression={'sstable_compression': 'LZ4Compressor'};\\n\\n2.0.15 works fine with the above query\", 'I even got the error with a simple \"ping\" query in cqlsh.\\n\\n2.1.7:\\n{code}\\nConnected to Cluster Fucked at 127.0.0.1:9042.\\n[cqlsh 5.0.1 | Cassandra 2.1.7 | CQL spec 3.2.0 | Native protocol v3]\\nUse HELP for help.\\nuser@cqlsh> select now() from system.local;\\nServerError: <ErrorMessage code=0000 [Server error] message=\"java.lang.AssertionError\">\\n{code}\\n\\n2.1.6:\\n{code}\\nConnected to Cluster-o-Nuts at 127.0.0.1:9042.\\n[cqlsh 5.0.1 | Cassandra 2.1.6 | CQL spec 3.2.0 | Native protocol v3]\\nUse HELP for help.\\nuser@cqlsh> select now() from system.local;\\n\\n now()\\n--------------------------------------\\n 577708f0-1ba9-11e5-85d0-df82a948a83c\\n\\n(1 rows)\\n{code}', \"In both cases mentioned in the comments, the issue is that the Selection contains a result column which doesn't map to any underlying column (as both use the no-arg {{now()}} function). This was overlooked in the unit tests, but the unit testing wasn't quite rigorous enough anyway as it only verified the mappings collected for a given query matched expectations. It needs to go further and actually execute the query to ensure that the resultset can be properly constructed from the mappings. I've made the necessary changes to the tests & pushed a fix for the no-arg function case ([2.0|https://github.com/beobal/cassandra/tree/9636-2.0], [2.1|https://github.com/beobal/cassandra/tree/9636-2.1]). I should note that this particular problem doesn't affect 2.2.\\n\\nRegarding the original problem regarding duplicates in the selection, my characterisation of the pre-9532 behaviour was slightly off, so for the sake of clarity:\\n\\n||Branch||pre-9532 behaviour||post-9532 behaviour||\\n|2.0|duplicates are included in results|AssertionError & error response|\\n|2.1|duplicates are collated|AssertionError & error response|\\n|2.2|AssertionError & error response|duplicates are collated|\\n \\nThe branches I've linked also revert 2.0 & 2.1 to their original behaviours.\", 'I noticed an issue with {{count(*)}} queries. \\nUp to 2.2, the count function was implemented in a different way than the other functions. It was some form of hack in {{SelectStatement}}. Due to that the mapping returned is wrong for this function.\\n\\nTo be on the safe side, I think it will be good to add some tests for duplicate function calls and for 2.2 some tests with aggregations. \\n', \"Thanks, you're right the mapping for {{count}} queries were definitely wrong for 2.0 & 2.1.\\nAlso, there were some issues on 2.2 with selecting the same column with multiple distinct aliases & with selecting duplicate unaliased functions.\\n\\nI've rebased & pushed fixes for 2.0 -> trunk; CI is running now\\n\\n[2.0|https://github.com/beobal/cassandra/tree/9636-2.0] [utest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.0-testall/] [dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.0-dtest/]\\n[2.1|https://github.com/beobal/cassandra/tree/9636-2.1] [utest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.1-testall/] [dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.1-dtest/]\\n[2.2|https://github.com/beobal/cassandra/tree/9636-2.2] [utest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.2-testall/] [dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.2-dtest/]\\n[trunk|https://github.com/beobal/cassandra/tree/9636-trunk] [utest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-trunk-testall/] [dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-trunk-dtest/]\\n\", '+1', \"alright, thanks. I've committed to 2.0 in {{2a294e45aa023af28ccc179c5f41410940ef40d7}} and merged up to trunk. There were quite a number of conflicts as this area has changed heavily across the versions, so for the record I've attached the branch specific patches.\", \"I'm a little confused after reading this ticket, is `select now() from system.local;` a good query that should work or  a bad one that needs a better error message?\", 'A good query that should work, the error was caused by the assumption that all columns in the selection ultimately map to one or more columns in the base table.']\n",
            "my_comment: <ErrorMessage code=0000 [Server error] message=\"java.lang.AssertionError\">\n",
            "{code}\n",
            "\n",
            "2.1.6:\n",
            "{code}\n",
            "Connected to Cluster-o-Nuts at 127.0.0.1:9042.\n",
            "[cqlsh 5.0.1 | Cassandra 2.1.6 | CQL spec 3.2.0 | Native protocol v3]\n",
            "Use HELP for help.\n",
            "user@cqlsh> select now() from system.local;\n",
            "\n",
            " now()\n",
            "--------------------------------------\n",
            " 577708f0-1ba9-11e5-85d0-df82a948a83c\n",
            "\n",
            "(1 rows)\n",
            "{code} \"In both cases mentioned in the comments the issue is that the Selection contains a result column which doesnt map to any underlying column (as both use the no-arg {{now()}} function). This was overlooked in the unit tests but the unit testing wasnt quite rigorous enough anyway as it only verified the mappings collected for a given query matched expectations. It needs to go further and actually execute the query to ensure that the resultset can be properly constructed from the mappings. Ive made the necessary changes to the tests & pushed a fix for the no-arg function case ([2.0|https://github.com/beobal/cassandra/tree/9636-2.0] [2.1|https://github.com/beobal/cassandra/tree/9636-2.1]). I should note that this particular problem doesnt affect 2.2.\n",
            "\n",
            "Regarding the original problem regarding duplicates in the selection my characterisation of the pre-9532 behaviour was slightly off so for the sake of clarity:\n",
            "\n",
            "||Branch||pre-9532 behaviour||post-9532 behaviour||\n",
            "|2.0|duplicates are included in results|AssertionError & error response|\n",
            "|2.1|duplicates are collated|AssertionError & error response|\n",
            "|2.2|AssertionError & error response|duplicates are collated|\n",
            " \n",
            "The branches Ive linked also revert 2.0 & 2.1 to their original behaviours.\" I noticed an issue with {{count(*)}} queries. \n",
            "Up to 2.2 the count function was implemented in a different way than the other functions. It was some form of hack in {{SelectStatement}}. Due to that the mapping returned is wrong for this function.\n",
            "\n",
            "To be on the safe side I think it will be good to add some tests for duplicate function calls and for 2.2 some tests with aggregations. \n",
            " \"Thanks youre right the mapping for {{count}} queries were definitely wrong for 2.0 & 2.1.\n",
            "Also there were some issues on 2.2 with selecting the same column with multiple distinct aliases & with selecting duplicate unaliased functions.\n",
            "\n",
            "Ive rebased & pushed fixes for 2.0 -> trunk; CI is running now\n",
            "\n",
            "[2.0|https://github.com/beobal/cassandra/tree/9636-2.0] [utest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.0-testall/] [dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.0-dtest/]\n",
            "[2.1|https://github.com/beobal/cassandra/tree/9636-2.1] [utest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.1-testall/] [dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.1-dtest/]\n",
            "[2.2|https://github.com/beobal/cassandra/tree/9636-2.2] [utest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.2-testall/] [dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-2.2-dtest/]\n",
            "[trunk|https://github.com/beobal/cassandra/tree/9636-trunk] [utest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-trunk-testall/] [dtest|http://cassci.datastax.com/view/Dev/view/beobal/job/beobal-9636-trunk-dtest/]\n",
            "\" +1 \"alright thanks. Ive committed to 2.0 in {{2a294e45aa023af28ccc179c5f41410940ef40d7}} and merged up to trunk. There were quite a number of conflicts as this area has changed heavily across the versions so for the record Ive attached the branch specific patches.\" \"Im a little confused after reading this ticket is `select now() from system.local;` a good query that should work or  a bad one that needs a better error message?\" A good query that should work the error was caused by the assumption that all columns in the selection ultimately map to one or more columns in the base table.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9858\n",
            "issue_type: Bug\n",
            "summary: SelectStatement.Parameters fields should be inspectable by custom indexes and query handlers\n",
            "description: SelectStatement.Parameters fields should be inspectable by custom indexes and query handlers\n",
            "architectural impact: NO\n",
            "comments: ['https://github.com/JeremiahDJordan/cassandra/tree/c9858-21\\nhttps://github.com/JeremiahDJordan/cassandra/tree/c9858-22\\nhttps://github.com/JeremiahDJordan/cassandra/tree/c9858-30\\n', 'Committed to 2.1 as {{e726cf6d6b1a21abd0b7cf35775b7a980b1009ed}} and merged into 2.2 and trunk, thanks.']\n",
            "my_comment: https://github.com/JeremiahDJordan/cassandra/tree/c9858-21\n",
            "https://github.com/JeremiahDJordan/cassandra/tree/c9858-22\n",
            "https://github.com/JeremiahDJordan/cassandra/tree/c9858-30\n",
            " Committed to 2.1 as {{e726cf6d6b1a21abd0b7cf35775b7a980b1009ed}} and merged into 2.2 and trunk thanks.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-9880\n",
            "issue_type: Bug\n",
            "summary: ScrubTest.testScrubOutOfOrder should generate test file on the fly\n",
            "description: ScrubTest#testScrubOutOfOrder is failing on trunk due to the serialization format change from pre-generated out-of-order SSTable.\n",
            "\n",
            "We should change that to generate out-of-order SSTable on the fly so that we don't need to bother generating SSTable by hand again.\n",
            "architectural impact: NO\n",
            "comments: [\"Patch attached as link.\\nLet's see what cassci says.\", 'testall: http://cassci.datastax.com/job/yukim-9880-testall/lastBuild/testReport/\\n\\nScrubTest passed.', '[~Stefania] to review', 'The code is +1 but we need to rebase. There are also a couple of unused imports.', 'Force pushed rebased version to: https://github.com/yukim/cassandra/tree/9880\\n\\nCan you check the code again?\\nScrubTest still passes, but new API is not yet familiar to me.\\n', 'I am not an expert with the new API either but it looks correct, so +1. \\n\\nI created a [pull request|https://github.com/yukim/cassandra/pull/1] with some, mostly unrelated, suggestions:\\n\\n- keepOriginals in scrubber is not used\\n- there is a BAD RELEASE error in sstable.validate(), if you are not comfortable committing this just open another ticket, exception details below\\n- there is a slightly more compact API taken from fillCF().\\n\\n{code}\\nERROR 23:46:59 BAD RELEASE: attempted to release a reference (org.apache.cassandra.utils.concurrent.Ref$State@59662a93) that has already been released\\nERROR 23:46:59 Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@59662a93:\\nThread[main,5,main]\\n  at java.lang.Thread.getStackTrace(Thread.java:1552)\\n  at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:218)\\n  at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:148)\\n  at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:70)\\n  at org.apache.cassandra.io.sstable.format.SSTableReader.<init>(SSTableReader.java:215)\\n  at org.apache.cassandra.io.sstable.format.big.BigTableReader.<init>(BigTableReader.java:57)\\n  at org.apache.cassandra.io.sstable.format.big.BigFormat$ReaderFactory.open(BigFormat.java:103)\\n  at org.apache.cassandra.io.sstable.format.SSTableReader.internalOpen(SSTableReader.java:600)\\n  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:464)\\n  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:362)\\n  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:357)\\n  at org.apache.cassandra.db.ScrubTest.testScrubOutOfOrder(ScrubTest.java:349)\\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n  at java.lang.reflect.Method.invoke(Method.java:497)\\n  at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\\n  at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\\n  at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\\n  at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\\n  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\\n  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\\n  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\\n  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)\\n  at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)\\n  at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)\\n  at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)\\n  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\\n  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\\n  at org.junit.runners.ParentRunner.run(ParentRunner.java:220)\\n  at org.junit.runner.JUnitCore.run(JUnitCore.java:159)\\n  at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)\\n  at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)\\n  at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)\\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n  at java.lang.reflect.Method.invoke(Method.java:497)\\n  at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)\\n\\nERROR 23:46:59 Deallocate trace org.apache.cassandra.utils.concurrent.Ref$State@59662a93:\\nThread[main,5,main]\\n  at java.lang.Thread.getStackTrace(Thread.java:1552)\\n  at org.apache.cassandra.utils.concurrent.Ref$Debug.deallocate(Ref.java:224)\\n  at org.apache.cassandra.utils.concurrent.Ref$State.release(Ref.java:203)\\n  at org.apache.cassandra.utils.concurrent.Ref.release(Ref.java:87)\\n  at org.apache.cassandra.io.sstable.format.SSTableReader.validate(SSTableReader.java:1254)\\n  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:482)\\n  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:362)\\n  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:357)\\n  at org.apache.cassandra.db.ScrubTest.testScrubOutOfOrder(ScrubTest.java:349)\\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n  at java.lang.reflect.Method.invoke(Method.java:497)\\n  at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\\n  at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\\n  at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\\n  at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\\n  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\\n  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\\n  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\\n  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)\\n  at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)\\n  at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)\\n  at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)\\n  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\\n  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\\n  at org.junit.runners.ParentRunner.run(ParentRunner.java:220)\\n  at org.junit.runner.JUnitCore.run(JUnitCore.java:159)\\n  at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)\\n  at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)\\n  at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)\\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n  at java.lang.reflect.Method.invoke(Method.java:497)\\n  at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)\\n{code}', 'Thanks for checking and follow up.\\nCommitted with your patch.']\n",
            "my_comment: attempted to release a reference (org.apache.cassandra.utils.concurrent.Ref$State@59662a93) that has already been released\n",
            "ERROR 23:46:59 Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@59662a93:\n",
            "Thread[main5main]\n",
            "  at java.lang.Thread.getStackTrace(Thread.java:1552)\n",
            "  at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:218)\n",
            "  at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:148)\n",
            "  at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:70)\n",
            "  at org.apache.cassandra.io.sstable.format.SSTableReader.<init>(SSTableReader.java:215)\n",
            "  at org.apache.cassandra.io.sstable.format.big.BigTableReader.<init>(BigTableReader.java:57)\n",
            "  at org.apache.cassandra.io.sstable.format.big.BigFormat$ReaderFactory.open(BigFormat.java:103)\n",
            "  at org.apache.cassandra.io.sstable.format.SSTableReader.internalOpen(SSTableReader.java:600)\n",
            "  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:464)\n",
            "  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:362)\n",
            "  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:357)\n",
            "  at org.apache.cassandra.db.ScrubTest.testScrubOutOfOrder(ScrubTest.java:349)\n",
            "  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "  at java.lang.reflect.Method.invoke(Method.java:497)\n",
            "  at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n",
            "  at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n",
            "  at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n",
            "  at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n",
            "  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n",
            "  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n",
            "  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n",
            "  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)\n",
            "  at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)\n",
            "  at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)\n",
            "  at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)\n",
            "  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n",
            "  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n",
            "  at org.junit.runners.ParentRunner.run(ParentRunner.java:220)\n",
            "  at org.junit.runner.JUnitCore.run(JUnitCore.java:159)\n",
            "  at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)\n",
            "  at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)\n",
            "  at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)\n",
            "  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "  at java.lang.reflect.Method.invoke(Method.java:497)\n",
            "  at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)\n",
            "\n",
            "ERROR 23:46:59 Deallocate trace org.apache.cassandra.utils.concurrent.Ref$State@59662a93:\n",
            "Thread[main5main]\n",
            "  at java.lang.Thread.getStackTrace(Thread.java:1552)\n",
            "  at org.apache.cassandra.utils.concurrent.Ref$Debug.deallocate(Ref.java:224)\n",
            "  at org.apache.cassandra.utils.concurrent.Ref$State.release(Ref.java:203)\n",
            "  at org.apache.cassandra.utils.concurrent.Ref.release(Ref.java:87)\n",
            "  at org.apache.cassandra.io.sstable.format.SSTableReader.validate(SSTableReader.java:1254)\n",
            "  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:482)\n",
            "  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:362)\n",
            "  at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:357)\n",
            "  at org.apache.cassandra.db.ScrubTest.testScrubOutOfOrder(ScrubTest.java:349)\n",
            "  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "  at java.lang.reflect.Method.invoke(Method.java:497)\n",
            "  at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n",
            "  at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n",
            "  at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n",
            "  at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n",
            "  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n",
            "  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n",
            "  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n",
            "  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)\n",
            "  at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)\n",
            "  at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)\n",
            "  at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)\n",
            "  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n",
            "  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n",
            "  at org.junit.runners.ParentRunner.run(ParentRunner.java:220)\n",
            "  at org.junit.runner.JUnitCore.run(JUnitCore.java:159)\n",
            "  at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)\n",
            "  at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)\n",
            "  at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)\n",
            "  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "  at java.lang.reflect.Method.invoke(Method.java:497)\n",
            "  at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)\n",
            "{code} Thanks for checking and follow up.\n",
            "Committed with your patch.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "issue_id: CASSANDRA-12251\n",
            "issue_type: Bug\n",
            "summary: Move migration tasks to non-periodic queue, assure flush executor shutdown after non-periodic executor\n",
            "description: example failure:\n",
            "\n",
            "http://cassci.datastax.com/job/cassandra-3.8_dtest_upgrade/1/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_x_To_indev_3_x/whole_list_conditional_test\n",
            "\n",
            "Failed on CassCI build cassandra-3.8_dtest_upgrade #1\n",
            "\n",
            "Relevant error in logs is\n",
            "{code}\n",
            "Unexpected error in node1 log, error: \n",
            "ERROR [InternalResponseStage:2] 2016-07-20 04:58:45,876 CassandraDaemon.java:217 - Exception in thread Thread[InternalResponseStage:2,5,main]\n",
            "java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down\n",
            "\tat org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:61) ~[apache-cassandra-3.7.jar:3.7]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) ~[na:1.8.0_51]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369) ~[na:1.8.0_51]\n",
            "\tat org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:165) ~[apache-cassandra-3.7.jar:3.7]\n",
            "\tat java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:112) ~[na:1.8.0_51]\n",
            "\tat org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:842) ~[apache-cassandra-3.7.jar:3.7]\n",
            "\tat org.apache.cassandra.db.ColumnFamilyStore.switchMemtableIfCurrent(ColumnFamilyStore.java:822) ~[apache-cassandra-3.7.jar:3.7]\n",
            "\tat org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:891) ~[apache-cassandra-3.7.jar:3.7]\n",
            "\tat org.apache.cassandra.schema.SchemaKeyspace.lambda$flush$1(SchemaKeyspace.java:279) ~[apache-cassandra-3.7.jar:3.7]\n",
            "\tat org.apache.cassandra.schema.SchemaKeyspace$$Lambda$200/1129213153.accept(Unknown Source) ~[na:na]\n",
            "\tat java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_51]\n",
            "\tat org.apache.cassandra.schema.SchemaKeyspace.flush(SchemaKeyspace.java:279) ~[apache-cassandra-3.7.jar:3.7]\n",
            "\tat org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1271) ~[apache-cassandra-3.7.jar:3.7]\n",
            "\tat org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1253) ~[apache-cassandra-3.7.jar:3.7]\n",
            "\tat org.apache.cassandra.service.MigrationTask$1.response(MigrationTask.java:92) ~[apache-cassandra-3.7.jar:3.7]\n",
            "\tat org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:53) ~[apache-cassandra-3.7.jar:3.7]\n",
            "\tat org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:64) ~[apache-cassandra-3.7.jar:3.7]\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_51]\n",
            "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]\n",
            "\tat java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]\n",
            "{code}\n",
            "This is on a mixed 3.0.8, 3.8-tentative cluster\n",
            "architectural impact: NO\n",
            "comments: [\"This looks to me like in drain/StorageServiceShutdownHook, the schema stage is not shutdown, so if you have a task submitted to that executor that doesn't execute until after the postflush executor has been terminated in drain, you'll hit this exception. This is a C* fix for sure.\", \"Unfortunately, I could not reproduce this locally. However, given the exception it's more or less clear what has happened. It's just much harder to land nodes into the same state. \\n\\nThe tasks executed by {{MigrationManager}} are landing in the {{optional}} tasks of scheduled executor [here|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/MigrationManager.java]. Optional tasks are not being waited for during shutdown or drain. Although even putting the task into the {{nonPeriodicTasks}} executor won't fix the problem since post-flush executor is being shut down before the periodic executor [here|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/StorageService.java#L4251]. \\n\\nIn order to fix the issue, we need to both move the migration task to non-periodic and make sure that shutdown order is correct. Since mutation stage is already shut down, I left commit log shutdown on ints place and only moved the post flush executor shutdown after the periodic tasks.\", '[~ifesdjeen] - I actually need to make a superset of these changes for [CASSANDRA-12260] and [CASSANDRA-12313]. It might make sense to just centralize review there and link this one as a duplicate of [CASSANDRA-12260].', \"[~jkni] I've checked the two other issues. Most likely I'm missing something, but I could not see similar exception there (as regards the duplicate). But we can / should definitely centralise the review.\", \"[~ifesdjeen] - good point, they aren't really duplicates in their symptoms since instead of hitting an exception, those two issues deadlock for a minute. They're the same source though, which is that the ordering of the nonPeriodicTasks/postFlush executors with other parts of the shutdown are not correct. I have a patch that I'll be posting on [CASSANDRA-12260] today which changes the order of executor shutdown. This ticket probably still needs to change the executor for the migration task (I haven't thought this through fully yet).\\n\\nEDIT: I should clarify that I was wrong here - my changes on those issues aren't a superset, just have significant overlap.\", '|[trunk|https://github.com/ifesdjeen/cassandra/tree/12251-upgrade-trunk]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-dtest/]|[testall|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-testall/]|[upgrade|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-upgrade/]|', 'Patch and CI look good to me. Is there a reason you only ran CI on trunk [~ifesdjeen]? I think this should go into 2.2+.', \"You're right. In fact, it all merges cleanly from 2.2 upwards. I've triggered CI as well: \\n\\n|[2.2|https://github.com/ifesdjeen/cassandra/tree/12251-upgrade-2.2]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-2.2-dtest/]|[testall|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-2.2-testall/]|\\n|[3.0|https://github.com/ifesdjeen/cassandra/tree/12251-upgrade-3.0]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-3.0-dtest/]|[testall|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-3.0-testall/]|\\n|[trunk|https://github.com/ifesdjeen/cassandra/tree/12251-upgrade-trunk]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-dtest/]|[testall|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-testall/]|\\n\\n|[upgrade tests|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-upgrade/]|\\n\\nI re-ran upgrade tests to use all patched branches. Results look clean. \", '+1 - lgtm. This should merge forward cleanly from 2.2.', 'Committed as [465bb5d45ccef337382592127e214a0ca16a3d88|https://github.com/apache/cassandra/commit/465bb5d45ccef337382592127e214a0ca16a3d88] to 2.2, and merged with 3.0, 3.0, and trunk. Thanks.']\n",
            "my_comment: \n",
            "\n",
            "|[2.2|https://github.com/ifesdjeen/cassandra/tree/12251-upgrade-2.2]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-2.2-dtest/]|[testall|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-2.2-testall/]|\n",
            "|[3.0|https://github.com/ifesdjeen/cassandra/tree/12251-upgrade-3.0]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-3.0-dtest/]|[testall|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-3.0-testall/]|\n",
            "|[trunk|https://github.com/ifesdjeen/cassandra/tree/12251-upgrade-trunk]|[dtest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-dtest/]|[testall|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-trunk-testall/]|\n",
            "\n",
            "|[upgrade tests|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-12251-upgrade-upgrade/]|\n",
            "\n",
            "I re-ran upgrade tests to use all patched branches. Results look clean. \" +1 - lgtm. This should merge forward cleanly from 2.2. Committed as [465bb5d45ccef337382592127e214a0ca16a3d88|https://github.com/apache/cassandra/commit/465bb5d45ccef337382592127e214a0ca16a3d88] to 2.2 and merged with 3.0 3.0 and trunk. Thanks.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}